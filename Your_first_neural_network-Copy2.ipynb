{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 你的第一个神经网络\n",
    "\n",
    "在此项目中，你将构建你的第一个神经网络，并用该网络预测每日自行车租客人数。我们提供了一些代码，但是需要你来实现神经网络（大部分内容）。提交此项目后，欢迎进一步探索该数据和模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载和准备数据\n",
    "\n",
    "构建神经网络的关键一步是正确地准备数据。不同尺度级别的变量使网络难以高效地掌握正确的权重。我们在下方已经提供了加载和准备数据的代码。你很快将进一步学习这些代码！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "\n",
    "rides = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据简介\n",
    "\n",
    "此数据集包含的是从 2011 年 1 月 1 日到 2012 年 12 月 31 日期间每天每小时的骑车人数。骑车用户分成临时用户和注册用户，cnt 列是骑车用户数汇总列。你可以在上方看到前几行数据。\n",
    "\n",
    "下图展示的是数据集中前 10 天左右的骑车人数（某些天不一定是 24 个条目，所以不是精确的 10 天）。你可以在这里看到每小时租金。这些数据很复杂！周末的骑行人数少些，工作日上下班期间是骑行高峰期。我们还可以从上方的数据中看到温度、湿度和风速信息，所有这些信息都会影响骑行人数。你需要用你的模型展示所有这些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2019b044630>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20198e4fb70>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 254,
       "width": 377
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rides[:24*10].plot(x='dteday', y='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 虚拟变量（哑变量）\n",
    "\n",
    "下面是一些分类变量，例如季节、天气、月份。要在我们的模型中包含这些数据，我们需要创建二进制虚拟变量。用 Pandas 库中的 `get_dummies()` 就可以轻松实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>...</th>\n",
       "      <th>hr_21</th>\n",
       "      <th>hr_22</th>\n",
       "      <th>hr_23</th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   yr  holiday  temp   hum  windspeed  casual  registered  cnt  season_1  \\\n",
       "0   0        0  0.24  0.81        0.0       3          13   16         1   \n",
       "1   0        0  0.22  0.80        0.0       8          32   40         1   \n",
       "2   0        0  0.22  0.80        0.0       5          27   32         1   \n",
       "3   0        0  0.24  0.75        0.0       3          10   13         1   \n",
       "4   0        0  0.24  0.75        0.0       0           1    1         1   \n",
       "\n",
       "   season_2    ...      hr_21  hr_22  hr_23  weekday_0  weekday_1  weekday_2  \\\n",
       "0         0    ...          0      0      0          0          0          0   \n",
       "1         0    ...          0      0      0          0          0          0   \n",
       "2         0    ...          0      0      0          0          0          0   \n",
       "3         0    ...          0      0      0          0          0          0   \n",
       "4         0    ...          0      0      0          0          0          0   \n",
       "\n",
       "   weekday_3  weekday_4  weekday_5  weekday_6  \n",
       "0          0          0          0          1  \n",
       "1          0          0          0          1  \n",
       "2          0          0          0          1  \n",
       "3          0          0          0          1  \n",
       "4          0          0          0          1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调整目标变量\n",
    "\n",
    "为了更轻松地训练网络，我们将对每个连续变量标准化，即转换和调整变量，使它们的均值为 0，标准差为 1。\n",
    "\n",
    "我们会保存换算因子，以便当我们使用网络进行预测时可以还原数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据拆分为训练、测试和验证数据集\n",
    "\n",
    "我们将大约最后 21 天的数据保存为测试数据集，这些数据集会在训练完网络后使用。我们将使用该数据集进行预测，并与实际的骑行人数进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for approximately the last 21 days \n",
    "test_data = data[-21*24:]\n",
    "\n",
    "# Now remove the test data from the data set \n",
    "data = data[:-21*24]\n",
    "\n",
    "# Separate the data into features and targets\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将数据拆分为两个数据集，一个用作训练，一个在网络训练完后用来验证网络。因为数据是有时间序列特性的，所以我们用历史数据进行训练，然后尝试预测未来数据（验证数据集）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out the last 60 days or so of the remaining data as a validation set\n",
    "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
    "val_features, val_targets = features[-60*24:], targets[-60*24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始构建网络\n",
    "\n",
    "下面你将构建自己的网络。我们已经构建好结构和反向传递部分。你将实现网络的前向传递部分。还需要设置超参数：学习速率、隐藏单元的数量，以及训练传递数量。\n",
    "\n",
    "<img src=\"C:\\Users\\carmen wong/A_uda-deep learning/1simple NN/Bike-Sharing-Dataset\" width=300px>\n",
    "\n",
    "该网络有两个层级，一个隐藏层和一个输出层。隐藏层级将使用 S 型函数作为激活函数。输出层只有一个节点，用于递归，节点的输出和节点的输入相同。即激活函数是 $f(x)=x$。这种函数获得输入信号，并生成输出信号，但是会考虑阈值，称为激活函数。我们完成网络的每个层级，并计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。这一流程叫做前向传播（forward propagation）。\n",
    "\n",
    "我们在神经网络中使用权重将信号从输入层传播到输出层。我们还使用权重将错误从输出层传播回网络，以便更新权重。这叫做反向传播（backpropagation）。\n",
    "\n",
    "> **提示**：你需要为反向传播实现计算输出激活函数 ($f(x) = x$) 的导数。如果你不熟悉微积分，其实该函数就等同于等式 $y = x$。该等式的斜率是多少？也就是导数 $f(x)$。\n",
    "\n",
    "\n",
    "你需要完成以下任务：\n",
    "\n",
    "1. 实现 S 型激活函数。将 `__init__` 中的 `self.activation_function`  设为你的 S 型函数。\n",
    "2. 在 `train` 方法中实现前向传递。\n",
    "3. 在 `train` 方法中实现反向传播算法，包括计算输出错误。\n",
    "4. 在 `run` 方法中实现前向传递。\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
    "                                       (self.input_nodes, self.hidden_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.output_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### TODO: Set self.activation_function to your implemented sigmoid function ####\n",
    "        #\n",
    "        # Note: in Python, you can define a function with a lambda expression,\n",
    "        # as shown below.\n",
    "        self.activation_function = lambda x : 1/(1+np.exp(-x))  # Replace 0 with your sigmoid calculation.\n",
    "        \n",
    "        ### If the lambda code above is not something you're familiar with,\n",
    "        # You can uncomment out the following three lines and put your \n",
    "        # implementation there instead.\n",
    "        #\n",
    "        #def sigmoid(x):\n",
    "        #    return 0  # Replace 0 with your sigmoid calculation here\n",
    "        #self.activation_function = sigmoid\n",
    "                    \n",
    "    \n",
    "    def train(self, features, targets):\n",
    "        ''' Train the network on batch of features and targets. \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            \n",
    "            features: 2D array, each row is one data record, each column is a feature\n",
    "            targets: 1D array of target values\n",
    "        \n",
    "        '''\n",
    "        n_records = features.shape[0]\n",
    "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n",
    "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n",
    "        for X, y in zip(features, targets):\n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "            # TODO: Hidden layer - Replace these values with your calculations.\n",
    "            hidden_inputs = np.dot(X,self.weights_input_to_hidden) # signals into hidden layer\n",
    "            hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "\n",
    "            # TODO: Output layer - Replace these values with your calculations.\n",
    "            final_inputs =np.dot(hidden_outputs,self.weights_hidden_to_output) # signals into final output layer\n",
    "            final_outputs =final_inputs  # signals from final output layer\n",
    "            (1,1)##助教说这里代码一定要写（1,1），因为（1，1）是矩阵，（1，）是向量\n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # TODO: Output error - Replace this value with your calculations.\n",
    "            #助教说这里代码一定要写（1,1），因为（1，1）是矩阵，（1，）是向量\n",
    "            error = y-final_outputs  # Output layer error is the difference between desired target and actual output.\n",
    "            (1,1)\n",
    "            # TODO: Calculate the hidden layer's contribution to the error\n",
    "            hidden_error = np.dot(self.weights_hidden_to_output,error)\n",
    "            \n",
    "            # TODO: Backpropagated error terms - Replace these values with your calculations.\n",
    "            output_error_term = np.array(1)\n",
    "            hidden_error_term = hidden_outputs*(1-hidden_outputs)\n",
    "\n",
    "            # Weight step (input to hidden)\n",
    "            delta_weights_i_h += np.dot(X.reshape((-1,1)),(hidden_error*hidden_error_term).reshape((1,-1)))\n",
    "            # Weight step (hidden to output)\n",
    "            delta_weights_h_o += np.dot(hidden_outputs.reshape((-1,1)),(error*output_error_term).reshape((1,-1)))\n",
    "\n",
    "        # TODO: Update the weights - Replace these values with your calculations.\n",
    "        self.weights_hidden_to_output += self.lr*delta_weights_h_o/n_records # update hidden-to-output weights with gradient descent step\n",
    "        self.weights_input_to_hidden += self.lr*delta_weights_i_h/n_records # update input-to-hidden weights with gradient descent step\n",
    " \n",
    "    def run(self, features):\n",
    "        ''' Run a forward pass through the network with input features \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            features: 1D array of feature values\n",
    "        '''\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer - replace these values with the appropriate calculations.\n",
    "        hidden_inputs = np.dot(features,self.weights_input_to_hidden) # signals into hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer - Replace these values with the appropriate calculations.\n",
    "        final_inputs =  np.dot(hidden_outputs,self.weights_hidden_to_output) # signals into final output layer\n",
    "        final_outputs = final_inputs # signals from final output layer \n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, Y):\n",
    "    return np.mean((y-Y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单元测试\n",
    "\n",
    "运行这些单元测试，检查你的网络实现是否正确。这样可以帮助你确保网络已正确实现，然后再开始训练网络。这些测试必须成功才能通过此项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.023s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "inputs = np.array([[0.5, -0.2, 0.1]])\n",
    "targets = np.array([[0.4]])\n",
    "test_w_i_h = np.array([[0.1, -0.2],\n",
    "                       [0.4, 0.5],\n",
    "                       [-0.3, 0.2]])\n",
    "test_w_h_o = np.array([[0.3],\n",
    "                       [-0.1]])\n",
    "\n",
    "class TestMethods(unittest.TestCase):\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for data loading\n",
    "    ##########\n",
    "    \n",
    "    def test_data_path(self):\n",
    "        # Test that file path to dataset has been unaltered\n",
    "        self.assertTrue(data_path.lower() == 'bike-sharing-dataset/hour.csv')\n",
    "        \n",
    "    def test_data_loaded(self):\n",
    "        # Test that data frame loaded\n",
    "        self.assertTrue(isinstance(rides, pd.DataFrame))\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for network functionality\n",
    "    ##########\n",
    "\n",
    "    def test_activation(self):\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        # Test that the activation function is a sigmoid\n",
    "        self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5))))\n",
    "\n",
    "    def test_train(self):\n",
    "        # Test that weights are updated correctly on training\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "        \n",
    "        network.train(inputs, targets)\n",
    "        self.assertTrue(np.allclose(network.weights_hidden_to_output, \n",
    "                                    np.array([[ 0.37275328], \n",
    "                                              [-0.03172939]])))\n",
    "        self.assertTrue(np.allclose(network.weights_input_to_hidden,\n",
    "                                    np.array([[ 0.10562014, -0.20185996], \n",
    "                                              [0.39775194, 0.50074398], \n",
    "                                              [-0.29887597, 0.19962801]])))\n",
    "\n",
    "    def test_run(self):\n",
    "        # Test correctness of run method\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "\n",
    "        self.assertTrue(np.allclose(network.run(inputs), 0.09998924))\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestMethods())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络\n",
    "\n",
    "现在你将设置网络的超参数。策略是设置的超参数使训练集上的错误很小但是数据不会过拟合。如果网络训练时间太长，或者有太多的隐藏节点，可能就会过于针对特定训练集，无法泛化到验证数据集。即当训练集的损失降低时，验证集的损失将开始增大。\n",
    "\n",
    "你还将采用随机梯度下降 (SGD) 方法训练网络。对于每次训练，都获取随机样本数据，而不是整个数据集。与普通梯度下降相比，训练次数要更多，但是每次时间更短。这样的话，网络训练效率更高。稍后你将详细了解 SGD。\n",
    "\n",
    "\n",
    "### 选择迭代次数\n",
    "\n",
    "也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。\n",
    "\n",
    "### 选择学习速率\n",
    "\n",
    "速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。\n",
    "\n",
    "\n",
    "### 选择隐藏节点数量\n",
    "\n",
    "隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0% ... Training loss: 0.930 ... Validation loss: 1.263iteration: 0\n",
      "train_loss: 0.9305221939227614\n",
      "val_loss: 1.263986978316298\n",
      "Progress: 0.0% ... Training loss: 0.880 ... Validation loss: 1.351iteration: 1\n",
      "train_loss: 0.8809668450420257\n",
      "val_loss: 1.351496685670542\n",
      "Progress: 0.0% ... Training loss: 0.868 ... Validation loss: 1.285iteration: 2\n",
      "train_loss: 0.8683427249203824\n",
      "val_loss: 1.285676443299597\n",
      "Progress: 0.0% ... Training loss: 0.847 ... Validation loss: 1.334"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3\n",
      "train_loss: 0.847914428928384\n",
      "val_loss: 1.3348236634410293\n",
      "Progress: 0.0% ... Training loss: 0.838 ... Validation loss: 1.234iteration: 4\n",
      "train_loss: 0.8386512862865876\n",
      "val_loss: 1.2347621355903757\n",
      "Progress: 0.1% ... Training loss: 0.823 ... Validation loss: 1.221iteration: 5\n",
      "train_loss: 0.8231528181204832\n",
      "val_loss: 1.2216029541831235\n",
      "Progress: 0.1% ... Training loss: 0.803 ... Validation loss: 1.367iteration: 6\n",
      "train_loss: 0.8031305992415131\n",
      "val_loss: 1.3676512913976695\n",
      "Progress: 0.1% ... Training loss: 0.807 ... Validation loss: 1.415iteration: 7\n",
      "train_loss: 0.807158567811122\n",
      "val_loss: 1.415108523690836\n",
      "Progress: 0.1% ... Training loss: 0.774 ... Validation loss: 1.247iteration: 8\n",
      "train_loss: 0.7744470923047433\n",
      "val_loss: 1.2477923552780135\n",
      "Progress: 0.1% ... Training loss: 0.786 ... Validation loss: 1.417iteration: 9\n",
      "train_loss: 0.7862795994391175\n",
      "val_loss: 1.417817816251733\n",
      "Progress: 0.1% ... Training loss: 0.746 ... Validation loss: 1.277iteration: 10\n",
      "train_loss: 0.7466136508175083\n",
      "val_loss: 1.2779516712687156\n",
      "Progress: 0.1% ... Training loss: 0.744 ... Validation loss: 1.160iteration: 11\n",
      "train_loss: 0.7440332417309947\n",
      "val_loss: 1.1605367280480374\n",
      "Progress: 0.1% ... Training loss: 0.729 ... Validation loss: 1.145iteration: 12\n",
      "train_loss: 0.7297578797769271\n",
      "val_loss: 1.1455525570526106\n",
      "Progress: 0.1% ... Training loss: 0.706 ... Validation loss: 1.202iteration: 13\n",
      "train_loss: 0.7067072331474848\n",
      "val_loss: 1.2026301485710535\n",
      "Progress: 0.1% ... Training loss: 0.716 ... Validation loss: 1.133iteration: 14\n",
      "train_loss: 0.7163702678732804\n",
      "val_loss: 1.1338322492510033\n",
      "Progress: 0.1% ... Training loss: 0.695 ... Validation loss: 1.180iteration: 15\n",
      "train_loss: 0.6955317192705689\n",
      "val_loss: 1.1807943952574655\n",
      "Progress: 0.2% ... Training loss: 0.687 ... Validation loss: 1.201iteration: 16\n",
      "train_loss: 0.687304321275942\n",
      "val_loss: 1.2018754086341592\n",
      "Progress: 0.2% ... Training loss: 0.680 ... Validation loss: 1.227iteration: 17\n",
      "train_loss: 0.6806172261718975\n",
      "val_loss: 1.2272743794780971\n",
      "Progress: 0.2% ... Training loss: 0.685 ... Validation loss: 1.301iteration: 18\n",
      "train_loss: 0.6852413186879477\n",
      "val_loss: 1.3015295016404618\n",
      "Progress: 0.2% ... Training loss: 0.670 ... Validation loss: 1.186iteration: 19\n",
      "train_loss: 0.6707953306621405\n",
      "val_loss: 1.1862033109828052\n",
      "Progress: 0.2% ... Training loss: 0.669 ... Validation loss: 1.158iteration: 20\n",
      "train_loss: 0.6690650138536081\n",
      "val_loss: 1.1580801030057892\n",
      "Progress: 0.2% ... Training loss: 0.682 ... Validation loss: 1.306iteration: 21\n",
      "train_loss: 0.6828159796449775\n",
      "val_loss: 1.3069764174986847\n",
      "Progress: 0.2% ... Training loss: 0.670 ... Validation loss: 1.251iteration: 22\n",
      "train_loss: 0.6704808223742543\n",
      "val_loss: 1.2514097127805723\n",
      "Progress: 0.2% ... Training loss: 0.660 ... Validation loss: 1.207iteration: 23\n",
      "train_loss: 0.6604113035927921\n",
      "val_loss: 1.2079025580524942\n",
      "Progress: 0.2% ... Training loss: 0.658 ... Validation loss: 1.095iteration: 24\n",
      "train_loss: 0.658848563493325\n",
      "val_loss: 1.0957644330631142\n",
      "Progress: 0.2% ... Training loss: 0.671 ... Validation loss: 1.302iteration: 25\n",
      "train_loss: 0.6713562419206024\n",
      "val_loss: 1.3026372270656286\n",
      "Progress: 0.3% ... Training loss: 0.648 ... Validation loss: 1.201iteration: 26\n",
      "train_loss: 0.648195255768531\n",
      "val_loss: 1.2016668144782425\n",
      "Progress: 0.3% ... Training loss: 0.638 ... Validation loss: 1.129iteration: 27\n",
      "train_loss: 0.6389874090031331\n",
      "val_loss: 1.1299236379428494\n",
      "Progress: 0.3% ... Training loss: 0.652 ... Validation loss: 1.234iteration: 28\n",
      "train_loss: 0.6524840272883684\n",
      "val_loss: 1.2349654097582945\n",
      "Progress: 0.3% ... Training loss: 0.635 ... Validation loss: 1.109iteration: 29\n",
      "train_loss: 0.635039769124654\n",
      "val_loss: 1.109030387876273\n",
      "Progress: 0.3% ... Training loss: 0.632 ... Validation loss: 1.135iteration: 30\n",
      "train_loss: 0.6328036063238413\n",
      "val_loss: 1.1350267966188146\n",
      "Progress: 0.3% ... Training loss: 0.644 ... Validation loss: 1.022iteration: 31\n",
      "train_loss: 0.6446194164827785\n",
      "val_loss: 1.0220695376711817\n",
      "Progress: 0.3% ... Training loss: 0.623 ... Validation loss: 1.100iteration: 32\n",
      "train_loss: 0.6237574041590936\n",
      "val_loss: 1.1004575224850466\n",
      "Progress: 0.3% ... Training loss: 0.620 ... Validation loss: 1.111iteration: 33\n",
      "train_loss: 0.6206794605140848\n",
      "val_loss: 1.1112853907391904\n",
      "Progress: 0.3% ... Training loss: 0.629 ... Validation loss: 1.179iteration: 34\n",
      "train_loss: 0.6297451611341555\n",
      "val_loss: 1.1793504967587267\n",
      "Progress: 0.3% ... Training loss: 0.615 ... Validation loss: 1.087iteration: 35\n",
      "train_loss: 0.6158810756221544\n",
      "val_loss: 1.0873233936446949\n",
      "Progress: 0.4% ... Training loss: 0.621 ... Validation loss: 1.011iteration: 36\n",
      "train_loss: 0.621447156212341\n",
      "val_loss: 1.0117597800192752\n",
      "Progress: 0.4% ... Training loss: 0.617 ... Validation loss: 1.134iteration: 37\n",
      "train_loss: 0.617651520821042\n",
      "val_loss: 1.1347484608938412\n",
      "Progress: 0.4% ... Training loss: 0.628 ... Validation loss: 1.182iteration: 38\n",
      "train_loss: 0.6282658504003996\n",
      "val_loss: 1.1821658794335999\n",
      "Progress: 0.4% ... Training loss: 0.607 ... Validation loss: 1.082iteration: 39\n",
      "train_loss: 0.6079153304294889\n",
      "val_loss: 1.0829253525731124\n",
      "Progress: 0.4% ... Training loss: 0.626 ... Validation loss: 1.182iteration: 40\n",
      "train_loss: 0.6266373763432087\n",
      "val_loss: 1.1828780661303058\n",
      "Progress: 0.4% ... Training loss: 0.605 ... Validation loss: 1.010iteration: 41\n",
      "train_loss: 0.6053287611303347\n",
      "val_loss: 1.0104734176060808\n",
      "Progress: 0.4% ... Training loss: 0.613 ... Validation loss: 0.969iteration: 42\n",
      "train_loss: 0.6135656916514184\n",
      "val_loss: 0.9699278607588869\n",
      "Progress: 0.4% ... Training loss: 0.624 ... Validation loss: 0.941iteration: 43\n",
      "train_loss: 0.624996779220746\n",
      "val_loss: 0.9419590193486799\n",
      "Progress: 0.4% ... Training loss: 0.591 ... Validation loss: 1.057iteration: 44\n",
      "train_loss: 0.5914679865855671\n",
      "val_loss: 1.0570243318530654\n",
      "Progress: 0.5% ... Training loss: 0.600 ... Validation loss: 0.952iteration: 45\n",
      "train_loss: 0.6002175845630172\n",
      "val_loss: 0.9520379355489498\n",
      "Progress: 0.5% ... Training loss: 0.597 ... Validation loss: 1.106iteration: 46\n",
      "train_loss: 0.5972384082049353\n",
      "val_loss: 1.1065979943006992\n",
      "Progress: 0.5% ... Training loss: 0.589 ... Validation loss: 0.946iteration: 47\n",
      "train_loss: 0.5897281427562392\n",
      "val_loss: 0.9461099692435373\n",
      "Progress: 0.5% ... Training loss: 0.577 ... Validation loss: 1.015iteration: 48\n",
      "train_loss: 0.5779904294035371\n",
      "val_loss: 1.0159107313124136\n",
      "Progress: 0.5% ... Training loss: 0.588 ... Validation loss: 1.076iteration: 49\n",
      "train_loss: 0.5888828068817394\n",
      "val_loss: 1.0765636298092227\n",
      "Progress: 0.5% ... Training loss: 0.573 ... Validation loss: 0.995iteration: 50\n",
      "train_loss: 0.5734819893869203\n",
      "val_loss: 0.9958480997294875\n",
      "Progress: 0.5% ... Training loss: 0.569 ... Validation loss: 0.963iteration: 51\n",
      "train_loss: 0.569864830075049\n",
      "val_loss: 0.9638868501498138\n",
      "Progress: 0.5% ... Training loss: 0.570 ... Validation loss: 0.927iteration: 52\n",
      "train_loss: 0.5705264932427994\n",
      "val_loss: 0.9275550982041804\n",
      "Progress: 0.5% ... Training loss: 0.563 ... Validation loss: 0.985iteration: 53\n",
      "train_loss: 0.5632861470197222\n",
      "val_loss: 0.9851950139457367\n",
      "Progress: 0.5% ... Training loss: 0.559 ... Validation loss: 0.940iteration: 54\n",
      "train_loss: 0.5593225096344111\n",
      "val_loss: 0.9407336315488114\n",
      "Progress: 0.6% ... Training loss: 0.555 ... Validation loss: 0.946iteration: 55\n",
      "train_loss: 0.5559989456796558\n",
      "val_loss: 0.9462663778643748\n",
      "Progress: 0.6% ... Training loss: 0.565 ... Validation loss: 1.004iteration: 56\n",
      "train_loss: 0.565173852373641\n",
      "val_loss: 1.004642804051478\n",
      "Progress: 0.6% ... Training loss: 0.597 ... Validation loss: 0.856iteration: 57\n",
      "train_loss: 0.5975350517200402\n",
      "val_loss: 0.8566445933999903\n",
      "Progress: 0.6% ... Training loss: 0.546 ... Validation loss: 0.919iteration: 58\n",
      "train_loss: 0.5469770906732738\n",
      "val_loss: 0.919150038952498\n",
      "Progress: 0.6% ... Training loss: 0.543 ... Validation loss: 0.915iteration: 59\n",
      "train_loss: 0.5438181898514995\n",
      "val_loss: 0.9156209304369535\n",
      "Progress: 0.6% ... Training loss: 0.551 ... Validation loss: 0.865iteration: 60\n",
      "train_loss: 0.5512353527445362\n",
      "val_loss: 0.8650431938271615\n",
      "Progress: 0.6% ... Training loss: 0.542 ... Validation loss: 0.943iteration: 61\n",
      "train_loss: 0.5421346383238193\n",
      "val_loss: 0.9435552797492401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.6% ... Training loss: 0.535 ... Validation loss: 0.914iteration: 62\n",
      "train_loss: 0.5352045519058685\n",
      "val_loss: 0.9141746713147312\n",
      "Progress: 0.6% ... Training loss: 0.540 ... Validation loss: 0.851iteration: 63\n",
      "train_loss: 0.540015979812777\n",
      "val_loss: 0.8512426725809717\n",
      "Progress: 0.6% ... Training loss: 0.531 ... Validation loss: 0.914iteration: 64\n",
      "train_loss: 0.531712507545926\n",
      "val_loss: 0.9140415748795745\n",
      "Progress: 0.7% ... Training loss: 0.526 ... Validation loss: 0.857iteration: 65\n",
      "train_loss: 0.526008875036568\n",
      "val_loss: 0.857669800941107\n",
      "Progress: 0.7% ... Training loss: 0.526 ... Validation loss: 0.907iteration: 66\n",
      "train_loss: 0.5262840214859571\n",
      "val_loss: 0.9074879477914728\n",
      "Progress: 0.7% ... Training loss: 0.528 ... Validation loss: 0.828iteration: 67\n",
      "train_loss: 0.5288445119060748\n",
      "val_loss: 0.8288768587004023\n",
      "Progress: 0.7% ... Training loss: 0.545 ... Validation loss: 0.953iteration: 68\n",
      "train_loss: 0.5451917412388514\n",
      "val_loss: 0.953274850980235\n",
      "Progress: 0.7% ... Training loss: 0.532 ... Validation loss: 0.819iteration: 69\n",
      "train_loss: 0.5326987244100972\n",
      "val_loss: 0.8196926709934257\n",
      "Progress: 0.7% ... Training loss: 0.539 ... Validation loss: 0.942iteration: 70\n",
      "train_loss: 0.5394322258359842\n",
      "val_loss: 0.9423672396934661\n",
      "Progress: 0.7% ... Training loss: 0.514 ... Validation loss: 0.821iteration: 71\n",
      "train_loss: 0.5145121776444358\n",
      "val_loss: 0.8216896688438542\n",
      "Progress: 0.7% ... Training loss: 0.505 ... Validation loss: 0.841iteration: 72\n",
      "train_loss: 0.5057808286940706\n",
      "val_loss: 0.8413352591722056\n",
      "Progress: 0.7% ... Training loss: 0.507 ... Validation loss: 0.813iteration: 73\n",
      "train_loss: 0.5074624125482529\n",
      "val_loss: 0.8139348946796434\n",
      "Progress: 0.7% ... Training loss: 0.502 ... Validation loss: 0.831iteration: 74\n",
      "train_loss: 0.5028043356656995\n",
      "val_loss: 0.8312581082455308\n",
      "Progress: 0.8% ... Training loss: 0.507 ... Validation loss: 0.863iteration: 75\n",
      "train_loss: 0.5077149122081495\n",
      "val_loss: 0.8630765474112154\n",
      "Progress: 0.8% ... Training loss: 0.502 ... Validation loss: 0.844iteration: 76\n",
      "train_loss: 0.5023233530096759\n",
      "val_loss: 0.8441150924683964\n",
      "Progress: 0.8% ... Training loss: 0.497 ... Validation loss: 0.835iteration: 77\n",
      "train_loss: 0.4978846447771262\n",
      "val_loss: 0.8350370366577092\n",
      "Progress: 0.8% ... Training loss: 0.494 ... Validation loss: 0.794iteration: 78\n",
      "train_loss: 0.4940345065016729\n",
      "val_loss: 0.794661327747113\n",
      "Progress: 0.8% ... Training loss: 0.491 ... Validation loss: 0.794iteration: 79\n",
      "train_loss: 0.49176348502033024\n",
      "val_loss: 0.7949257207949332\n",
      "Progress: 0.8% ... Training loss: 0.497 ... Validation loss: 0.838iteration: 80\n",
      "train_loss: 0.4972681950557944\n",
      "val_loss: 0.838420034060837\n",
      "Progress: 0.8% ... Training loss: 0.518 ... Validation loss: 0.766iteration: 81\n",
      "train_loss: 0.5186648771506233\n",
      "val_loss: 0.7669246217520408\n",
      "Progress: 0.8% ... Training loss: 0.497 ... Validation loss: 0.835iteration: 82\n",
      "train_loss: 0.4971601974040495\n",
      "val_loss: 0.8355871991255801\n",
      "Progress: 0.8% ... Training loss: 0.488 ... Validation loss: 0.814iteration: 83\n",
      "train_loss: 0.4881588990510681\n",
      "val_loss: 0.8147405112740476\n",
      "Progress: 0.8% ... Training loss: 0.495 ... Validation loss: 0.758iteration: 84\n",
      "train_loss: 0.4957812411485343\n",
      "val_loss: 0.7589308142011725\n",
      "Progress: 0.8% ... Training loss: 0.474 ... Validation loss: 0.786iteration: 85\n",
      "train_loss: 0.47424964290126487\n",
      "val_loss: 0.7866993802782386\n",
      "Progress: 0.9% ... Training loss: 0.497 ... Validation loss: 0.756iteration: 86\n",
      "train_loss: 0.4973728085858253\n",
      "val_loss: 0.7569151149363454\n",
      "Progress: 0.9% ... Training loss: 0.466 ... Validation loss: 0.749iteration: 87\n",
      "train_loss: 0.466799130938499\n",
      "val_loss: 0.7490180528193318\n",
      "Progress: 0.9% ... Training loss: 0.479 ... Validation loss: 0.808iteration: 88\n",
      "train_loss: 0.4796186030648579\n",
      "val_loss: 0.8089052500808972\n",
      "Progress: 0.9% ... Training loss: 0.465 ... Validation loss: 0.738iteration: 89\n",
      "train_loss: 0.4654848646599749\n",
      "val_loss: 0.7380189667152902\n",
      "Progress: 0.9% ... Training loss: 0.458 ... Validation loss: 0.740iteration: 90\n",
      "train_loss: 0.45836694161546343\n",
      "val_loss: 0.7401395649757736\n",
      "Progress: 0.9% ... Training loss: 0.504 ... Validation loss: 0.837iteration: 91\n",
      "train_loss: 0.5045351735488626\n",
      "val_loss: 0.83742659981711\n",
      "Progress: 0.9% ... Training loss: 0.474 ... Validation loss: 0.737iteration: 92\n",
      "train_loss: 0.4743449472011311\n",
      "val_loss: 0.737984271293333\n",
      "Progress: 0.9% ... Training loss: 0.525 ... Validation loss: 0.863iteration: 93\n",
      "train_loss: 0.5254984159480383\n",
      "val_loss: 0.8631316855289716\n",
      "Progress: 0.9% ... Training loss: 0.448 ... Validation loss: 0.739iteration: 94\n",
      "train_loss: 0.44820131714741895\n",
      "val_loss: 0.7397171831533293\n",
      "Progress: 0.9% ... Training loss: 0.453 ... Validation loss: 0.728iteration: 95\n",
      "train_loss: 0.45302400227142525\n",
      "val_loss: 0.7284159257875946\n",
      "Progress: 1.0% ... Training loss: 0.442 ... Validation loss: 0.722iteration: 96\n",
      "train_loss: 0.4422514619698204\n",
      "val_loss: 0.7225965391746755\n",
      "Progress: 1.0% ... Training loss: 0.440 ... Validation loss: 0.709iteration: 97\n",
      "train_loss: 0.44036881354835405\n",
      "val_loss: 0.7090845473427709\n",
      "Progress: 1.0% ... Training loss: 0.436 ... Validation loss: 0.723iteration: 98\n",
      "train_loss: 0.4367471039869262\n",
      "val_loss: 0.7232972660296699\n",
      "Progress: 1.0% ... Training loss: 0.440 ... Validation loss: 0.704iteration: 99\n",
      "train_loss: 0.44056052818018204\n",
      "val_loss: 0.7040740869561949\n",
      "Progress: 1.0% ... Training loss: 0.433 ... Validation loss: 0.698iteration: 100\n",
      "train_loss: 0.43378081263153245\n",
      "val_loss: 0.6982889393499245\n",
      "Progress: 1.0% ... Training loss: 0.429 ... Validation loss: 0.692iteration: 101\n",
      "train_loss: 0.4299129224477303\n",
      "val_loss: 0.6921819175167507\n",
      "Progress: 1.0% ... Training loss: 0.427 ... Validation loss: 0.699iteration: 102\n",
      "train_loss: 0.4270016479569934\n",
      "val_loss: 0.6994500995661135\n",
      "Progress: 1.0% ... Training loss: 0.434 ... Validation loss: 0.692iteration: 103\n",
      "train_loss: 0.4347305981307814\n",
      "val_loss: 0.6922898081354041\n",
      "Progress: 1.0% ... Training loss: 0.425 ... Validation loss: 0.692iteration: 104\n",
      "train_loss: 0.42551205090936634\n",
      "val_loss: 0.6926649298744446\n",
      "Progress: 1.1% ... Training loss: 0.416 ... Validation loss: 0.681iteration: 105\n",
      "train_loss: 0.4163804806189582\n",
      "val_loss: 0.6811596914622094\n",
      "Progress: 1.1% ... Training loss: 0.414 ... Validation loss: 0.674iteration: 106\n",
      "train_loss: 0.4141163402785187\n",
      "val_loss: 0.6749436531344952\n",
      "Progress: 1.1% ... Training loss: 0.409 ... Validation loss: 0.671iteration: 107\n",
      "train_loss: 0.4090546780058132\n",
      "val_loss: 0.671229288847996\n",
      "Progress: 1.1% ... Training loss: 0.419 ... Validation loss: 0.685iteration: 108\n",
      "train_loss: 0.41923281899566156\n",
      "val_loss: 0.6853107315635182\n",
      "Progress: 1.1% ... Training loss: 0.413 ... Validation loss: 0.670iteration: 109\n",
      "train_loss: 0.4138782310237709\n",
      "val_loss: 0.6700058097532232\n",
      "Progress: 1.1% ... Training loss: 0.432 ... Validation loss: 0.694iteration: 110\n",
      "train_loss: 0.4326523746306837\n",
      "val_loss: 0.6945356687822818\n",
      "Progress: 1.1% ... Training loss: 0.460 ... Validation loss: 0.699iteration: 111\n",
      "train_loss: 0.460811548455623\n",
      "val_loss: 0.6999535299799571\n",
      "Progress: 1.1% ... Training loss: 0.467 ... Validation loss: 0.728iteration: 112\n",
      "train_loss: 0.4671324561944184\n",
      "val_loss: 0.728556281026117\n",
      "Progress: 1.1% ... Training loss: 0.415 ... Validation loss: 0.663iteration: 113\n",
      "train_loss: 0.41527232934325614\n",
      "val_loss: 0.6634670877057963\n",
      "Progress: 1.1% ... Training loss: 0.421 ... Validation loss: 0.680iteration: 114\n",
      "train_loss: 0.42141275616206636\n",
      "val_loss: 0.6807107365189226\n",
      "Progress: 1.1% ... Training loss: 0.402 ... Validation loss: 0.653iteration: 115\n",
      "train_loss: 0.4022842777796473\n",
      "val_loss: 0.6536944897387732\n",
      "Progress: 1.2% ... Training loss: 0.393 ... Validation loss: 0.646iteration: 116\n",
      "train_loss: 0.39359221895904767\n",
      "val_loss: 0.6460607455254397\n",
      "Progress: 1.2% ... Training loss: 0.392 ... Validation loss: 0.639iteration: 117\n",
      "train_loss: 0.3929755508687858\n",
      "val_loss: 0.6397276120029144\n",
      "Progress: 1.2% ... Training loss: 0.398 ... Validation loss: 0.635iteration: 118\n",
      "train_loss: 0.3986232466835986\n",
      "val_loss: 0.635066578311175\n",
      "Progress: 1.2% ... Training loss: 0.410 ... Validation loss: 0.662iteration: 119\n",
      "train_loss: 0.41058014336436566\n",
      "val_loss: 0.6628625288165718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.2% ... Training loss: 0.401 ... Validation loss: 0.635iteration: 120\n",
      "train_loss: 0.4016689697245631\n",
      "val_loss: 0.6350833145303205\n",
      "Progress: 1.2% ... Training loss: 0.412 ... Validation loss: 0.658iteration: 121\n",
      "train_loss: 0.41225794881362926\n",
      "val_loss: 0.6588210956141082\n",
      "Progress: 1.2% ... Training loss: 0.382 ... Validation loss: 0.627iteration: 122\n",
      "train_loss: 0.3827538805073876\n",
      "val_loss: 0.6272947234505676\n",
      "Progress: 1.2% ... Training loss: 0.401 ... Validation loss: 0.642iteration: 123\n",
      "train_loss: 0.4013179548244593\n",
      "val_loss: 0.6429242059537041\n",
      "Progress: 1.2% ... Training loss: 0.384 ... Validation loss: 0.627iteration: 124\n",
      "train_loss: 0.38431959710384117\n",
      "val_loss: 0.627732381620869\n",
      "Progress: 1.2% ... Training loss: 0.383 ... Validation loss: 0.623iteration: 125\n",
      "train_loss: 0.3832182104403969\n",
      "val_loss: 0.6234556646618052\n",
      "Progress: 1.3% ... Training loss: 0.378 ... Validation loss: 0.618iteration: 126\n",
      "train_loss: 0.37856702468937964\n",
      "val_loss: 0.6182209302862185\n",
      "Progress: 1.3% ... Training loss: 0.382 ... Validation loss: 0.621iteration: 127\n",
      "train_loss: 0.38203687578060885\n",
      "val_loss: 0.6217026493946305\n",
      "Progress: 1.3% ... Training loss: 0.375 ... Validation loss: 0.614iteration: 128\n",
      "train_loss: 0.37556487599507493\n",
      "val_loss: 0.6140171847337156\n",
      "Progress: 1.3% ... Training loss: 0.373 ... Validation loss: 0.609iteration: 129\n",
      "train_loss: 0.37370368534498116\n",
      "val_loss: 0.6095390660212194\n",
      "Progress: 1.3% ... Training loss: 0.373 ... Validation loss: 0.609iteration: 130\n",
      "train_loss: 0.373608919080175\n",
      "val_loss: 0.6090116662454059\n",
      "Progress: 1.3% ... Training loss: 0.376 ... Validation loss: 0.610iteration: 131\n",
      "train_loss: 0.37654788018869956\n",
      "val_loss: 0.6106357339614621\n",
      "Progress: 1.3% ... Training loss: 0.368 ... Validation loss: 0.604iteration: 132\n",
      "train_loss: 0.3682914762847422\n",
      "val_loss: 0.6043665574817969\n",
      "Progress: 1.3% ... Training loss: 0.377 ... Validation loss: 0.612iteration: 133\n",
      "train_loss: 0.37721417603527496\n",
      "val_loss: 0.6129276698947608\n",
      "Progress: 1.3% ... Training loss: 0.368 ... Validation loss: 0.603iteration: 134\n",
      "train_loss: 0.3682415618117323\n",
      "val_loss: 0.6039635960665326\n",
      "Progress: 1.4% ... Training loss: 0.361 ... Validation loss: 0.590iteration: 135\n",
      "train_loss: 0.3614484146594028\n",
      "val_loss: 0.5904955371932261\n",
      "Progress: 1.4% ... Training loss: 0.360 ... Validation loss: 0.587iteration: 136\n",
      "train_loss: 0.3606519267042767\n",
      "val_loss: 0.587375557724328\n",
      "Progress: 1.4% ... Training loss: 0.373 ... Validation loss: 0.585iteration: 137\n",
      "train_loss: 0.37310097813748583\n",
      "val_loss: 0.5854884298291785\n",
      "Progress: 1.4% ... Training loss: 0.359 ... Validation loss: 0.586iteration: 138\n",
      "train_loss: 0.35925145267290276\n",
      "val_loss: 0.5866537870206553\n",
      "Progress: 1.4% ... Training loss: 0.373 ... Validation loss: 0.592iteration: 139\n",
      "train_loss: 0.37355621158097363\n",
      "val_loss: 0.5927120037327527\n",
      "Progress: 1.4% ... Training loss: 0.353 ... Validation loss: 0.575iteration: 140\n",
      "train_loss: 0.3537213268979766\n",
      "val_loss: 0.5755662203334981\n",
      "Progress: 1.4% ... Training loss: 0.354 ... Validation loss: 0.577iteration: 141\n",
      "train_loss: 0.35432080036757485\n",
      "val_loss: 0.5770267556878983\n",
      "Progress: 1.4% ... Training loss: 0.353 ... Validation loss: 0.574iteration: 142\n",
      "train_loss: 0.35338676993437523\n",
      "val_loss: 0.5748911569520656\n",
      "Progress: 1.4% ... Training loss: 0.352 ... Validation loss: 0.574iteration: 143\n",
      "train_loss: 0.35227277200282614\n",
      "val_loss: 0.5748318906517376\n",
      "Progress: 1.4% ... Training loss: 0.385 ... Validation loss: 0.611iteration: 144\n",
      "train_loss: 0.3850375767127787\n",
      "val_loss: 0.611419350963248\n",
      "Progress: 1.4% ... Training loss: 0.407 ... Validation loss: 0.604iteration: 145\n",
      "train_loss: 0.4079777202937996\n",
      "val_loss: 0.6044379396018178\n",
      "Progress: 1.5% ... Training loss: 0.353 ... Validation loss: 0.563iteration: 146\n",
      "train_loss: 0.3530743448998335\n",
      "val_loss: 0.5637094271781187\n",
      "Progress: 1.5% ... Training loss: 0.348 ... Validation loss: 0.557iteration: 147\n",
      "train_loss: 0.3482871218947414\n",
      "val_loss: 0.5573809077529093\n",
      "Progress: 1.5% ... Training loss: 0.344 ... Validation loss: 0.547iteration: 148\n",
      "train_loss: 0.34440302667767336\n",
      "val_loss: 0.5470892247153815\n",
      "Progress: 1.5% ... Training loss: 0.342 ... Validation loss: 0.544iteration: 149\n",
      "train_loss: 0.342176299175186\n",
      "val_loss: 0.5442891863026179\n",
      "Progress: 1.5% ... Training loss: 0.346 ... Validation loss: 0.541iteration: 150\n",
      "train_loss: 0.3468452968466708\n",
      "val_loss: 0.5415217341865559\n",
      "Progress: 1.5% ... Training loss: 0.344 ... Validation loss: 0.553iteration: 151\n",
      "train_loss: 0.3441819712416568\n",
      "val_loss: 0.5532155038651376\n",
      "Progress: 1.5% ... Training loss: 0.341 ... Validation loss: 0.535iteration: 152\n",
      "train_loss: 0.3414488833642663\n",
      "val_loss: 0.5359741035218316\n",
      "Progress: 1.5% ... Training loss: 0.339 ... Validation loss: 0.536iteration: 153\n",
      "train_loss: 0.3391453625235092\n",
      "val_loss: 0.536392890622148\n",
      "Progress: 1.5% ... Training loss: 0.361 ... Validation loss: 0.541iteration: 154\n",
      "train_loss: 0.3614977997692694\n",
      "val_loss: 0.5410116561956257\n",
      "Progress: 1.6% ... Training loss: 0.334 ... Validation loss: 0.538iteration: 155\n",
      "train_loss: 0.3349528011892543\n",
      "val_loss: 0.5389712932694971\n",
      "Progress: 1.6% ... Training loss: 0.345 ... Validation loss: 0.523iteration: 156\n",
      "train_loss: 0.34532041663826624\n",
      "val_loss: 0.5234071947500027\n",
      "Progress: 1.6% ... Training loss: 0.354 ... Validation loss: 0.564iteration: 157\n",
      "train_loss: 0.3546685046333658\n",
      "val_loss: 0.5642777972996821\n",
      "Progress: 1.6% ... Training loss: 0.347 ... Validation loss: 0.530iteration: 158\n",
      "train_loss: 0.34765451521663515\n",
      "val_loss: 0.5302005505916082\n",
      "Progress: 1.6% ... Training loss: 0.342 ... Validation loss: 0.545iteration: 159\n",
      "train_loss: 0.3427856282724637\n",
      "val_loss: 0.545874602384771\n",
      "Progress: 1.6% ... Training loss: 0.328 ... Validation loss: 0.532iteration: 160\n",
      "train_loss: 0.3287921952228283\n",
      "val_loss: 0.5326367641659521\n",
      "Progress: 1.6% ... Training loss: 0.327 ... Validation loss: 0.525iteration: 161\n",
      "train_loss: 0.327685153543796\n",
      "val_loss: 0.5250633315551787\n",
      "Progress: 1.6% ... Training loss: 0.337 ... Validation loss: 0.525iteration: 162\n",
      "train_loss: 0.33778814973480015\n",
      "val_loss: 0.5252587021637078\n",
      "Progress: 1.6% ... Training loss: 0.359 ... Validation loss: 0.569iteration: 163\n",
      "train_loss: 0.35901179354151014\n",
      "val_loss: 0.5696677480839127\n",
      "Progress: 1.6% ... Training loss: 0.331 ... Validation loss: 0.534iteration: 164\n",
      "train_loss: 0.3317966646809045\n",
      "val_loss: 0.5345141992858427\n",
      "Progress: 1.6% ... Training loss: 0.327 ... Validation loss: 0.530iteration: 165\n",
      "train_loss: 0.32703137181146535\n",
      "val_loss: 0.53027812866618\n",
      "Progress: 1.7% ... Training loss: 0.335 ... Validation loss: 0.548iteration: 166\n",
      "train_loss: 0.3359462241226083\n",
      "val_loss: 0.5481581349093084\n",
      "Progress: 1.7% ... Training loss: 0.331 ... Validation loss: 0.535iteration: 167\n",
      "train_loss: 0.3315378842103482\n",
      "val_loss: 0.5354666055218966\n",
      "Progress: 1.7% ... Training loss: 0.341 ... Validation loss: 0.555iteration: 168\n",
      "train_loss: 0.34194692783230834\n",
      "val_loss: 0.555174685018113\n",
      "Progress: 1.7% ... Training loss: 0.336 ... Validation loss: 0.525iteration: 169\n",
      "train_loss: 0.33658376506356313\n",
      "val_loss: 0.5258029017764928\n",
      "Progress: 1.7% ... Training loss: 0.381 ... Validation loss: 0.603iteration: 170\n",
      "train_loss: 0.38117900016479545\n",
      "val_loss: 0.6034731699154686\n",
      "Progress: 1.7% ... Training loss: 0.363 ... Validation loss: 0.543iteration: 171\n",
      "train_loss: 0.3635000031618594\n",
      "val_loss: 0.543517093870685\n",
      "Progress: 1.7% ... Training loss: 0.333 ... Validation loss: 0.534iteration: 172\n",
      "train_loss: 0.33360847993427256\n",
      "val_loss: 0.5347031273057654\n",
      "Progress: 1.7% ... Training loss: 0.344 ... Validation loss: 0.519iteration: 173\n",
      "train_loss: 0.3445308306704257\n",
      "val_loss: 0.5198753855737114\n",
      "Progress: 1.7% ... Training loss: 0.323 ... Validation loss: 0.524iteration: 174\n",
      "train_loss: 0.32343516685361273\n",
      "val_loss: 0.5242168325845001\n",
      "Progress: 1.8% ... Training loss: 0.369 ... Validation loss: 0.534iteration: 175\n",
      "train_loss: 0.36925621662473107\n",
      "val_loss: 0.5341768122589438\n",
      "Progress: 1.8% ... Training loss: 0.354 ... Validation loss: 0.563iteration: 176\n",
      "train_loss: 0.3542269077592608\n",
      "val_loss: 0.5630159308655801\n",
      "Progress: 1.8% ... Training loss: 0.327 ... Validation loss: 0.515iteration: 177\n",
      "train_loss: 0.32716295649608146\n",
      "val_loss: 0.5150124846342934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.8% ... Training loss: 0.319 ... Validation loss: 0.515iteration: 178\n",
      "train_loss: 0.3192075569169959\n",
      "val_loss: 0.5152390512699582\n",
      "Progress: 1.8% ... Training loss: 0.328 ... Validation loss: 0.516iteration: 179\n",
      "train_loss: 0.32810255906625063\n",
      "val_loss: 0.5161893881824844\n",
      "Progress: 1.8% ... Training loss: 0.319 ... Validation loss: 0.512iteration: 180\n",
      "train_loss: 0.31910692762811316\n",
      "val_loss: 0.5123654606528781\n",
      "Progress: 1.8% ... Training loss: 0.322 ... Validation loss: 0.498iteration: 181\n",
      "train_loss: 0.32257108147849284\n",
      "val_loss: 0.4988774909646089\n",
      "Progress: 1.8% ... Training loss: 0.323 ... Validation loss: 0.516iteration: 182\n",
      "train_loss: 0.323159639069863\n",
      "val_loss: 0.5165809393134357\n",
      "Progress: 1.8% ... Training loss: 0.339 ... Validation loss: 0.527iteration: 183\n",
      "train_loss: 0.3398124099960122\n",
      "val_loss: 0.5278623332841947\n",
      "Progress: 1.8% ... Training loss: 0.316 ... Validation loss: 0.504iteration: 184\n",
      "train_loss: 0.31650063522631183\n",
      "val_loss: 0.5047474536093974\n",
      "Progress: 1.9% ... Training loss: 0.325 ... Validation loss: 0.510iteration: 185\n",
      "train_loss: 0.32503745877006673\n",
      "val_loss: 0.5101347903453408\n",
      "Progress: 1.9% ... Training loss: 0.319 ... Validation loss: 0.506iteration: 186\n",
      "train_loss: 0.3198080928623927\n",
      "val_loss: 0.5065588898779676\n",
      "Progress: 1.9% ... Training loss: 0.316 ... Validation loss: 0.503iteration: 187\n",
      "train_loss: 0.3169579973136276\n",
      "val_loss: 0.5030706655126659\n",
      "Progress: 1.9% ... Training loss: 0.314 ... Validation loss: 0.505iteration: 188\n",
      "train_loss: 0.3147001754500681\n",
      "val_loss: 0.5058206443637064\n",
      "Progress: 1.9% ... Training loss: 0.313 ... Validation loss: 0.498iteration: 189\n",
      "train_loss: 0.3130011912098686\n",
      "val_loss: 0.49896661871667064\n",
      "Progress: 1.9% ... Training loss: 0.314 ... Validation loss: 0.499iteration: 190\n",
      "train_loss: 0.3140404657548861\n",
      "val_loss: 0.4996644257884053\n",
      "Progress: 1.9% ... Training loss: 0.312 ... Validation loss: 0.489iteration: 191\n",
      "train_loss: 0.31292017590573207\n",
      "val_loss: 0.48973182470160564\n",
      "Progress: 1.9% ... Training loss: 0.323 ... Validation loss: 0.488iteration: 192\n",
      "train_loss: 0.3232569496556278\n",
      "val_loss: 0.4880063958326716\n",
      "Progress: 1.9% ... Training loss: 0.317 ... Validation loss: 0.493iteration: 193\n",
      "train_loss: 0.3174775394157091\n",
      "val_loss: 0.4939185783338571\n",
      "Progress: 1.9% ... Training loss: 0.322 ... Validation loss: 0.497iteration: 194\n",
      "train_loss: 0.3224049510940411\n",
      "val_loss: 0.4978140110501258\n",
      "Progress: 1.9% ... Training loss: 0.312 ... Validation loss: 0.484iteration: 195\n",
      "train_loss: 0.31205454306549846\n",
      "val_loss: 0.4843000047632829\n",
      "Progress: 2.0% ... Training loss: 0.342 ... Validation loss: 0.538iteration: 196\n",
      "train_loss: 0.34269342040576434\n",
      "val_loss: 0.5388237166211911\n",
      "Progress: 2.0% ... Training loss: 0.331 ... Validation loss: 0.507iteration: 197\n",
      "train_loss: 0.331650075791934\n",
      "val_loss: 0.5076347329893672\n",
      "Progress: 2.0% ... Training loss: 0.312 ... Validation loss: 0.500iteration: 198\n",
      "train_loss: 0.31289495773238635\n",
      "val_loss: 0.5007967272298671\n",
      "Progress: 2.0% ... Training loss: 0.315 ... Validation loss: 0.487iteration: 199\n",
      "train_loss: 0.3156893100551496\n",
      "val_loss: 0.48713925178541495\n",
      "Progress: 2.0% ... Training loss: 0.313 ... Validation loss: 0.486iteration: 200\n",
      "train_loss: 0.3137360238940038\n",
      "val_loss: 0.48641715888471304\n",
      "Progress: 2.0% ... Training loss: 0.346 ... Validation loss: 0.526iteration: 201\n",
      "train_loss: 0.346434689883069\n",
      "val_loss: 0.5261657381485009\n",
      "Progress: 2.0% ... Training loss: 0.320 ... Validation loss: 0.492iteration: 202\n",
      "train_loss: 0.32004974085976684\n",
      "val_loss: 0.49219796273943234\n",
      "Progress: 2.0% ... Training loss: 0.331 ... Validation loss: 0.532iteration: 203\n",
      "train_loss: 0.33108103122945975\n",
      "val_loss: 0.5324899292327544\n",
      "Progress: 2.0% ... Training loss: 0.312 ... Validation loss: 0.490iteration: 204\n",
      "train_loss: 0.31243371468223347\n",
      "val_loss: 0.4908156128607361\n",
      "Progress: 2.0% ... Training loss: 0.316 ... Validation loss: 0.494iteration: 205\n",
      "train_loss: 0.3165508104963081\n",
      "val_loss: 0.49437895387844033\n",
      "Progress: 2.1% ... Training loss: 0.318 ... Validation loss: 0.506iteration: 206\n",
      "train_loss: 0.3184306078880348\n",
      "val_loss: 0.5068066403482613\n",
      "Progress: 2.1% ... Training loss: 0.318 ... Validation loss: 0.495iteration: 207\n",
      "train_loss: 0.31870156650605264\n",
      "val_loss: 0.495659388918916\n",
      "Progress: 2.1% ... Training loss: 0.328 ... Validation loss: 0.528iteration: 208\n",
      "train_loss: 0.32833926338211106\n",
      "val_loss: 0.5280863838599231\n",
      "Progress: 2.1% ... Training loss: 0.334 ... Validation loss: 0.504iteration: 209\n",
      "train_loss: 0.33495564568051434\n",
      "val_loss: 0.5045906382679419\n",
      "Progress: 2.1% ... Training loss: 0.324 ... Validation loss: 0.511iteration: 210\n",
      "train_loss: 0.32422741586784115\n",
      "val_loss: 0.5116244803925292\n",
      "Progress: 2.1% ... Training loss: 0.309 ... Validation loss: 0.483iteration: 211\n",
      "train_loss: 0.3096414249368326\n",
      "val_loss: 0.48359866784500877\n",
      "Progress: 2.1% ... Training loss: 0.312 ... Validation loss: 0.483iteration: 212\n",
      "train_loss: 0.3128973727083794\n",
      "val_loss: 0.4835311425388799\n",
      "Progress: 2.1% ... Training loss: 0.307 ... Validation loss: 0.481iteration: 213\n",
      "train_loss: 0.3077722921810985\n",
      "val_loss: 0.4818312409297328\n",
      "Progress: 2.1% ... Training loss: 0.309 ... Validation loss: 0.478iteration: 214\n",
      "train_loss: 0.3097623537120803\n",
      "val_loss: 0.4782834393714052\n",
      "Progress: 2.1% ... Training loss: 0.314 ... Validation loss: 0.497iteration: 215\n",
      "train_loss: 0.31461322241958084\n",
      "val_loss: 0.4971636286946065\n",
      "Progress: 2.2% ... Training loss: 0.326 ... Validation loss: 0.483iteration: 216\n",
      "train_loss: 0.32658978294475977\n",
      "val_loss: 0.4831363973269807\n",
      "Progress: 2.2% ... Training loss: 0.307 ... Validation loss: 0.471iteration: 217\n",
      "train_loss: 0.3073178324018316\n",
      "val_loss: 0.4719756810806232\n",
      "Progress: 2.2% ... Training loss: 0.324 ... Validation loss: 0.489iteration: 218\n",
      "train_loss: 0.3240767040996545\n",
      "val_loss: 0.4892539225281204\n",
      "Progress: 2.2% ... Training loss: 0.312 ... Validation loss: 0.486iteration: 219\n",
      "train_loss: 0.3129246396236859\n",
      "val_loss: 0.4869549560141686\n",
      "Progress: 2.2% ... Training loss: 0.307 ... Validation loss: 0.470iteration: 220\n",
      "train_loss: 0.30756924459439466\n",
      "val_loss: 0.47081136565633463\n",
      "Progress: 2.2% ... Training loss: 0.323 ... Validation loss: 0.458iteration: 221\n",
      "train_loss: 0.32316379433287196\n",
      "val_loss: 0.4589681017190838\n",
      "Progress: 2.2% ... Training loss: 0.351 ... Validation loss: 0.516iteration: 222\n",
      "train_loss: 0.3518613505596872\n",
      "val_loss: 0.5164188987843458\n",
      "Progress: 2.2% ... Training loss: 0.361 ... Validation loss: 0.510iteration: 223\n",
      "train_loss: 0.3615778082594461\n",
      "val_loss: 0.5107233066248484\n",
      "Progress: 2.2% ... Training loss: 0.332 ... Validation loss: 0.498iteration: 224\n",
      "train_loss: 0.33278823653136624\n",
      "val_loss: 0.49871752377753154\n",
      "Progress: 2.2% ... Training loss: 0.313 ... Validation loss: 0.486iteration: 225\n",
      "train_loss: 0.3137885996806494\n",
      "val_loss: 0.4861206404578594\n",
      "Progress: 2.3% ... Training loss: 0.307 ... Validation loss: 0.481iteration: 226\n",
      "train_loss: 0.30799685945864674\n",
      "val_loss: 0.4817307858894567\n",
      "Progress: 2.3% ... Training loss: 0.306 ... Validation loss: 0.477iteration: 227\n",
      "train_loss: 0.30639391379706404\n",
      "val_loss: 0.4776067174603848\n",
      "Progress: 2.3% ... Training loss: 0.305 ... Validation loss: 0.478iteration: 228\n",
      "train_loss: 0.30592091939330834\n",
      "val_loss: 0.47838838713492443\n",
      "Progress: 2.3% ... Training loss: 0.310 ... Validation loss: 0.481iteration: 229\n",
      "train_loss: 0.31001746383392387\n",
      "val_loss: 0.48115030055196084\n",
      "Progress: 2.3% ... Training loss: 0.307 ... Validation loss: 0.468iteration: 230\n",
      "train_loss: 0.30731197429620244\n",
      "val_loss: 0.46834979990112674\n",
      "Progress: 2.3% ... Training loss: 0.304 ... Validation loss: 0.473iteration: 231\n",
      "train_loss: 0.30496116233114157\n",
      "val_loss: 0.4738167743398055\n",
      "Progress: 2.3% ... Training loss: 0.305 ... Validation loss: 0.478iteration: 232\n",
      "train_loss: 0.30507447102040053\n",
      "val_loss: 0.4783581254922695\n",
      "Progress: 2.3% ... Training loss: 0.311 ... Validation loss: 0.493iteration: 233\n",
      "train_loss: 0.31135091690962235\n",
      "val_loss: 0.4934409975566224\n",
      "Progress: 2.3% ... Training loss: 0.314 ... Validation loss: 0.471iteration: 234\n",
      "train_loss: 0.31422035713723884\n",
      "val_loss: 0.47107855345113403\n",
      "Progress: 2.4% ... Training loss: 0.305 ... Validation loss: 0.467iteration: 235\n",
      "train_loss: 0.3052505411528586\n",
      "val_loss: 0.46755468508391973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 2.4% ... Training loss: 0.327 ... Validation loss: 0.507iteration: 236\n",
      "train_loss: 0.3277871856953315\n",
      "val_loss: 0.5075486963333445\n",
      "Progress: 2.4% ... Training loss: 0.335 ... Validation loss: 0.502iteration: 237\n",
      "train_loss: 0.3354530728104609\n",
      "val_loss: 0.5021043081794366\n",
      "Progress: 2.4% ... Training loss: 0.322 ... Validation loss: 0.487iteration: 238\n",
      "train_loss: 0.3221659332828317\n",
      "val_loss: 0.48766107020816557\n",
      "Progress: 2.4% ... Training loss: 0.306 ... Validation loss: 0.475iteration: 239\n",
      "train_loss: 0.30631601577577067\n",
      "val_loss: 0.4753968550786928\n",
      "Progress: 2.4% ... Training loss: 0.304 ... Validation loss: 0.473iteration: 240\n",
      "train_loss: 0.30410104203310784\n",
      "val_loss: 0.4735118394760001\n",
      "Progress: 2.4% ... Training loss: 0.310 ... Validation loss: 0.468iteration: 241\n",
      "train_loss: 0.3102919179478342\n",
      "val_loss: 0.46805995684748986\n",
      "Progress: 2.4% ... Training loss: 0.327 ... Validation loss: 0.500iteration: 242\n",
      "train_loss: 0.32730411686202443\n",
      "val_loss: 0.5006872778729984\n",
      "Progress: 2.4% ... Training loss: 0.316 ... Validation loss: 0.471iteration: 243\n",
      "train_loss: 0.31647848382861055\n",
      "val_loss: 0.47188891121481175\n",
      "Progress: 2.4% ... Training loss: 0.303 ... Validation loss: 0.471iteration: 244\n",
      "train_loss: 0.303849251979037\n",
      "val_loss: 0.4714584108334531\n",
      "Progress: 2.5% ... Training loss: 0.310 ... Validation loss: 0.482iteration: 245\n",
      "train_loss: 0.31043605707199246\n",
      "val_loss: 0.482396571414474\n",
      "Progress: 2.5% ... Training loss: 0.318 ... Validation loss: 0.478iteration: 246\n",
      "train_loss: 0.3180042583344001\n",
      "val_loss: 0.47852486725846827\n",
      "Progress: 2.5% ... Training loss: 0.311 ... Validation loss: 0.472iteration: 247\n",
      "train_loss: 0.31114848335657774\n",
      "val_loss: 0.47276715987477996\n",
      "Progress: 2.5% ... Training loss: 0.307 ... Validation loss: 0.465iteration: 248\n",
      "train_loss: 0.3076554593849418\n",
      "val_loss: 0.46559627942401666\n",
      "Progress: 2.5% ... Training loss: 0.304 ... Validation loss: 0.466iteration: 249\n",
      "train_loss: 0.3045404688235093\n",
      "val_loss: 0.46658021402319083\n",
      "Progress: 2.5% ... Training loss: 0.313 ... Validation loss: 0.482iteration: 250\n",
      "train_loss: 0.3135093687588499\n",
      "val_loss: 0.4824863452132398\n",
      "Progress: 2.5% ... Training loss: 0.327 ... Validation loss: 0.481iteration: 251\n",
      "train_loss: 0.32757233406310166\n",
      "val_loss: 0.48128867614313403\n",
      "Progress: 2.5% ... Training loss: 0.310 ... Validation loss: 0.469iteration: 252\n",
      "train_loss: 0.3107992624464581\n",
      "val_loss: 0.46919270977263927\n",
      "Progress: 2.5% ... Training loss: 0.301 ... Validation loss: 0.467iteration: 253\n",
      "train_loss: 0.3015236084492837\n",
      "val_loss: 0.46787634182190646\n",
      "Progress: 2.5% ... Training loss: 0.320 ... Validation loss: 0.492iteration: 254\n",
      "train_loss: 0.32094564796306657\n",
      "val_loss: 0.492297321619356\n",
      "Progress: 2.5% ... Training loss: 0.308 ... Validation loss: 0.471iteration: 255\n",
      "train_loss: 0.30858388216425703\n",
      "val_loss: 0.47177626982967696\n",
      "Progress: 2.6% ... Training loss: 0.355 ... Validation loss: 0.488iteration: 256\n",
      "train_loss: 0.35569319980125913\n",
      "val_loss: 0.4887975523007755\n",
      "Progress: 2.6% ... Training loss: 0.336 ... Validation loss: 0.503iteration: 257\n",
      "train_loss: 0.33612652539820154\n",
      "val_loss: 0.5031963113198231\n",
      "Progress: 2.6% ... Training loss: 0.327 ... Validation loss: 0.477iteration: 258\n",
      "train_loss: 0.3272485215734527\n",
      "val_loss: 0.4773928167043582\n",
      "Progress: 2.6% ... Training loss: 0.315 ... Validation loss: 0.484iteration: 259\n",
      "train_loss: 0.3158723612457074\n",
      "val_loss: 0.48451285002946587\n",
      "Progress: 2.6% ... Training loss: 0.332 ... Validation loss: 0.483iteration: 260\n",
      "train_loss: 0.3320216380523375\n",
      "val_loss: 0.48312676760266127\n",
      "Progress: 2.6% ... Training loss: 0.328 ... Validation loss: 0.489iteration: 261\n",
      "train_loss: 0.3286652448794788\n",
      "val_loss: 0.48954806605270446\n",
      "Progress: 2.6% ... Training loss: 0.301 ... Validation loss: 0.461iteration: 262\n",
      "train_loss: 0.3014952741817533\n",
      "val_loss: 0.4610417950182578\n",
      "Progress: 2.6% ... Training loss: 0.303 ... Validation loss: 0.457iteration: 263\n",
      "train_loss: 0.3036432374641026\n",
      "val_loss: 0.45754499293924433\n",
      "Progress: 2.6% ... Training loss: 0.335 ... Validation loss: 0.499iteration: 264\n",
      "train_loss: 0.3356459598756085\n",
      "val_loss: 0.49958839325269516\n",
      "Progress: 2.6% ... Training loss: 0.322 ... Validation loss: 0.454iteration: 265\n",
      "train_loss: 0.3222506140498808\n",
      "val_loss: 0.4540076337934178\n",
      "Progress: 2.7% ... Training loss: 0.307 ... Validation loss: 0.464iteration: 266\n",
      "train_loss: 0.307748331631382\n",
      "val_loss: 0.46484139254627244\n",
      "Progress: 2.7% ... Training loss: 0.309 ... Validation loss: 0.460iteration: 267\n",
      "train_loss: 0.30901500959208067\n",
      "val_loss: 0.4606855564546549\n",
      "Progress: 2.7% ... Training loss: 0.300 ... Validation loss: 0.464iteration: 268\n",
      "train_loss: 0.30072426869313496\n",
      "val_loss: 0.4645655088537385\n",
      "Progress: 2.7% ... Training loss: 0.304 ... Validation loss: 0.455iteration: 269\n",
      "train_loss: 0.3044593506862343\n",
      "val_loss: 0.45541107156678307\n",
      "Progress: 2.7% ... Training loss: 0.303 ... Validation loss: 0.455iteration: 270\n",
      "train_loss: 0.30312492463009727\n",
      "val_loss: 0.4550306977305474\n",
      "Progress: 2.7% ... Training loss: 0.305 ... Validation loss: 0.450iteration: 271\n",
      "train_loss: 0.305667614821612\n",
      "val_loss: 0.4507444516743185\n",
      "Progress: 2.7% ... Training loss: 0.302 ... Validation loss: 0.457iteration: 272\n",
      "train_loss: 0.3020436947295272\n",
      "val_loss: 0.4577991162498309\n",
      "Progress: 2.7% ... Training loss: 0.312 ... Validation loss: 0.478iteration: 273\n",
      "train_loss: 0.3129495474443394\n",
      "val_loss: 0.4782018956849901\n",
      "Progress: 2.7% ... Training loss: 0.305 ... Validation loss: 0.485iteration: 274\n",
      "train_loss: 0.3057384367776622\n",
      "val_loss: 0.48591221562806924\n",
      "Progress: 2.8% ... Training loss: 0.308 ... Validation loss: 0.478iteration: 275\n",
      "train_loss: 0.3080638324038696\n",
      "val_loss: 0.47820406649391395\n",
      "Progress: 2.8% ... Training loss: 0.298 ... Validation loss: 0.460iteration: 276\n",
      "train_loss: 0.29821738226883887\n",
      "val_loss: 0.460718882150622\n",
      "Progress: 2.8% ... Training loss: 0.298 ... Validation loss: 0.458iteration: 277\n",
      "train_loss: 0.2989655276012298\n",
      "val_loss: 0.45898697989929693\n",
      "Progress: 2.8% ... Training loss: 0.330 ... Validation loss: 0.478iteration: 278\n",
      "train_loss: 0.3306407008140684\n",
      "val_loss: 0.4789976352745487\n",
      "Progress: 2.8% ... Training loss: 0.301 ... Validation loss: 0.456iteration: 279\n",
      "train_loss: 0.3011928423786302\n",
      "val_loss: 0.4562768044851366\n",
      "Progress: 2.8% ... Training loss: 0.300 ... Validation loss: 0.463iteration: 280\n",
      "train_loss: 0.30021971728650526\n",
      "val_loss: 0.4633442158931824\n",
      "Progress: 2.8% ... Training loss: 0.303 ... Validation loss: 0.469iteration: 281\n",
      "train_loss: 0.30337499827042574\n",
      "val_loss: 0.4694844922896389\n",
      "Progress: 2.8% ... Training loss: 0.307 ... Validation loss: 0.477iteration: 282\n",
      "train_loss: 0.3073442778077149\n",
      "val_loss: 0.4772836073174305\n",
      "Progress: 2.8% ... Training loss: 0.307 ... Validation loss: 0.472iteration: 283\n",
      "train_loss: 0.3073773675250682\n",
      "val_loss: 0.47245796267786433\n",
      "Progress: 2.8% ... Training loss: 0.300 ... Validation loss: 0.453iteration: 284\n",
      "train_loss: 0.3008062922281568\n",
      "val_loss: 0.45359800526537664\n",
      "Progress: 2.9% ... Training loss: 0.306 ... Validation loss: 0.456iteration: 285\n",
      "train_loss: 0.3067868561477375\n",
      "val_loss: 0.4561445630912633\n",
      "Progress: 2.9% ... Training loss: 0.317 ... Validation loss: 0.467iteration: 286\n",
      "train_loss: 0.31748414993235374\n",
      "val_loss: 0.46751650322488253\n",
      "Progress: 2.9% ... Training loss: 0.326 ... Validation loss: 0.487iteration: 287\n",
      "train_loss: 0.32647054561779576\n",
      "val_loss: 0.48713626697504275\n",
      "Progress: 2.9% ... Training loss: 0.298 ... Validation loss: 0.466iteration: 288\n",
      "train_loss: 0.2983113773953878\n",
      "val_loss: 0.4669728723520064\n",
      "Progress: 2.9% ... Training loss: 0.298 ... Validation loss: 0.464iteration: 289\n",
      "train_loss: 0.298273831310974\n",
      "val_loss: 0.4647404421970736\n",
      "Progress: 2.9% ... Training loss: 0.296 ... Validation loss: 0.461iteration: 290\n",
      "train_loss: 0.29651505304142\n",
      "val_loss: 0.46160415078817846\n",
      "Progress: 2.9% ... Training loss: 0.353 ... Validation loss: 0.500iteration: 291\n",
      "train_loss: 0.353683658253236\n",
      "val_loss: 0.5005921344711217\n",
      "Progress: 2.9% ... Training loss: 0.366 ... Validation loss: 0.525iteration: 292\n",
      "train_loss: 0.3668097303984833\n",
      "val_loss: 0.5258575547934691\n",
      "Progress: 2.9% ... Training loss: 0.357 ... Validation loss: 0.507iteration: 293\n",
      "train_loss: 0.3579122441263918\n",
      "val_loss: 0.5070080324023942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 2.9% ... Training loss: 0.303 ... Validation loss: 0.471iteration: 294\n",
      "train_loss: 0.30389755186813583\n",
      "val_loss: 0.47153131513306434\n",
      "Progress: 3.0% ... Training loss: 0.311 ... Validation loss: 0.481iteration: 295\n",
      "train_loss: 0.31107972517176363\n",
      "val_loss: 0.4816782590911059\n",
      "Progress: 3.0% ... Training loss: 0.356 ... Validation loss: 0.514iteration: 296\n",
      "train_loss: 0.35677392711633665\n",
      "val_loss: 0.5143211369883767\n",
      "Progress: 3.0% ... Training loss: 0.321 ... Validation loss: 0.500iteration: 297\n",
      "train_loss: 0.321304822823218\n",
      "val_loss: 0.5005512526312663\n",
      "Progress: 3.0% ... Training loss: 0.337 ... Validation loss: 0.500iteration: 298\n",
      "train_loss: 0.3375369134811655\n",
      "val_loss: 0.5000361753823996\n",
      "Progress: 3.0% ... Training loss: 0.300 ... Validation loss: 0.477iteration: 299\n",
      "train_loss: 0.30075503797489855\n",
      "val_loss: 0.4770700871640638\n",
      "Progress: 3.0% ... Training loss: 0.316 ... Validation loss: 0.479iteration: 300\n",
      "train_loss: 0.31609511023835657\n",
      "val_loss: 0.4792610562550275\n",
      "Progress: 3.0% ... Training loss: 0.316 ... Validation loss: 0.490iteration: 301\n",
      "train_loss: 0.3164249560707993\n",
      "val_loss: 0.49005408079827695\n",
      "Progress: 3.0% ... Training loss: 0.297 ... Validation loss: 0.465iteration: 302\n",
      "train_loss: 0.2979656826139976\n",
      "val_loss: 0.4651776979152038\n",
      "Progress: 3.0% ... Training loss: 0.308 ... Validation loss: 0.479iteration: 303\n",
      "train_loss: 0.3081437111184686\n",
      "val_loss: 0.47949832829733513\n",
      "Progress: 3.0% ... Training loss: 0.300 ... Validation loss: 0.479iteration: 304\n",
      "train_loss: 0.30063944462097497\n",
      "val_loss: 0.4797905033514149\n",
      "Progress: 3.0% ... Training loss: 0.295 ... Validation loss: 0.464iteration: 305\n",
      "train_loss: 0.29575069522543146\n",
      "val_loss: 0.46419996911933403\n",
      "Progress: 3.1% ... Training loss: 0.294 ... Validation loss: 0.462iteration: 306\n",
      "train_loss: 0.29494404788454587\n",
      "val_loss: 0.46207050703210606\n",
      "Progress: 3.1% ... Training loss: 0.300 ... Validation loss: 0.466iteration: 307\n",
      "train_loss: 0.3009318509079679\n",
      "val_loss: 0.4662977381910287\n",
      "Progress: 3.1% ... Training loss: 0.297 ... Validation loss: 0.463iteration: 308\n",
      "train_loss: 0.2970543555083333\n",
      "val_loss: 0.46358105690559837\n",
      "Progress: 3.1% ... Training loss: 0.305 ... Validation loss: 0.471iteration: 309\n",
      "train_loss: 0.30519786326075604\n",
      "val_loss: 0.4714729198203233\n",
      "Progress: 3.1% ... Training loss: 0.295 ... Validation loss: 0.453iteration: 310\n",
      "train_loss: 0.2952596324025491\n",
      "val_loss: 0.4539629385472502\n",
      "Progress: 3.1% ... Training loss: 0.301 ... Validation loss: 0.471iteration: 311\n",
      "train_loss: 0.30158096765357006\n",
      "val_loss: 0.4712459741547263\n",
      "Progress: 3.1% ... Training loss: 0.299 ... Validation loss: 0.470iteration: 312\n",
      "train_loss: 0.2994463446070055\n",
      "val_loss: 0.4709869790280265\n",
      "Progress: 3.1% ... Training loss: 0.296 ... Validation loss: 0.460iteration: 313\n",
      "train_loss: 0.2968068189728976\n",
      "val_loss: 0.4604612980648876\n",
      "Progress: 3.1% ... Training loss: 0.294 ... Validation loss: 0.452iteration: 314\n",
      "train_loss: 0.2944506885824663\n",
      "val_loss: 0.45264204943490116\n",
      "Progress: 3.1% ... Training loss: 0.295 ... Validation loss: 0.459iteration: 315\n",
      "train_loss: 0.2959810795584912\n",
      "val_loss: 0.45938972176104015\n",
      "Progress: 3.2% ... Training loss: 0.306 ... Validation loss: 0.474iteration: 316\n",
      "train_loss: 0.30625322083048157\n",
      "val_loss: 0.4742503907833202\n",
      "Progress: 3.2% ... Training loss: 0.298 ... Validation loss: 0.459iteration: 317\n",
      "train_loss: 0.29809897785834655\n",
      "val_loss: 0.459501300129284\n",
      "Progress: 3.2% ... Training loss: 0.302 ... Validation loss: 0.479iteration: 318\n",
      "train_loss: 0.30218665499759506\n",
      "val_loss: 0.4799063753014947\n",
      "Progress: 3.2% ... Training loss: 0.313 ... Validation loss: 0.455iteration: 319\n",
      "train_loss: 0.3134920590024216\n",
      "val_loss: 0.455422367663238\n",
      "Progress: 3.2% ... Training loss: 0.297 ... Validation loss: 0.448iteration: 320\n",
      "train_loss: 0.2975163074556585\n",
      "val_loss: 0.4488592066528085\n",
      "Progress: 3.2% ... Training loss: 0.297 ... Validation loss: 0.450iteration: 321\n",
      "train_loss: 0.2974920064958002\n",
      "val_loss: 0.45068997399400523\n",
      "Progress: 3.2% ... Training loss: 0.331 ... Validation loss: 0.509iteration: 322\n",
      "train_loss: 0.33153735619832725\n",
      "val_loss: 0.5096366446292536\n",
      "Progress: 3.2% ... Training loss: 0.292 ... Validation loss: 0.459iteration: 323\n",
      "train_loss: 0.292777464310478\n",
      "val_loss: 0.45915913795804325\n",
      "Progress: 3.2% ... Training loss: 0.307 ... Validation loss: 0.455iteration: 324\n",
      "train_loss: 0.30747679961910007\n",
      "val_loss: 0.4557671956873616\n",
      "Progress: 3.2% ... Training loss: 0.351 ... Validation loss: 0.540iteration: 325\n",
      "train_loss: 0.3515488614586627\n",
      "val_loss: 0.5400769311861096\n",
      "Progress: 3.3% ... Training loss: 0.292 ... Validation loss: 0.458iteration: 326\n",
      "train_loss: 0.2925565323056043\n",
      "val_loss: 0.4589451467210917\n",
      "Progress: 3.3% ... Training loss: 0.315 ... Validation loss: 0.497iteration: 327\n",
      "train_loss: 0.3156343022792688\n",
      "val_loss: 0.49713140781079246\n",
      "Progress: 3.3% ... Training loss: 0.391 ... Validation loss: 0.508iteration: 328\n",
      "train_loss: 0.39167233655265227\n",
      "val_loss: 0.5082426340850278\n",
      "Progress: 3.3% ... Training loss: 0.330 ... Validation loss: 0.516iteration: 329\n",
      "train_loss: 0.33065110808940823\n",
      "val_loss: 0.5164380470728154\n",
      "Progress: 3.3% ... Training loss: 0.301 ... Validation loss: 0.460iteration: 330\n",
      "train_loss: 0.30130668280100126\n",
      "val_loss: 0.4607476225124367\n",
      "Progress: 3.3% ... Training loss: 0.296 ... Validation loss: 0.467iteration: 331\n",
      "train_loss: 0.2960034963430363\n",
      "val_loss: 0.46766109901205916\n",
      "Progress: 3.3% ... Training loss: 0.297 ... Validation loss: 0.462iteration: 332\n",
      "train_loss: 0.29740225686943944\n",
      "val_loss: 0.4624261942225682\n",
      "Progress: 3.3% ... Training loss: 0.299 ... Validation loss: 0.471iteration: 333\n",
      "train_loss: 0.2991680003836606\n",
      "val_loss: 0.47192486250354526\n",
      "Progress: 3.3% ... Training loss: 0.321 ... Validation loss: 0.453iteration: 334\n",
      "train_loss: 0.3216024284948283\n",
      "val_loss: 0.4538812681958872\n",
      "Progress: 3.4% ... Training loss: 0.339 ... Validation loss: 0.517iteration: 335\n",
      "train_loss: 0.3393474198905602\n",
      "val_loss: 0.5179021463778758\n",
      "Progress: 3.4% ... Training loss: 0.369 ... Validation loss: 0.523iteration: 336\n",
      "train_loss: 0.3695422834803738\n",
      "val_loss: 0.5233667403223058\n",
      "Progress: 3.4% ... Training loss: 0.385 ... Validation loss: 0.553iteration: 337\n",
      "train_loss: 0.3851419731248725\n",
      "val_loss: 0.553097045128934\n",
      "Progress: 3.4% ... Training loss: 0.410 ... Validation loss: 0.526iteration: 338\n",
      "train_loss: 0.41078953138354124\n",
      "val_loss: 0.526543164389146\n",
      "Progress: 3.4% ... Training loss: 0.315 ... Validation loss: 0.475iteration: 339\n",
      "train_loss: 0.31590732335978294\n",
      "val_loss: 0.4751729582173547\n",
      "Progress: 3.4% ... Training loss: 0.291 ... Validation loss: 0.450iteration: 340\n",
      "train_loss: 0.29188992588333607\n",
      "val_loss: 0.4503281455217127\n",
      "Progress: 3.4% ... Training loss: 0.291 ... Validation loss: 0.449iteration: 341\n",
      "train_loss: 0.29198258481684175\n",
      "val_loss: 0.4499100824361012\n",
      "Progress: 3.4% ... Training loss: 0.292 ... Validation loss: 0.453iteration: 342\n",
      "train_loss: 0.2921291080353498\n",
      "val_loss: 0.4536484222831539\n",
      "Progress: 3.4% ... Training loss: 0.291 ... Validation loss: 0.459iteration: 343\n",
      "train_loss: 0.2911506653496578\n",
      "val_loss: 0.459141635918978\n",
      "Progress: 3.4% ... Training loss: 0.302 ... Validation loss: 0.463iteration: 344\n",
      "train_loss: 0.30265249375154624\n",
      "val_loss: 0.4638069277718025\n",
      "Progress: 3.5% ... Training loss: 0.296 ... Validation loss: 0.446iteration: 345\n",
      "train_loss: 0.296482823570502\n",
      "val_loss: 0.4465820709550897\n",
      "Progress: 3.5% ... Training loss: 0.295 ... Validation loss: 0.443iteration: 346\n",
      "train_loss: 0.2955028450493572\n",
      "val_loss: 0.4436083038461527\n",
      "Progress: 3.5% ... Training loss: 0.293 ... Validation loss: 0.457iteration: 347\n",
      "train_loss: 0.29356387787990723\n",
      "val_loss: 0.4572200161648804\n",
      "Progress: 3.5% ... Training loss: 0.297 ... Validation loss: 0.452iteration: 348\n",
      "train_loss: 0.29734053936083843\n",
      "val_loss: 0.4528631699684556\n",
      "Progress: 3.5% ... Training loss: 0.292 ... Validation loss: 0.443iteration: 349\n",
      "train_loss: 0.2927291751360482\n",
      "val_loss: 0.443685441110591\n",
      "Progress: 3.5% ... Training loss: 0.293 ... Validation loss: 0.447iteration: 350\n",
      "train_loss: 0.29382704219449113\n",
      "val_loss: 0.4476040230031763\n",
      "Progress: 3.5% ... Training loss: 0.291 ... Validation loss: 0.450iteration: 351\n",
      "train_loss: 0.2912725141296565\n",
      "val_loss: 0.4508300863819512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 3.5% ... Training loss: 0.289 ... Validation loss: 0.452iteration: 352\n",
      "train_loss: 0.28957992143164757\n",
      "val_loss: 0.4521624227501328\n",
      "Progress: 3.5% ... Training loss: 0.305 ... Validation loss: 0.473iteration: 353\n",
      "train_loss: 0.30529390012505503\n",
      "val_loss: 0.47308760465848554\n",
      "Progress: 3.5% ... Training loss: 0.385 ... Validation loss: 0.517iteration: 354\n",
      "train_loss: 0.3856063678690764\n",
      "val_loss: 0.5173141051403973\n",
      "Progress: 3.5% ... Training loss: 0.326 ... Validation loss: 0.520iteration: 355\n",
      "train_loss: 0.3264874159778399\n",
      "val_loss: 0.5200138113709794\n",
      "Progress: 3.6% ... Training loss: 0.303 ... Validation loss: 0.457iteration: 356\n",
      "train_loss: 0.30303960543234637\n",
      "val_loss: 0.4575337667250662\n",
      "Progress: 3.6% ... Training loss: 0.294 ... Validation loss: 0.469iteration: 357\n",
      "train_loss: 0.2940102677570817\n",
      "val_loss: 0.4695157265422662\n",
      "Progress: 3.6% ... Training loss: 0.292 ... Validation loss: 0.457iteration: 358\n",
      "train_loss: 0.2921555510240358\n",
      "val_loss: 0.45732537089550873\n",
      "Progress: 3.6% ... Training loss: 0.291 ... Validation loss: 0.466iteration: 359\n",
      "train_loss: 0.29174410034667386\n",
      "val_loss: 0.4660713853640336\n",
      "Progress: 3.6% ... Training loss: 0.289 ... Validation loss: 0.450iteration: 360\n",
      "train_loss: 0.28985145723337763\n",
      "val_loss: 0.45016987386134116\n",
      "Progress: 3.6% ... Training loss: 0.294 ... Validation loss: 0.450iteration: 361\n",
      "train_loss: 0.2943256102023624\n",
      "val_loss: 0.45037403293377465\n",
      "Progress: 3.6% ... Training loss: 0.291 ... Validation loss: 0.457iteration: 362\n",
      "train_loss: 0.2912159678217529\n",
      "val_loss: 0.4576701643908832\n",
      "Progress: 3.6% ... Training loss: 0.319 ... Validation loss: 0.470iteration: 363\n",
      "train_loss: 0.3193732052032653\n",
      "val_loss: 0.4705297419285524\n",
      "Progress: 3.6% ... Training loss: 0.289 ... Validation loss: 0.466iteration: 364\n",
      "train_loss: 0.28932383731858674\n",
      "val_loss: 0.4662278388161349\n",
      "Progress: 3.6% ... Training loss: 0.292 ... Validation loss: 0.465iteration: 365\n",
      "train_loss: 0.2928633960352835\n",
      "val_loss: 0.46545035147519176\n",
      "Progress: 3.7% ... Training loss: 0.296 ... Validation loss: 0.471iteration: 366\n",
      "train_loss: 0.2962497181606941\n",
      "val_loss: 0.4713176022334136\n",
      "Progress: 3.7% ... Training loss: 0.325 ... Validation loss: 0.491iteration: 367\n",
      "train_loss: 0.32513039986824754\n",
      "val_loss: 0.4911731321328864\n",
      "Progress: 3.7% ... Training loss: 0.299 ... Validation loss: 0.481iteration: 368\n",
      "train_loss: 0.2990481921299131\n",
      "val_loss: 0.481321931373392\n",
      "Progress: 3.7% ... Training loss: 0.301 ... Validation loss: 0.474iteration: 369\n",
      "train_loss: 0.30187410028302747\n",
      "val_loss: 0.47460599474972537\n",
      "Progress: 3.7% ... Training loss: 0.312 ... Validation loss: 0.505iteration: 370\n",
      "train_loss: 0.31296644149085673\n",
      "val_loss: 0.5059360779618225\n",
      "Progress: 3.7% ... Training loss: 0.294 ... Validation loss: 0.465iteration: 371\n",
      "train_loss: 0.2945014738292144\n",
      "val_loss: 0.4657266139950416\n",
      "Progress: 3.7% ... Training loss: 0.292 ... Validation loss: 0.472iteration: 372\n",
      "train_loss: 0.2924461191438315\n",
      "val_loss: 0.4724211377465183\n",
      "Progress: 3.7% ... Training loss: 0.304 ... Validation loss: 0.458iteration: 373\n",
      "train_loss: 0.3041264849882386\n",
      "val_loss: 0.458777102080509\n",
      "Progress: 3.7% ... Training loss: 0.308 ... Validation loss: 0.477iteration: 374\n",
      "train_loss: 0.3087952437147368\n",
      "val_loss: 0.4770367676749045\n",
      "Progress: 3.8% ... Training loss: 0.291 ... Validation loss: 0.464iteration: 375\n",
      "train_loss: 0.2919576827390588\n",
      "val_loss: 0.46427931313051274\n",
      "Progress: 3.8% ... Training loss: 0.350 ... Validation loss: 0.519iteration: 376\n",
      "train_loss: 0.35068203519563934\n",
      "val_loss: 0.519602537679045\n",
      "Progress: 3.8% ... Training loss: 0.330 ... Validation loss: 0.510iteration: 377\n",
      "train_loss: 0.3308090385602885\n",
      "val_loss: 0.5101164529020822\n",
      "Progress: 3.8% ... Training loss: 0.312 ... Validation loss: 0.487iteration: 378\n",
      "train_loss: 0.3127378375457002\n",
      "val_loss: 0.4870947163496702\n",
      "Progress: 3.8% ... Training loss: 0.302 ... Validation loss: 0.477iteration: 379\n",
      "train_loss: 0.30239499232680705\n",
      "val_loss: 0.47728431287952555\n",
      "Progress: 3.8% ... Training loss: 0.288 ... Validation loss: 0.453iteration: 380\n",
      "train_loss: 0.2882412364335342\n",
      "val_loss: 0.45300324143056625\n",
      "Progress: 3.8% ... Training loss: 0.299 ... Validation loss: 0.461iteration: 381\n",
      "train_loss: 0.2998789012728879\n",
      "val_loss: 0.46194543697749424\n",
      "Progress: 3.8% ... Training loss: 0.289 ... Validation loss: 0.462iteration: 382\n",
      "train_loss: 0.28909950661551603\n",
      "val_loss: 0.4628973454502353\n",
      "Progress: 3.8% ... Training loss: 0.300 ... Validation loss: 0.457iteration: 383\n",
      "train_loss: 0.3009942230375273\n",
      "val_loss: 0.4576860859842338\n",
      "Progress: 3.8% ... Training loss: 0.286 ... Validation loss: 0.457iteration: 384\n",
      "train_loss: 0.28601245107315587\n",
      "val_loss: 0.4574174892643501\n",
      "Progress: 3.9% ... Training loss: 0.290 ... Validation loss: 0.463iteration: 385\n",
      "train_loss: 0.29069830892624327\n",
      "val_loss: 0.46310062831333265\n",
      "Progress: 3.9% ... Training loss: 0.300 ... Validation loss: 0.466iteration: 386\n",
      "train_loss: 0.30061979577035214\n",
      "val_loss: 0.46676086740879996\n",
      "Progress: 3.9% ... Training loss: 0.291 ... Validation loss: 0.468iteration: 387\n",
      "train_loss: 0.2915485620265121\n",
      "val_loss: 0.46862124109171954\n",
      "Progress: 3.9% ... Training loss: 0.291 ... Validation loss: 0.459iteration: 388\n",
      "train_loss: 0.29198212102374344\n",
      "val_loss: 0.4593940846228724\n",
      "Progress: 3.9% ... Training loss: 0.284 ... Validation loss: 0.455iteration: 389\n",
      "train_loss: 0.28429683683776913\n",
      "val_loss: 0.45537815015655303\n",
      "Progress: 3.9% ... Training loss: 0.308 ... Validation loss: 0.481iteration: 390\n",
      "train_loss: 0.3086246838800457\n",
      "val_loss: 0.48108661319174834\n",
      "Progress: 3.9% ... Training loss: 0.317 ... Validation loss: 0.489iteration: 391\n",
      "train_loss: 0.3177465375956089\n",
      "val_loss: 0.4896635829805468\n",
      "Progress: 3.9% ... Training loss: 0.290 ... Validation loss: 0.462iteration: 392\n",
      "train_loss: 0.2900663137165008\n",
      "val_loss: 0.4626203776548206\n",
      "Progress: 3.9% ... Training loss: 0.288 ... Validation loss: 0.467iteration: 393\n",
      "train_loss: 0.2888258889711193\n",
      "val_loss: 0.4674792704870742\n",
      "Progress: 3.9% ... Training loss: 0.287 ... Validation loss: 0.457iteration: 394\n",
      "train_loss: 0.2879081800581375\n",
      "val_loss: 0.457462334336179\n",
      "Progress: 4.0% ... Training loss: 0.298 ... Validation loss: 0.472iteration: 395\n",
      "train_loss: 0.2984391073491124\n",
      "val_loss: 0.4728842463896126\n",
      "Progress: 4.0% ... Training loss: 0.299 ... Validation loss: 0.479iteration: 396\n",
      "train_loss: 0.29949284467630627\n",
      "val_loss: 0.47973860076571334\n",
      "Progress: 4.0% ... Training loss: 0.286 ... Validation loss: 0.446iteration: 397\n",
      "train_loss: 0.2867565919321704\n",
      "val_loss: 0.44661350963231694\n",
      "Progress: 4.0% ... Training loss: 0.286 ... Validation loss: 0.448iteration: 398\n",
      "train_loss: 0.28669508179926767\n",
      "val_loss: 0.44849376284953624\n",
      "Progress: 4.0% ... Training loss: 0.309 ... Validation loss: 0.474iteration: 399\n",
      "train_loss: 0.30985719616230717\n",
      "val_loss: 0.4748569344729984\n",
      "Progress: 4.0% ... Training loss: 0.283 ... Validation loss: 0.446iteration: 400\n",
      "train_loss: 0.2839479659282374\n",
      "val_loss: 0.4464611031050715\n",
      "Progress: 4.0% ... Training loss: 0.292 ... Validation loss: 0.463iteration: 401\n",
      "train_loss: 0.29256971354611866\n",
      "val_loss: 0.46306621976407947\n",
      "Progress: 4.0% ... Training loss: 0.286 ... Validation loss: 0.448iteration: 402\n",
      "train_loss: 0.28696798637743387\n",
      "val_loss: 0.4484105984882828\n",
      "Progress: 4.0% ... Training loss: 0.286 ... Validation loss: 0.456iteration: 403\n",
      "train_loss: 0.28663363777675294\n",
      "val_loss: 0.4563648066366633\n",
      "Progress: 4.0% ... Training loss: 0.295 ... Validation loss: 0.454iteration: 404\n",
      "train_loss: 0.29559993769536175\n",
      "val_loss: 0.45460233115630744\n",
      "Progress: 4.0% ... Training loss: 0.302 ... Validation loss: 0.467iteration: 405\n",
      "train_loss: 0.30291684182809686\n",
      "val_loss: 0.46735929223032247\n",
      "Progress: 4.1% ... Training loss: 0.284 ... Validation loss: 0.444iteration: 406\n",
      "train_loss: 0.28430874914346793\n",
      "val_loss: 0.44462873311951434\n",
      "Progress: 4.1% ... Training loss: 0.282 ... Validation loss: 0.450iteration: 407\n",
      "train_loss: 0.28298878505732533\n",
      "val_loss: 0.45084816521931004\n",
      "Progress: 4.1% ... Training loss: 0.290 ... Validation loss: 0.462iteration: 408\n",
      "train_loss: 0.29078333745341656\n",
      "val_loss: 0.46248125472667473\n",
      "Progress: 4.1% ... Training loss: 0.282 ... Validation loss: 0.457iteration: 409\n",
      "train_loss: 0.28222183290268643\n",
      "val_loss: 0.4577025868289567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 4.1% ... Training loss: 0.283 ... Validation loss: 0.463iteration: 410\n",
      "train_loss: 0.28313343693901255\n",
      "val_loss: 0.4633453643015588\n",
      "Progress: 4.1% ... Training loss: 0.282 ... Validation loss: 0.458iteration: 411\n",
      "train_loss: 0.28240752065534436\n",
      "val_loss: 0.45841042088069944\n",
      "Progress: 4.1% ... Training loss: 0.288 ... Validation loss: 0.464iteration: 412\n",
      "train_loss: 0.2886600947508168\n",
      "val_loss: 0.46477509400987127\n",
      "Progress: 4.1% ... Training loss: 0.293 ... Validation loss: 0.455iteration: 413\n",
      "train_loss: 0.2938332501800394\n",
      "val_loss: 0.4557005755145894\n",
      "Progress: 4.1% ... Training loss: 0.287 ... Validation loss: 0.451iteration: 414\n",
      "train_loss: 0.2877095334970807\n",
      "val_loss: 0.45124332535073663\n",
      "Progress: 4.2% ... Training loss: 0.288 ... Validation loss: 0.471iteration: 415\n",
      "train_loss: 0.28810339569182153\n",
      "val_loss: 0.47143384117530646\n",
      "Progress: 4.2% ... Training loss: 0.287 ... Validation loss: 0.454iteration: 416\n",
      "train_loss: 0.28716860645790043\n",
      "val_loss: 0.4541888851687626\n",
      "Progress: 4.2% ... Training loss: 0.284 ... Validation loss: 0.471iteration: 417\n",
      "train_loss: 0.28451888936527275\n",
      "val_loss: 0.4711684962505795\n",
      "Progress: 4.2% ... Training loss: 0.301 ... Validation loss: 0.466iteration: 418\n",
      "train_loss: 0.3013427578403221\n",
      "val_loss: 0.4665342085966114\n",
      "Progress: 4.2% ... Training loss: 0.293 ... Validation loss: 0.484iteration: 419\n",
      "train_loss: 0.2932337459977318\n",
      "val_loss: 0.4840046634208795\n",
      "Progress: 4.2% ... Training loss: 0.280 ... Validation loss: 0.453iteration: 420\n",
      "train_loss: 0.28025733770457906\n",
      "val_loss: 0.4533446751557367\n",
      "Progress: 4.2% ... Training loss: 0.279 ... Validation loss: 0.454iteration: 421\n",
      "train_loss: 0.27983100209755835\n",
      "val_loss: 0.4542089964225754\n",
      "Progress: 4.2% ... Training loss: 0.282 ... Validation loss: 0.452iteration: 422\n",
      "train_loss: 0.2827163162376721\n",
      "val_loss: 0.45259594859924346\n",
      "Progress: 4.2% ... Training loss: 0.282 ... Validation loss: 0.443iteration: 423\n",
      "train_loss: 0.2827509966714549\n",
      "val_loss: 0.4432446533621398\n",
      "Progress: 4.2% ... Training loss: 0.282 ... Validation loss: 0.447iteration: 424\n",
      "train_loss: 0.2822693422839047\n",
      "val_loss: 0.44750513222600874\n",
      "Progress: 4.2% ... Training loss: 0.293 ... Validation loss: 0.461iteration: 425\n",
      "train_loss: 0.2936960791565829\n",
      "val_loss: 0.46133071616172255\n",
      "Progress: 4.3% ... Training loss: 0.284 ... Validation loss: 0.448iteration: 426\n",
      "train_loss: 0.28417581896609295\n",
      "val_loss: 0.44898172762221833\n",
      "Progress: 4.3% ... Training loss: 0.285 ... Validation loss: 0.462iteration: 427\n",
      "train_loss: 0.28574208599543094\n",
      "val_loss: 0.46280214090353866\n",
      "Progress: 4.3% ... Training loss: 0.296 ... Validation loss: 0.468iteration: 428\n",
      "train_loss: 0.2969999152939952\n",
      "val_loss: 0.46882092643043627\n",
      "Progress: 4.3% ... Training loss: 0.285 ... Validation loss: 0.451iteration: 429\n",
      "train_loss: 0.2853784898190383\n",
      "val_loss: 0.4516131123948868\n",
      "Progress: 4.3% ... Training loss: 0.281 ... Validation loss: 0.450iteration: 430\n",
      "train_loss: 0.28177445878800433\n",
      "val_loss: 0.45073912059581794\n",
      "Progress: 4.3% ... Training loss: 0.286 ... Validation loss: 0.456iteration: 431\n",
      "train_loss: 0.2868770122805184\n",
      "val_loss: 0.4567927035047579\n",
      "Progress: 4.3% ... Training loss: 0.288 ... Validation loss: 0.468iteration: 432\n",
      "train_loss: 0.28828136437549834\n",
      "val_loss: 0.46822216931090455\n",
      "Progress: 4.3% ... Training loss: 0.285 ... Validation loss: 0.466iteration: 433\n",
      "train_loss: 0.28541707689761775\n",
      "val_loss: 0.4669758491501513\n",
      "Progress: 4.3% ... Training loss: 0.279 ... Validation loss: 0.452iteration: 434\n",
      "train_loss: 0.2797899917308385\n",
      "val_loss: 0.4522471502214914\n",
      "Progress: 4.3% ... Training loss: 0.289 ... Validation loss: 0.474iteration: 435\n",
      "train_loss: 0.2899466802594026\n",
      "val_loss: 0.47409357158529297\n",
      "Progress: 4.4% ... Training loss: 0.278 ... Validation loss: 0.458iteration: 436\n",
      "train_loss: 0.2782875506954176\n",
      "val_loss: 0.45886913559175174\n",
      "Progress: 4.4% ... Training loss: 0.286 ... Validation loss: 0.466iteration: 437\n",
      "train_loss: 0.2861122852696511\n",
      "val_loss: 0.46625297503004987\n",
      "Progress: 4.4% ... Training loss: 0.278 ... Validation loss: 0.456iteration: 438\n",
      "train_loss: 0.2786602072015701\n",
      "val_loss: 0.4568244824581057\n",
      "Progress: 4.4% ... Training loss: 0.278 ... Validation loss: 0.449iteration: 439\n",
      "train_loss: 0.27869209527679284\n",
      "val_loss: 0.4499988386096493\n",
      "Progress: 4.4% ... Training loss: 0.280 ... Validation loss: 0.460iteration: 440\n",
      "train_loss: 0.2809229639202476\n",
      "val_loss: 0.4602597154303234\n",
      "Progress: 4.4% ... Training loss: 0.284 ... Validation loss: 0.458iteration: 441\n",
      "train_loss: 0.28463140630069217\n",
      "val_loss: 0.45861655587335565\n",
      "Progress: 4.4% ... Training loss: 0.278 ... Validation loss: 0.449iteration: 442\n",
      "train_loss: 0.2783974709291738\n",
      "val_loss: 0.4492653760406012\n",
      "Progress: 4.4% ... Training loss: 0.278 ... Validation loss: 0.449iteration: 443\n",
      "train_loss: 0.27838148220936654\n",
      "val_loss: 0.4497560012592113\n",
      "Progress: 4.4% ... Training loss: 0.288 ... Validation loss: 0.455iteration: 444\n",
      "train_loss: 0.2880216126784001\n",
      "val_loss: 0.45597903417209396\n",
      "Progress: 4.5% ... Training loss: 0.285 ... Validation loss: 0.454iteration: 445\n",
      "train_loss: 0.28595508273896897\n",
      "val_loss: 0.45469259734764716\n",
      "Progress: 4.5% ... Training loss: 0.300 ... Validation loss: 0.473iteration: 446\n",
      "train_loss: 0.3007887042438525\n",
      "val_loss: 0.4739176702907179\n",
      "Progress: 4.5% ... Training loss: 0.288 ... Validation loss: 0.459iteration: 447\n",
      "train_loss: 0.28845035182084544\n",
      "val_loss: 0.459643743974517\n",
      "Progress: 4.5% ... Training loss: 0.278 ... Validation loss: 0.456iteration: 448\n",
      "train_loss: 0.27831747937714957\n",
      "val_loss: 0.45625721600929325\n",
      "Progress: 4.5% ... Training loss: 0.280 ... Validation loss: 0.465iteration: 449\n",
      "train_loss: 0.28062414026024163\n",
      "val_loss: 0.46567850269975586\n",
      "Progress: 4.5% ... Training loss: 0.279 ... Validation loss: 0.461iteration: 450\n",
      "train_loss: 0.27922513085721296\n",
      "val_loss: 0.4611972024836902\n",
      "Progress: 4.5% ... Training loss: 0.277 ... Validation loss: 0.458iteration: 451\n",
      "train_loss: 0.27727662472542974\n",
      "val_loss: 0.4589718844051512\n",
      "Progress: 4.5% ... Training loss: 0.288 ... Validation loss: 0.469iteration: 452\n",
      "train_loss: 0.28883760902463873\n",
      "val_loss: 0.4696670445257264\n",
      "Progress: 4.5% ... Training loss: 0.309 ... Validation loss: 0.485iteration: 453\n",
      "train_loss: 0.309157572481435\n",
      "val_loss: 0.4854750177480964\n",
      "Progress: 4.5% ... Training loss: 0.281 ... Validation loss: 0.465iteration: 454\n",
      "train_loss: 0.28163511167420713\n",
      "val_loss: 0.465521005887625\n",
      "Progress: 4.5% ... Training loss: 0.287 ... Validation loss: 0.466iteration: 455\n",
      "train_loss: 0.2872127546312833\n",
      "val_loss: 0.4665747729052124\n",
      "Progress: 4.6% ... Training loss: 0.299 ... Validation loss: 0.469iteration: 456\n",
      "train_loss: 0.29999511629143955\n",
      "val_loss: 0.4696454035930668\n",
      "Progress: 4.6% ... Training loss: 0.281 ... Validation loss: 0.451iteration: 457\n",
      "train_loss: 0.28133023115459166\n",
      "val_loss: 0.4514149564462545\n",
      "Progress: 4.6% ... Training loss: 0.276 ... Validation loss: 0.452iteration: 458\n",
      "train_loss: 0.27639519359349557\n",
      "val_loss: 0.45299108031610175\n",
      "Progress: 4.6% ... Training loss: 0.276 ... Validation loss: 0.457iteration: 459\n",
      "train_loss: 0.27605685843537314\n",
      "val_loss: 0.4578032747390154\n",
      "Progress: 4.6% ... Training loss: 0.284 ... Validation loss: 0.473iteration: 460\n",
      "train_loss: 0.28448247868055654\n",
      "val_loss: 0.4737279730163825\n",
      "Progress: 4.6% ... Training loss: 0.279 ... Validation loss: 0.458iteration: 461\n",
      "train_loss: 0.2794294231720759\n",
      "val_loss: 0.4581847108775082\n",
      "Progress: 4.6% ... Training loss: 0.276 ... Validation loss: 0.452iteration: 462\n",
      "train_loss: 0.27676515059056733\n",
      "val_loss: 0.4525346851618737\n",
      "Progress: 4.6% ... Training loss: 0.279 ... Validation loss: 0.443iteration: 463\n",
      "train_loss: 0.27942679357374195\n",
      "val_loss: 0.4431022313302497\n",
      "Progress: 4.6% ... Training loss: 0.276 ... Validation loss: 0.453iteration: 464\n",
      "train_loss: 0.2760673271188386\n",
      "val_loss: 0.45375247664979085\n",
      "Progress: 4.7% ... Training loss: 0.288 ... Validation loss: 0.445iteration: 465\n",
      "train_loss: 0.2880185131394664\n",
      "val_loss: 0.4458611172094428\n",
      "Progress: 4.7% ... Training loss: 0.275 ... Validation loss: 0.448iteration: 466\n",
      "train_loss: 0.27536341605954556\n",
      "val_loss: 0.44801175328211074\n",
      "Progress: 4.7% ... Training loss: 0.275 ... Validation loss: 0.452iteration: 467\n",
      "train_loss: 0.27586831328684963\n",
      "val_loss: 0.45217097453851746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 4.7% ... Training loss: 0.279 ... Validation loss: 0.459iteration: 468\n",
      "train_loss: 0.27929241748726996\n",
      "val_loss: 0.45907521001797197\n",
      "Progress: 4.7% ... Training loss: 0.276 ... Validation loss: 0.461iteration: 469\n",
      "train_loss: 0.27649384881256484\n",
      "val_loss: 0.4614968436802515\n",
      "Progress: 4.7% ... Training loss: 0.275 ... Validation loss: 0.449iteration: 470\n",
      "train_loss: 0.27577583648312387\n",
      "val_loss: 0.449881869153747\n",
      "Progress: 4.7% ... Training loss: 0.275 ... Validation loss: 0.452iteration: 471\n",
      "train_loss: 0.2755543393116397\n",
      "val_loss: 0.45230069356039987\n",
      "Progress: 4.7% ... Training loss: 0.281 ... Validation loss: 0.450iteration: 472\n",
      "train_loss: 0.28152967679951846\n",
      "val_loss: 0.4507925840416572\n",
      "Progress: 4.7% ... Training loss: 0.282 ... Validation loss: 0.456iteration: 473\n",
      "train_loss: 0.28293427164414237\n",
      "val_loss: 0.45664268179688994\n",
      "Progress: 4.7% ... Training loss: 0.279 ... Validation loss: 0.454iteration: 474\n",
      "train_loss: 0.27928303739867\n",
      "val_loss: 0.45457036800671385\n",
      "Progress: 4.8% ... Training loss: 0.281 ... Validation loss: 0.455iteration: 475\n",
      "train_loss: 0.2814461140090725\n",
      "val_loss: 0.45593219578315947\n",
      "Progress: 4.8% ... Training loss: 0.275 ... Validation loss: 0.457iteration: 476\n",
      "train_loss: 0.2753421692233736\n",
      "val_loss: 0.4572650628067389\n",
      "Progress: 4.8% ... Training loss: 0.274 ... Validation loss: 0.458iteration: 477\n",
      "train_loss: 0.27427923517851194\n",
      "val_loss: 0.4585025781148504\n",
      "Progress: 4.8% ... Training loss: 0.279 ... Validation loss: 0.462iteration: 478\n",
      "train_loss: 0.27938210091714966\n",
      "val_loss: 0.46230287758865674\n",
      "Progress: 4.8% ... Training loss: 0.274 ... Validation loss: 0.461iteration: 479\n",
      "train_loss: 0.27451185150338436\n",
      "val_loss: 0.46196153877965096\n",
      "Progress: 4.8% ... Training loss: 0.274 ... Validation loss: 0.457iteration: 480\n",
      "train_loss: 0.27485082393149124\n",
      "val_loss: 0.4575453676043311\n",
      "Progress: 4.8% ... Training loss: 0.275 ... Validation loss: 0.469iteration: 481\n",
      "train_loss: 0.275580208456317\n",
      "val_loss: 0.46989178242890006\n",
      "Progress: 4.8% ... Training loss: 0.279 ... Validation loss: 0.460iteration: 482\n",
      "train_loss: 0.2795115144205628\n",
      "val_loss: 0.4609478501136427\n",
      "Progress: 4.8% ... Training loss: 0.275 ... Validation loss: 0.458iteration: 483\n",
      "train_loss: 0.2759651163778799\n",
      "val_loss: 0.45893070384169116\n",
      "Progress: 4.8% ... Training loss: 0.305 ... Validation loss: 0.494iteration: 484\n",
      "train_loss: 0.30547622806261243\n",
      "val_loss: 0.4944510840978912\n",
      "Progress: 4.8% ... Training loss: 0.285 ... Validation loss: 0.463iteration: 485\n",
      "train_loss: 0.2858169820700996\n",
      "val_loss: 0.46351271460705995\n",
      "Progress: 4.9% ... Training loss: 0.284 ... Validation loss: 0.487iteration: 486\n",
      "train_loss: 0.2848874351503256\n",
      "val_loss: 0.4870877470240111\n",
      "Progress: 4.9% ... Training loss: 0.282 ... Validation loss: 0.492iteration: 487\n",
      "train_loss: 0.28214491380962853\n",
      "val_loss: 0.4927707382051008\n",
      "Progress: 4.9% ... Training loss: 0.279 ... Validation loss: 0.463iteration: 488\n",
      "train_loss: 0.2797358690181132\n",
      "val_loss: 0.4639703516874679\n",
      "Progress: 4.9% ... Training loss: 0.278 ... Validation loss: 0.462iteration: 489\n",
      "train_loss: 0.2780401010864656\n",
      "val_loss: 0.4625396485223758\n",
      "Progress: 4.9% ... Training loss: 0.282 ... Validation loss: 0.475iteration: 490\n",
      "train_loss: 0.28242529344711953\n",
      "val_loss: 0.47591488547569694\n",
      "Progress: 4.9% ... Training loss: 0.276 ... Validation loss: 0.462iteration: 491\n",
      "train_loss: 0.27619020402938077\n",
      "val_loss: 0.46296383005292036\n",
      "Progress: 4.9% ... Training loss: 0.279 ... Validation loss: 0.471iteration: 492\n",
      "train_loss: 0.27954331827931406\n",
      "val_loss: 0.4711679997716009\n",
      "Progress: 4.9% ... Training loss: 0.273 ... Validation loss: 0.461iteration: 493\n",
      "train_loss: 0.2738899362681865\n",
      "val_loss: 0.46135611966540907\n",
      "Progress: 4.9% ... Training loss: 0.273 ... Validation loss: 0.447iteration: 494\n",
      "train_loss: 0.2733895093309896\n",
      "val_loss: 0.44746815003157425\n",
      "Progress: 5.0% ... Training loss: 0.280 ... Validation loss: 0.460iteration: 495\n",
      "train_loss: 0.2806114189719843\n",
      "val_loss: 0.4602893582222884\n",
      "Progress: 5.0% ... Training loss: 0.276 ... Validation loss: 0.465iteration: 496\n",
      "train_loss: 0.2768708873217734\n",
      "val_loss: 0.4655456695049733\n",
      "Progress: 5.0% ... Training loss: 0.285 ... Validation loss: 0.461iteration: 497\n",
      "train_loss: 0.2854529443820638\n",
      "val_loss: 0.46165939441979037\n",
      "Progress: 5.0% ... Training loss: 0.288 ... Validation loss: 0.483iteration: 498\n",
      "train_loss: 0.2886050053100589\n",
      "val_loss: 0.48394501176379284\n",
      "Progress: 5.0% ... Training loss: 0.276 ... Validation loss: 0.458iteration: 499\n",
      "train_loss: 0.2765806078899183\n",
      "val_loss: 0.4582172855511904\n",
      "Progress: 5.0% ... Training loss: 0.279 ... Validation loss: 0.454iteration: 500\n",
      "train_loss: 0.27953386696315907\n",
      "val_loss: 0.45446058167329617\n",
      "Progress: 5.0% ... Training loss: 0.289 ... Validation loss: 0.467iteration: 501\n",
      "train_loss: 0.2898392941343549\n",
      "val_loss: 0.46746771904050227\n",
      "Progress: 5.0% ... Training loss: 0.277 ... Validation loss: 0.451iteration: 502\n",
      "train_loss: 0.2777158953581541\n",
      "val_loss: 0.45122314838996735\n",
      "Progress: 5.0% ... Training loss: 0.298 ... Validation loss: 0.490iteration: 503\n",
      "train_loss: 0.2989466226273279\n",
      "val_loss: 0.4907884731067211\n",
      "Progress: 5.0% ... Training loss: 0.275 ... Validation loss: 0.455iteration: 504\n",
      "train_loss: 0.2758592261267415\n",
      "val_loss: 0.4556238698687175\n",
      "Progress: 5.0% ... Training loss: 0.272 ... Validation loss: 0.460iteration: 505\n",
      "train_loss: 0.27296444831695443\n",
      "val_loss: 0.460676238286869\n",
      "Progress: 5.1% ... Training loss: 0.275 ... Validation loss: 0.451iteration: 506\n",
      "train_loss: 0.2755230763905606\n",
      "val_loss: 0.4512631092861034\n",
      "Progress: 5.1% ... Training loss: 0.272 ... Validation loss: 0.443iteration: 507\n",
      "train_loss: 0.2725152601483096\n",
      "val_loss: 0.44357640707979074\n",
      "Progress: 5.1% ... Training loss: 0.274 ... Validation loss: 0.444iteration: 508\n",
      "train_loss: 0.2740156495221914\n",
      "val_loss: 0.4446478284516003\n",
      "Progress: 5.1% ... Training loss: 0.273 ... Validation loss: 0.452iteration: 509\n",
      "train_loss: 0.27329317315545826\n",
      "val_loss: 0.4529534699808627\n",
      "Progress: 5.1% ... Training loss: 0.287 ... Validation loss: 0.456iteration: 510\n",
      "train_loss: 0.28773876474859944\n",
      "val_loss: 0.456409234664388\n",
      "Progress: 5.1% ... Training loss: 0.295 ... Validation loss: 0.455iteration: 511\n",
      "train_loss: 0.29584014504516926\n",
      "val_loss: 0.45542191081277716\n",
      "Progress: 5.1% ... Training loss: 0.321 ... Validation loss: 0.485iteration: 512\n",
      "train_loss: 0.32159399803567074\n",
      "val_loss: 0.48580909424865176\n",
      "Progress: 5.1% ... Training loss: 0.362 ... Validation loss: 0.550iteration: 513\n",
      "train_loss: 0.36201915317585576\n",
      "val_loss: 0.5507328408213118\n",
      "Progress: 5.1% ... Training loss: 0.309 ... Validation loss: 0.467iteration: 514\n",
      "train_loss: 0.3095194666765619\n",
      "val_loss: 0.4677081055250675\n",
      "Progress: 5.2% ... Training loss: 0.304 ... Validation loss: 0.480iteration: 515\n",
      "train_loss: 0.3042815779982155\n",
      "val_loss: 0.48071090250680126\n",
      "Progress: 5.2% ... Training loss: 0.273 ... Validation loss: 0.445iteration: 516\n",
      "train_loss: 0.27316798934065384\n",
      "val_loss: 0.445780494992678\n",
      "Progress: 5.2% ... Training loss: 0.272 ... Validation loss: 0.450iteration: 517\n",
      "train_loss: 0.2726826639441303\n",
      "val_loss: 0.45023281636167184\n",
      "Progress: 5.2% ... Training loss: 0.312 ... Validation loss: 0.477iteration: 518\n",
      "train_loss: 0.3129549921879985\n",
      "val_loss: 0.47723126380205316\n",
      "Progress: 5.2% ... Training loss: 0.297 ... Validation loss: 0.478iteration: 519\n",
      "train_loss: 0.2976735957845843\n",
      "val_loss: 0.4789030371095804\n",
      "Progress: 5.2% ... Training loss: 0.292 ... Validation loss: 0.473iteration: 520\n",
      "train_loss: 0.2929278614579204\n",
      "val_loss: 0.4730348226544379\n",
      "Progress: 5.2% ... Training loss: 0.273 ... Validation loss: 0.460iteration: 521\n",
      "train_loss: 0.27371028997453717\n",
      "val_loss: 0.46079932859649847\n",
      "Progress: 5.2% ... Training loss: 0.271 ... Validation loss: 0.458iteration: 522\n",
      "train_loss: 0.271168798894505\n",
      "val_loss: 0.4588934591323423\n",
      "Progress: 5.2% ... Training loss: 0.274 ... Validation loss: 0.443iteration: 523\n",
      "train_loss: 0.2744679681349703\n",
      "val_loss: 0.44332438722553164\n",
      "Progress: 5.2% ... Training loss: 0.269 ... Validation loss: 0.445iteration: 524\n",
      "train_loss: 0.26919754358062187\n",
      "val_loss: 0.4454510413654437\n",
      "Progress: 5.2% ... Training loss: 0.269 ... Validation loss: 0.454iteration: 525\n",
      "train_loss: 0.2692769956702494\n",
      "val_loss: 0.4548114674615357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 5.3% ... Training loss: 0.276 ... Validation loss: 0.458iteration: 526\n",
      "train_loss: 0.27680982169700435\n",
      "val_loss: 0.45854789522622763\n",
      "Progress: 5.3% ... Training loss: 0.275 ... Validation loss: 0.459iteration: 527\n",
      "train_loss: 0.2753684957111453\n",
      "val_loss: 0.45933077614467854\n",
      "Progress: 5.3% ... Training loss: 0.269 ... Validation loss: 0.446iteration: 528\n",
      "train_loss: 0.2691396943525338\n",
      "val_loss: 0.4467572076600471\n",
      "Progress: 5.3% ... Training loss: 0.270 ... Validation loss: 0.444iteration: 529\n",
      "train_loss: 0.27056633494931387\n",
      "val_loss: 0.44487334703043324\n",
      "Progress: 5.3% ... Training loss: 0.270 ... Validation loss: 0.446iteration: 530\n",
      "train_loss: 0.270641028105806\n",
      "val_loss: 0.44694644669835715\n",
      "Progress: 5.3% ... Training loss: 0.268 ... Validation loss: 0.446iteration: 531\n",
      "train_loss: 0.26876421362194625\n",
      "val_loss: 0.4465355145869866\n",
      "Progress: 5.3% ... Training loss: 0.276 ... Validation loss: 0.464iteration: 532\n",
      "train_loss: 0.2769078474285469\n",
      "val_loss: 0.46481027605716413\n",
      "Progress: 5.3% ... Training loss: 0.271 ... Validation loss: 0.450iteration: 533\n",
      "train_loss: 0.2715898941343169\n",
      "val_loss: 0.45007444782784467\n",
      "Progress: 5.3% ... Training loss: 0.274 ... Validation loss: 0.447iteration: 534\n",
      "train_loss: 0.27416338844951577\n",
      "val_loss: 0.44715291542828495\n",
      "Progress: 5.3% ... Training loss: 0.269 ... Validation loss: 0.447iteration: 535\n",
      "train_loss: 0.2697739342770295\n",
      "val_loss: 0.447196545864917\n",
      "Progress: 5.4% ... Training loss: 0.285 ... Validation loss: 0.467iteration: 536\n",
      "train_loss: 0.28586845845138587\n",
      "val_loss: 0.46757402493467826\n",
      "Progress: 5.4% ... Training loss: 0.268 ... Validation loss: 0.459iteration: 537\n",
      "train_loss: 0.2688765241108422\n",
      "val_loss: 0.4590686004261696\n",
      "Progress: 5.4% ... Training loss: 0.269 ... Validation loss: 0.448iteration: 538\n",
      "train_loss: 0.26953589391010774\n",
      "val_loss: 0.4484350433114867\n",
      "Progress: 5.4% ... Training loss: 0.269 ... Validation loss: 0.445iteration: 539\n",
      "train_loss: 0.26914849297812443\n",
      "val_loss: 0.4452102067873164\n",
      "Progress: 5.4% ... Training loss: 0.267 ... Validation loss: 0.448iteration: 540\n",
      "train_loss: 0.26784820615383503\n",
      "val_loss: 0.4487609049084676\n",
      "Progress: 5.4% ... Training loss: 0.269 ... Validation loss: 0.459iteration: 541\n",
      "train_loss: 0.2697053012167716\n",
      "val_loss: 0.4591775333021274\n",
      "Progress: 5.4% ... Training loss: 0.269 ... Validation loss: 0.447iteration: 542\n",
      "train_loss: 0.26954463015364044\n",
      "val_loss: 0.44798570143829053\n",
      "Progress: 5.4% ... Training loss: 0.292 ... Validation loss: 0.472iteration: 543\n",
      "train_loss: 0.29228163805346413\n",
      "val_loss: 0.47209189628877457\n",
      "Progress: 5.4% ... Training loss: 0.288 ... Validation loss: 0.468iteration: 544\n",
      "train_loss: 0.2885687835658105\n",
      "val_loss: 0.4685224292728939\n",
      "Progress: 5.5% ... Training loss: 0.269 ... Validation loss: 0.454iteration: 545\n",
      "train_loss: 0.26943631507931426\n",
      "val_loss: 0.4545526778792167\n",
      "Progress: 5.5% ... Training loss: 0.277 ... Validation loss: 0.448iteration: 546\n",
      "train_loss: 0.27740872662363786\n",
      "val_loss: 0.4489071964784905\n",
      "Progress: 5.5% ... Training loss: 0.287 ... Validation loss: 0.468iteration: 547\n",
      "train_loss: 0.28761437102059384\n",
      "val_loss: 0.4686275136832925\n",
      "Progress: 5.5% ... Training loss: 0.289 ... Validation loss: 0.459iteration: 548\n",
      "train_loss: 0.2896577535772676\n",
      "val_loss: 0.4592154513936383\n",
      "Progress: 5.5% ... Training loss: 0.274 ... Validation loss: 0.458iteration: 549\n",
      "train_loss: 0.2744021200885036\n",
      "val_loss: 0.4581808706178565\n",
      "Progress: 5.5% ... Training loss: 0.298 ... Validation loss: 0.462iteration: 550\n",
      "train_loss: 0.2980340568343853\n",
      "val_loss: 0.46224943542363345\n",
      "Progress: 5.5% ... Training loss: 0.287 ... Validation loss: 0.472iteration: 551\n",
      "train_loss: 0.2873813405744454\n",
      "val_loss: 0.472147054154271\n",
      "Progress: 5.5% ... Training loss: 0.290 ... Validation loss: 0.441iteration: 552\n",
      "train_loss: 0.2909133329228927\n",
      "val_loss: 0.4414322447582017\n",
      "Progress: 5.5% ... Training loss: 0.272 ... Validation loss: 0.453iteration: 553\n",
      "train_loss: 0.27202421729185994\n",
      "val_loss: 0.4530930691058642\n",
      "Progress: 5.5% ... Training loss: 0.268 ... Validation loss: 0.450iteration: 554\n",
      "train_loss: 0.26857889796968615\n",
      "val_loss: 0.45072944780975777\n",
      "Progress: 5.5% ... Training loss: 0.268 ... Validation loss: 0.445iteration: 555\n",
      "train_loss: 0.2683092964547567\n",
      "val_loss: 0.4458788843544798\n",
      "Progress: 5.6% ... Training loss: 0.289 ... Validation loss: 0.471iteration: 556\n",
      "train_loss: 0.28987846564042774\n",
      "val_loss: 0.4711638745598502\n",
      "Progress: 5.6% ... Training loss: 0.268 ... Validation loss: 0.450iteration: 557\n",
      "train_loss: 0.2680815151682741\n",
      "val_loss: 0.4506390846405718\n",
      "Progress: 5.6% ... Training loss: 0.267 ... Validation loss: 0.449iteration: 558\n",
      "train_loss: 0.26775904982350557\n",
      "val_loss: 0.4495865907635173\n",
      "Progress: 5.6% ... Training loss: 0.268 ... Validation loss: 0.446iteration: 559\n",
      "train_loss: 0.2689529724955783\n",
      "val_loss: 0.44699367805907647\n",
      "Progress: 5.6% ... Training loss: 0.281 ... Validation loss: 0.442iteration: 560\n",
      "train_loss: 0.281018605626662\n",
      "val_loss: 0.44262421838501353\n",
      "Progress: 5.6% ... Training loss: 0.269 ... Validation loss: 0.449iteration: 561\n",
      "train_loss: 0.26915861493897675\n",
      "val_loss: 0.44909375306189353\n",
      "Progress: 5.6% ... Training loss: 0.271 ... Validation loss: 0.446iteration: 562\n",
      "train_loss: 0.271695605715597\n",
      "val_loss: 0.44629464411750946\n",
      "Progress: 5.6% ... Training loss: 0.273 ... Validation loss: 0.453iteration: 563\n",
      "train_loss: 0.2735500829401805\n",
      "val_loss: 0.4539860447791475\n",
      "Progress: 5.6% ... Training loss: 0.267 ... Validation loss: 0.453iteration: 564\n",
      "train_loss: 0.2676879582966782\n",
      "val_loss: 0.45373337184152357\n",
      "Progress: 5.7% ... Training loss: 0.266 ... Validation loss: 0.453iteration: 565\n",
      "train_loss: 0.2662682888374542\n",
      "val_loss: 0.4537523464377615\n",
      "Progress: 5.7% ... Training loss: 0.266 ... Validation loss: 0.451iteration: 566\n",
      "train_loss: 0.2660677262093076\n",
      "val_loss: 0.4519502164573382\n",
      "Progress: 5.7% ... Training loss: 0.302 ... Validation loss: 0.504iteration: 567\n",
      "train_loss: 0.3023640502528913\n",
      "val_loss: 0.5044810498514601\n",
      "Progress: 5.7% ... Training loss: 0.294 ... Validation loss: 0.452iteration: 568\n",
      "train_loss: 0.29419070649004647\n",
      "val_loss: 0.45211483437405603\n",
      "Progress: 5.7% ... Training loss: 0.269 ... Validation loss: 0.455iteration: 569\n",
      "train_loss: 0.2691138040422615\n",
      "val_loss: 0.45571539933247945\n",
      "Progress: 5.7% ... Training loss: 0.272 ... Validation loss: 0.461iteration: 570\n",
      "train_loss: 0.2728296592203384\n",
      "val_loss: 0.46162406002751055\n",
      "Progress: 5.7% ... Training loss: 0.287 ... Validation loss: 0.456iteration: 571\n",
      "train_loss: 0.28767051765700724\n",
      "val_loss: 0.456307028071036\n",
      "Progress: 5.7% ... Training loss: 0.268 ... Validation loss: 0.449iteration: 572\n",
      "train_loss: 0.2682018507289593\n",
      "val_loss: 0.44962857995977484\n",
      "Progress: 5.7% ... Training loss: 0.270 ... Validation loss: 0.459iteration: 573\n",
      "train_loss: 0.2700283143463252\n",
      "val_loss: 0.45964975688142495\n",
      "Progress: 5.7% ... Training loss: 0.281 ... Validation loss: 0.464iteration: 574\n",
      "train_loss: 0.28105079901075075\n",
      "val_loss: 0.46476132524578573\n",
      "Progress: 5.8% ... Training loss: 0.268 ... Validation loss: 0.459iteration: 575\n",
      "train_loss: 0.26803466875400866\n",
      "val_loss: 0.4596355958968456\n",
      "Progress: 5.8% ... Training loss: 0.270 ... Validation loss: 0.469iteration: 576\n",
      "train_loss: 0.27092223218213807\n",
      "val_loss: 0.4691199914750805\n",
      "Progress: 5.8% ... Training loss: 0.269 ... Validation loss: 0.463iteration: 577\n",
      "train_loss: 0.2695947448174463\n",
      "val_loss: 0.463728465056703\n",
      "Progress: 5.8% ... Training loss: 0.270 ... Validation loss: 0.463iteration: 578\n",
      "train_loss: 0.2705423961608926\n",
      "val_loss: 0.4631486480565389\n",
      "Progress: 5.8% ... Training loss: 0.266 ... Validation loss: 0.453iteration: 579\n",
      "train_loss: 0.26681318568885315\n",
      "val_loss: 0.4538457294341678\n",
      "Progress: 5.8% ... Training loss: 0.266 ... Validation loss: 0.458iteration: 580\n",
      "train_loss: 0.2667603942083032\n",
      "val_loss: 0.45846597313017434\n",
      "Progress: 5.8% ... Training loss: 0.270 ... Validation loss: 0.459iteration: 581\n",
      "train_loss: 0.2709080770829283\n",
      "val_loss: 0.4594978652554764\n",
      "Progress: 5.8% ... Training loss: 0.271 ... Validation loss: 0.453iteration: 582\n",
      "train_loss: 0.2714078221118809\n",
      "val_loss: 0.4530188963569865\n",
      "Progress: 5.8% ... Training loss: 0.270 ... Validation loss: 0.445iteration: 583\n",
      "train_loss: 0.270927276095525\n",
      "val_loss: 0.44591632907011036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 5.8% ... Training loss: 0.269 ... Validation loss: 0.458iteration: 584\n",
      "train_loss: 0.26955082275146963\n",
      "val_loss: 0.4584828425313066\n",
      "Progress: 5.8% ... Training loss: 0.289 ... Validation loss: 0.463iteration: 585\n",
      "train_loss: 0.28933347027721806\n",
      "val_loss: 0.46323956907197406\n",
      "Progress: 5.9% ... Training loss: 0.271 ... Validation loss: 0.456iteration: 586\n",
      "train_loss: 0.27112911141494844\n",
      "val_loss: 0.45678483180829993\n",
      "Progress: 5.9% ... Training loss: 0.275 ... Validation loss: 0.451iteration: 587\n",
      "train_loss: 0.2753944358574998\n",
      "val_loss: 0.4512828661182141\n",
      "Progress: 5.9% ... Training loss: 0.275 ... Validation loss: 0.461iteration: 588\n",
      "train_loss: 0.27513354399057827\n",
      "val_loss: 0.4611586071037824\n",
      "Progress: 5.9% ... Training loss: 0.279 ... Validation loss: 0.453iteration: 589\n",
      "train_loss: 0.2792063360461917\n",
      "val_loss: 0.45366771846905835\n",
      "Progress: 5.9% ... Training loss: 0.287 ... Validation loss: 0.491iteration: 590\n",
      "train_loss: 0.2876753509494626\n",
      "val_loss: 0.49110275586374963\n",
      "Progress: 5.9% ... Training loss: 0.267 ... Validation loss: 0.442iteration: 591\n",
      "train_loss: 0.2670847191355117\n",
      "val_loss: 0.4422773816262915\n",
      "Progress: 5.9% ... Training loss: 0.266 ... Validation loss: 0.450iteration: 592\n",
      "train_loss: 0.2664368116870046\n",
      "val_loss: 0.45095078726779536\n",
      "Progress: 5.9% ... Training loss: 0.283 ... Validation loss: 0.476iteration: 593\n",
      "train_loss: 0.28372221588729163\n",
      "val_loss: 0.4761177140211179\n",
      "Progress: 5.9% ... Training loss: 0.269 ... Validation loss: 0.448iteration: 594\n",
      "train_loss: 0.26916643991024936\n",
      "val_loss: 0.44857619201316745\n",
      "Progress: 6.0% ... Training loss: 0.278 ... Validation loss: 0.459iteration: 595\n",
      "train_loss: 0.27864500783202356\n",
      "val_loss: 0.45974491522790173\n",
      "Progress: 6.0% ... Training loss: 0.270 ... Validation loss: 0.458iteration: 596\n",
      "train_loss: 0.27047939200522775\n",
      "val_loss: 0.45845145736323645\n",
      "Progress: 6.0% ... Training loss: 0.276 ... Validation loss: 0.458iteration: 597\n",
      "train_loss: 0.2767058497696902\n",
      "val_loss: 0.45863563489236536\n",
      "Progress: 6.0% ... Training loss: 0.269 ... Validation loss: 0.453iteration: 598\n",
      "train_loss: 0.2694978734342685\n",
      "val_loss: 0.4533159758306778\n",
      "Progress: 6.0% ... Training loss: 0.269 ... Validation loss: 0.455iteration: 599\n",
      "train_loss: 0.26950215071713834\n",
      "val_loss: 0.45595740489403663\n",
      "Progress: 6.0% ... Training loss: 0.267 ... Validation loss: 0.453iteration: 600\n",
      "train_loss: 0.2671958875043594\n",
      "val_loss: 0.45342722037240085\n",
      "Progress: 6.0% ... Training loss: 0.267 ... Validation loss: 0.459iteration: 601\n",
      "train_loss: 0.26799943997526404\n",
      "val_loss: 0.4592789248550558\n",
      "Progress: 6.0% ... Training loss: 0.293 ... Validation loss: 0.449iteration: 602\n",
      "train_loss: 0.2931944763930305\n",
      "val_loss: 0.4493918575708152\n",
      "Progress: 6.0% ... Training loss: 0.265 ... Validation loss: 0.443iteration: 603\n",
      "train_loss: 0.2651832806605304\n",
      "val_loss: 0.44314917953478683\n",
      "Progress: 6.0% ... Training loss: 0.270 ... Validation loss: 0.455iteration: 604\n",
      "train_loss: 0.2707639550800796\n",
      "val_loss: 0.45593179596569433\n",
      "Progress: 6.0% ... Training loss: 0.276 ... Validation loss: 0.475iteration: 605\n",
      "train_loss: 0.2766305904391424\n",
      "val_loss: 0.47552816128207115\n",
      "Progress: 6.1% ... Training loss: 0.269 ... Validation loss: 0.465iteration: 606\n",
      "train_loss: 0.2696837388857133\n",
      "val_loss: 0.4653779801636211\n",
      "Progress: 6.1% ... Training loss: 0.265 ... Validation loss: 0.454iteration: 607\n",
      "train_loss: 0.2658558941489433\n",
      "val_loss: 0.45487184436528355\n",
      "Progress: 6.1% ... Training loss: 0.278 ... Validation loss: 0.465iteration: 608\n",
      "train_loss: 0.278249639359124\n",
      "val_loss: 0.46586488216139543\n",
      "Progress: 6.1% ... Training loss: 0.268 ... Validation loss: 0.466iteration: 609\n",
      "train_loss: 0.2686416255759759\n",
      "val_loss: 0.46668911554591286\n",
      "Progress: 6.1% ... Training loss: 0.265 ... Validation loss: 0.456iteration: 610\n",
      "train_loss: 0.2656680786006088\n",
      "val_loss: 0.4560417691661386\n",
      "Progress: 6.1% ... Training loss: 0.273 ... Validation loss: 0.466iteration: 611\n",
      "train_loss: 0.27328969816223064\n",
      "val_loss: 0.46640204531733803\n",
      "Progress: 6.1% ... Training loss: 0.270 ... Validation loss: 0.454iteration: 612\n",
      "train_loss: 0.2706692597909208\n",
      "val_loss: 0.45488280235264644\n",
      "Progress: 6.1% ... Training loss: 0.264 ... Validation loss: 0.445iteration: 613\n",
      "train_loss: 0.2649554661218885\n",
      "val_loss: 0.4457308653057104\n",
      "Progress: 6.1% ... Training loss: 0.274 ... Validation loss: 0.460iteration: 614\n",
      "train_loss: 0.2746063402577053\n",
      "val_loss: 0.4606177092415372\n",
      "Progress: 6.2% ... Training loss: 0.277 ... Validation loss: 0.452iteration: 615\n",
      "train_loss: 0.27715123337961056\n",
      "val_loss: 0.4525388307422851\n",
      "Progress: 6.2% ... Training loss: 0.279 ... Validation loss: 0.467iteration: 616\n",
      "train_loss: 0.27967641562158646\n",
      "val_loss: 0.46712415835987525\n",
      "Progress: 6.2% ... Training loss: 0.263 ... Validation loss: 0.447iteration: 617\n",
      "train_loss: 0.2637199155115515\n",
      "val_loss: 0.44740248263183685\n",
      "Progress: 6.2% ... Training loss: 0.265 ... Validation loss: 0.440iteration: 618\n",
      "train_loss: 0.2657021996364431\n",
      "val_loss: 0.4409679070469639\n",
      "Progress: 6.2% ... Training loss: 0.265 ... Validation loss: 0.447iteration: 619\n",
      "train_loss: 0.2656772269293004\n",
      "val_loss: 0.44776512491085557\n",
      "Progress: 6.2% ... Training loss: 0.264 ... Validation loss: 0.442iteration: 620\n",
      "train_loss: 0.2643867756827163\n",
      "val_loss: 0.442003817687822\n",
      "Progress: 6.2% ... Training loss: 0.290 ... Validation loss: 0.463iteration: 621\n",
      "train_loss: 0.2900507739648796\n",
      "val_loss: 0.4636374099198916\n",
      "Progress: 6.2% ... Training loss: 0.272 ... Validation loss: 0.452iteration: 622\n",
      "train_loss: 0.2726774959026597\n",
      "val_loss: 0.4524157591183371\n",
      "Progress: 6.2% ... Training loss: 0.276 ... Validation loss: 0.469iteration: 623\n",
      "train_loss: 0.2763505601246632\n",
      "val_loss: 0.4695160108950856\n",
      "Progress: 6.2% ... Training loss: 0.263 ... Validation loss: 0.445iteration: 624\n",
      "train_loss: 0.26388452773050036\n",
      "val_loss: 0.44559007710917375\n",
      "Progress: 6.2% ... Training loss: 0.267 ... Validation loss: 0.449iteration: 625\n",
      "train_loss: 0.26733512120119574\n",
      "val_loss: 0.4499232961921462\n",
      "Progress: 6.3% ... Training loss: 0.264 ... Validation loss: 0.449iteration: 626\n",
      "train_loss: 0.2647457508961338\n",
      "val_loss: 0.4498564654929868\n",
      "Progress: 6.3% ... Training loss: 0.265 ... Validation loss: 0.455iteration: 627\n",
      "train_loss: 0.26525398433166736\n",
      "val_loss: 0.4556127651312155\n",
      "Progress: 6.3% ... Training loss: 0.263 ... Validation loss: 0.448iteration: 628\n",
      "train_loss: 0.2639327069479651\n",
      "val_loss: 0.44874382942841845\n",
      "Progress: 6.3% ... Training loss: 0.270 ... Validation loss: 0.459iteration: 629\n",
      "train_loss: 0.27061740823174596\n",
      "val_loss: 0.45952352448511835\n",
      "Progress: 6.3% ... Training loss: 0.278 ... Validation loss: 0.461iteration: 630\n",
      "train_loss: 0.2780394561726918\n",
      "val_loss: 0.4617064136092797\n",
      "Progress: 6.3% ... Training loss: 0.274 ... Validation loss: 0.459iteration: 631\n",
      "train_loss: 0.2742475746198517\n",
      "val_loss: 0.45931170831155854\n",
      "Progress: 6.3% ... Training loss: 0.268 ... Validation loss: 0.450iteration: 632\n",
      "train_loss: 0.2684412894320839\n",
      "val_loss: 0.45000819923402025\n",
      "Progress: 6.3% ... Training loss: 0.271 ... Validation loss: 0.439iteration: 633\n",
      "train_loss: 0.2710864875792855\n",
      "val_loss: 0.4398067341774344\n",
      "Progress: 6.3% ... Training loss: 0.286 ... Validation loss: 0.474iteration: 634\n",
      "train_loss: 0.2864996097109852\n",
      "val_loss: 0.4744457483781885\n",
      "Progress: 6.3% ... Training loss: 0.288 ... Validation loss: 0.465iteration: 635\n",
      "train_loss: 0.28811312139166567\n",
      "val_loss: 0.46591711210707953\n",
      "Progress: 6.4% ... Training loss: 0.264 ... Validation loss: 0.451iteration: 636\n",
      "train_loss: 0.26495852845470796\n",
      "val_loss: 0.45145762109520243\n",
      "Progress: 6.4% ... Training loss: 0.265 ... Validation loss: 0.447iteration: 637\n",
      "train_loss: 0.2650258183549953\n",
      "val_loss: 0.44784614439513665\n",
      "Progress: 6.4% ... Training loss: 0.266 ... Validation loss: 0.444iteration: 638\n",
      "train_loss: 0.2667870448325158\n",
      "val_loss: 0.44430794552747366\n",
      "Progress: 6.4% ... Training loss: 0.270 ... Validation loss: 0.452iteration: 639\n",
      "train_loss: 0.2705601639938918\n",
      "val_loss: 0.45295847533038874\n",
      "Progress: 6.4% ... Training loss: 0.267 ... Validation loss: 0.439iteration: 640\n",
      "train_loss: 0.2673444449860026\n",
      "val_loss: 0.4391452083688913\n",
      "Progress: 6.4% ... Training loss: 0.264 ... Validation loss: 0.445iteration: 641\n",
      "train_loss: 0.26402013022570575\n",
      "val_loss: 0.4454363955140643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 6.4% ... Training loss: 0.265 ... Validation loss: 0.450iteration: 642\n",
      "train_loss: 0.26583499740266003\n",
      "val_loss: 0.4503913375686788\n",
      "Progress: 6.4% ... Training loss: 0.265 ... Validation loss: 0.454iteration: 643\n",
      "train_loss: 0.26501043695184945\n",
      "val_loss: 0.45402511141452917\n",
      "Progress: 6.4% ... Training loss: 0.281 ... Validation loss: 0.454iteration: 644\n",
      "train_loss: 0.2817258472220909\n",
      "val_loss: 0.4547704829988499\n",
      "Progress: 6.5% ... Training loss: 0.264 ... Validation loss: 0.452iteration: 645\n",
      "train_loss: 0.2642212154123989\n",
      "val_loss: 0.4528870857617581\n",
      "Progress: 6.5% ... Training loss: 0.267 ... Validation loss: 0.449iteration: 646\n",
      "train_loss: 0.2672700938475119\n",
      "val_loss: 0.4494912696929664\n",
      "Progress: 6.5% ... Training loss: 0.264 ... Validation loss: 0.451iteration: 647\n",
      "train_loss: 0.26433116280140845\n",
      "val_loss: 0.4511871598638354\n",
      "Progress: 6.5% ... Training loss: 0.272 ... Validation loss: 0.464iteration: 648\n",
      "train_loss: 0.2729889691141816\n",
      "val_loss: 0.46482316310088084\n",
      "Progress: 6.5% ... Training loss: 0.266 ... Validation loss: 0.460iteration: 649\n",
      "train_loss: 0.2661104261174165\n",
      "val_loss: 0.4602595895109793\n",
      "Progress: 6.5% ... Training loss: 0.266 ... Validation loss: 0.442iteration: 650\n",
      "train_loss: 0.2668300015089748\n",
      "val_loss: 0.4423334779604968\n",
      "Progress: 6.5% ... Training loss: 0.263 ... Validation loss: 0.444iteration: 651\n",
      "train_loss: 0.2636711691915213\n",
      "val_loss: 0.444269706861169\n",
      "Progress: 6.5% ... Training loss: 0.280 ... Validation loss: 0.448iteration: 652\n",
      "train_loss: 0.280184349705934\n",
      "val_loss: 0.44808486785011825\n",
      "Progress: 6.5% ... Training loss: 0.268 ... Validation loss: 0.444iteration: 653\n",
      "train_loss: 0.2680000010379593\n",
      "val_loss: 0.44436601161104783\n",
      "Progress: 6.5% ... Training loss: 0.350 ... Validation loss: 0.518iteration: 654\n",
      "train_loss: 0.3506014618432629\n",
      "val_loss: 0.5181164406112357\n",
      "Progress: 6.5% ... Training loss: 0.329 ... Validation loss: 0.514iteration: 655\n",
      "train_loss: 0.329330178653617\n",
      "val_loss: 0.5149014955484388\n",
      "Progress: 6.6% ... Training loss: 0.313 ... Validation loss: 0.491iteration: 656\n",
      "train_loss: 0.31370856767666794\n",
      "val_loss: 0.4910599993960548\n",
      "Progress: 6.6% ... Training loss: 0.270 ... Validation loss: 0.459iteration: 657\n",
      "train_loss: 0.2704233397556463\n",
      "val_loss: 0.45917660516988223\n",
      "Progress: 6.6% ... Training loss: 0.271 ... Validation loss: 0.454iteration: 658\n",
      "train_loss: 0.2715887923268372\n",
      "val_loss: 0.4547360626332998\n",
      "Progress: 6.6% ... Training loss: 0.261 ... Validation loss: 0.445iteration: 659\n",
      "train_loss: 0.26195377463105507\n",
      "val_loss: 0.44533003638850865\n",
      "Progress: 6.6% ... Training loss: 0.263 ... Validation loss: 0.444iteration: 660\n",
      "train_loss: 0.2630776509936817\n",
      "val_loss: 0.4447995402505943\n",
      "Progress: 6.6% ... Training loss: 0.263 ... Validation loss: 0.455iteration: 661\n",
      "train_loss: 0.26375305356879997\n",
      "val_loss: 0.45561113118868535\n",
      "Progress: 6.6% ... Training loss: 0.263 ... Validation loss: 0.452iteration: 662\n",
      "train_loss: 0.26305589124185247\n",
      "val_loss: 0.452713316414702\n",
      "Progress: 6.6% ... Training loss: 0.278 ... Validation loss: 0.463iteration: 663\n",
      "train_loss: 0.27899081348955856\n",
      "val_loss: 0.4632225580543917\n",
      "Progress: 6.6% ... Training loss: 0.280 ... Validation loss: 0.467iteration: 664\n",
      "train_loss: 0.28087262309179356\n",
      "val_loss: 0.46753184155369953\n",
      "Progress: 6.7% ... Training loss: 0.288 ... Validation loss: 0.493iteration: 665\n",
      "train_loss: 0.2882458962579337\n",
      "val_loss: 0.49387802957413557\n",
      "Progress: 6.7% ... Training loss: 0.266 ... Validation loss: 0.458iteration: 666\n",
      "train_loss: 0.26636670653421063\n",
      "val_loss: 0.45821342069313714\n",
      "Progress: 6.7% ... Training loss: 0.268 ... Validation loss: 0.470iteration: 667\n",
      "train_loss: 0.26899764477346333\n",
      "val_loss: 0.470020957726964\n",
      "Progress: 6.7% ... Training loss: 0.270 ... Validation loss: 0.463iteration: 668\n",
      "train_loss: 0.27088756328561175\n",
      "val_loss: 0.4635939863346885\n",
      "Progress: 6.7% ... Training loss: 0.262 ... Validation loss: 0.450iteration: 669\n",
      "train_loss: 0.26200097996966976\n",
      "val_loss: 0.450138646713767\n",
      "Progress: 6.7% ... Training loss: 0.277 ... Validation loss: 0.473iteration: 670\n",
      "train_loss: 0.27765489498697354\n",
      "val_loss: 0.47341442298868486\n",
      "Progress: 6.7% ... Training loss: 0.261 ... Validation loss: 0.444iteration: 671\n",
      "train_loss: 0.26168572880609564\n",
      "val_loss: 0.4448983990562848\n",
      "Progress: 6.7% ... Training loss: 0.287 ... Validation loss: 0.467iteration: 672\n",
      "train_loss: 0.28725431407052526\n",
      "val_loss: 0.4671171877753368\n",
      "Progress: 6.7% ... Training loss: 0.281 ... Validation loss: 0.470iteration: 673\n",
      "train_loss: 0.281156133107934\n",
      "val_loss: 0.47047420656620365\n",
      "Progress: 6.7% ... Training loss: 0.264 ... Validation loss: 0.448iteration: 674\n",
      "train_loss: 0.2644252884541074\n",
      "val_loss: 0.4480576450330686\n",
      "Progress: 6.8% ... Training loss: 0.265 ... Validation loss: 0.449iteration: 675\n",
      "train_loss: 0.2650729990838515\n",
      "val_loss: 0.4492164567468842\n",
      "Progress: 6.8% ... Training loss: 0.269 ... Validation loss: 0.461iteration: 676\n",
      "train_loss: 0.269904929573375\n",
      "val_loss: 0.46193331742902755\n",
      "Progress: 6.8% ... Training loss: 0.268 ... Validation loss: 0.443iteration: 677\n",
      "train_loss: 0.26822085381314814\n",
      "val_loss: 0.4436407627641022\n",
      "Progress: 6.8% ... Training loss: 0.275 ... Validation loss: 0.467iteration: 678\n",
      "train_loss: 0.2751082959736439\n",
      "val_loss: 0.4675313690481191\n",
      "Progress: 6.8% ... Training loss: 0.264 ... Validation loss: 0.451iteration: 679\n",
      "train_loss: 0.26462425242827076\n",
      "val_loss: 0.4517892293743761\n",
      "Progress: 6.8% ... Training loss: 0.267 ... Validation loss: 0.462iteration: 680\n",
      "train_loss: 0.26788581542820594\n",
      "val_loss: 0.4623769653063645\n",
      "Progress: 6.8% ... Training loss: 0.265 ... Validation loss: 0.458iteration: 681\n",
      "train_loss: 0.2654764622498465\n",
      "val_loss: 0.4589130373105671\n",
      "Progress: 6.8% ... Training loss: 0.270 ... Validation loss: 0.449iteration: 682\n",
      "train_loss: 0.27082865768233966\n",
      "val_loss: 0.44910149930072035\n",
      "Progress: 6.8% ... Training loss: 0.267 ... Validation loss: 0.453iteration: 683\n",
      "train_loss: 0.2672792130516876\n",
      "val_loss: 0.45341091588827764\n",
      "Progress: 6.8% ... Training loss: 0.262 ... Validation loss: 0.444iteration: 684\n",
      "train_loss: 0.2628089727130766\n",
      "val_loss: 0.44401467271341044\n",
      "Progress: 6.8% ... Training loss: 0.276 ... Validation loss: 0.443iteration: 685\n",
      "train_loss: 0.27675759265199334\n",
      "val_loss: 0.4439232924528805\n",
      "Progress: 6.9% ... Training loss: 0.321 ... Validation loss: 0.500iteration: 686\n",
      "train_loss: 0.3215649326822988\n",
      "val_loss: 0.5006265058969886\n",
      "Progress: 6.9% ... Training loss: 0.322 ... Validation loss: 0.485iteration: 687\n",
      "train_loss: 0.32257221593859486\n",
      "val_loss: 0.48525970011947284\n",
      "Progress: 6.9% ... Training loss: 0.276 ... Validation loss: 0.450iteration: 688\n",
      "train_loss: 0.27649392459841265\n",
      "val_loss: 0.4507300596969511\n",
      "Progress: 6.9% ... Training loss: 0.273 ... Validation loss: 0.446iteration: 689\n",
      "train_loss: 0.27371934234051154\n",
      "val_loss: 0.4464773134796585\n",
      "Progress: 6.9% ... Training loss: 0.262 ... Validation loss: 0.445iteration: 690\n",
      "train_loss: 0.2624766891039304\n",
      "val_loss: 0.445437695126697\n",
      "Progress: 6.9% ... Training loss: 0.270 ... Validation loss: 0.459iteration: 691\n",
      "train_loss: 0.2702693513831568\n",
      "val_loss: 0.45975381579996016\n",
      "Progress: 6.9% ... Training loss: 0.276 ... Validation loss: 0.451iteration: 692\n",
      "train_loss: 0.27646141001477526\n",
      "val_loss: 0.451105421225649\n",
      "Progress: 6.9% ... Training loss: 0.279 ... Validation loss: 0.463iteration: 693\n",
      "train_loss: 0.27998154194194275\n",
      "val_loss: 0.4630934934154765\n",
      "Progress: 6.9% ... Training loss: 0.270 ... Validation loss: 0.447iteration: 694\n",
      "train_loss: 0.2708306784385524\n",
      "val_loss: 0.4474060968750316\n",
      "Progress: 7.0% ... Training loss: 0.268 ... Validation loss: 0.445iteration: 695\n",
      "train_loss: 0.2683109828005683\n",
      "val_loss: 0.44565502106515015\n",
      "Progress: 7.0% ... Training loss: 0.265 ... Validation loss: 0.449iteration: 696\n",
      "train_loss: 0.2656355810355579\n",
      "val_loss: 0.4499139513503983\n",
      "Progress: 7.0% ... Training loss: 0.264 ... Validation loss: 0.452iteration: 697\n",
      "train_loss: 0.2641396446470488\n",
      "val_loss: 0.45225016773270915\n",
      "Progress: 7.0% ... Training loss: 0.265 ... Validation loss: 0.439iteration: 698\n",
      "train_loss: 0.26535771836434313\n",
      "val_loss: 0.43907429984035096\n",
      "Progress: 7.0% ... Training loss: 0.293 ... Validation loss: 0.472iteration: 699\n",
      "train_loss: 0.2930149473059895\n",
      "val_loss: 0.4725657890307207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 7.0% ... Training loss: 0.267 ... Validation loss: 0.460iteration: 700\n",
      "train_loss: 0.26712133028643725\n",
      "val_loss: 0.4606949233846415\n",
      "Progress: 7.0% ... Training loss: 0.269 ... Validation loss: 0.463iteration: 701\n",
      "train_loss: 0.269538290439212\n",
      "val_loss: 0.4635521975815514\n",
      "Progress: 7.0% ... Training loss: 0.276 ... Validation loss: 0.469iteration: 702\n",
      "train_loss: 0.27676828332587916\n",
      "val_loss: 0.46985864012578055\n",
      "Progress: 7.0% ... Training loss: 0.264 ... Validation loss: 0.454iteration: 703\n",
      "train_loss: 0.2641404426250103\n",
      "val_loss: 0.4542281159299583\n",
      "Progress: 7.0% ... Training loss: 0.275 ... Validation loss: 0.466iteration: 704\n",
      "train_loss: 0.2757998911125097\n",
      "val_loss: 0.46627274096395227\n",
      "Progress: 7.0% ... Training loss: 0.301 ... Validation loss: 0.477iteration: 705\n",
      "train_loss: 0.3019249239155794\n",
      "val_loss: 0.4773078310151434\n",
      "Progress: 7.1% ... Training loss: 0.268 ... Validation loss: 0.455iteration: 706\n",
      "train_loss: 0.26850339594453604\n",
      "val_loss: 0.4550762228800243\n",
      "Progress: 7.1% ... Training loss: 0.264 ... Validation loss: 0.447iteration: 707\n",
      "train_loss: 0.26479096811815217\n",
      "val_loss: 0.4471658525118102\n",
      "Progress: 7.1% ... Training loss: 0.263 ... Validation loss: 0.443iteration: 708\n",
      "train_loss: 0.2630237195461367\n",
      "val_loss: 0.44324715585077584\n",
      "Progress: 7.1% ... Training loss: 0.270 ... Validation loss: 0.446iteration: 709\n",
      "train_loss: 0.2704539350261935\n",
      "val_loss: 0.44602817429692815\n",
      "Progress: 7.1% ... Training loss: 0.271 ... Validation loss: 0.449iteration: 710\n",
      "train_loss: 0.2712407580957596\n",
      "val_loss: 0.4492901451624794\n",
      "Progress: 7.1% ... Training loss: 0.272 ... Validation loss: 0.445iteration: 711\n",
      "train_loss: 0.2722752248872743\n",
      "val_loss: 0.44547607331168193\n",
      "Progress: 7.1% ... Training loss: 0.274 ... Validation loss: 0.450iteration: 712\n",
      "train_loss: 0.27479506975415396\n",
      "val_loss: 0.4503348385319931\n",
      "Progress: 7.1% ... Training loss: 0.262 ... Validation loss: 0.449iteration: 713\n",
      "train_loss: 0.2628874771533106\n",
      "val_loss: 0.4499981265292188\n",
      "Progress: 7.1% ... Training loss: 0.263 ... Validation loss: 0.438iteration: 714\n",
      "train_loss: 0.2632716545398476\n",
      "val_loss: 0.43846714132995446\n",
      "Progress: 7.2% ... Training loss: 0.263 ... Validation loss: 0.439iteration: 715\n",
      "train_loss: 0.26362568673110376\n",
      "val_loss: 0.43925150179669226\n",
      "Progress: 7.2% ... Training loss: 0.262 ... Validation loss: 0.441iteration: 716\n",
      "train_loss: 0.2624368378303505\n",
      "val_loss: 0.44131533568751785\n",
      "Progress: 7.2% ... Training loss: 0.262 ... Validation loss: 0.435iteration: 717\n",
      "train_loss: 0.2623891725220038\n",
      "val_loss: 0.4358370217442851\n",
      "Progress: 7.2% ... Training loss: 0.267 ... Validation loss: 0.433iteration: 718\n",
      "train_loss: 0.26735814380532097\n",
      "val_loss: 0.43318414062498506\n",
      "Progress: 7.2% ... Training loss: 0.261 ... Validation loss: 0.438iteration: 719\n",
      "train_loss: 0.26160057713426094\n",
      "val_loss: 0.4383109536147191\n",
      "Progress: 7.2% ... Training loss: 0.263 ... Validation loss: 0.448iteration: 720\n",
      "train_loss: 0.26348098429176087\n",
      "val_loss: 0.4487859752847812\n",
      "Progress: 7.2% ... Training loss: 0.267 ... Validation loss: 0.458iteration: 721\n",
      "train_loss: 0.2678319463183546\n",
      "val_loss: 0.4584052290364868\n",
      "Progress: 7.2% ... Training loss: 0.262 ... Validation loss: 0.449iteration: 722\n",
      "train_loss: 0.26299786429058974\n",
      "val_loss: 0.44998753175693385\n",
      "Progress: 7.2% ... Training loss: 0.267 ... Validation loss: 0.457iteration: 723\n",
      "train_loss: 0.26769749010107957\n",
      "val_loss: 0.4570148410662791\n",
      "Progress: 7.2% ... Training loss: 0.267 ... Validation loss: 0.439iteration: 724\n",
      "train_loss: 0.26728325397984504\n",
      "val_loss: 0.4394179930165106\n",
      "Progress: 7.2% ... Training loss: 0.265 ... Validation loss: 0.440iteration: 725\n",
      "train_loss: 0.26567387285449184\n",
      "val_loss: 0.4408387614949549\n",
      "Progress: 7.3% ... Training loss: 0.264 ... Validation loss: 0.446iteration: 726\n",
      "train_loss: 0.26429978303638524\n",
      "val_loss: 0.44622018322424395\n",
      "Progress: 7.3% ... Training loss: 0.271 ... Validation loss: 0.449iteration: 727\n",
      "train_loss: 0.27108334525908\n",
      "val_loss: 0.4496327401320146\n",
      "Progress: 7.3% ... Training loss: 0.292 ... Validation loss: 0.476iteration: 728\n",
      "train_loss: 0.29214707750602104\n",
      "val_loss: 0.47687660845802726\n",
      "Progress: 7.3% ... Training loss: 0.265 ... Validation loss: 0.431iteration: 729\n",
      "train_loss: 0.26511507915444654\n",
      "val_loss: 0.4318803071393364\n",
      "Progress: 7.3% ... Training loss: 0.262 ... Validation loss: 0.441iteration: 730\n",
      "train_loss: 0.26294112085652\n",
      "val_loss: 0.4410026487086512\n",
      "Progress: 7.3% ... Training loss: 0.262 ... Validation loss: 0.441iteration: 731\n",
      "train_loss: 0.2626650314442139\n",
      "val_loss: 0.44127699036984497\n",
      "Progress: 7.3% ... Training loss: 0.264 ... Validation loss: 0.434iteration: 732\n",
      "train_loss: 0.2647685148329876\n",
      "val_loss: 0.4347860776624722\n",
      "Progress: 7.3% ... Training loss: 0.269 ... Validation loss: 0.442iteration: 733\n",
      "train_loss: 0.2691823124395158\n",
      "val_loss: 0.44282137262117466\n",
      "Progress: 7.3% ... Training loss: 0.261 ... Validation loss: 0.453iteration: 734\n",
      "train_loss: 0.26197339240982387\n",
      "val_loss: 0.45396243905355915\n",
      "Progress: 7.3% ... Training loss: 0.264 ... Validation loss: 0.450iteration: 735\n",
      "train_loss: 0.264201464147473\n",
      "val_loss: 0.4509119297418263\n",
      "Progress: 7.4% ... Training loss: 0.272 ... Validation loss: 0.464iteration: 736\n",
      "train_loss: 0.272730564117685\n",
      "val_loss: 0.464539970340557\n",
      "Progress: 7.4% ... Training loss: 0.263 ... Validation loss: 0.442iteration: 737\n",
      "train_loss: 0.26363620127109694\n",
      "val_loss: 0.4425420346237626\n",
      "Progress: 7.4% ... Training loss: 0.275 ... Validation loss: 0.469iteration: 738\n",
      "train_loss: 0.27596560496693856\n",
      "val_loss: 0.4698465062959278\n",
      "Progress: 7.4% ... Training loss: 0.261 ... Validation loss: 0.443iteration: 739\n",
      "train_loss: 0.2613717368475523\n",
      "val_loss: 0.44362123688753996\n",
      "Progress: 7.4% ... Training loss: 0.260 ... Validation loss: 0.437iteration: 740\n",
      "train_loss: 0.26009609760631064\n",
      "val_loss: 0.4375182092087344\n",
      "Progress: 7.4% ... Training loss: 0.267 ... Validation loss: 0.450iteration: 741\n",
      "train_loss: 0.2677708990033998\n",
      "val_loss: 0.45005001966708214\n",
      "Progress: 7.4% ... Training loss: 0.260 ... Validation loss: 0.434iteration: 742\n",
      "train_loss: 0.26053873317110066\n",
      "val_loss: 0.43478434499891044\n",
      "Progress: 7.4% ... Training loss: 0.263 ... Validation loss: 0.447iteration: 743\n",
      "train_loss: 0.2637873339789654\n",
      "val_loss: 0.4479184743608761\n",
      "Progress: 7.4% ... Training loss: 0.261 ... Validation loss: 0.449iteration: 744\n",
      "train_loss: 0.2618480333230928\n",
      "val_loss: 0.44981994333815434\n",
      "Progress: 7.5% ... Training loss: 0.271 ... Validation loss: 0.463iteration: 745\n",
      "train_loss: 0.27119516572795144\n",
      "val_loss: 0.4636908459357811\n",
      "Progress: 7.5% ... Training loss: 0.260 ... Validation loss: 0.440iteration: 746\n",
      "train_loss: 0.2604248777244293\n",
      "val_loss: 0.4409908181344758\n",
      "Progress: 7.5% ... Training loss: 0.261 ... Validation loss: 0.441iteration: 747\n",
      "train_loss: 0.26127824221971313\n",
      "val_loss: 0.44169234907365507\n",
      "Progress: 7.5% ... Training loss: 0.278 ... Validation loss: 0.452iteration: 748\n",
      "train_loss: 0.27867380590889557\n",
      "val_loss: 0.4520932251131236\n",
      "Progress: 7.5% ... Training loss: 0.265 ... Validation loss: 0.452iteration: 749\n",
      "train_loss: 0.2652183779876019\n",
      "val_loss: 0.45200811106766\n",
      "Progress: 7.5% ... Training loss: 0.263 ... Validation loss: 0.449iteration: 750\n",
      "train_loss: 0.26321641201506696\n",
      "val_loss: 0.44910631759919745\n",
      "Progress: 7.5% ... Training loss: 0.261 ... Validation loss: 0.443iteration: 751\n",
      "train_loss: 0.2612604372818357\n",
      "val_loss: 0.44394428724219054\n",
      "Progress: 7.5% ... Training loss: 0.264 ... Validation loss: 0.444iteration: 752\n",
      "train_loss: 0.2640554482991931\n",
      "val_loss: 0.4441294216349413\n",
      "Progress: 7.5% ... Training loss: 0.264 ... Validation loss: 0.454iteration: 753\n",
      "train_loss: 0.2645168737324443\n",
      "val_loss: 0.45460593377603814\n",
      "Progress: 7.5% ... Training loss: 0.276 ... Validation loss: 0.464iteration: 754\n",
      "train_loss: 0.2768865164304672\n",
      "val_loss: 0.4649775246969489\n",
      "Progress: 7.5% ... Training loss: 0.262 ... Validation loss: 0.449iteration: 755\n",
      "train_loss: 0.2627882706386976\n",
      "val_loss: 0.44910143656855744\n",
      "Progress: 7.6% ... Training loss: 0.262 ... Validation loss: 0.452iteration: 756\n",
      "train_loss: 0.2629786077643607\n",
      "val_loss: 0.4526358869441885\n",
      "Progress: 7.6% ... Training loss: 0.262 ... Validation loss: 0.444iteration: 757\n",
      "train_loss: 0.2623536800375263\n",
      "val_loss: 0.44430550783576933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 7.6% ... Training loss: 0.259 ... Validation loss: 0.441iteration: 758\n",
      "train_loss: 0.25951967986484886\n",
      "val_loss: 0.44130232328232194\n",
      "Progress: 7.6% ... Training loss: 0.263 ... Validation loss: 0.444iteration: 759\n",
      "train_loss: 0.2632722946647635\n",
      "val_loss: 0.444961278212166\n",
      "Progress: 7.6% ... Training loss: 0.276 ... Validation loss: 0.455iteration: 760\n",
      "train_loss: 0.27607907306102497\n",
      "val_loss: 0.45581567070404155\n",
      "Progress: 7.6% ... Training loss: 0.262 ... Validation loss: 0.443iteration: 761\n",
      "train_loss: 0.26279670425016805\n",
      "val_loss: 0.4438608906988435\n",
      "Progress: 7.6% ... Training loss: 0.266 ... Validation loss: 0.457iteration: 762\n",
      "train_loss: 0.2660309820372125\n",
      "val_loss: 0.4575332330795313\n",
      "Progress: 7.6% ... Training loss: 0.268 ... Validation loss: 0.454iteration: 763\n",
      "train_loss: 0.2686951911810942\n",
      "val_loss: 0.45451650512291764\n",
      "Progress: 7.6% ... Training loss: 0.259 ... Validation loss: 0.443iteration: 764\n",
      "train_loss: 0.25928600294629\n",
      "val_loss: 0.44306660748268506\n",
      "Progress: 7.7% ... Training loss: 0.271 ... Validation loss: 0.455iteration: 765\n",
      "train_loss: 0.2716026993249862\n",
      "val_loss: 0.45502682271742745\n",
      "Progress: 7.7% ... Training loss: 0.269 ... Validation loss: 0.456iteration: 766\n",
      "train_loss: 0.26918964067888596\n",
      "val_loss: 0.45686993974448936\n",
      "Progress: 7.7% ... Training loss: 0.270 ... Validation loss: 0.460iteration: 767\n",
      "train_loss: 0.270615212672773\n",
      "val_loss: 0.4603810325771776\n",
      "Progress: 7.7% ... Training loss: 0.271 ... Validation loss: 0.464iteration: 768\n",
      "train_loss: 0.2718177708558052\n",
      "val_loss: 0.4645214941828125\n",
      "Progress: 7.7% ... Training loss: 0.263 ... Validation loss: 0.455iteration: 769\n",
      "train_loss: 0.2637902732444434\n",
      "val_loss: 0.45528346846103807\n",
      "Progress: 7.7% ... Training loss: 0.259 ... Validation loss: 0.438iteration: 770\n",
      "train_loss: 0.259061913969478\n",
      "val_loss: 0.43882394140695286\n",
      "Progress: 7.7% ... Training loss: 0.270 ... Validation loss: 0.447iteration: 771\n",
      "train_loss: 0.27016360786923177\n",
      "val_loss: 0.4470532569470484\n",
      "Progress: 7.7% ... Training loss: 0.263 ... Validation loss: 0.444iteration: 772\n",
      "train_loss: 0.2639391299424411\n",
      "val_loss: 0.44414542415034125\n",
      "Progress: 7.7% ... Training loss: 0.270 ... Validation loss: 0.445iteration: 773\n",
      "train_loss: 0.2702080032815591\n",
      "val_loss: 0.44508965696509173\n",
      "Progress: 7.7% ... Training loss: 0.266 ... Validation loss: 0.440iteration: 774\n",
      "train_loss: 0.266303187260756\n",
      "val_loss: 0.4405004760455793\n",
      "Progress: 7.8% ... Training loss: 0.263 ... Validation loss: 0.442iteration: 775\n",
      "train_loss: 0.26315524148248487\n",
      "val_loss: 0.44239873347126674\n",
      "Progress: 7.8% ... Training loss: 0.275 ... Validation loss: 0.444iteration: 776\n",
      "train_loss: 0.275841712005981\n",
      "val_loss: 0.44429932540735484\n",
      "Progress: 7.8% ... Training loss: 0.263 ... Validation loss: 0.438iteration: 777\n",
      "train_loss: 0.2631633597335981\n",
      "val_loss: 0.43856452422005004\n",
      "Progress: 7.8% ... Training loss: 0.321 ... Validation loss: 0.479iteration: 778\n",
      "train_loss: 0.32154881505625627\n",
      "val_loss: 0.47920828865374604\n",
      "Progress: 7.8% ... Training loss: 0.329 ... Validation loss: 0.510iteration: 779\n",
      "train_loss: 0.329056737938421\n",
      "val_loss: 0.5101051839043598\n",
      "Progress: 7.8% ... Training loss: 0.281 ... Validation loss: 0.450iteration: 780\n",
      "train_loss: 0.2815041889357851\n",
      "val_loss: 0.45085032898560196\n",
      "Progress: 7.8% ... Training loss: 0.327 ... Validation loss: 0.510iteration: 781\n",
      "train_loss: 0.32733324725223667\n",
      "val_loss: 0.5109671809788098\n",
      "Progress: 7.8% ... Training loss: 0.335 ... Validation loss: 0.485iteration: 782\n",
      "train_loss: 0.3357385845771924\n",
      "val_loss: 0.4853690416070878\n",
      "Progress: 7.8% ... Training loss: 0.268 ... Validation loss: 0.451iteration: 783\n",
      "train_loss: 0.2682851617269658\n",
      "val_loss: 0.45107670308543485\n",
      "Progress: 7.8% ... Training loss: 0.272 ... Validation loss: 0.444iteration: 784\n",
      "train_loss: 0.27249039076421805\n",
      "val_loss: 0.4442438062780119\n",
      "Progress: 7.8% ... Training loss: 0.262 ... Validation loss: 0.441iteration: 785\n",
      "train_loss: 0.2627353132780096\n",
      "val_loss: 0.44182010008363176\n",
      "Progress: 7.9% ... Training loss: 0.258 ... Validation loss: 0.438iteration: 786\n",
      "train_loss: 0.25861501150173316\n",
      "val_loss: 0.43874938648733997\n",
      "Progress: 7.9% ... Training loss: 0.258 ... Validation loss: 0.440iteration: 787\n",
      "train_loss: 0.2586496546850395\n",
      "val_loss: 0.4404776730068131\n",
      "Progress: 7.9% ... Training loss: 0.265 ... Validation loss: 0.440iteration: 788\n",
      "train_loss: 0.2650115951682531\n",
      "val_loss: 0.44084475085737956\n",
      "Progress: 7.9% ... Training loss: 0.261 ... Validation loss: 0.442iteration: 789\n",
      "train_loss: 0.2611119514152364\n",
      "val_loss: 0.4428762785365097\n",
      "Progress: 7.9% ... Training loss: 0.268 ... Validation loss: 0.447iteration: 790\n",
      "train_loss: 0.26889801709158234\n",
      "val_loss: 0.44784224744741263\n",
      "Progress: 7.9% ... Training loss: 0.265 ... Validation loss: 0.449iteration: 791\n",
      "train_loss: 0.26548234653574193\n",
      "val_loss: 0.4492068962087972\n",
      "Progress: 7.9% ... Training loss: 0.262 ... Validation loss: 0.446iteration: 792\n",
      "train_loss: 0.2626210855367357\n",
      "val_loss: 0.4464725349561555\n",
      "Progress: 7.9% ... Training loss: 0.259 ... Validation loss: 0.443iteration: 793\n",
      "train_loss: 0.25969488236266786\n",
      "val_loss: 0.443370885427899\n",
      "Progress: 7.9% ... Training loss: 0.258 ... Validation loss: 0.439iteration: 794\n",
      "train_loss: 0.25895491208699256\n",
      "val_loss: 0.4398993241308332\n",
      "Progress: 8.0% ... Training loss: 0.270 ... Validation loss: 0.442iteration: 795\n",
      "train_loss: 0.2700900799968386\n",
      "val_loss: 0.44256133532776354\n",
      "Progress: 8.0% ... Training loss: 0.266 ... Validation loss: 0.446iteration: 796\n",
      "train_loss: 0.2664337705129421\n",
      "val_loss: 0.4465632617293732\n",
      "Progress: 8.0% ... Training loss: 0.271 ... Validation loss: 0.441iteration: 797\n",
      "train_loss: 0.27129315613435845\n",
      "val_loss: 0.44153632938557685\n",
      "Progress: 8.0% ... Training loss: 0.271 ... Validation loss: 0.449iteration: 798\n",
      "train_loss: 0.2717831630321907\n",
      "val_loss: 0.44931556138024725\n",
      "Progress: 8.0% ... Training loss: 0.275 ... Validation loss: 0.461iteration: 799\n",
      "train_loss: 0.2755269075775183\n",
      "val_loss: 0.4610488779518928\n",
      "Progress: 8.0% ... Training loss: 0.264 ... Validation loss: 0.449iteration: 800\n",
      "train_loss: 0.26457392128165974\n",
      "val_loss: 0.4496305230355624\n",
      "Progress: 8.0% ... Training loss: 0.262 ... Validation loss: 0.445iteration: 801\n",
      "train_loss: 0.2621607576524463\n",
      "val_loss: 0.44535853353349936\n",
      "Progress: 8.0% ... Training loss: 0.262 ... Validation loss: 0.447iteration: 802\n",
      "train_loss: 0.2621087291622662\n",
      "val_loss: 0.4470577496155411\n",
      "Progress: 8.0% ... Training loss: 0.266 ... Validation loss: 0.444iteration: 803\n",
      "train_loss: 0.2665214135536142\n",
      "val_loss: 0.44418979723969554\n",
      "Progress: 8.0% ... Training loss: 0.260 ... Validation loss: 0.432iteration: 804\n",
      "train_loss: 0.26078929206135426\n",
      "val_loss: 0.4329633591258182\n",
      "Progress: 8.1% ... Training loss: 0.265 ... Validation loss: 0.430iteration: 805\n",
      "train_loss: 0.26507204104097865\n",
      "val_loss: 0.430860822662483\n",
      "Progress: 8.1% ... Training loss: 0.258 ... Validation loss: 0.433iteration: 806\n",
      "train_loss: 0.2589429599283794\n",
      "val_loss: 0.43342041879883547\n",
      "Progress: 8.1% ... Training loss: 0.259 ... Validation loss: 0.437iteration: 807\n",
      "train_loss: 0.2592257957804454\n",
      "val_loss: 0.43766508257939585\n",
      "Progress: 8.1% ... Training loss: 0.259 ... Validation loss: 0.435iteration: 808\n",
      "train_loss: 0.2591770246647219\n",
      "val_loss: 0.43556333616597886\n",
      "Progress: 8.1% ... Training loss: 0.258 ... Validation loss: 0.436iteration: 809\n",
      "train_loss: 0.2580073716826146\n",
      "val_loss: 0.4369739834571113\n",
      "Progress: 8.1% ... Training loss: 0.260 ... Validation loss: 0.448iteration: 810\n",
      "train_loss: 0.2608237975431307\n",
      "val_loss: 0.4480981344244341\n",
      "Progress: 8.1% ... Training loss: 0.261 ... Validation loss: 0.441iteration: 811\n",
      "train_loss: 0.2615686896835219\n",
      "val_loss: 0.44142239642049663\n",
      "Progress: 8.1% ... Training loss: 0.273 ... Validation loss: 0.451iteration: 812\n",
      "train_loss: 0.27397324475306084\n",
      "val_loss: 0.45144119048449166\n",
      "Progress: 8.1% ... Training loss: 0.272 ... Validation loss: 0.453iteration: 813\n",
      "train_loss: 0.272762865827686\n",
      "val_loss: 0.4536059780520047\n",
      "Progress: 8.1% ... Training loss: 0.259 ... Validation loss: 0.438iteration: 814\n",
      "train_loss: 0.2590145804592417\n",
      "val_loss: 0.43843060739000983\n",
      "Progress: 8.2% ... Training loss: 0.262 ... Validation loss: 0.439iteration: 815\n",
      "train_loss: 0.2628438175512406\n",
      "val_loss: 0.43988140750394694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 8.2% ... Training loss: 0.260 ... Validation loss: 0.431iteration: 816\n",
      "train_loss: 0.2600876708073371\n",
      "val_loss: 0.43127918735647497\n",
      "Progress: 8.2% ... Training loss: 0.265 ... Validation loss: 0.443iteration: 817\n",
      "train_loss: 0.2654268941150273\n",
      "val_loss: 0.443895404650548\n",
      "Progress: 8.2% ... Training loss: 0.276 ... Validation loss: 0.444iteration: 818\n",
      "train_loss: 0.27695825695921256\n",
      "val_loss: 0.44456538824513536\n",
      "Progress: 8.2% ... Training loss: 0.257 ... Validation loss: 0.433iteration: 819\n",
      "train_loss: 0.2575261008949498\n",
      "val_loss: 0.4339630582402745\n",
      "Progress: 8.2% ... Training loss: 0.262 ... Validation loss: 0.455iteration: 820\n",
      "train_loss: 0.26221298713733643\n",
      "val_loss: 0.45540818234662395\n",
      "Progress: 8.2% ... Training loss: 0.261 ... Validation loss: 0.445iteration: 821\n",
      "train_loss: 0.2613918703776736\n",
      "val_loss: 0.4456568936088503\n",
      "Progress: 8.2% ... Training loss: 0.259 ... Validation loss: 0.437iteration: 822\n",
      "train_loss: 0.2594257720957763\n",
      "val_loss: 0.43768770584661226\n",
      "Progress: 8.2% ... Training loss: 0.267 ... Validation loss: 0.438iteration: 823\n",
      "train_loss: 0.2671097266443396\n",
      "val_loss: 0.4381383738329297\n",
      "Progress: 8.2% ... Training loss: 0.261 ... Validation loss: 0.427iteration: 824\n",
      "train_loss: 0.26150736959954884\n",
      "val_loss: 0.4277801560169213\n",
      "Progress: 8.2% ... Training loss: 0.264 ... Validation loss: 0.442iteration: 825\n",
      "train_loss: 0.2647611380515962\n",
      "val_loss: 0.44292473012350536\n",
      "Progress: 8.3% ... Training loss: 0.286 ... Validation loss: 0.456iteration: 826\n",
      "train_loss: 0.28662817054088896\n",
      "val_loss: 0.45650020936992347\n",
      "Progress: 8.3% ... Training loss: 0.262 ... Validation loss: 0.443iteration: 827\n",
      "train_loss: 0.2621510098302455\n",
      "val_loss: 0.44366473082111363\n",
      "Progress: 8.3% ... Training loss: 0.281 ... Validation loss: 0.465iteration: 828\n",
      "train_loss: 0.2819367721153367\n",
      "val_loss: 0.46512460716068266\n",
      "Progress: 8.3% ... Training loss: 0.286 ... Validation loss: 0.456iteration: 829\n",
      "train_loss: 0.2868224464375462\n",
      "val_loss: 0.45617677080395047\n",
      "Progress: 8.3% ... Training loss: 0.297 ... Validation loss: 0.475iteration: 830\n",
      "train_loss: 0.29785881252229884\n",
      "val_loss: 0.4751374304849239\n",
      "Progress: 8.3% ... Training loss: 0.266 ... Validation loss: 0.440iteration: 831\n",
      "train_loss: 0.2667839429762144\n",
      "val_loss: 0.4402199988043975\n",
      "Progress: 8.3% ... Training loss: 0.258 ... Validation loss: 0.439iteration: 832\n",
      "train_loss: 0.2583007766537135\n",
      "val_loss: 0.439400147445815\n",
      "Progress: 8.3% ... Training loss: 0.260 ... Validation loss: 0.442iteration: 833\n",
      "train_loss: 0.26085409521990316\n",
      "val_loss: 0.4422202871845965\n",
      "Progress: 8.3% ... Training loss: 0.261 ... Validation loss: 0.434iteration: 834\n",
      "train_loss: 0.26188100156922967\n",
      "val_loss: 0.4344083947729957\n",
      "Progress: 8.3% ... Training loss: 0.273 ... Validation loss: 0.459iteration: 835\n",
      "train_loss: 0.2737311481895821\n",
      "val_loss: 0.4593562680552793\n",
      "Progress: 8.4% ... Training loss: 0.258 ... Validation loss: 0.437iteration: 836\n",
      "train_loss: 0.25874025799935096\n",
      "val_loss: 0.4375485346167169\n",
      "Progress: 8.4% ... Training loss: 0.262 ... Validation loss: 0.428iteration: 837\n",
      "train_loss: 0.26206980757634857\n",
      "val_loss: 0.42834351854027575\n",
      "Progress: 8.4% ... Training loss: 0.262 ... Validation loss: 0.430iteration: 838\n",
      "train_loss: 0.26295960699804244\n",
      "val_loss: 0.43097366901980083\n",
      "Progress: 8.4% ... Training loss: 0.257 ... Validation loss: 0.437iteration: 839\n",
      "train_loss: 0.2578988454099147\n",
      "val_loss: 0.43788114710297255\n",
      "Progress: 8.4% ... Training loss: 0.271 ... Validation loss: 0.457iteration: 840\n",
      "train_loss: 0.27129259428128943\n",
      "val_loss: 0.4570693906465157\n",
      "Progress: 8.4% ... Training loss: 0.257 ... Validation loss: 0.435iteration: 841\n",
      "train_loss: 0.25710770370679714\n",
      "val_loss: 0.43589027069691294\n",
      "Progress: 8.4% ... Training loss: 0.259 ... Validation loss: 0.439iteration: 842\n",
      "train_loss: 0.2592443034605159\n",
      "val_loss: 0.4394215318796099\n",
      "Progress: 8.4% ... Training loss: 0.256 ... Validation loss: 0.436iteration: 843\n",
      "train_loss: 0.25665268306292366\n",
      "val_loss: 0.4365204814730316\n",
      "Progress: 8.4% ... Training loss: 0.264 ... Validation loss: 0.445iteration: 844\n",
      "train_loss: 0.2641367396327044\n",
      "val_loss: 0.44570943747851904\n",
      "Progress: 8.4% ... Training loss: 0.258 ... Validation loss: 0.438iteration: 845\n",
      "train_loss: 0.258647252814059\n",
      "val_loss: 0.4380264844435925\n",
      "Progress: 8.5% ... Training loss: 0.256 ... Validation loss: 0.439iteration: 846\n",
      "train_loss: 0.25664413963855226\n",
      "val_loss: 0.43992358233453716\n",
      "Progress: 8.5% ... Training loss: 0.257 ... Validation loss: 0.439iteration: 847\n",
      "train_loss: 0.2577491901659509\n",
      "val_loss: 0.43987638982770805\n",
      "Progress: 8.5% ... Training loss: 0.262 ... Validation loss: 0.435iteration: 848\n",
      "train_loss: 0.26219825990640716\n",
      "val_loss: 0.4355091442269521\n",
      "Progress: 8.5% ... Training loss: 0.266 ... Validation loss: 0.438iteration: 849\n",
      "train_loss: 0.266488851069686\n",
      "val_loss: 0.43811104578373866\n",
      "Progress: 8.5% ... Training loss: 0.256 ... Validation loss: 0.434iteration: 850\n",
      "train_loss: 0.25615013454564195\n",
      "val_loss: 0.434896333245143\n",
      "Progress: 8.5% ... Training loss: 0.259 ... Validation loss: 0.435iteration: 851\n",
      "train_loss: 0.2596233474482319\n",
      "val_loss: 0.4356962172248184\n",
      "Progress: 8.5% ... Training loss: 0.257 ... Validation loss: 0.435iteration: 852\n",
      "train_loss: 0.25762486990766614\n",
      "val_loss: 0.4359906142777978\n",
      "Progress: 8.5% ... Training loss: 0.256 ... Validation loss: 0.437iteration: 853\n",
      "train_loss: 0.2565069770091331\n",
      "val_loss: 0.4377369625578055\n",
      "Progress: 8.5% ... Training loss: 0.258 ... Validation loss: 0.435iteration: 854\n",
      "train_loss: 0.2580791860318991\n",
      "val_loss: 0.4357909978634931\n",
      "Progress: 8.6% ... Training loss: 0.257 ... Validation loss: 0.431iteration: 855\n",
      "train_loss: 0.2574191650729286\n",
      "val_loss: 0.43111958136505263\n",
      "Progress: 8.6% ... Training loss: 0.271 ... Validation loss: 0.452iteration: 856\n",
      "train_loss: 0.27175551857566\n",
      "val_loss: 0.45273596417016665\n",
      "Progress: 8.6% ... Training loss: 0.255 ... Validation loss: 0.445iteration: 857\n",
      "train_loss: 0.255687112581946\n",
      "val_loss: 0.4455105871252932\n",
      "Progress: 8.6% ... Training loss: 0.257 ... Validation loss: 0.445iteration: 858\n",
      "train_loss: 0.2570140496953584\n",
      "val_loss: 0.4451281730151428\n",
      "Progress: 8.6% ... Training loss: 0.256 ... Validation loss: 0.448iteration: 859\n",
      "train_loss: 0.25661908269155537\n",
      "val_loss: 0.4486428202383322\n",
      "Progress: 8.6% ... Training loss: 0.261 ... Validation loss: 0.450iteration: 860\n",
      "train_loss: 0.26136740414260207\n",
      "val_loss: 0.4501453487627284\n",
      "Progress: 8.6% ... Training loss: 0.257 ... Validation loss: 0.452iteration: 861\n",
      "train_loss: 0.2574902179228674\n",
      "val_loss: 0.45269872556634205\n",
      "Progress: 8.6% ... Training loss: 0.257 ... Validation loss: 0.452iteration: 862\n",
      "train_loss: 0.2577328742478593\n",
      "val_loss: 0.45208267443570016\n",
      "Progress: 8.6% ... Training loss: 0.255 ... Validation loss: 0.448iteration: 863\n",
      "train_loss: 0.25596062161874306\n",
      "val_loss: 0.44898642973213676\n",
      "Progress: 8.6% ... Training loss: 0.255 ... Validation loss: 0.450iteration: 864\n",
      "train_loss: 0.2554820771193008\n",
      "val_loss: 0.4504943817095756\n",
      "Progress: 8.7% ... Training loss: 0.258 ... Validation loss: 0.446iteration: 865\n",
      "train_loss: 0.2582113268382699\n",
      "val_loss: 0.44634283370289674\n",
      "Progress: 8.7% ... Training loss: 0.257 ... Validation loss: 0.447iteration: 866\n",
      "train_loss: 0.25730249708632996\n",
      "val_loss: 0.44755983582304076\n",
      "Progress: 8.7% ... Training loss: 0.257 ... Validation loss: 0.453iteration: 867\n",
      "train_loss: 0.2572109901464728\n",
      "val_loss: 0.45347717053172654\n",
      "Progress: 8.7% ... Training loss: 0.278 ... Validation loss: 0.465iteration: 868\n",
      "train_loss: 0.2784559988839251\n",
      "val_loss: 0.4651443029610615\n",
      "Progress: 8.7% ... Training loss: 0.259 ... Validation loss: 0.457iteration: 869\n",
      "train_loss: 0.2592475106255473\n",
      "val_loss: 0.45772099890697526\n",
      "Progress: 8.7% ... Training loss: 0.260 ... Validation loss: 0.447iteration: 870\n",
      "train_loss: 0.26060182027114864\n",
      "val_loss: 0.44738520844044694\n",
      "Progress: 8.7% ... Training loss: 0.255 ... Validation loss: 0.443iteration: 871\n",
      "train_loss: 0.25568182999747197\n",
      "val_loss: 0.4439867659066225\n",
      "Progress: 8.7% ... Training loss: 0.264 ... Validation loss: 0.445iteration: 872\n",
      "train_loss: 0.2648978848849879\n",
      "val_loss: 0.44555333263010005\n",
      "Progress: 8.7% ... Training loss: 0.255 ... Validation loss: 0.445iteration: 873\n",
      "train_loss: 0.25532148755844253\n",
      "val_loss: 0.445573892668692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 8.7% ... Training loss: 0.257 ... Validation loss: 0.451iteration: 874\n",
      "train_loss: 0.2575347617320374\n",
      "val_loss: 0.4518757222464098\n",
      "Progress: 8.8% ... Training loss: 0.257 ... Validation loss: 0.452iteration: 875\n",
      "train_loss: 0.2573351775716062\n",
      "val_loss: 0.45250754526278963\n",
      "Progress: 8.8% ... Training loss: 0.257 ... Validation loss: 0.454iteration: 876\n",
      "train_loss: 0.2571783881086556\n",
      "val_loss: 0.45433176099341394\n",
      "Progress: 8.8% ... Training loss: 0.277 ... Validation loss: 0.464iteration: 877\n",
      "train_loss: 0.27746659136175406\n",
      "val_loss: 0.4646049274402929\n",
      "Progress: 8.8% ... Training loss: 0.270 ... Validation loss: 0.463iteration: 878\n",
      "train_loss: 0.2704390659136243\n",
      "val_loss: 0.4638106297329718\n",
      "Progress: 8.8% ... Training loss: 0.260 ... Validation loss: 0.457iteration: 879\n",
      "train_loss: 0.2604288599882194\n",
      "val_loss: 0.45720830029047743\n",
      "Progress: 8.8% ... Training loss: 0.273 ... Validation loss: 0.467iteration: 880\n",
      "train_loss: 0.27336893217368424\n",
      "val_loss: 0.4678333017235632\n",
      "Progress: 8.8% ... Training loss: 0.256 ... Validation loss: 0.454iteration: 881\n",
      "train_loss: 0.25674291598677274\n",
      "val_loss: 0.4549934970783484\n",
      "Progress: 8.8% ... Training loss: 0.257 ... Validation loss: 0.458iteration: 882\n",
      "train_loss: 0.25754998284112846\n",
      "val_loss: 0.45891212902101064\n",
      "Progress: 8.8% ... Training loss: 0.260 ... Validation loss: 0.466iteration: 883\n",
      "train_loss: 0.26005038428106536\n",
      "val_loss: 0.46634964807018237\n",
      "Progress: 8.8% ... Training loss: 0.260 ... Validation loss: 0.461iteration: 884\n",
      "train_loss: 0.260623306746598\n",
      "val_loss: 0.4612324587205822\n",
      "Progress: 8.8% ... Training loss: 0.264 ... Validation loss: 0.467iteration: 885\n",
      "train_loss: 0.26496453251828067\n",
      "val_loss: 0.467879579788981\n",
      "Progress: 8.9% ... Training loss: 0.276 ... Validation loss: 0.449iteration: 886\n",
      "train_loss: 0.2769201496009724\n",
      "val_loss: 0.44998813920082176\n",
      "Progress: 8.9% ... Training loss: 0.284 ... Validation loss: 0.493iteration: 887\n",
      "train_loss: 0.2847003874782336\n",
      "val_loss: 0.49381478540658114\n",
      "Progress: 8.9% ... Training loss: 0.260 ... Validation loss: 0.434iteration: 888\n",
      "train_loss: 0.26069482316805387\n",
      "val_loss: 0.43432517365627316\n",
      "Progress: 8.9% ... Training loss: 0.259 ... Validation loss: 0.444iteration: 889\n",
      "train_loss: 0.25916151309618746\n",
      "val_loss: 0.4444065134133715\n",
      "Progress: 8.9% ... Training loss: 0.266 ... Validation loss: 0.436iteration: 890\n",
      "train_loss: 0.2664643421336742\n",
      "val_loss: 0.43620367951558053\n",
      "Progress: 8.9% ... Training loss: 0.262 ... Validation loss: 0.459iteration: 891\n",
      "train_loss: 0.26267768130659636\n",
      "val_loss: 0.45913741896950233\n",
      "Progress: 8.9% ... Training loss: 0.257 ... Validation loss: 0.437iteration: 892\n",
      "train_loss: 0.25785581204029484\n",
      "val_loss: 0.4374832183883364\n",
      "Progress: 8.9% ... Training loss: 0.255 ... Validation loss: 0.443iteration: 893\n",
      "train_loss: 0.2550431233671527\n",
      "val_loss: 0.4437484334606251\n",
      "Progress: 8.9% ... Training loss: 0.256 ... Validation loss: 0.443iteration: 894\n",
      "train_loss: 0.2564081602971974\n",
      "val_loss: 0.4439906720502811\n",
      "Progress: 8.9% ... Training loss: 0.257 ... Validation loss: 0.447iteration: 895\n",
      "train_loss: 0.25751521650962145\n",
      "val_loss: 0.44744255103867364\n",
      "Progress: 9.0% ... Training loss: 0.272 ... Validation loss: 0.445iteration: 896\n",
      "train_loss: 0.2721345872206597\n",
      "val_loss: 0.44513924137833194\n",
      "Progress: 9.0% ... Training loss: 0.275 ... Validation loss: 0.471iteration: 897\n",
      "train_loss: 0.2755153714912862\n",
      "val_loss: 0.4719040190497506\n",
      "Progress: 9.0% ... Training loss: 0.275 ... Validation loss: 0.460iteration: 898\n",
      "train_loss: 0.2758382851447605\n",
      "val_loss: 0.4609839937983241\n",
      "Progress: 9.0% ... Training loss: 0.257 ... Validation loss: 0.442iteration: 899\n",
      "train_loss: 0.25720147080911554\n",
      "val_loss: 0.4427600280512772\n",
      "Progress: 9.0% ... Training loss: 0.255 ... Validation loss: 0.448iteration: 900\n",
      "train_loss: 0.2554800551821636\n",
      "val_loss: 0.44804788099282195\n",
      "Progress: 9.0% ... Training loss: 0.264 ... Validation loss: 0.450iteration: 901\n",
      "train_loss: 0.2643338913054526\n",
      "val_loss: 0.45039220846941563\n",
      "Progress: 9.0% ... Training loss: 0.257 ... Validation loss: 0.445iteration: 902\n",
      "train_loss: 0.25778574204740795\n",
      "val_loss: 0.4450998776386924\n",
      "Progress: 9.0% ... Training loss: 0.260 ... Validation loss: 0.445iteration: 903\n",
      "train_loss: 0.2602397936206922\n",
      "val_loss: 0.4453912594617004\n",
      "Progress: 9.0% ... Training loss: 0.256 ... Validation loss: 0.453iteration: 904\n",
      "train_loss: 0.2565719558681925\n",
      "val_loss: 0.4530077026209496\n",
      "Progress: 9.1% ... Training loss: 0.254 ... Validation loss: 0.446iteration: 905\n",
      "train_loss: 0.2545932404498295\n",
      "val_loss: 0.4460679440860045\n",
      "Progress: 9.1% ... Training loss: 0.255 ... Validation loss: 0.439iteration: 906\n",
      "train_loss: 0.25559147773004587\n",
      "val_loss: 0.4398835432255119\n",
      "Progress: 9.1% ... Training loss: 0.256 ... Validation loss: 0.434iteration: 907\n",
      "train_loss: 0.25606093831977766\n",
      "val_loss: 0.4342437623599753\n",
      "Progress: 9.1% ... Training loss: 0.261 ... Validation loss: 0.441iteration: 908\n",
      "train_loss: 0.2616453296858703\n",
      "val_loss: 0.4410056024706383\n",
      "Progress: 9.1% ... Training loss: 0.256 ... Validation loss: 0.435iteration: 909\n",
      "train_loss: 0.25610938164471536\n",
      "val_loss: 0.43523083310582567\n",
      "Progress: 9.1% ... Training loss: 0.254 ... Validation loss: 0.439iteration: 910\n",
      "train_loss: 0.2543516918844521\n",
      "val_loss: 0.43992267123436163\n",
      "Progress: 9.1% ... Training loss: 0.257 ... Validation loss: 0.442iteration: 911\n",
      "train_loss: 0.25742046826767695\n",
      "val_loss: 0.44223181927936267\n",
      "Progress: 9.1% ... Training loss: 0.258 ... Validation loss: 0.448iteration: 912\n",
      "train_loss: 0.25859048185682537\n",
      "val_loss: 0.4480590790349143\n",
      "Progress: 9.1% ... Training loss: 0.256 ... Validation loss: 0.445iteration: 913\n",
      "train_loss: 0.25659152947877034\n",
      "val_loss: 0.4453244954953825\n",
      "Progress: 9.1% ... Training loss: 0.258 ... Validation loss: 0.441iteration: 914\n",
      "train_loss: 0.2584174933090472\n",
      "val_loss: 0.4410924047443753\n",
      "Progress: 9.2% ... Training loss: 0.260 ... Validation loss: 0.446iteration: 915\n",
      "train_loss: 0.26031168170799646\n",
      "val_loss: 0.44697576628247515\n",
      "Progress: 9.2% ... Training loss: 0.254 ... Validation loss: 0.439iteration: 916\n",
      "train_loss: 0.254860096916639\n",
      "val_loss: 0.4396841926568271\n",
      "Progress: 9.2% ... Training loss: 0.254 ... Validation loss: 0.442iteration: 917\n",
      "train_loss: 0.2540344670691544\n",
      "val_loss: 0.4428297709857433\n",
      "Progress: 9.2% ... Training loss: 0.261 ... Validation loss: 0.448iteration: 918\n",
      "train_loss: 0.26136327576852525\n",
      "val_loss: 0.4488913512338273\n",
      "Progress: 9.2% ... Training loss: 0.255 ... Validation loss: 0.439iteration: 919\n",
      "train_loss: 0.2554358722419921\n",
      "val_loss: 0.43916100353282733\n",
      "Progress: 9.2% ... Training loss: 0.262 ... Validation loss: 0.445iteration: 920\n",
      "train_loss: 0.26209642587637066\n",
      "val_loss: 0.44508983192846946\n",
      "Progress: 9.2% ... Training loss: 0.255 ... Validation loss: 0.439iteration: 921\n",
      "train_loss: 0.25541947013802924\n",
      "val_loss: 0.4391903428369907\n",
      "Progress: 9.2% ... Training loss: 0.256 ... Validation loss: 0.441iteration: 922\n",
      "train_loss: 0.2561369426919034\n",
      "val_loss: 0.441638064984276\n",
      "Progress: 9.2% ... Training loss: 0.254 ... Validation loss: 0.441iteration: 923\n",
      "train_loss: 0.25474459232720414\n",
      "val_loss: 0.4419624192137074\n",
      "Progress: 9.2% ... Training loss: 0.255 ... Validation loss: 0.434iteration: 924\n",
      "train_loss: 0.2551773236290895\n",
      "val_loss: 0.4341350498113903\n",
      "Progress: 9.2% ... Training loss: 0.254 ... Validation loss: 0.435iteration: 925\n",
      "train_loss: 0.25461835057845394\n",
      "val_loss: 0.4359252168623092\n",
      "Progress: 9.3% ... Training loss: 0.260 ... Validation loss: 0.445iteration: 926\n",
      "train_loss: 0.260050424246419\n",
      "val_loss: 0.4456110349781031\n",
      "Progress: 9.3% ... Training loss: 0.257 ... Validation loss: 0.441iteration: 927\n",
      "train_loss: 0.2576808681109197\n",
      "val_loss: 0.44104758642547603\n",
      "Progress: 9.3% ... Training loss: 0.264 ... Validation loss: 0.449iteration: 928\n",
      "train_loss: 0.26429938156202326\n",
      "val_loss: 0.44911898186763255\n",
      "Progress: 9.3% ... Training loss: 0.274 ... Validation loss: 0.460iteration: 929\n",
      "train_loss: 0.2744646478385606\n",
      "val_loss: 0.46072670422108075\n",
      "Progress: 9.3% ... Training loss: 0.259 ... Validation loss: 0.445iteration: 930\n",
      "train_loss: 0.25973172238008285\n",
      "val_loss: 0.4459085866084305\n",
      "Progress: 9.3% ... Training loss: 0.255 ... Validation loss: 0.441iteration: 931\n",
      "train_loss: 0.2552111004467313\n",
      "val_loss: 0.44156613590204585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 9.3% ... Training loss: 0.257 ... Validation loss: 0.451iteration: 932\n",
      "train_loss: 0.2571854897370629\n",
      "val_loss: 0.45147330508793604\n",
      "Progress: 9.3% ... Training loss: 0.259 ... Validation loss: 0.442iteration: 933\n",
      "train_loss: 0.25906239447880086\n",
      "val_loss: 0.44213755778944314\n",
      "Progress: 9.3% ... Training loss: 0.256 ... Validation loss: 0.437iteration: 934\n",
      "train_loss: 0.2563725076851228\n",
      "val_loss: 0.43754174695930126\n",
      "Progress: 9.3% ... Training loss: 0.257 ... Validation loss: 0.436iteration: 935\n",
      "train_loss: 0.25786979765285933\n",
      "val_loss: 0.4366336099364103\n",
      "Progress: 9.4% ... Training loss: 0.267 ... Validation loss: 0.462iteration: 936\n",
      "train_loss: 0.2670487428512591\n",
      "val_loss: 0.4623819318829173\n",
      "Progress: 9.4% ... Training loss: 0.256 ... Validation loss: 0.446iteration: 937\n",
      "train_loss: 0.2563645260101751\n",
      "val_loss: 0.4461912771185252\n",
      "Progress: 9.4% ... Training loss: 0.256 ... Validation loss: 0.437iteration: 938\n",
      "train_loss: 0.2560766863679972\n",
      "val_loss: 0.4377541632684742\n",
      "Progress: 9.4% ... Training loss: 0.258 ... Validation loss: 0.433iteration: 939\n",
      "train_loss: 0.2583345733412683\n",
      "val_loss: 0.4334138506694591\n",
      "Progress: 9.4% ... Training loss: 0.256 ... Validation loss: 0.443iteration: 940\n",
      "train_loss: 0.2563217861926784\n",
      "val_loss: 0.4436364151027205\n",
      "Progress: 9.4% ... Training loss: 0.254 ... Validation loss: 0.443iteration: 941\n",
      "train_loss: 0.25462370784425137\n",
      "val_loss: 0.4439146716221455\n",
      "Progress: 9.4% ... Training loss: 0.254 ... Validation loss: 0.434iteration: 942\n",
      "train_loss: 0.2542939161870053\n",
      "val_loss: 0.4347478449223327\n",
      "Progress: 9.4% ... Training loss: 0.276 ... Validation loss: 0.452iteration: 943\n",
      "train_loss: 0.27692447187334657\n",
      "val_loss: 0.4520831297662878\n",
      "Progress: 9.4% ... Training loss: 0.294 ... Validation loss: 0.476iteration: 944\n",
      "train_loss: 0.2943891526672587\n",
      "val_loss: 0.47681137821805475\n",
      "Progress: 9.4% ... Training loss: 0.255 ... Validation loss: 0.443iteration: 945\n",
      "train_loss: 0.2555774166274839\n",
      "val_loss: 0.4439692007729908\n",
      "Progress: 9.5% ... Training loss: 0.253 ... Validation loss: 0.441iteration: 946\n",
      "train_loss: 0.25351616153273165\n",
      "val_loss: 0.44137952193193014\n",
      "Progress: 9.5% ... Training loss: 0.264 ... Validation loss: 0.435iteration: 947\n",
      "train_loss: 0.2641531562477437\n",
      "val_loss: 0.43545758107454163\n",
      "Progress: 9.5% ... Training loss: 0.255 ... Validation loss: 0.444iteration: 948\n",
      "train_loss: 0.25580866689612547\n",
      "val_loss: 0.44432847482491805\n",
      "Progress: 9.5% ... Training loss: 0.254 ... Validation loss: 0.440iteration: 949\n",
      "train_loss: 0.2540380760634387\n",
      "val_loss: 0.4408954901041524\n",
      "Progress: 9.5% ... Training loss: 0.258 ... Validation loss: 0.451iteration: 950\n",
      "train_loss: 0.2585201932602079\n",
      "val_loss: 0.45192386311628086\n",
      "Progress: 9.5% ... Training loss: 0.259 ... Validation loss: 0.440iteration: 951\n",
      "train_loss: 0.2598201196712089\n",
      "val_loss: 0.44021156431742126\n",
      "Progress: 9.5% ... Training loss: 0.253 ... Validation loss: 0.442iteration: 952\n",
      "train_loss: 0.25357174142201666\n",
      "val_loss: 0.4426865128091644\n",
      "Progress: 9.5% ... Training loss: 0.260 ... Validation loss: 0.452iteration: 953\n",
      "train_loss: 0.2604211267225044\n",
      "val_loss: 0.4521918109247025\n",
      "Progress: 9.5% ... Training loss: 0.254 ... Validation loss: 0.451iteration: 954\n",
      "train_loss: 0.2547751799440747\n",
      "val_loss: 0.4518935633363285\n",
      "Progress: 9.6% ... Training loss: 0.255 ... Validation loss: 0.459iteration: 955\n",
      "train_loss: 0.255768767502601\n",
      "val_loss: 0.4590547131660005\n",
      "Progress: 9.6% ... Training loss: 0.281 ... Validation loss: 0.487iteration: 956\n",
      "train_loss: 0.28160225071293016\n",
      "val_loss: 0.48714787601136395\n",
      "Progress: 9.6% ... Training loss: 0.254 ... Validation loss: 0.458iteration: 957\n",
      "train_loss: 0.2546872773456809\n",
      "val_loss: 0.45828730925737227\n",
      "Progress: 9.6% ... Training loss: 0.253 ... Validation loss: 0.443iteration: 958\n",
      "train_loss: 0.2538003581393238\n",
      "val_loss: 0.44376949492248785\n",
      "Progress: 9.6% ... Training loss: 0.255 ... Validation loss: 0.439iteration: 959\n",
      "train_loss: 0.25592350421701787\n",
      "val_loss: 0.43945525593483314\n",
      "Progress: 9.6% ... Training loss: 0.255 ... Validation loss: 0.445iteration: 960\n",
      "train_loss: 0.2550713134282604\n",
      "val_loss: 0.4451788258394272\n",
      "Progress: 9.6% ... Training loss: 0.256 ... Validation loss: 0.439iteration: 961\n",
      "train_loss: 0.2568576762282689\n",
      "val_loss: 0.43947093623062583\n",
      "Progress: 9.6% ... Training loss: 0.265 ... Validation loss: 0.469iteration: 962\n",
      "train_loss: 0.2650759199443844\n",
      "val_loss: 0.4699343936416175\n",
      "Progress: 9.6% ... Training loss: 0.292 ... Validation loss: 0.473iteration: 963\n",
      "train_loss: 0.2929517762681073\n",
      "val_loss: 0.4736260442624887\n",
      "Progress: 9.6% ... Training loss: 0.254 ... Validation loss: 0.447iteration: 964\n",
      "train_loss: 0.25444869410768317\n",
      "val_loss: 0.44761659154580813\n",
      "Progress: 9.7% ... Training loss: 0.259 ... Validation loss: 0.454iteration: 965\n",
      "train_loss: 0.2595313429176032\n",
      "val_loss: 0.4541590639261183\n",
      "Progress: 9.7% ... Training loss: 0.254 ... Validation loss: 0.444iteration: 966\n",
      "train_loss: 0.2542334050872513\n",
      "val_loss: 0.4446900018677086\n",
      "Progress: 9.7% ... Training loss: 0.257 ... Validation loss: 0.453iteration: 967\n",
      "train_loss: 0.25796296145675274\n",
      "val_loss: 0.45354548035674297\n",
      "Progress: 9.7% ... Training loss: 0.254 ... Validation loss: 0.447iteration: 968\n",
      "train_loss: 0.25434847966216034\n",
      "val_loss: 0.447712317045581\n",
      "Progress: 9.7% ... Training loss: 0.258 ... Validation loss: 0.457iteration: 969\n",
      "train_loss: 0.2583265849250083\n",
      "val_loss: 0.4574791527736641\n",
      "Progress: 9.7% ... Training loss: 0.254 ... Validation loss: 0.447iteration: 970\n",
      "train_loss: 0.2547603650558844\n",
      "val_loss: 0.447959849524836\n",
      "Progress: 9.7% ... Training loss: 0.253 ... Validation loss: 0.449iteration: 971\n",
      "train_loss: 0.2533168359406179\n",
      "val_loss: 0.4499487743331937\n",
      "Progress: 9.7% ... Training loss: 0.270 ... Validation loss: 0.462iteration: 972\n",
      "train_loss: 0.27046563474226526\n",
      "val_loss: 0.4620938516682324\n",
      "Progress: 9.7% ... Training loss: 0.253 ... Validation loss: 0.444iteration: 973\n",
      "train_loss: 0.2531547564097686\n",
      "val_loss: 0.4448665990115772\n",
      "Progress: 9.7% ... Training loss: 0.258 ... Validation loss: 0.460iteration: 974\n",
      "train_loss: 0.25813596023511104\n",
      "val_loss: 0.460286898880646\n",
      "Progress: 9.8% ... Training loss: 0.255 ... Validation loss: 0.443iteration: 975\n",
      "train_loss: 0.2552989853799441\n",
      "val_loss: 0.44329648573680275\n",
      "Progress: 9.8% ... Training loss: 0.252 ... Validation loss: 0.441iteration: 976\n",
      "train_loss: 0.25283953833004946\n",
      "val_loss: 0.44166572592576125\n",
      "Progress: 9.8% ... Training loss: 0.270 ... Validation loss: 0.479iteration: 977\n",
      "train_loss: 0.27042313643754207\n",
      "val_loss: 0.47921014423089553\n",
      "Progress: 9.8% ... Training loss: 0.254 ... Validation loss: 0.444iteration: 978\n",
      "train_loss: 0.25451620449747897\n",
      "val_loss: 0.4449139013102955\n",
      "Progress: 9.8% ... Training loss: 0.287 ... Validation loss: 0.453iteration: 979\n",
      "train_loss: 0.2874121620406553\n",
      "val_loss: 0.45372251999891106\n",
      "Progress: 9.8% ... Training loss: 0.266 ... Validation loss: 0.463iteration: 980\n",
      "train_loss: 0.2665845148953432\n",
      "val_loss: 0.4636964770820646\n",
      "Progress: 9.8% ... Training loss: 0.261 ... Validation loss: 0.447iteration: 981\n",
      "train_loss: 0.261049367355774\n",
      "val_loss: 0.447684428432297\n",
      "Progress: 9.8% ... Training loss: 0.258 ... Validation loss: 0.451iteration: 982\n",
      "train_loss: 0.2582158431492591\n",
      "val_loss: 0.451193258217461\n",
      "Progress: 9.8% ... Training loss: 0.256 ... Validation loss: 0.453iteration: 983\n",
      "train_loss: 0.25613852367627\n",
      "val_loss: 0.45368369526653035\n",
      "Progress: 9.8% ... Training loss: 0.259 ... Validation loss: 0.451iteration: 984\n",
      "train_loss: 0.2595650775279591\n",
      "val_loss: 0.4515135219581628\n",
      "Progress: 9.8% ... Training loss: 0.300 ... Validation loss: 0.498iteration: 985\n",
      "train_loss: 0.30040287914769553\n",
      "val_loss: 0.4981057282706039\n",
      "Progress: 9.9% ... Training loss: 0.266 ... Validation loss: 0.461iteration: 986\n",
      "train_loss: 0.26698162656866653\n",
      "val_loss: 0.46116702811542964\n",
      "Progress: 9.9% ... Training loss: 0.300 ... Validation loss: 0.501iteration: 987\n",
      "train_loss: 0.30037387202164084\n",
      "val_loss: 0.5010998422156921\n",
      "Progress: 9.9% ... Training loss: 0.299 ... Validation loss: 0.492iteration: 988\n",
      "train_loss: 0.2994886068730819\n",
      "val_loss: 0.4929261125510885\n",
      "Progress: 9.9% ... Training loss: 0.270 ... Validation loss: 0.474iteration: 989\n",
      "train_loss: 0.27080034944813175\n",
      "val_loss: 0.47424261665349754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 9.9% ... Training loss: 0.259 ... Validation loss: 0.450iteration: 990\n",
      "train_loss: 0.2597286345597087\n",
      "val_loss: 0.4504771869145956\n",
      "Progress: 9.9% ... Training loss: 0.259 ... Validation loss: 0.452iteration: 991\n",
      "train_loss: 0.25946863448078245\n",
      "val_loss: 0.452498030353938\n",
      "Progress: 9.9% ... Training loss: 0.267 ... Validation loss: 0.450iteration: 992\n",
      "train_loss: 0.2674025539913747\n",
      "val_loss: 0.450367638496158\n",
      "Progress: 9.9% ... Training loss: 0.255 ... Validation loss: 0.446iteration: 993\n",
      "train_loss: 0.2558091878126293\n",
      "val_loss: 0.44688271569937804\n",
      "Progress: 9.9% ... Training loss: 0.258 ... Validation loss: 0.455iteration: 994\n",
      "train_loss: 0.25823529096220205\n",
      "val_loss: 0.45526811805949685\n",
      "Progress: 9.9% ... Training loss: 0.252 ... Validation loss: 0.454iteration: 995\n",
      "train_loss: 0.25283302344170333\n",
      "val_loss: 0.4549842832344645\n",
      "Progress: 10.0% ... Training loss: 0.253 ... Validation loss: 0.453iteration: 996\n",
      "train_loss: 0.2530833214888711\n",
      "val_loss: 0.45323083427364835\n",
      "Progress: 10.0% ... Training loss: 0.251 ... Validation loss: 0.446iteration: 997\n",
      "train_loss: 0.25161994003783034\n",
      "val_loss: 0.4467656901409432\n",
      "Progress: 10.0% ... Training loss: 0.255 ... Validation loss: 0.442iteration: 998\n",
      "train_loss: 0.2556675961132778\n",
      "val_loss: 0.4425500478841654\n",
      "Progress: 10.0% ... Training loss: 0.258 ... Validation loss: 0.458iteration: 999\n",
      "train_loss: 0.25889215706764523\n",
      "val_loss: 0.4587572593461194\n",
      "Progress: 10.0% ... Training loss: 0.255 ... Validation loss: 0.456iteration: 1000\n",
      "train_loss: 0.25543944459886336\n",
      "val_loss: 0.45641641359001867\n",
      "Progress: 10.0% ... Training loss: 0.252 ... Validation loss: 0.454iteration: 1001\n",
      "train_loss: 0.2523949990813214\n",
      "val_loss: 0.4543157379241244\n",
      "Progress: 10.0% ... Training loss: 0.265 ... Validation loss: 0.470iteration: 1002\n",
      "train_loss: 0.26549080898624766\n",
      "val_loss: 0.4701561923552128\n",
      "Progress: 10.0% ... Training loss: 0.260 ... Validation loss: 0.460iteration: 1003\n",
      "train_loss: 0.26003424882015397\n",
      "val_loss: 0.4605761480355924\n",
      "Progress: 10.0% ... Training loss: 0.283 ... Validation loss: 0.461iteration: 1004\n",
      "train_loss: 0.28369469936596026\n",
      "val_loss: 0.4610122722787553\n",
      "Progress: 10.1% ... Training loss: 0.256 ... Validation loss: 0.445iteration: 1005\n",
      "train_loss: 0.25643863313968157\n",
      "val_loss: 0.4458529252335802\n",
      "Progress: 10.1% ... Training loss: 0.254 ... Validation loss: 0.438iteration: 1006\n",
      "train_loss: 0.25494898055541265\n",
      "val_loss: 0.4382061030663523\n",
      "Progress: 10.1% ... Training loss: 0.270 ... Validation loss: 0.469iteration: 1007\n",
      "train_loss: 0.2708533564974127\n",
      "val_loss: 0.46997089581582324\n",
      "Progress: 10.1% ... Training loss: 0.273 ... Validation loss: 0.459iteration: 1008\n",
      "train_loss: 0.27398221521349053\n",
      "val_loss: 0.45999941183029136\n",
      "Progress: 10.1% ... Training loss: 0.254 ... Validation loss: 0.456iteration: 1009\n",
      "train_loss: 0.2546543661014145\n",
      "val_loss: 0.45691204871002783\n",
      "Progress: 10.1% ... Training loss: 0.255 ... Validation loss: 0.449iteration: 1010\n",
      "train_loss: 0.2554112619571112\n",
      "val_loss: 0.4498858197085323\n",
      "Progress: 10.1% ... Training loss: 0.258 ... Validation loss: 0.452iteration: 1011\n",
      "train_loss: 0.25880738421094457\n",
      "val_loss: 0.4529490164589077\n",
      "Progress: 10.1% ... Training loss: 0.251 ... Validation loss: 0.441iteration: 1012\n",
      "train_loss: 0.2516239958340534\n",
      "val_loss: 0.4416198863358304\n",
      "Progress: 10.1% ... Training loss: 0.274 ... Validation loss: 0.446iteration: 1013\n",
      "train_loss: 0.2745515125653553\n",
      "val_loss: 0.446881926583421\n",
      "Progress: 10.1% ... Training loss: 0.252 ... Validation loss: 0.439iteration: 1014\n",
      "train_loss: 0.2526705809274962\n",
      "val_loss: 0.4390050527466899\n",
      "Progress: 10.2% ... Training loss: 0.254 ... Validation loss: 0.450iteration: 1015\n",
      "train_loss: 0.2544329651424414\n",
      "val_loss: 0.4505725752404159\n",
      "Progress: 10.2% ... Training loss: 0.250 ... Validation loss: 0.440iteration: 1016\n",
      "train_loss: 0.2508750531310473\n",
      "val_loss: 0.4403939459514756\n",
      "Progress: 10.2% ... Training loss: 0.257 ... Validation loss: 0.436iteration: 1017\n",
      "train_loss: 0.2573399025017396\n",
      "val_loss: 0.43609141533631685\n",
      "Progress: 10.2% ... Training loss: 0.252 ... Validation loss: 0.432iteration: 1018\n",
      "train_loss: 0.252764512658073\n",
      "val_loss: 0.43295188255478423\n",
      "Progress: 10.2% ... Training loss: 0.252 ... Validation loss: 0.430iteration: 1019\n",
      "train_loss: 0.25295703321369495\n",
      "val_loss: 0.43048408458250176\n",
      "Progress: 10.2% ... Training loss: 0.252 ... Validation loss: 0.433iteration: 1020\n",
      "train_loss: 0.252706910151651\n",
      "val_loss: 0.4331165995450386\n",
      "Progress: 10.2% ... Training loss: 0.254 ... Validation loss: 0.430iteration: 1021\n",
      "train_loss: 0.2547932253356021\n",
      "val_loss: 0.4303553594216944\n",
      "Progress: 10.2% ... Training loss: 0.250 ... Validation loss: 0.440iteration: 1022\n",
      "train_loss: 0.25040960792961015\n",
      "val_loss: 0.4408356359738082\n",
      "Progress: 10.2% ... Training loss: 0.270 ... Validation loss: 0.470iteration: 1023\n",
      "train_loss: 0.270626875895163\n",
      "val_loss: 0.4702175941124918\n",
      "Progress: 10.2% ... Training loss: 0.252 ... Validation loss: 0.450iteration: 1024\n",
      "train_loss: 0.2529229596871337\n",
      "val_loss: 0.45019757533492344\n",
      "Progress: 10.2% ... Training loss: 0.270 ... Validation loss: 0.446iteration: 1025\n",
      "train_loss: 0.270773483294725\n",
      "val_loss: 0.44604899426302924\n",
      "Progress: 10.3% ... Training loss: 0.253 ... Validation loss: 0.447iteration: 1026\n",
      "train_loss: 0.25300695564519216\n",
      "val_loss: 0.4479911592517252\n",
      "Progress: 10.3% ... Training loss: 0.265 ... Validation loss: 0.446iteration: 1027\n",
      "train_loss: 0.2655514054627021\n",
      "val_loss: 0.44689788171182404\n",
      "Progress: 10.3% ... Training loss: 0.263 ... Validation loss: 0.458iteration: 1028\n",
      "train_loss: 0.2630917141732656\n",
      "val_loss: 0.45855146608043157\n",
      "Progress: 10.3% ... Training loss: 0.252 ... Validation loss: 0.452iteration: 1029\n",
      "train_loss: 0.25258199595853215\n",
      "val_loss: 0.4523948547315791\n",
      "Progress: 10.3% ... Training loss: 0.262 ... Validation loss: 0.452iteration: 1030\n",
      "train_loss: 0.2623280875016741\n",
      "val_loss: 0.4524589963627376\n",
      "Progress: 10.3% ... Training loss: 0.256 ... Validation loss: 0.442iteration: 1031\n",
      "train_loss: 0.256548526686164\n",
      "val_loss: 0.44223949108567356\n",
      "Progress: 10.3% ... Training loss: 0.256 ... Validation loss: 0.446iteration: 1032\n",
      "train_loss: 0.25699693761355036\n",
      "val_loss: 0.4460120948479272\n",
      "Progress: 10.3% ... Training loss: 0.254 ... Validation loss: 0.438iteration: 1033\n",
      "train_loss: 0.2544734979140534\n",
      "val_loss: 0.4387972359357809\n",
      "Progress: 10.3% ... Training loss: 0.254 ... Validation loss: 0.442iteration: 1034\n",
      "train_loss: 0.25413995345762025\n",
      "val_loss: 0.4423547720065577\n",
      "Progress: 10.3% ... Training loss: 0.258 ... Validation loss: 0.444iteration: 1035\n",
      "train_loss: 0.2586264247158348\n",
      "val_loss: 0.4441424089754332\n",
      "Progress: 10.4% ... Training loss: 0.256 ... Validation loss: 0.445iteration: 1036\n",
      "train_loss: 0.256233701631958\n",
      "val_loss: 0.44513026877099043\n",
      "Progress: 10.4% ... Training loss: 0.255 ... Validation loss: 0.452iteration: 1037\n",
      "train_loss: 0.2553553061018611\n",
      "val_loss: 0.4527239764326006\n",
      "Progress: 10.4% ... Training loss: 0.250 ... Validation loss: 0.439iteration: 1038\n",
      "train_loss: 0.25090964927715165\n",
      "val_loss: 0.43925713325409566\n",
      "Progress: 10.4% ... Training loss: 0.250 ... Validation loss: 0.442iteration: 1039\n",
      "train_loss: 0.25024071990518093\n",
      "val_loss: 0.44263383188542177\n",
      "Progress: 10.4% ... Training loss: 0.250 ... Validation loss: 0.444iteration: 1040\n",
      "train_loss: 0.2501583608393732\n",
      "val_loss: 0.4444109510592464\n",
      "Progress: 10.4% ... Training loss: 0.253 ... Validation loss: 0.453iteration: 1041\n",
      "train_loss: 0.2530353036498888\n",
      "val_loss: 0.453025867823105\n",
      "Progress: 10.4% ... Training loss: 0.253 ... Validation loss: 0.443iteration: 1042\n",
      "train_loss: 0.2539589778200848\n",
      "val_loss: 0.4430682853661785\n",
      "Progress: 10.4% ... Training loss: 0.257 ... Validation loss: 0.440iteration: 1043\n",
      "train_loss: 0.2572310602629229\n",
      "val_loss: 0.44078771306410247\n",
      "Progress: 10.4% ... Training loss: 0.250 ... Validation loss: 0.446iteration: 1044\n",
      "train_loss: 0.25085548501105887\n",
      "val_loss: 0.4460231307904233\n",
      "Progress: 10.4% ... Training loss: 0.249 ... Validation loss: 0.441iteration: 1045\n",
      "train_loss: 0.24929636865197335\n",
      "val_loss: 0.44160575585675066\n",
      "Progress: 10.5% ... Training loss: 0.252 ... Validation loss: 0.447iteration: 1046\n",
      "train_loss: 0.25209879492187187\n",
      "val_loss: 0.44795530484548174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10.5% ... Training loss: 0.252 ... Validation loss: 0.440iteration: 1047\n",
      "train_loss: 0.2524516101262584\n",
      "val_loss: 0.4405584682848236\n",
      "Progress: 10.5% ... Training loss: 0.252 ... Validation loss: 0.437iteration: 1048\n",
      "train_loss: 0.2525512306705887\n",
      "val_loss: 0.4379496131080279\n",
      "Progress: 10.5% ... Training loss: 0.253 ... Validation loss: 0.444iteration: 1049\n",
      "train_loss: 0.2535743918950314\n",
      "val_loss: 0.4443662675452786\n",
      "Progress: 10.5% ... Training loss: 0.260 ... Validation loss: 0.435iteration: 1050\n",
      "train_loss: 0.26032455543362626\n",
      "val_loss: 0.43537276006994763\n",
      "Progress: 10.5% ... Training loss: 0.263 ... Validation loss: 0.462iteration: 1051\n",
      "train_loss: 0.2631603988310139\n",
      "val_loss: 0.4623391716525725\n",
      "Progress: 10.5% ... Training loss: 0.252 ... Validation loss: 0.437iteration: 1052\n",
      "train_loss: 0.2523598199800397\n",
      "val_loss: 0.4372955179558086\n",
      "Progress: 10.5% ... Training loss: 0.252 ... Validation loss: 0.434iteration: 1053\n",
      "train_loss: 0.2529436073903767\n",
      "val_loss: 0.4340484036029211\n",
      "Progress: 10.5% ... Training loss: 0.254 ... Validation loss: 0.444iteration: 1054\n",
      "train_loss: 0.25402039552752204\n",
      "val_loss: 0.4443324017747075\n",
      "Progress: 10.6% ... Training loss: 0.252 ... Validation loss: 0.452iteration: 1055\n",
      "train_loss: 0.2524235906491256\n",
      "val_loss: 0.452808282590706\n",
      "Progress: 10.6% ... Training loss: 0.250 ... Validation loss: 0.441iteration: 1056\n",
      "train_loss: 0.250489250269877\n",
      "val_loss: 0.44187898711522977\n",
      "Progress: 10.6% ... Training loss: 0.249 ... Validation loss: 0.443iteration: 1057\n",
      "train_loss: 0.24940086707979844\n",
      "val_loss: 0.44382326480315465\n",
      "Progress: 10.6% ... Training loss: 0.249 ... Validation loss: 0.446iteration: 1058\n",
      "train_loss: 0.24975685879880508\n",
      "val_loss: 0.44611173609974225\n",
      "Progress: 10.6% ... Training loss: 0.249 ... Validation loss: 0.448iteration: 1059\n",
      "train_loss: 0.24928227714993978\n",
      "val_loss: 0.4482669942503312\n",
      "Progress: 10.6% ... Training loss: 0.249 ... Validation loss: 0.454iteration: 1060\n",
      "train_loss: 0.24981615041863883\n",
      "val_loss: 0.45410519885509765\n",
      "Progress: 10.6% ... Training loss: 0.251 ... Validation loss: 0.450iteration: 1061\n",
      "train_loss: 0.2512976168618383\n",
      "val_loss: 0.4502677803391503\n",
      "Progress: 10.6% ... Training loss: 0.253 ... Validation loss: 0.442iteration: 1062\n",
      "train_loss: 0.2531030027272676\n",
      "val_loss: 0.44292826967085847\n",
      "Progress: 10.6% ... Training loss: 0.255 ... Validation loss: 0.454iteration: 1063\n",
      "train_loss: 0.25537331419198805\n",
      "val_loss: 0.4546166526941385\n",
      "Progress: 10.6% ... Training loss: 0.250 ... Validation loss: 0.451iteration: 1064\n",
      "train_loss: 0.2504478363295287\n",
      "val_loss: 0.451578857636822\n",
      "Progress: 10.7% ... Training loss: 0.259 ... Validation loss: 0.453iteration: 1065\n",
      "train_loss: 0.2591399093777349\n",
      "val_loss: 0.4530270352189649\n",
      "Progress: 10.7% ... Training loss: 0.250 ... Validation loss: 0.453iteration: 1066\n",
      "train_loss: 0.2503362852069261\n",
      "val_loss: 0.4534972918408882\n",
      "Progress: 10.7% ... Training loss: 0.252 ... Validation loss: 0.439iteration: 1067\n",
      "train_loss: 0.2523967430948782\n",
      "val_loss: 0.43961442643851034\n",
      "Progress: 10.7% ... Training loss: 0.255 ... Validation loss: 0.458iteration: 1068\n",
      "train_loss: 0.2555840113426058\n",
      "val_loss: 0.4585830067776096\n",
      "Progress: 10.7% ... Training loss: 0.254 ... Validation loss: 0.443iteration: 1069\n",
      "train_loss: 0.2549447499717457\n",
      "val_loss: 0.44331822008702293\n",
      "Progress: 10.7% ... Training loss: 0.255 ... Validation loss: 0.461iteration: 1070\n",
      "train_loss: 0.25585502018306444\n",
      "val_loss: 0.46133865791999334\n",
      "Progress: 10.7% ... Training loss: 0.249 ... Validation loss: 0.448iteration: 1071\n",
      "train_loss: 0.24936112443702274\n",
      "val_loss: 0.44855255005173705\n",
      "Progress: 10.7% ... Training loss: 0.249 ... Validation loss: 0.448iteration: 1072\n",
      "train_loss: 0.24951874473113536\n",
      "val_loss: 0.4486824161000047\n",
      "Progress: 10.7% ... Training loss: 0.254 ... Validation loss: 0.454iteration: 1073\n",
      "train_loss: 0.25493367694690555\n",
      "val_loss: 0.45465986394343083\n",
      "Progress: 10.7% ... Training loss: 0.249 ... Validation loss: 0.439iteration: 1074\n",
      "train_loss: 0.24920685085710428\n",
      "val_loss: 0.43910860179796457\n",
      "Progress: 10.8% ... Training loss: 0.283 ... Validation loss: 0.455iteration: 1075\n",
      "train_loss: 0.2838170088752435\n",
      "val_loss: 0.45513135626700585\n",
      "Progress: 10.8% ... Training loss: 0.269 ... Validation loss: 0.461iteration: 1076\n",
      "train_loss: 0.26916066793326654\n",
      "val_loss: 0.4612547199087607\n",
      "Progress: 10.8% ... Training loss: 0.296 ... Validation loss: 0.506iteration: 1077\n",
      "train_loss: 0.29632806429703007\n",
      "val_loss: 0.5066358813254402\n",
      "Progress: 10.8% ... Training loss: 0.256 ... Validation loss: 0.448iteration: 1078\n",
      "train_loss: 0.2566886616500649\n",
      "val_loss: 0.4488291603350017\n",
      "Progress: 10.8% ... Training loss: 0.253 ... Validation loss: 0.439iteration: 1079\n",
      "train_loss: 0.25352904009719013\n",
      "val_loss: 0.4398314101592987\n",
      "Progress: 10.8% ... Training loss: 0.253 ... Validation loss: 0.445iteration: 1080\n",
      "train_loss: 0.2531515538085444\n",
      "val_loss: 0.44592015903099047\n",
      "Progress: 10.8% ... Training loss: 0.261 ... Validation loss: 0.453iteration: 1081\n",
      "train_loss: 0.2616170810380609\n",
      "val_loss: 0.45390285093967414\n",
      "Progress: 10.8% ... Training loss: 0.250 ... Validation loss: 0.434iteration: 1082\n",
      "train_loss: 0.25082385578423494\n",
      "val_loss: 0.43489012995744536\n",
      "Progress: 10.8% ... Training loss: 0.250 ... Validation loss: 0.433iteration: 1083\n",
      "train_loss: 0.25034190548104673\n",
      "val_loss: 0.43383324909592635\n",
      "Progress: 10.8% ... Training loss: 0.251 ... Validation loss: 0.442iteration: 1084\n",
      "train_loss: 0.2514888519780928\n",
      "val_loss: 0.44275548136218146\n",
      "Progress: 10.8% ... Training loss: 0.250 ... Validation loss: 0.438iteration: 1085\n",
      "train_loss: 0.2506228193986216\n",
      "val_loss: 0.4387865486357269\n",
      "Progress: 10.9% ... Training loss: 0.258 ... Validation loss: 0.452iteration: 1086\n",
      "train_loss: 0.25886989201959154\n",
      "val_loss: 0.4523721068717653\n",
      "Progress: 10.9% ... Training loss: 0.249 ... Validation loss: 0.439iteration: 1087\n",
      "train_loss: 0.24998631938838417\n",
      "val_loss: 0.4398565928498047\n",
      "Progress: 10.9% ... Training loss: 0.248 ... Validation loss: 0.446iteration: 1088\n",
      "train_loss: 0.24868577369002512\n",
      "val_loss: 0.44651463065911173\n",
      "Progress: 10.9% ... Training loss: 0.252 ... Validation loss: 0.454iteration: 1089\n",
      "train_loss: 0.25271152000029856\n",
      "val_loss: 0.4543498219230721\n",
      "Progress: 10.9% ... Training loss: 0.259 ... Validation loss: 0.455iteration: 1090\n",
      "train_loss: 0.2592385414238262\n",
      "val_loss: 0.45583261771730205\n",
      "Progress: 10.9% ... Training loss: 0.259 ... Validation loss: 0.457iteration: 1091\n",
      "train_loss: 0.2592366684139264\n",
      "val_loss: 0.45731584225854677\n",
      "Progress: 10.9% ... Training loss: 0.258 ... Validation loss: 0.452iteration: 1092\n",
      "train_loss: 0.2582713993383979\n",
      "val_loss: 0.45207198065532406\n",
      "Progress: 10.9% ... Training loss: 0.251 ... Validation loss: 0.448iteration: 1093\n",
      "train_loss: 0.25130835345603936\n",
      "val_loss: 0.44880508655212065\n",
      "Progress: 10.9% ... Training loss: 0.248 ... Validation loss: 0.437iteration: 1094\n",
      "train_loss: 0.24807927731613758\n",
      "val_loss: 0.4370587334079603\n",
      "Progress: 10.9% ... Training loss: 0.249 ... Validation loss: 0.433iteration: 1095\n",
      "train_loss: 0.24999718884322733\n",
      "val_loss: 0.43383835896704076\n",
      "Progress: 11.0% ... Training loss: 0.249 ... Validation loss: 0.434iteration: 1096\n",
      "train_loss: 0.2494851176553609\n",
      "val_loss: 0.43483377868382195\n",
      "Progress: 11.0% ... Training loss: 0.250 ... Validation loss: 0.437iteration: 1097\n",
      "train_loss: 0.25059987829440544\n",
      "val_loss: 0.4375805906026249\n",
      "Progress: 11.0% ... Training loss: 0.249 ... Validation loss: 0.432iteration: 1098\n",
      "train_loss: 0.24941231397588162\n",
      "val_loss: 0.4320753792869905\n",
      "Progress: 11.0% ... Training loss: 0.247 ... Validation loss: 0.435iteration: 1099\n",
      "train_loss: 0.24790432887463912\n",
      "val_loss: 0.43575060171659885\n",
      "Progress: 11.0% ... Training loss: 0.248 ... Validation loss: 0.435iteration: 1100\n",
      "train_loss: 0.24825174678223283\n",
      "val_loss: 0.43592327616869175\n",
      "Progress: 11.0% ... Training loss: 0.248 ... Validation loss: 0.442iteration: 1101\n",
      "train_loss: 0.24872315926059926\n",
      "val_loss: 0.44244933345232795\n",
      "Progress: 11.0% ... Training loss: 0.251 ... Validation loss: 0.439iteration: 1102\n",
      "train_loss: 0.2516485997501729\n",
      "val_loss: 0.439098102919341\n",
      "Progress: 11.0% ... Training loss: 0.247 ... Validation loss: 0.439iteration: 1103\n",
      "train_loss: 0.24765487960457389\n",
      "val_loss: 0.4397226789169231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 11.0% ... Training loss: 0.251 ... Validation loss: 0.446iteration: 1104\n",
      "train_loss: 0.25116310380238777\n",
      "val_loss: 0.4468742802141063\n",
      "Progress: 11.1% ... Training loss: 0.250 ... Validation loss: 0.440iteration: 1105\n",
      "train_loss: 0.25071808297697395\n",
      "val_loss: 0.4401357048307188\n",
      "Progress: 11.1% ... Training loss: 0.247 ... Validation loss: 0.437iteration: 1106\n",
      "train_loss: 0.2477211178111947\n",
      "val_loss: 0.43745802544304085\n",
      "Progress: 11.1% ... Training loss: 0.249 ... Validation loss: 0.435iteration: 1107\n",
      "train_loss: 0.24919514087133146\n",
      "val_loss: 0.43558305196125374\n",
      "Progress: 11.1% ... Training loss: 0.256 ... Validation loss: 0.442iteration: 1108\n",
      "train_loss: 0.25611481996451796\n",
      "val_loss: 0.44215154278494234\n",
      "Progress: 11.1% ... Training loss: 0.291 ... Validation loss: 0.463iteration: 1109\n",
      "train_loss: 0.29135100334031805\n",
      "val_loss: 0.4633072936636668\n",
      "Progress: 11.1% ... Training loss: 0.256 ... Validation loss: 0.449iteration: 1110\n",
      "train_loss: 0.256710497044176\n",
      "val_loss: 0.4496585951407543\n",
      "Progress: 11.1% ... Training loss: 0.248 ... Validation loss: 0.443iteration: 1111\n",
      "train_loss: 0.2488020542858846\n",
      "val_loss: 0.44336759946811066\n",
      "Progress: 11.1% ... Training loss: 0.256 ... Validation loss: 0.452iteration: 1112\n",
      "train_loss: 0.2560538070588501\n",
      "val_loss: 0.452311182201711\n",
      "Progress: 11.1% ... Training loss: 0.252 ... Validation loss: 0.453iteration: 1113\n",
      "train_loss: 0.2520977152592215\n",
      "val_loss: 0.4539167169960017\n",
      "Progress: 11.1% ... Training loss: 0.252 ... Validation loss: 0.455iteration: 1114\n",
      "train_loss: 0.2528768513470164\n",
      "val_loss: 0.45514059121998046\n",
      "Progress: 11.2% ... Training loss: 0.252 ... Validation loss: 0.447iteration: 1115\n",
      "train_loss: 0.2527210342833578\n",
      "val_loss: 0.4472510913995273\n",
      "Progress: 11.2% ... Training loss: 0.252 ... Validation loss: 0.437iteration: 1116\n",
      "train_loss: 0.2529689479297475\n",
      "val_loss: 0.4375569143795565\n",
      "Progress: 11.2% ... Training loss: 0.247 ... Validation loss: 0.433iteration: 1117\n",
      "train_loss: 0.24771868622696433\n",
      "val_loss: 0.43362196323683105\n",
      "Progress: 11.2% ... Training loss: 0.247 ... Validation loss: 0.428iteration: 1118\n",
      "train_loss: 0.2476891570145726\n",
      "val_loss: 0.42859646379602323\n",
      "Progress: 11.2% ... Training loss: 0.246 ... Validation loss: 0.429iteration: 1119\n",
      "train_loss: 0.24699950855689132\n",
      "val_loss: 0.42935962821358215\n",
      "Progress: 11.2% ... Training loss: 0.248 ... Validation loss: 0.435iteration: 1120\n",
      "train_loss: 0.24824123466244477\n",
      "val_loss: 0.43522181179524844\n",
      "Progress: 11.2% ... Training loss: 0.247 ... Validation loss: 0.440iteration: 1121\n",
      "train_loss: 0.24786558325973262\n",
      "val_loss: 0.4401374982172412\n",
      "Progress: 11.2% ... Training loss: 0.252 ... Validation loss: 0.436iteration: 1122\n",
      "train_loss: 0.25228063692528935\n",
      "val_loss: 0.4362138558771695\n",
      "Progress: 11.2% ... Training loss: 0.260 ... Validation loss: 0.446iteration: 1123\n",
      "train_loss: 0.26063126960826377\n",
      "val_loss: 0.4461465025731963\n",
      "Progress: 11.2% ... Training loss: 0.247 ... Validation loss: 0.426iteration: 1124\n",
      "train_loss: 0.24768147045860414\n",
      "val_loss: 0.4266343426028744\n",
      "Progress: 11.2% ... Training loss: 0.249 ... Validation loss: 0.435iteration: 1125\n",
      "train_loss: 0.24993888113432328\n",
      "val_loss: 0.4357196761180898\n",
      "Progress: 11.3% ... Training loss: 0.253 ... Validation loss: 0.439iteration: 1126\n",
      "train_loss: 0.2530649681207513\n",
      "val_loss: 0.4397805534434494\n",
      "Progress: 11.3% ... Training loss: 0.246 ... Validation loss: 0.432iteration: 1127\n",
      "train_loss: 0.24639452653624333\n",
      "val_loss: 0.43285847698606267\n",
      "Progress: 11.3% ... Training loss: 0.247 ... Validation loss: 0.432iteration: 1128\n",
      "train_loss: 0.24728162127972306\n",
      "val_loss: 0.43219178784998685\n",
      "Progress: 11.3% ... Training loss: 0.249 ... Validation loss: 0.434iteration: 1129\n",
      "train_loss: 0.24927014610099887\n",
      "val_loss: 0.4347306403093026\n",
      "Progress: 11.3% ... Training loss: 0.254 ... Validation loss: 0.436iteration: 1130\n",
      "train_loss: 0.25496223396803064\n",
      "val_loss: 0.4367873501714482\n",
      "Progress: 11.3% ... Training loss: 0.254 ... Validation loss: 0.437iteration: 1131\n",
      "train_loss: 0.25452401000646097\n",
      "val_loss: 0.43751267433081803\n",
      "Progress: 11.3% ... Training loss: 0.249 ... Validation loss: 0.435iteration: 1132\n",
      "train_loss: 0.24917703216376683\n",
      "val_loss: 0.43503310170953474\n",
      "Progress: 11.3% ... Training loss: 0.251 ... Validation loss: 0.431iteration: 1133\n",
      "train_loss: 0.2516066985588687\n",
      "val_loss: 0.43199888416106397\n",
      "Progress: 11.3% ... Training loss: 0.247 ... Validation loss: 0.432iteration: 1134\n",
      "train_loss: 0.24794526088106864\n",
      "val_loss: 0.43293856371168365\n",
      "Progress: 11.3% ... Training loss: 0.248 ... Validation loss: 0.432iteration: 1135\n",
      "train_loss: 0.24857390936839127\n",
      "val_loss: 0.4325654592100412\n",
      "Progress: 11.4% ... Training loss: 0.252 ... Validation loss: 0.433iteration: 1136\n",
      "train_loss: 0.2522055097651116\n",
      "val_loss: 0.4331901971405191\n",
      "Progress: 11.4% ... Training loss: 0.252 ... Validation loss: 0.434iteration: 1137\n",
      "train_loss: 0.25247132036010184\n",
      "val_loss: 0.43404124119120197\n",
      "Progress: 11.4% ... Training loss: 0.249 ... Validation loss: 0.425iteration: 1138\n",
      "train_loss: 0.24913002105943505\n",
      "val_loss: 0.42548448847381004\n",
      "Progress: 11.4% ... Training loss: 0.251 ... Validation loss: 0.426iteration: 1139\n",
      "train_loss: 0.2512556959585767\n",
      "val_loss: 0.42604319846079614\n",
      "Progress: 11.4% ... Training loss: 0.250 ... Validation loss: 0.432iteration: 1140\n",
      "train_loss: 0.25046830051289387\n",
      "val_loss: 0.43226984410817676\n",
      "Progress: 11.4% ... Training loss: 0.249 ... Validation loss: 0.428iteration: 1141\n",
      "train_loss: 0.24914635713019492\n",
      "val_loss: 0.4280110827095521\n",
      "Progress: 11.4% ... Training loss: 0.252 ... Validation loss: 0.438iteration: 1142\n",
      "train_loss: 0.25220675288543687\n",
      "val_loss: 0.43873741087232726\n",
      "Progress: 11.4% ... Training loss: 0.247 ... Validation loss: 0.430iteration: 1143\n",
      "train_loss: 0.24714416527201954\n",
      "val_loss: 0.430251639521995\n",
      "Progress: 11.4% ... Training loss: 0.247 ... Validation loss: 0.433iteration: 1144\n",
      "train_loss: 0.24727198271815798\n",
      "val_loss: 0.43335030015126813\n",
      "Progress: 11.4% ... Training loss: 0.245 ... Validation loss: 0.433iteration: 1145\n",
      "train_loss: 0.24582634348451546\n",
      "val_loss: 0.43365536223247075\n",
      "Progress: 11.5% ... Training loss: 0.245 ... Validation loss: 0.433iteration: 1146\n",
      "train_loss: 0.24532084546327113\n",
      "val_loss: 0.43347350595447953\n",
      "Progress: 11.5% ... Training loss: 0.247 ... Validation loss: 0.428iteration: 1147\n",
      "train_loss: 0.24711131083656435\n",
      "val_loss: 0.42873644242607994\n",
      "Progress: 11.5% ... Training loss: 0.245 ... Validation loss: 0.434iteration: 1148\n",
      "train_loss: 0.24562362200599172\n",
      "val_loss: 0.4340653137882856\n",
      "Progress: 11.5% ... Training loss: 0.248 ... Validation loss: 0.444iteration: 1149\n",
      "train_loss: 0.24899337551585718\n",
      "val_loss: 0.4440536987848047\n",
      "Progress: 11.5% ... Training loss: 0.248 ... Validation loss: 0.438iteration: 1150\n",
      "train_loss: 0.24819858041930615\n",
      "val_loss: 0.43897235230293397\n",
      "Progress: 11.5% ... Training loss: 0.245 ... Validation loss: 0.434iteration: 1151\n",
      "train_loss: 0.24578722642827358\n",
      "val_loss: 0.4341261128253644\n",
      "Progress: 11.5% ... Training loss: 0.247 ... Validation loss: 0.441iteration: 1152\n",
      "train_loss: 0.24756267481431837\n",
      "val_loss: 0.44100385164437184\n",
      "Progress: 11.5% ... Training loss: 0.247 ... Validation loss: 0.442iteration: 1153\n",
      "train_loss: 0.2471467915764033\n",
      "val_loss: 0.44269237273645917\n",
      "Progress: 11.5% ... Training loss: 0.245 ... Validation loss: 0.436iteration: 1154\n",
      "train_loss: 0.2456464701726407\n",
      "val_loss: 0.436668522044176\n",
      "Progress: 11.6% ... Training loss: 0.246 ... Validation loss: 0.436iteration: 1155\n",
      "train_loss: 0.24616828532556087\n",
      "val_loss: 0.4367298500879866\n",
      "Progress: 11.6% ... Training loss: 0.250 ... Validation loss: 0.428iteration: 1156\n",
      "train_loss: 0.25089574528227604\n",
      "val_loss: 0.42826421667593767\n",
      "Progress: 11.6% ... Training loss: 0.251 ... Validation loss: 0.438iteration: 1157\n",
      "train_loss: 0.2515567815516282\n",
      "val_loss: 0.4384864635941191\n",
      "Progress: 11.6% ... Training loss: 0.245 ... Validation loss: 0.433iteration: 1158\n",
      "train_loss: 0.24545253539453757\n",
      "val_loss: 0.4339430902516145\n",
      "Progress: 11.6% ... Training loss: 0.245 ... Validation loss: 0.434iteration: 1159\n",
      "train_loss: 0.24537851903215516\n",
      "val_loss: 0.434129273441785\n",
      "Progress: 11.6% ... Training loss: 0.246 ... Validation loss: 0.439iteration: 1160\n",
      "train_loss: 0.24612183545333194\n",
      "val_loss: 0.439727846374889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 11.6% ... Training loss: 0.246 ... Validation loss: 0.433iteration: 1161\n",
      "train_loss: 0.24695849590500837\n",
      "val_loss: 0.4334350338798455\n",
      "Progress: 11.6% ... Training loss: 0.246 ... Validation loss: 0.437iteration: 1162\n",
      "train_loss: 0.24666681520509662\n",
      "val_loss: 0.4379909588799714\n",
      "Progress: 11.6% ... Training loss: 0.247 ... Validation loss: 0.435iteration: 1163\n",
      "train_loss: 0.24791731747950635\n",
      "val_loss: 0.4355560760586993\n",
      "Progress: 11.6% ... Training loss: 0.244 ... Validation loss: 0.429iteration: 1164\n",
      "train_loss: 0.24487044442964423\n",
      "val_loss: 0.4295321322570419\n",
      "Progress: 11.7% ... Training loss: 0.250 ... Validation loss: 0.428iteration: 1165\n",
      "train_loss: 0.2500431889063178\n",
      "val_loss: 0.4283147323414909\n",
      "Progress: 11.7% ... Training loss: 0.269 ... Validation loss: 0.428iteration: 1166\n",
      "train_loss: 0.2695921455150998\n",
      "val_loss: 0.42818142653369345\n",
      "Progress: 11.7% ... Training loss: 0.254 ... Validation loss: 0.422iteration: 1167\n",
      "train_loss: 0.25444040739323454\n",
      "val_loss: 0.4229741469116244\n",
      "Progress: 11.7% ... Training loss: 0.249 ... Validation loss: 0.441iteration: 1168\n",
      "train_loss: 0.24925555362380267\n",
      "val_loss: 0.44192296795851577\n",
      "Progress: 11.7% ... Training loss: 0.255 ... Validation loss: 0.441iteration: 1169\n",
      "train_loss: 0.25596827451537907\n",
      "val_loss: 0.4411488073714753\n",
      "Progress: 11.7% ... Training loss: 0.247 ... Validation loss: 0.439iteration: 1170\n",
      "train_loss: 0.24712131938588197\n",
      "val_loss: 0.43996662978626305\n",
      "Progress: 11.7% ... Training loss: 0.249 ... Validation loss: 0.438iteration: 1171\n",
      "train_loss: 0.24943656260672656\n",
      "val_loss: 0.43823239868743336\n",
      "Progress: 11.7% ... Training loss: 0.252 ... Validation loss: 0.450iteration: 1172\n",
      "train_loss: 0.2522654831552157\n",
      "val_loss: 0.4505979225175635\n",
      "Progress: 11.7% ... Training loss: 0.245 ... Validation loss: 0.437iteration: 1173\n",
      "train_loss: 0.24518454729768513\n",
      "val_loss: 0.43702035857109806\n",
      "Progress: 11.7% ... Training loss: 0.255 ... Validation loss: 0.453iteration: 1174\n",
      "train_loss: 0.255013908422556\n",
      "val_loss: 0.45303430080598045\n",
      "Progress: 11.8% ... Training loss: 0.244 ... Validation loss: 0.435iteration: 1175\n",
      "train_loss: 0.2448045671945078\n",
      "val_loss: 0.4354962517157168\n",
      "Progress: 11.8% ... Training loss: 0.250 ... Validation loss: 0.445iteration: 1176\n",
      "train_loss: 0.2505795230510205\n",
      "val_loss: 0.44583406255456526\n",
      "Progress: 11.8% ... Training loss: 0.250 ... Validation loss: 0.439iteration: 1177\n",
      "train_loss: 0.2505432358721556\n",
      "val_loss: 0.43977458523962315\n",
      "Progress: 11.8% ... Training loss: 0.243 ... Validation loss: 0.433iteration: 1178\n",
      "train_loss: 0.24334972589572795\n",
      "val_loss: 0.43355306452130643\n",
      "Progress: 11.8% ... Training loss: 0.243 ... Validation loss: 0.433iteration: 1179\n",
      "train_loss: 0.24371399404056787\n",
      "val_loss: 0.43395742089121747\n",
      "Progress: 11.8% ... Training loss: 0.251 ... Validation loss: 0.436iteration: 1180\n",
      "train_loss: 0.2519838314000068\n",
      "val_loss: 0.43661260931982565\n",
      "Progress: 11.8% ... Training loss: 0.245 ... Validation loss: 0.424iteration: 1181\n",
      "train_loss: 0.24515083115595843\n",
      "val_loss: 0.42490618788279166\n",
      "Progress: 11.8% ... Training loss: 0.252 ... Validation loss: 0.432iteration: 1182\n",
      "train_loss: 0.252169463491834\n",
      "val_loss: 0.43258620883554744\n",
      "Progress: 11.8% ... Training loss: 0.255 ... Validation loss: 0.438iteration: 1183\n",
      "train_loss: 0.25590843720017814\n",
      "val_loss: 0.43885102604085247\n",
      "Progress: 11.8% ... Training loss: 0.246 ... Validation loss: 0.433iteration: 1184\n",
      "train_loss: 0.24600643370073452\n",
      "val_loss: 0.4332798896817177\n",
      "Progress: 11.8% ... Training loss: 0.248 ... Validation loss: 0.430iteration: 1185\n",
      "train_loss: 0.24859292896941793\n",
      "val_loss: 0.43002587843184176\n",
      "Progress: 11.9% ... Training loss: 0.243 ... Validation loss: 0.432iteration: 1186\n",
      "train_loss: 0.24373641750083794\n",
      "val_loss: 0.43246039770716355\n",
      "Progress: 11.9% ... Training loss: 0.248 ... Validation loss: 0.434iteration: 1187\n",
      "train_loss: 0.24813226221007673\n",
      "val_loss: 0.4345357376474807\n",
      "Progress: 11.9% ... Training loss: 0.251 ... Validation loss: 0.451iteration: 1188\n",
      "train_loss: 0.2517338337224225\n",
      "val_loss: 0.45121351574054086\n",
      "Progress: 11.9% ... Training loss: 0.244 ... Validation loss: 0.431iteration: 1189\n",
      "train_loss: 0.24417476701595453\n",
      "val_loss: 0.43110859795928724\n",
      "Progress: 11.9% ... Training loss: 0.249 ... Validation loss: 0.432iteration: 1190\n",
      "train_loss: 0.24952696981928527\n",
      "val_loss: 0.43292554224101737\n",
      "Progress: 11.9% ... Training loss: 0.243 ... Validation loss: 0.430iteration: 1191\n",
      "train_loss: 0.24364482404670992\n",
      "val_loss: 0.43004610772061763\n",
      "Progress: 11.9% ... Training loss: 0.242 ... Validation loss: 0.431iteration: 1192\n",
      "train_loss: 0.2421634137440332\n",
      "val_loss: 0.43107278339207983\n",
      "Progress: 11.9% ... Training loss: 0.246 ... Validation loss: 0.426iteration: 1193\n",
      "train_loss: 0.24655931494241737\n",
      "val_loss: 0.42678529215295163\n",
      "Progress: 11.9% ... Training loss: 0.247 ... Validation loss: 0.435iteration: 1194\n",
      "train_loss: 0.2474877280501465\n",
      "val_loss: 0.4356916644257745\n",
      "Progress: 11.9% ... Training loss: 0.243 ... Validation loss: 0.439iteration: 1195\n",
      "train_loss: 0.24398973834118715\n",
      "val_loss: 0.4393482659508463\n",
      "Progress: 12.0% ... Training loss: 0.244 ... Validation loss: 0.432iteration: 1196\n",
      "train_loss: 0.24452917170931238\n",
      "val_loss: 0.43284645053125087\n",
      "Progress: 12.0% ... Training loss: 0.246 ... Validation loss: 0.437iteration: 1197\n",
      "train_loss: 0.2466291954027941\n",
      "val_loss: 0.43708297456451684\n",
      "Progress: 12.0% ... Training loss: 0.248 ... Validation loss: 0.434iteration: 1198\n",
      "train_loss: 0.24860322646887698\n",
      "val_loss: 0.43460234669921993\n",
      "Progress: 12.0% ... Training loss: 0.256 ... Validation loss: 0.444iteration: 1199\n",
      "train_loss: 0.2561016633863774\n",
      "val_loss: 0.44495146191499996\n",
      "Progress: 12.0% ... Training loss: 0.246 ... Validation loss: 0.446iteration: 1200\n",
      "train_loss: 0.24621416638462798\n",
      "val_loss: 0.44611665304589615\n",
      "Progress: 12.0% ... Training loss: 0.264 ... Validation loss: 0.451iteration: 1201\n",
      "train_loss: 0.26426453860909854\n",
      "val_loss: 0.4511524772712611\n",
      "Progress: 12.0% ... Training loss: 0.245 ... Validation loss: 0.447iteration: 1202\n",
      "train_loss: 0.24562769846745683\n",
      "val_loss: 0.4470945099712694\n",
      "Progress: 12.0% ... Training loss: 0.251 ... Validation loss: 0.455iteration: 1203\n",
      "train_loss: 0.2518477346170586\n",
      "val_loss: 0.45523172246585913\n",
      "Progress: 12.0% ... Training loss: 0.272 ... Validation loss: 0.475iteration: 1204\n",
      "train_loss: 0.2722699133052982\n",
      "val_loss: 0.4759155890542603\n",
      "Progress: 12.1% ... Training loss: 0.247 ... Validation loss: 0.439iteration: 1205\n",
      "train_loss: 0.2476746495471823\n",
      "val_loss: 0.43912100878253996\n",
      "Progress: 12.1% ... Training loss: 0.247 ... Validation loss: 0.442iteration: 1206\n",
      "train_loss: 0.2479335937409926\n",
      "val_loss: 0.44237534594104805\n",
      "Progress: 12.1% ... Training loss: 0.246 ... Validation loss: 0.438iteration: 1207\n",
      "train_loss: 0.2466060015600364\n",
      "val_loss: 0.43814581723124457\n",
      "Progress: 12.1% ... Training loss: 0.243 ... Validation loss: 0.440iteration: 1208\n",
      "train_loss: 0.2432500243750692\n",
      "val_loss: 0.44049005940928526\n",
      "Progress: 12.1% ... Training loss: 0.251 ... Validation loss: 0.447iteration: 1209\n",
      "train_loss: 0.2515927214228359\n",
      "val_loss: 0.4471328499458538\n",
      "Progress: 12.1% ... Training loss: 0.250 ... Validation loss: 0.451iteration: 1210\n",
      "train_loss: 0.2500840137401996\n",
      "val_loss: 0.45135816263712025\n",
      "Progress: 12.1% ... Training loss: 0.243 ... Validation loss: 0.435iteration: 1211\n",
      "train_loss: 0.24361665498315113\n",
      "val_loss: 0.4352731760335729\n",
      "Progress: 12.1% ... Training loss: 0.246 ... Validation loss: 0.446iteration: 1212\n",
      "train_loss: 0.24694020422414006\n",
      "val_loss: 0.4465953055391514\n",
      "Progress: 12.1% ... Training loss: 0.242 ... Validation loss: 0.440iteration: 1213\n",
      "train_loss: 0.24268938296057527\n",
      "val_loss: 0.44083620558781944\n",
      "Progress: 12.1% ... Training loss: 0.246 ... Validation loss: 0.438iteration: 1214\n",
      "train_loss: 0.24670513578586772\n",
      "val_loss: 0.4388632710688935\n",
      "Progress: 12.2% ... Training loss: 0.242 ... Validation loss: 0.441iteration: 1215\n",
      "train_loss: 0.2425668058381136\n",
      "val_loss: 0.4415274024040865\n",
      "Progress: 12.2% ... Training loss: 0.242 ... Validation loss: 0.439iteration: 1216\n",
      "train_loss: 0.24227753601529978\n",
      "val_loss: 0.43995637761577217\n",
      "Progress: 12.2% ... Training loss: 0.247 ... Validation loss: 0.455iteration: 1217\n",
      "train_loss: 0.24799608246703886\n",
      "val_loss: 0.4559301337993293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 12.2% ... Training loss: 0.247 ... Validation loss: 0.451iteration: 1218\n",
      "train_loss: 0.24756727657507255\n",
      "val_loss: 0.45106695313834144\n",
      "Progress: 12.2% ... Training loss: 0.249 ... Validation loss: 0.449iteration: 1219\n",
      "train_loss: 0.24975454781307455\n",
      "val_loss: 0.4499746498742511\n",
      "Progress: 12.2% ... Training loss: 0.241 ... Validation loss: 0.435iteration: 1220\n",
      "train_loss: 0.24143024293431126\n",
      "val_loss: 0.43556645058419546\n",
      "Progress: 12.2% ... Training loss: 0.241 ... Validation loss: 0.431iteration: 1221\n",
      "train_loss: 0.24154791293095482\n",
      "val_loss: 0.431511178571644\n",
      "Progress: 12.2% ... Training loss: 0.241 ... Validation loss: 0.430iteration: 1222\n",
      "train_loss: 0.24153175424166362\n",
      "val_loss: 0.43009002613457487\n",
      "Progress: 12.2% ... Training loss: 0.241 ... Validation loss: 0.427iteration: 1223\n",
      "train_loss: 0.24159702092860585\n",
      "val_loss: 0.4273563658361926\n",
      "Progress: 12.2% ... Training loss: 0.240 ... Validation loss: 0.427iteration: 1224\n",
      "train_loss: 0.24087705678719482\n",
      "val_loss: 0.4274576626311175\n",
      "Progress: 12.2% ... Training loss: 0.241 ... Validation loss: 0.425iteration: 1225\n",
      "train_loss: 0.24163073584411388\n",
      "val_loss: 0.4258114931533878\n",
      "Progress: 12.3% ... Training loss: 0.242 ... Validation loss: 0.424iteration: 1226\n",
      "train_loss: 0.24266739840512255\n",
      "val_loss: 0.42476884877094756\n",
      "Progress: 12.3% ... Training loss: 0.241 ... Validation loss: 0.422iteration: 1227\n",
      "train_loss: 0.24146786026960548\n",
      "val_loss: 0.4223596777917309\n",
      "Progress: 12.3% ... Training loss: 0.241 ... Validation loss: 0.423iteration: 1228\n",
      "train_loss: 0.24108737309851577\n",
      "val_loss: 0.4239469639386884\n",
      "Progress: 12.3% ... Training loss: 0.258 ... Validation loss: 0.426iteration: 1229\n",
      "train_loss: 0.25867294003799407\n",
      "val_loss: 0.4263118390317148\n",
      "Progress: 12.3% ... Training loss: 0.243 ... Validation loss: 0.422iteration: 1230\n",
      "train_loss: 0.24373040214556513\n",
      "val_loss: 0.4229195840543399\n",
      "Progress: 12.3% ... Training loss: 0.241 ... Validation loss: 0.432iteration: 1231\n",
      "train_loss: 0.24193265264642366\n",
      "val_loss: 0.4320328360042093\n",
      "Progress: 12.3% ... Training loss: 0.244 ... Validation loss: 0.436iteration: 1232\n",
      "train_loss: 0.24478777813911007\n",
      "val_loss: 0.4369647097834338\n",
      "Progress: 12.3% ... Training loss: 0.241 ... Validation loss: 0.427iteration: 1233\n",
      "train_loss: 0.24124055650323703\n",
      "val_loss: 0.42734571871125504\n",
      "Progress: 12.3% ... Training loss: 0.262 ... Validation loss: 0.459iteration: 1234\n",
      "train_loss: 0.26211740875568473\n",
      "val_loss: 0.45951871600842187\n",
      "Progress: 12.3% ... Training loss: 0.245 ... Validation loss: 0.443iteration: 1235\n",
      "train_loss: 0.24599975168666566\n",
      "val_loss: 0.443460755506875\n",
      "Progress: 12.4% ... Training loss: 0.243 ... Validation loss: 0.436iteration: 1236\n",
      "train_loss: 0.24363268715007666\n",
      "val_loss: 0.4364968688823678\n",
      "Progress: 12.4% ... Training loss: 0.241 ... Validation loss: 0.440iteration: 1237\n",
      "train_loss: 0.24152426190757212\n",
      "val_loss: 0.44043909736615566\n",
      "Progress: 12.4% ... Training loss: 0.248 ... Validation loss: 0.453iteration: 1238\n",
      "train_loss: 0.2489303945546469\n",
      "val_loss: 0.4530294707481649\n",
      "Progress: 12.4% ... Training loss: 0.252 ... Validation loss: 0.462iteration: 1239\n",
      "train_loss: 0.2529447628059973\n",
      "val_loss: 0.46248091638093763\n",
      "Progress: 12.4% ... Training loss: 0.240 ... Validation loss: 0.438iteration: 1240\n",
      "train_loss: 0.2401247385428417\n",
      "val_loss: 0.43880939370121536\n",
      "Progress: 12.4% ... Training loss: 0.242 ... Validation loss: 0.439iteration: 1241\n",
      "train_loss: 0.24287416851830437\n",
      "val_loss: 0.4399568102636334\n",
      "Progress: 12.4% ... Training loss: 0.240 ... Validation loss: 0.443iteration: 1242\n",
      "train_loss: 0.24037254104388053\n",
      "val_loss: 0.4438451713634254\n",
      "Progress: 12.4% ... Training loss: 0.241 ... Validation loss: 0.438iteration: 1243\n",
      "train_loss: 0.24113948546795927\n",
      "val_loss: 0.43818424775075354\n",
      "Progress: 12.4% ... Training loss: 0.250 ... Validation loss: 0.453iteration: 1244\n",
      "train_loss: 0.2501426080767951\n",
      "val_loss: 0.45356733139771455\n",
      "Progress: 12.4% ... Training loss: 0.242 ... Validation loss: 0.448iteration: 1245\n",
      "train_loss: 0.24283108283675886\n",
      "val_loss: 0.4480876080704234\n",
      "Progress: 12.5% ... Training loss: 0.241 ... Validation loss: 0.433iteration: 1246\n",
      "train_loss: 0.24140088936584325\n",
      "val_loss: 0.43345301003590514\n",
      "Progress: 12.5% ... Training loss: 0.240 ... Validation loss: 0.442iteration: 1247\n",
      "train_loss: 0.24046388810958963\n",
      "val_loss: 0.4427444333444544\n",
      "Progress: 12.5% ... Training loss: 0.243 ... Validation loss: 0.440iteration: 1248\n",
      "train_loss: 0.2431423632651495\n",
      "val_loss: 0.44093913948398406\n",
      "Progress: 12.5% ... Training loss: 0.239 ... Validation loss: 0.437iteration: 1249\n",
      "train_loss: 0.2396065581687799\n",
      "val_loss: 0.4376195262553296\n",
      "Progress: 12.5% ... Training loss: 0.241 ... Validation loss: 0.442iteration: 1250\n",
      "train_loss: 0.24112107128928664\n",
      "val_loss: 0.4421390731436587\n",
      "Progress: 12.5% ... Training loss: 0.241 ... Validation loss: 0.434iteration: 1251\n",
      "train_loss: 0.24124608445682486\n",
      "val_loss: 0.4346631629931666\n",
      "Progress: 12.5% ... Training loss: 0.239 ... Validation loss: 0.435iteration: 1252\n",
      "train_loss: 0.23921095718276125\n",
      "val_loss: 0.43520859888063956\n",
      "Progress: 12.5% ... Training loss: 0.239 ... Validation loss: 0.438iteration: 1253\n",
      "train_loss: 0.2397350143423505\n",
      "val_loss: 0.43856477476919503\n",
      "Progress: 12.5% ... Training loss: 0.245 ... Validation loss: 0.437iteration: 1254\n",
      "train_loss: 0.24502667929927238\n",
      "val_loss: 0.4373337696297216\n",
      "Progress: 12.6% ... Training loss: 0.259 ... Validation loss: 0.459iteration: 1255\n",
      "train_loss: 0.2594009046302629\n",
      "val_loss: 0.45902241958530726\n",
      "Progress: 12.6% ... Training loss: 0.243 ... Validation loss: 0.435iteration: 1256\n",
      "train_loss: 0.24306499556137948\n",
      "val_loss: 0.43548459515611576\n",
      "Progress: 12.6% ... Training loss: 0.239 ... Validation loss: 0.435iteration: 1257\n",
      "train_loss: 0.2393081968684154\n",
      "val_loss: 0.43543986299953574\n",
      "Progress: 12.6% ... Training loss: 0.247 ... Validation loss: 0.439iteration: 1258\n",
      "train_loss: 0.24771097554222202\n",
      "val_loss: 0.4398831413371099\n",
      "Progress: 12.6% ... Training loss: 0.239 ... Validation loss: 0.436iteration: 1259\n",
      "train_loss: 0.2394203689085975\n",
      "val_loss: 0.4365179799606063\n",
      "Progress: 12.6% ... Training loss: 0.245 ... Validation loss: 0.444iteration: 1260\n",
      "train_loss: 0.24557122821922484\n",
      "val_loss: 0.44481748032481017\n",
      "Progress: 12.6% ... Training loss: 0.238 ... Validation loss: 0.433iteration: 1261\n",
      "train_loss: 0.23872654779387553\n",
      "val_loss: 0.4335819378163265\n",
      "Progress: 12.6% ... Training loss: 0.240 ... Validation loss: 0.435iteration: 1262\n",
      "train_loss: 0.24012959377360213\n",
      "val_loss: 0.4350957419564355\n",
      "Progress: 12.6% ... Training loss: 0.253 ... Validation loss: 0.443iteration: 1263\n",
      "train_loss: 0.25315835689546473\n",
      "val_loss: 0.4437542534274232\n",
      "Progress: 12.6% ... Training loss: 0.238 ... Validation loss: 0.429iteration: 1264\n",
      "train_loss: 0.23894448317308387\n",
      "val_loss: 0.4294010725136507\n",
      "Progress: 12.7% ... Training loss: 0.240 ... Validation loss: 0.433iteration: 1265\n",
      "train_loss: 0.2403201744251651\n",
      "val_loss: 0.433863312377512\n",
      "Progress: 12.7% ... Training loss: 0.240 ... Validation loss: 0.425iteration: 1266\n",
      "train_loss: 0.2402325677629567\n",
      "val_loss: 0.4259985214283972\n",
      "Progress: 12.7% ... Training loss: 0.240 ... Validation loss: 0.432iteration: 1267\n",
      "train_loss: 0.24056732264552522\n",
      "val_loss: 0.43202606894870704\n",
      "Progress: 12.7% ... Training loss: 0.237 ... Validation loss: 0.430iteration: 1268\n",
      "train_loss: 0.23790517873720246\n",
      "val_loss: 0.4307244977425376\n",
      "Progress: 12.7% ... Training loss: 0.239 ... Validation loss: 0.432iteration: 1269\n",
      "train_loss: 0.2390132591542976\n",
      "val_loss: 0.43207680586595926\n",
      "Progress: 12.7% ... Training loss: 0.246 ... Validation loss: 0.432iteration: 1270\n",
      "train_loss: 0.24620286294473703\n",
      "val_loss: 0.43264959952762205\n",
      "Progress: 12.7% ... Training loss: 0.251 ... Validation loss: 0.446iteration: 1271\n",
      "train_loss: 0.251625163616956\n",
      "val_loss: 0.44646023217962305\n",
      "Progress: 12.7% ... Training loss: 0.239 ... Validation loss: 0.427iteration: 1272\n",
      "train_loss: 0.23906975263890518\n",
      "val_loss: 0.42762824673307104\n",
      "Progress: 12.7% ... Training loss: 0.242 ... Validation loss: 0.433iteration: 1273\n",
      "train_loss: 0.24290237020733332\n",
      "val_loss: 0.4330069228160099\n",
      "Progress: 12.7% ... Training loss: 0.242 ... Validation loss: 0.432iteration: 1274\n",
      "train_loss: 0.24202319802613234\n",
      "val_loss: 0.4321487413505096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 12.8% ... Training loss: 0.240 ... Validation loss: 0.435iteration: 1275\n",
      "train_loss: 0.24028613395707238\n",
      "val_loss: 0.43508850392240767\n",
      "Progress: 12.8% ... Training loss: 0.251 ... Validation loss: 0.446iteration: 1276\n",
      "train_loss: 0.2511922669916092\n",
      "val_loss: 0.44696323876073835\n",
      "Progress: 12.8% ... Training loss: 0.243 ... Validation loss: 0.445iteration: 1277\n",
      "train_loss: 0.2434200815567963\n",
      "val_loss: 0.44549328438156616\n",
      "Progress: 12.8% ... Training loss: 0.241 ... Validation loss: 0.437iteration: 1278\n",
      "train_loss: 0.24196840973191291\n",
      "val_loss: 0.43715810012592227\n",
      "Progress: 12.8% ... Training loss: 0.238 ... Validation loss: 0.431iteration: 1279\n",
      "train_loss: 0.23832741514710598\n",
      "val_loss: 0.4318609726259496\n",
      "Progress: 12.8% ... Training loss: 0.237 ... Validation loss: 0.430iteration: 1280\n",
      "train_loss: 0.23783842955105\n",
      "val_loss: 0.4302964878296025\n",
      "Progress: 12.8% ... Training loss: 0.244 ... Validation loss: 0.443iteration: 1281\n",
      "train_loss: 0.24463140792348168\n",
      "val_loss: 0.4439757231783327\n",
      "Progress: 12.8% ... Training loss: 0.244 ... Validation loss: 0.432iteration: 1282\n",
      "train_loss: 0.24484307971178226\n",
      "val_loss: 0.43258471341125126\n",
      "Progress: 12.8% ... Training loss: 0.254 ... Validation loss: 0.456iteration: 1283\n",
      "train_loss: 0.2541376397457244\n",
      "val_loss: 0.45679743825367736\n",
      "Progress: 12.8% ... Training loss: 0.244 ... Validation loss: 0.435iteration: 1284\n",
      "train_loss: 0.24431093466976148\n",
      "val_loss: 0.43546119556832696\n",
      "Progress: 12.8% ... Training loss: 0.238 ... Validation loss: 0.425iteration: 1285\n",
      "train_loss: 0.238547397502844\n",
      "val_loss: 0.4256015274479804\n",
      "Progress: 12.9% ... Training loss: 0.237 ... Validation loss: 0.428iteration: 1286\n",
      "train_loss: 0.23786304083378426\n",
      "val_loss: 0.4280068194158528\n",
      "Progress: 12.9% ... Training loss: 0.246 ... Validation loss: 0.424iteration: 1287\n",
      "train_loss: 0.24668036902233345\n",
      "val_loss: 0.4241391551346006\n",
      "Progress: 12.9% ... Training loss: 0.242 ... Validation loss: 0.419iteration: 1288\n",
      "train_loss: 0.2423165478451159\n",
      "val_loss: 0.41942886752463393\n",
      "Progress: 12.9% ... Training loss: 0.238 ... Validation loss: 0.419iteration: 1289\n",
      "train_loss: 0.2389393388066145\n",
      "val_loss: 0.419848225984862\n",
      "Progress: 12.9% ... Training loss: 0.237 ... Validation loss: 0.424iteration: 1290\n",
      "train_loss: 0.23765037899683986\n",
      "val_loss: 0.42422113797301125\n",
      "Progress: 12.9% ... Training loss: 0.238 ... Validation loss: 0.427iteration: 1291\n",
      "train_loss: 0.2382541780674737\n",
      "val_loss: 0.42745197425162285\n",
      "Progress: 12.9% ... Training loss: 0.239 ... Validation loss: 0.419iteration: 1292\n",
      "train_loss: 0.2390645819346259\n",
      "val_loss: 0.41954863454384334\n",
      "Progress: 12.9% ... Training loss: 0.243 ... Validation loss: 0.436iteration: 1293\n",
      "train_loss: 0.24334977984093062\n",
      "val_loss: 0.4363712847790467\n",
      "Progress: 12.9% ... Training loss: 0.238 ... Validation loss: 0.426iteration: 1294\n",
      "train_loss: 0.2385011352246545\n",
      "val_loss: 0.4266503820055768\n",
      "Progress: 12.9% ... Training loss: 0.236 ... Validation loss: 0.420iteration: 1295\n",
      "train_loss: 0.2369541648621477\n",
      "val_loss: 0.42017093074920525\n",
      "Progress: 13.0% ... Training loss: 0.236 ... Validation loss: 0.425iteration: 1296\n",
      "train_loss: 0.23632763655294697\n",
      "val_loss: 0.4253144136070391\n",
      "Progress: 13.0% ... Training loss: 0.238 ... Validation loss: 0.419iteration: 1297\n",
      "train_loss: 0.23806150982308608\n",
      "val_loss: 0.4195362334043464\n",
      "Progress: 13.0% ... Training loss: 0.238 ... Validation loss: 0.422iteration: 1298\n",
      "train_loss: 0.23833038755265226\n",
      "val_loss: 0.4225611866868257\n",
      "Progress: 13.0% ... Training loss: 0.246 ... Validation loss: 0.423iteration: 1299\n",
      "train_loss: 0.24674577899569883\n",
      "val_loss: 0.42333137494748185\n",
      "Progress: 13.0% ... Training loss: 0.236 ... Validation loss: 0.418iteration: 1300\n",
      "train_loss: 0.23686018083920393\n",
      "val_loss: 0.4185735913421014\n",
      "Progress: 13.0% ... Training loss: 0.237 ... Validation loss: 0.420iteration: 1301\n",
      "train_loss: 0.23769667620943666\n",
      "val_loss: 0.4208710976314531\n",
      "Progress: 13.0% ... Training loss: 0.239 ... Validation loss: 0.429iteration: 1302\n",
      "train_loss: 0.23969025676669437\n",
      "val_loss: 0.42918680186360836\n",
      "Progress: 13.0% ... Training loss: 0.241 ... Validation loss: 0.428iteration: 1303\n",
      "train_loss: 0.2418559753560101\n",
      "val_loss: 0.42889787347841984\n",
      "Progress: 13.0% ... Training loss: 0.236 ... Validation loss: 0.417iteration: 1304\n",
      "train_loss: 0.2366910868206882\n",
      "val_loss: 0.4172279970132152\n",
      "Progress: 13.1% ... Training loss: 0.248 ... Validation loss: 0.436iteration: 1305\n",
      "train_loss: 0.2484001682317974\n",
      "val_loss: 0.43609993080636605\n",
      "Progress: 13.1% ... Training loss: 0.236 ... Validation loss: 0.420iteration: 1306\n",
      "train_loss: 0.2365323481434908\n",
      "val_loss: 0.42053401094098997\n",
      "Progress: 13.1% ... Training loss: 0.243 ... Validation loss: 0.426iteration: 1307\n",
      "train_loss: 0.24381232581311438\n",
      "val_loss: 0.4262747904968347\n",
      "Progress: 13.1% ... Training loss: 0.251 ... Validation loss: 0.438iteration: 1308\n",
      "train_loss: 0.2518543465377333\n",
      "val_loss: 0.4389497688584424\n",
      "Progress: 13.1% ... Training loss: 0.241 ... Validation loss: 0.428iteration: 1309\n",
      "train_loss: 0.2417123935754572\n",
      "val_loss: 0.42899414025978394\n",
      "Progress: 13.1% ... Training loss: 0.236 ... Validation loss: 0.424iteration: 1310\n",
      "train_loss: 0.23639771539963836\n",
      "val_loss: 0.42400130331598146\n",
      "Progress: 13.1% ... Training loss: 0.235 ... Validation loss: 0.417iteration: 1311\n",
      "train_loss: 0.23562827557954302\n",
      "val_loss: 0.4172212517654954\n",
      "Progress: 13.1% ... Training loss: 0.236 ... Validation loss: 0.421iteration: 1312\n",
      "train_loss: 0.2364509253178465\n",
      "val_loss: 0.4215558122306384\n",
      "Progress: 13.1% ... Training loss: 0.240 ... Validation loss: 0.428iteration: 1313\n",
      "train_loss: 0.2404773059428058\n",
      "val_loss: 0.4288405827843489\n",
      "Progress: 13.1% ... Training loss: 0.240 ... Validation loss: 0.436iteration: 1314\n",
      "train_loss: 0.240126375614574\n",
      "val_loss: 0.4369952361015112\n",
      "Progress: 13.2% ... Training loss: 0.238 ... Validation loss: 0.424iteration: 1315\n",
      "train_loss: 0.23863709394027366\n",
      "val_loss: 0.42430174675606913\n",
      "Progress: 13.2% ... Training loss: 0.239 ... Validation loss: 0.424iteration: 1316\n",
      "train_loss: 0.23929497729527155\n",
      "val_loss: 0.42489068039550404\n",
      "Progress: 13.2% ... Training loss: 0.236 ... Validation loss: 0.418iteration: 1317\n",
      "train_loss: 0.236803870448567\n",
      "val_loss: 0.4189036660605272\n",
      "Progress: 13.2% ... Training loss: 0.246 ... Validation loss: 0.417iteration: 1318\n",
      "train_loss: 0.24699428401059204\n",
      "val_loss: 0.41721861848646363\n",
      "Progress: 13.2% ... Training loss: 0.290 ... Validation loss: 0.465iteration: 1319\n",
      "train_loss: 0.2905246357798436\n",
      "val_loss: 0.465413746916701\n",
      "Progress: 13.2% ... Training loss: 0.260 ... Validation loss: 0.447iteration: 1320\n",
      "train_loss: 0.260962507792131\n",
      "val_loss: 0.44731725977641074\n",
      "Progress: 13.2% ... Training loss: 0.239 ... Validation loss: 0.420iteration: 1321\n",
      "train_loss: 0.23983852137715617\n",
      "val_loss: 0.42074964342487003\n",
      "Progress: 13.2% ... Training loss: 0.236 ... Validation loss: 0.417iteration: 1322\n",
      "train_loss: 0.2360939873395353\n",
      "val_loss: 0.4175611475992818\n",
      "Progress: 13.2% ... Training loss: 0.238 ... Validation loss: 0.427iteration: 1323\n",
      "train_loss: 0.2383225887838446\n",
      "val_loss: 0.42729552537304427\n",
      "Progress: 13.2% ... Training loss: 0.235 ... Validation loss: 0.423iteration: 1324\n",
      "train_loss: 0.23514062847058442\n",
      "val_loss: 0.42300441807267364\n",
      "Progress: 13.2% ... Training loss: 0.238 ... Validation loss: 0.427iteration: 1325\n",
      "train_loss: 0.2383308857836276\n",
      "val_loss: 0.4273609257587828\n",
      "Progress: 13.3% ... Training loss: 0.253 ... Validation loss: 0.435iteration: 1326\n",
      "train_loss: 0.25324813131334356\n",
      "val_loss: 0.4354680497460008\n",
      "Progress: 13.3% ... Training loss: 0.252 ... Validation loss: 0.441iteration: 1327\n",
      "train_loss: 0.2522518887656905\n",
      "val_loss: 0.4417495630506099\n",
      "Progress: 13.3% ... Training loss: 0.237 ... Validation loss: 0.430iteration: 1328\n",
      "train_loss: 0.23783404597573785\n",
      "val_loss: 0.4308965088389174\n",
      "Progress: 13.3% ... Training loss: 0.237 ... Validation loss: 0.433iteration: 1329\n",
      "train_loss: 0.2373172027206814\n",
      "val_loss: 0.4336644156220452\n",
      "Progress: 13.3% ... Training loss: 0.235 ... Validation loss: 0.425iteration: 1330\n",
      "train_loss: 0.23590276817270064\n",
      "val_loss: 0.4254553620284566\n",
      "Progress: 13.3% ... Training loss: 0.237 ... Validation loss: 0.422iteration: 1331\n",
      "train_loss: 0.23752284929738007\n",
      "val_loss: 0.42244231887955275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 13.3% ... Training loss: 0.236 ... Validation loss: 0.424iteration: 1332\n",
      "train_loss: 0.23662808845833677\n",
      "val_loss: 0.42461691926764805\n",
      "Progress: 13.3% ... Training loss: 0.235 ... Validation loss: 0.423iteration: 1333\n",
      "train_loss: 0.23533120696526855\n",
      "val_loss: 0.42301884558495984\n",
      "Progress: 13.3% ... Training loss: 0.234 ... Validation loss: 0.422iteration: 1334\n",
      "train_loss: 0.23428881638077792\n",
      "val_loss: 0.4223213285654927\n",
      "Progress: 13.3% ... Training loss: 0.242 ... Validation loss: 0.425iteration: 1335\n",
      "train_loss: 0.2422246718940994\n",
      "val_loss: 0.4253505188847579\n",
      "Progress: 13.4% ... Training loss: 0.236 ... Validation loss: 0.424iteration: 1336\n",
      "train_loss: 0.23612868673789633\n",
      "val_loss: 0.42471046056082107\n",
      "Progress: 13.4% ... Training loss: 0.238 ... Validation loss: 0.431iteration: 1337\n",
      "train_loss: 0.23834863039810092\n",
      "val_loss: 0.43175658718170035\n",
      "Progress: 13.4% ... Training loss: 0.234 ... Validation loss: 0.419iteration: 1338\n",
      "train_loss: 0.2346507915088417\n",
      "val_loss: 0.4194735312861643\n",
      "Progress: 13.4% ... Training loss: 0.236 ... Validation loss: 0.420iteration: 1339\n",
      "train_loss: 0.23640735104027583\n",
      "val_loss: 0.4200153731310973\n",
      "Progress: 13.4% ... Training loss: 0.238 ... Validation loss: 0.421iteration: 1340\n",
      "train_loss: 0.23834541783959035\n",
      "val_loss: 0.42187524936725734\n",
      "Progress: 13.4% ... Training loss: 0.238 ... Validation loss: 0.422iteration: 1341\n",
      "train_loss: 0.23870397115549516\n",
      "val_loss: 0.422597860811928\n",
      "Progress: 13.4% ... Training loss: 0.239 ... Validation loss: 0.432iteration: 1342\n",
      "train_loss: 0.239630014880576\n",
      "val_loss: 0.4320733679345515\n",
      "Progress: 13.4% ... Training loss: 0.234 ... Validation loss: 0.419iteration: 1343\n",
      "train_loss: 0.23430358530566178\n",
      "val_loss: 0.4197706126041254\n",
      "Progress: 13.4% ... Training loss: 0.233 ... Validation loss: 0.416iteration: 1344\n",
      "train_loss: 0.23337312654734346\n",
      "val_loss: 0.4163964274649044\n",
      "Progress: 13.4% ... Training loss: 0.232 ... Validation loss: 0.421iteration: 1345\n",
      "train_loss: 0.23228365135828302\n",
      "val_loss: 0.4218900964966284\n",
      "Progress: 13.5% ... Training loss: 0.239 ... Validation loss: 0.421iteration: 1346\n",
      "train_loss: 0.23907127367216174\n",
      "val_loss: 0.4215949935816442\n",
      "Progress: 13.5% ... Training loss: 0.237 ... Validation loss: 0.421iteration: 1347\n",
      "train_loss: 0.2377894645630356\n",
      "val_loss: 0.42194449602490824\n",
      "Progress: 13.5% ... Training loss: 0.235 ... Validation loss: 0.424iteration: 1348\n",
      "train_loss: 0.23567422829550116\n",
      "val_loss: 0.4246625288484119\n",
      "Progress: 13.5% ... Training loss: 0.242 ... Validation loss: 0.431iteration: 1349\n",
      "train_loss: 0.2426525531557448\n",
      "val_loss: 0.43143374122439426\n",
      "Progress: 13.5% ... Training loss: 0.245 ... Validation loss: 0.424iteration: 1350\n",
      "train_loss: 0.24544880874136013\n",
      "val_loss: 0.4246365196276367\n",
      "Progress: 13.5% ... Training loss: 0.233 ... Validation loss: 0.424iteration: 1351\n",
      "train_loss: 0.2336706087239279\n",
      "val_loss: 0.42420147565402133\n",
      "Progress: 13.5% ... Training loss: 0.236 ... Validation loss: 0.434iteration: 1352\n",
      "train_loss: 0.236078331583479\n",
      "val_loss: 0.4349084578899535\n",
      "Progress: 13.5% ... Training loss: 0.236 ... Validation loss: 0.424iteration: 1353\n",
      "train_loss: 0.23648032753352138\n",
      "val_loss: 0.4246148441709306\n",
      "Progress: 13.5% ... Training loss: 0.237 ... Validation loss: 0.425iteration: 1354\n",
      "train_loss: 0.23718667549847328\n",
      "val_loss: 0.42536082310550083\n",
      "Progress: 13.6% ... Training loss: 0.237 ... Validation loss: 0.425iteration: 1355\n",
      "train_loss: 0.23759844674710026\n",
      "val_loss: 0.42563125862112217\n",
      "Progress: 13.6% ... Training loss: 0.239 ... Validation loss: 0.433iteration: 1356\n",
      "train_loss: 0.23991559883279817\n",
      "val_loss: 0.43337015541325175\n",
      "Progress: 13.6% ... Training loss: 0.233 ... Validation loss: 0.421iteration: 1357\n",
      "train_loss: 0.23342477664105873\n",
      "val_loss: 0.4211573571762384\n",
      "Progress: 13.6% ... Training loss: 0.234 ... Validation loss: 0.425iteration: 1358\n",
      "train_loss: 0.23441553923832623\n",
      "val_loss: 0.425176854961047\n",
      "Progress: 13.6% ... Training loss: 0.232 ... Validation loss: 0.422iteration: 1359\n",
      "train_loss: 0.23250369265241877\n",
      "val_loss: 0.42299248665264333\n",
      "Progress: 13.6% ... Training loss: 0.234 ... Validation loss: 0.425iteration: 1360\n",
      "train_loss: 0.23485731428334963\n",
      "val_loss: 0.4254217315997749\n",
      "Progress: 13.6% ... Training loss: 0.238 ... Validation loss: 0.426iteration: 1361\n",
      "train_loss: 0.23898138159059637\n",
      "val_loss: 0.42652942468372407\n",
      "Progress: 13.6% ... Training loss: 0.240 ... Validation loss: 0.431iteration: 1362\n",
      "train_loss: 0.24087677967256185\n",
      "val_loss: 0.4314828997930312\n",
      "Progress: 13.6% ... Training loss: 0.231 ... Validation loss: 0.419iteration: 1363\n",
      "train_loss: 0.23156392283542837\n",
      "val_loss: 0.4199755809065207\n",
      "Progress: 13.6% ... Training loss: 0.237 ... Validation loss: 0.430iteration: 1364\n",
      "train_loss: 0.23723272319915212\n",
      "val_loss: 0.43012747286927977\n",
      "Progress: 13.7% ... Training loss: 0.231 ... Validation loss: 0.425iteration: 1365\n",
      "train_loss: 0.23131172120916443\n",
      "val_loss: 0.4252147375608718\n",
      "Progress: 13.7% ... Training loss: 0.231 ... Validation loss: 0.426iteration: 1366\n",
      "train_loss: 0.23102387527784712\n",
      "val_loss: 0.4265084310823109\n",
      "Progress: 13.7% ... Training loss: 0.233 ... Validation loss: 0.426iteration: 1367\n",
      "train_loss: 0.23346980532940675\n",
      "val_loss: 0.4260831201863398\n",
      "Progress: 13.7% ... Training loss: 0.230 ... Validation loss: 0.418iteration: 1368\n",
      "train_loss: 0.23093859601175937\n",
      "val_loss: 0.41887167761885313\n",
      "Progress: 13.7% ... Training loss: 0.231 ... Validation loss: 0.423iteration: 1369\n",
      "train_loss: 0.23152148374244194\n",
      "val_loss: 0.42370702841262936\n",
      "Progress: 13.7% ... Training loss: 0.231 ... Validation loss: 0.418iteration: 1370\n",
      "train_loss: 0.2313065585868583\n",
      "val_loss: 0.4180259918776441\n",
      "Progress: 13.7% ... Training loss: 0.239 ... Validation loss: 0.418iteration: 1371\n",
      "train_loss: 0.23901028213703787\n",
      "val_loss: 0.4182886989647691\n",
      "Progress: 13.7% ... Training loss: 0.232 ... Validation loss: 0.412iteration: 1372\n",
      "train_loss: 0.2324479623090571\n",
      "val_loss: 0.41255249489888873\n",
      "Progress: 13.7% ... Training loss: 0.231 ... Validation loss: 0.418iteration: 1373\n",
      "train_loss: 0.23155236246093527\n",
      "val_loss: 0.4183247421174951\n",
      "Progress: 13.7% ... Training loss: 0.232 ... Validation loss: 0.422iteration: 1374\n",
      "train_loss: 0.23275606633261153\n",
      "val_loss: 0.42229551477834715\n",
      "Progress: 13.8% ... Training loss: 0.230 ... Validation loss: 0.423iteration: 1375\n",
      "train_loss: 0.23031768146153836\n",
      "val_loss: 0.42356060061264206\n",
      "Progress: 13.8% ... Training loss: 0.249 ... Validation loss: 0.433iteration: 1376\n",
      "train_loss: 0.24902875553307346\n",
      "val_loss: 0.43301133027975314\n",
      "Progress: 13.8% ... Training loss: 0.245 ... Validation loss: 0.433iteration: 1377\n",
      "train_loss: 0.2457727513561211\n",
      "val_loss: 0.43324555349580074\n",
      "Progress: 13.8% ... Training loss: 0.234 ... Validation loss: 0.426iteration: 1378\n",
      "train_loss: 0.23406314896614686\n",
      "val_loss: 0.4268245925488498\n",
      "Progress: 13.8% ... Training loss: 0.234 ... Validation loss: 0.424iteration: 1379\n",
      "train_loss: 0.23468846856482908\n",
      "val_loss: 0.42411505731691884\n",
      "Progress: 13.8% ... Training loss: 0.230 ... Validation loss: 0.423iteration: 1380\n",
      "train_loss: 0.2304521774509149\n",
      "val_loss: 0.4230948921370984\n",
      "Progress: 13.8% ... Training loss: 0.234 ... Validation loss: 0.422iteration: 1381\n",
      "train_loss: 0.23420680858838508\n",
      "val_loss: 0.42269811881289976\n",
      "Progress: 13.8% ... Training loss: 0.229 ... Validation loss: 0.412iteration: 1382\n",
      "train_loss: 0.22912194140117748\n",
      "val_loss: 0.41221534767262713\n",
      "Progress: 13.8% ... Training loss: 0.231 ... Validation loss: 0.420iteration: 1383\n",
      "train_loss: 0.2312101430317911\n",
      "val_loss: 0.420716687392189\n",
      "Progress: 13.8% ... Training loss: 0.229 ... Validation loss: 0.418iteration: 1384\n",
      "train_loss: 0.22990333568888288\n",
      "val_loss: 0.41883576207447965\n",
      "Progress: 13.8% ... Training loss: 0.237 ... Validation loss: 0.419iteration: 1385\n",
      "train_loss: 0.23753646898913008\n",
      "val_loss: 0.4194685535383861\n",
      "Progress: 13.9% ... Training loss: 0.231 ... Validation loss: 0.415iteration: 1386\n",
      "train_loss: 0.23155009988708747\n",
      "val_loss: 0.41520097504245457\n",
      "Progress: 13.9% ... Training loss: 0.243 ... Validation loss: 0.433iteration: 1387\n",
      "train_loss: 0.24382459332868384\n",
      "val_loss: 0.4336675699863087\n",
      "Progress: 13.9% ... Training loss: 0.243 ... Validation loss: 0.419iteration: 1388\n",
      "train_loss: 0.24340774647147204\n",
      "val_loss: 0.41956570231537427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 13.9% ... Training loss: 0.228 ... Validation loss: 0.414iteration: 1389\n",
      "train_loss: 0.22880357676589697\n",
      "val_loss: 0.4144244111864905\n",
      "Progress: 13.9% ... Training loss: 0.228 ... Validation loss: 0.413iteration: 1390\n",
      "train_loss: 0.2289832725475713\n",
      "val_loss: 0.4139765873643122\n",
      "Progress: 13.9% ... Training loss: 0.228 ... Validation loss: 0.416iteration: 1391\n",
      "train_loss: 0.22893298566539352\n",
      "val_loss: 0.41642726492062165\n",
      "Progress: 13.9% ... Training loss: 0.229 ... Validation loss: 0.420iteration: 1392\n",
      "train_loss: 0.2290648335857497\n",
      "val_loss: 0.4201317255425938\n",
      "Progress: 13.9% ... Training loss: 0.228 ... Validation loss: 0.416iteration: 1393\n",
      "train_loss: 0.22842654901761317\n",
      "val_loss: 0.4165465926012924\n",
      "Progress: 13.9% ... Training loss: 0.236 ... Validation loss: 0.423iteration: 1394\n",
      "train_loss: 0.23638912253954952\n",
      "val_loss: 0.4234061544663907\n",
      "Progress: 13.9% ... Training loss: 0.229 ... Validation loss: 0.418iteration: 1395\n",
      "train_loss: 0.22921188978234303\n",
      "val_loss: 0.4182493628104723\n",
      "Progress: 14.0% ... Training loss: 0.233 ... Validation loss: 0.424iteration: 1396\n",
      "train_loss: 0.23343749452106693\n",
      "val_loss: 0.42498797014505624\n",
      "Progress: 14.0% ... Training loss: 0.230 ... Validation loss: 0.426iteration: 1397\n",
      "train_loss: 0.23005019524064074\n",
      "val_loss: 0.4260528905808586\n",
      "Progress: 14.0% ... Training loss: 0.231 ... Validation loss: 0.416iteration: 1398\n",
      "train_loss: 0.23112919742595353\n",
      "val_loss: 0.4168927107445493\n",
      "Progress: 14.0% ... Training loss: 0.230 ... Validation loss: 0.417iteration: 1399\n",
      "train_loss: 0.23039454332922224\n",
      "val_loss: 0.4172621676153891\n",
      "Progress: 14.0% ... Training loss: 0.227 ... Validation loss: 0.418iteration: 1400\n",
      "train_loss: 0.22794233788538934\n",
      "val_loss: 0.41854719261898227\n",
      "Progress: 14.0% ... Training loss: 0.236 ... Validation loss: 0.422iteration: 1401\n",
      "train_loss: 0.23621307407509332\n",
      "val_loss: 0.4222361000209492\n",
      "Progress: 14.0% ... Training loss: 0.248 ... Validation loss: 0.433iteration: 1402\n",
      "train_loss: 0.24837069458338923\n",
      "val_loss: 0.43397160480434754\n",
      "Progress: 14.0% ... Training loss: 0.235 ... Validation loss: 0.422iteration: 1403\n",
      "train_loss: 0.23537711932047795\n",
      "val_loss: 0.422619358349937\n",
      "Progress: 14.0% ... Training loss: 0.228 ... Validation loss: 0.413iteration: 1404\n",
      "train_loss: 0.2282375989401528\n",
      "val_loss: 0.4139459761799513\n",
      "Progress: 14.1% ... Training loss: 0.230 ... Validation loss: 0.412iteration: 1405\n",
      "train_loss: 0.23070641098117717\n",
      "val_loss: 0.41233207239810843\n",
      "Progress: 14.1% ... Training loss: 0.244 ... Validation loss: 0.423iteration: 1406\n",
      "train_loss: 0.2441488560849282\n",
      "val_loss: 0.42382763654971933\n",
      "Progress: 14.1% ... Training loss: 0.233 ... Validation loss: 0.412iteration: 1407\n",
      "train_loss: 0.23304744057326113\n",
      "val_loss: 0.4128909856936801\n",
      "Progress: 14.1% ... Training loss: 0.229 ... Validation loss: 0.408iteration: 1408\n",
      "train_loss: 0.22979837438906506\n",
      "val_loss: 0.408850803248373\n",
      "Progress: 14.1% ... Training loss: 0.228 ... Validation loss: 0.408iteration: 1409\n",
      "train_loss: 0.22824926638021042\n",
      "val_loss: 0.40844430846880614\n",
      "Progress: 14.1% ... Training loss: 0.230 ... Validation loss: 0.411iteration: 1410\n",
      "train_loss: 0.23059454895259868\n",
      "val_loss: 0.4114962174204684\n",
      "Progress: 14.1% ... Training loss: 0.231 ... Validation loss: 0.413iteration: 1411\n",
      "train_loss: 0.23165664289243396\n",
      "val_loss: 0.41317167399665106\n",
      "Progress: 14.1% ... Training loss: 0.229 ... Validation loss: 0.405iteration: 1412\n",
      "train_loss: 0.2294555165842237\n",
      "val_loss: 0.40502045277198867\n",
      "Progress: 14.1% ... Training loss: 0.227 ... Validation loss: 0.409iteration: 1413\n",
      "train_loss: 0.22727246810741855\n",
      "val_loss: 0.409694208303272\n",
      "Progress: 14.1% ... Training loss: 0.227 ... Validation loss: 0.406iteration: 1414\n",
      "train_loss: 0.22708656907346206\n",
      "val_loss: 0.4067793042825912\n",
      "Progress: 14.2% ... Training loss: 0.236 ... Validation loss: 0.419iteration: 1415\n",
      "train_loss: 0.2365834521503543\n",
      "val_loss: 0.41987849800884486\n",
      "Progress: 14.2% ... Training loss: 0.228 ... Validation loss: 0.409iteration: 1416\n",
      "train_loss: 0.2281697975219899\n",
      "val_loss: 0.40951645057169733\n",
      "Progress: 14.2% ... Training loss: 0.226 ... Validation loss: 0.408iteration: 1417\n",
      "train_loss: 0.22663486241277633\n",
      "val_loss: 0.4083322482588783\n",
      "Progress: 14.2% ... Training loss: 0.228 ... Validation loss: 0.414iteration: 1418\n",
      "train_loss: 0.22873678031119304\n",
      "val_loss: 0.41464445410827083\n",
      "Progress: 14.2% ... Training loss: 0.244 ... Validation loss: 0.429iteration: 1419\n",
      "train_loss: 0.2445336087415537\n",
      "val_loss: 0.429507461130831\n",
      "Progress: 14.2% ... Training loss: 0.232 ... Validation loss: 0.414iteration: 1420\n",
      "train_loss: 0.23225223753314156\n",
      "val_loss: 0.4148056627265569\n",
      "Progress: 14.2% ... Training loss: 0.226 ... Validation loss: 0.413iteration: 1421\n",
      "train_loss: 0.22670981867660178\n",
      "val_loss: 0.41318859840714167\n",
      "Progress: 14.2% ... Training loss: 0.228 ... Validation loss: 0.415iteration: 1422\n",
      "train_loss: 0.22806035312174902\n",
      "val_loss: 0.41549594450616356\n",
      "Progress: 14.2% ... Training loss: 0.228 ... Validation loss: 0.409iteration: 1423\n",
      "train_loss: 0.22881890554090922\n",
      "val_loss: 0.40946765200486873\n",
      "Progress: 14.2% ... Training loss: 0.225 ... Validation loss: 0.408iteration: 1424\n",
      "train_loss: 0.22578337645714747\n",
      "val_loss: 0.40825416397901054\n",
      "Progress: 14.2% ... Training loss: 0.232 ... Validation loss: 0.407iteration: 1425\n",
      "train_loss: 0.23253044094049974\n",
      "val_loss: 0.4073045535451168\n",
      "Progress: 14.3% ... Training loss: 0.228 ... Validation loss: 0.409iteration: 1426\n",
      "train_loss: 0.22880742027475443\n",
      "val_loss: 0.4094257714615681\n",
      "Progress: 14.3% ... Training loss: 0.229 ... Validation loss: 0.404iteration: 1427\n",
      "train_loss: 0.2293379393085208\n",
      "val_loss: 0.4049317360156948\n",
      "Progress: 14.3% ... Training loss: 0.227 ... Validation loss: 0.409iteration: 1428\n",
      "train_loss: 0.22774253769325856\n",
      "val_loss: 0.40948852979425426\n",
      "Progress: 14.3% ... Training loss: 0.229 ... Validation loss: 0.413iteration: 1429\n",
      "train_loss: 0.22999116948519466\n",
      "val_loss: 0.41383566695671803\n",
      "Progress: 14.3% ... Training loss: 0.231 ... Validation loss: 0.410iteration: 1430\n",
      "train_loss: 0.23124710558015374\n",
      "val_loss: 0.41095580800318765\n",
      "Progress: 14.3% ... Training loss: 0.227 ... Validation loss: 0.404iteration: 1431\n",
      "train_loss: 0.22701919675875654\n",
      "val_loss: 0.4040901236772751\n",
      "Progress: 14.3% ... Training loss: 0.229 ... Validation loss: 0.401iteration: 1432\n",
      "train_loss: 0.22987946058047862\n",
      "val_loss: 0.40166521504013725\n",
      "Progress: 14.3% ... Training loss: 0.249 ... Validation loss: 0.433iteration: 1433\n",
      "train_loss: 0.24991827401744912\n",
      "val_loss: 0.43303623783513223\n",
      "Progress: 14.3% ... Training loss: 0.224 ... Validation loss: 0.407iteration: 1434\n",
      "train_loss: 0.22425603758567642\n",
      "val_loss: 0.4077067101378227\n",
      "Progress: 14.3% ... Training loss: 0.229 ... Validation loss: 0.414iteration: 1435\n",
      "train_loss: 0.2294427742639525\n",
      "val_loss: 0.4140466817427582\n",
      "Progress: 14.4% ... Training loss: 0.227 ... Validation loss: 0.410iteration: 1436\n",
      "train_loss: 0.22780560388844454\n",
      "val_loss: 0.41081743097933915\n",
      "Progress: 14.4% ... Training loss: 0.225 ... Validation loss: 0.411iteration: 1437\n",
      "train_loss: 0.2259600706565424\n",
      "val_loss: 0.4119666852585379\n",
      "Progress: 14.4% ... Training loss: 0.225 ... Validation loss: 0.414iteration: 1438\n",
      "train_loss: 0.2255990165452846\n",
      "val_loss: 0.4147870320229466\n",
      "Progress: 14.4% ... Training loss: 0.227 ... Validation loss: 0.413iteration: 1439\n",
      "train_loss: 0.22731032202113652\n",
      "val_loss: 0.413852727734532\n",
      "Progress: 14.4% ... Training loss: 0.232 ... Validation loss: 0.418iteration: 1440\n",
      "train_loss: 0.23273093627575328\n",
      "val_loss: 0.4180479141177203\n",
      "Progress: 14.4% ... Training loss: 0.229 ... Validation loss: 0.423iteration: 1441\n",
      "train_loss: 0.2295755507684593\n",
      "val_loss: 0.4230291979682167\n",
      "Progress: 14.4% ... Training loss: 0.227 ... Validation loss: 0.420iteration: 1442\n",
      "train_loss: 0.22756209196144025\n",
      "val_loss: 0.4205869134661247\n",
      "Progress: 14.4% ... Training loss: 0.225 ... Validation loss: 0.420iteration: 1443\n",
      "train_loss: 0.22560069162632884\n",
      "val_loss: 0.4209307009221176\n",
      "Progress: 14.4% ... Training loss: 0.224 ... Validation loss: 0.419iteration: 1444\n",
      "train_loss: 0.2249484138950008\n",
      "val_loss: 0.41982512241031655\n",
      "Progress: 14.4% ... Training loss: 0.225 ... Validation loss: 0.410iteration: 1445\n",
      "train_loss: 0.22534799970822839\n",
      "val_loss: 0.4107671294692406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 14.5% ... Training loss: 0.225 ... Validation loss: 0.412iteration: 1446\n",
      "train_loss: 0.22545652535227081\n",
      "val_loss: 0.41270954485636097\n",
      "Progress: 14.5% ... Training loss: 0.225 ... Validation loss: 0.409iteration: 1447\n",
      "train_loss: 0.22554478227791563\n",
      "val_loss: 0.4098872041692679\n",
      "Progress: 14.5% ... Training loss: 0.229 ... Validation loss: 0.425iteration: 1448\n",
      "train_loss: 0.22915570803808896\n",
      "val_loss: 0.42565484686103516\n",
      "Progress: 14.5% ... Training loss: 0.228 ... Validation loss: 0.408iteration: 1449\n",
      "train_loss: 0.22830033737128433\n",
      "val_loss: 0.40852135876842205\n",
      "Progress: 14.5% ... Training loss: 0.240 ... Validation loss: 0.446iteration: 1450\n",
      "train_loss: 0.24028652181051538\n",
      "val_loss: 0.4461045468801808\n",
      "Progress: 14.5% ... Training loss: 0.241 ... Validation loss: 0.408iteration: 1451\n",
      "train_loss: 0.24149205899260615\n",
      "val_loss: 0.4089417259738941\n",
      "Progress: 14.5% ... Training loss: 0.229 ... Validation loss: 0.429iteration: 1452\n",
      "train_loss: 0.229093287946945\n",
      "val_loss: 0.42942378016919286\n",
      "Progress: 14.5% ... Training loss: 0.222 ... Validation loss: 0.409iteration: 1453\n",
      "train_loss: 0.22293683811774737\n",
      "val_loss: 0.4092259079624774\n",
      "Progress: 14.5% ... Training loss: 0.226 ... Validation loss: 0.404iteration: 1454\n",
      "train_loss: 0.22621119140287527\n",
      "val_loss: 0.4048682634105073\n",
      "Progress: 14.6% ... Training loss: 0.222 ... Validation loss: 0.407iteration: 1455\n",
      "train_loss: 0.22291859893220936\n",
      "val_loss: 0.40712482097706576\n",
      "Progress: 14.6% ... Training loss: 0.228 ... Validation loss: 0.411iteration: 1456\n",
      "train_loss: 0.22894149950458692\n",
      "val_loss: 0.41198632431026083\n",
      "Progress: 14.6% ... Training loss: 0.223 ... Validation loss: 0.405iteration: 1457\n",
      "train_loss: 0.22373715699942098\n",
      "val_loss: 0.4057414059744011\n",
      "Progress: 14.6% ... Training loss: 0.233 ... Validation loss: 0.423iteration: 1458\n",
      "train_loss: 0.233240088150964\n",
      "val_loss: 0.42377007734429484\n",
      "Progress: 14.6% ... Training loss: 0.224 ... Validation loss: 0.413iteration: 1459\n",
      "train_loss: 0.2242838695444152\n",
      "val_loss: 0.41379624353282257\n",
      "Progress: 14.6% ... Training loss: 0.225 ... Validation loss: 0.402iteration: 1460\n",
      "train_loss: 0.22509268383215522\n",
      "val_loss: 0.4023724984057586\n",
      "Progress: 14.6% ... Training loss: 0.224 ... Validation loss: 0.400iteration: 1461\n",
      "train_loss: 0.2240611127180618\n",
      "val_loss: 0.400755654293242\n",
      "Progress: 14.6% ... Training loss: 0.221 ... Validation loss: 0.403iteration: 1462\n",
      "train_loss: 0.22190368262079957\n",
      "val_loss: 0.4033472636725394\n",
      "Progress: 14.6% ... Training loss: 0.228 ... Validation loss: 0.403iteration: 1463\n",
      "train_loss: 0.22810196097857668\n",
      "val_loss: 0.40387415428340256\n",
      "Progress: 14.6% ... Training loss: 0.222 ... Validation loss: 0.401iteration: 1464\n",
      "train_loss: 0.22233270467649183\n",
      "val_loss: 0.40176095812319146\n",
      "Progress: 14.7% ... Training loss: 0.230 ... Validation loss: 0.412iteration: 1465\n",
      "train_loss: 0.23095874747330317\n",
      "val_loss: 0.41293123795899994\n",
      "Progress: 14.7% ... Training loss: 0.222 ... Validation loss: 0.399iteration: 1466\n",
      "train_loss: 0.22250124751938263\n",
      "val_loss: 0.3999345024783342\n",
      "Progress: 14.7% ... Training loss: 0.221 ... Validation loss: 0.402iteration: 1467\n",
      "train_loss: 0.22160038280134123\n",
      "val_loss: 0.402787856963486\n",
      "Progress: 14.7% ... Training loss: 0.224 ... Validation loss: 0.404iteration: 1468\n",
      "train_loss: 0.22484236015894507\n",
      "val_loss: 0.4045386407413614\n",
      "Progress: 14.7% ... Training loss: 0.222 ... Validation loss: 0.408iteration: 1469\n",
      "train_loss: 0.22239950897660157\n",
      "val_loss: 0.40848495525850637\n",
      "Progress: 14.7% ... Training loss: 0.221 ... Validation loss: 0.405iteration: 1470\n",
      "train_loss: 0.22161611848361054\n",
      "val_loss: 0.40579058500303233\n",
      "Progress: 14.7% ... Training loss: 0.222 ... Validation loss: 0.402iteration: 1471\n",
      "train_loss: 0.2229184462255921\n",
      "val_loss: 0.40243263707781407\n",
      "Progress: 14.7% ... Training loss: 0.222 ... Validation loss: 0.402iteration: 1472\n",
      "train_loss: 0.22229526965849364\n",
      "val_loss: 0.40287101416694693\n",
      "Progress: 14.7% ... Training loss: 0.224 ... Validation loss: 0.407iteration: 1473\n",
      "train_loss: 0.22462005809474492\n",
      "val_loss: 0.407928801783822\n",
      "Progress: 14.7% ... Training loss: 0.221 ... Validation loss: 0.409iteration: 1474\n",
      "train_loss: 0.22166416315379758\n",
      "val_loss: 0.409668987119608\n",
      "Progress: 14.8% ... Training loss: 0.222 ... Validation loss: 0.408iteration: 1475\n",
      "train_loss: 0.22222434829873244\n",
      "val_loss: 0.408360776609779\n",
      "Progress: 14.8% ... Training loss: 0.221 ... Validation loss: 0.405iteration: 1476\n",
      "train_loss: 0.22158122825779314\n",
      "val_loss: 0.4054074225786938\n",
      "Progress: 14.8% ... Training loss: 0.221 ... Validation loss: 0.407iteration: 1477\n",
      "train_loss: 0.22151662232915562\n",
      "val_loss: 0.4079179984915795\n",
      "Progress: 14.8% ... Training loss: 0.224 ... Validation loss: 0.418iteration: 1478\n",
      "train_loss: 0.22479948138354142\n",
      "val_loss: 0.41846818029520655\n",
      "Progress: 14.8% ... Training loss: 0.222 ... Validation loss: 0.412iteration: 1479\n",
      "train_loss: 0.22234829182315408\n",
      "val_loss: 0.41268103404958406\n",
      "Progress: 14.8% ... Training loss: 0.224 ... Validation loss: 0.408iteration: 1480\n",
      "train_loss: 0.22467164817217075\n",
      "val_loss: 0.4080377449968558\n",
      "Progress: 14.8% ... Training loss: 0.225 ... Validation loss: 0.418iteration: 1481\n",
      "train_loss: 0.2256469593830072\n",
      "val_loss: 0.41872408213183776\n",
      "Progress: 14.8% ... Training loss: 0.225 ... Validation loss: 0.404iteration: 1482\n",
      "train_loss: 0.225308942883286\n",
      "val_loss: 0.40427330691583846\n",
      "Progress: 14.8% ... Training loss: 0.220 ... Validation loss: 0.402iteration: 1483\n",
      "train_loss: 0.220774628215054\n",
      "val_loss: 0.4025400442429325\n",
      "Progress: 14.8% ... Training loss: 0.221 ... Validation loss: 0.402iteration: 1484\n",
      "train_loss: 0.2210070671187402\n",
      "val_loss: 0.4027127464308759\n",
      "Progress: 14.8% ... Training loss: 0.220 ... Validation loss: 0.398iteration: 1485\n",
      "train_loss: 0.2209223405741716\n",
      "val_loss: 0.3983410898898402\n",
      "Progress: 14.9% ... Training loss: 0.225 ... Validation loss: 0.403iteration: 1486\n",
      "train_loss: 0.22596790090818858\n",
      "val_loss: 0.4034158389914332\n",
      "Progress: 14.9% ... Training loss: 0.223 ... Validation loss: 0.409iteration: 1487\n",
      "train_loss: 0.22335679045002851\n",
      "val_loss: 0.40950718249772733\n",
      "Progress: 14.9% ... Training loss: 0.237 ... Validation loss: 0.420iteration: 1488\n",
      "train_loss: 0.23799312030790643\n",
      "val_loss: 0.4202341755804048\n",
      "Progress: 14.9% ... Training loss: 0.228 ... Validation loss: 0.410iteration: 1489\n",
      "train_loss: 0.22821537296027572\n",
      "val_loss: 0.4108915509683797\n",
      "Progress: 14.9% ... Training loss: 0.221 ... Validation loss: 0.403iteration: 1490\n",
      "train_loss: 0.22186246517028982\n",
      "val_loss: 0.4036322211590633\n",
      "Progress: 14.9% ... Training loss: 0.220 ... Validation loss: 0.404iteration: 1491\n",
      "train_loss: 0.22084980349287633\n",
      "val_loss: 0.404572991909392\n",
      "Progress: 14.9% ... Training loss: 0.222 ... Validation loss: 0.395iteration: 1492\n",
      "train_loss: 0.22250902079119733\n",
      "val_loss: 0.3956734498055797\n",
      "Progress: 14.9% ... Training loss: 0.220 ... Validation loss: 0.399iteration: 1493\n",
      "train_loss: 0.22097088352898833\n",
      "val_loss: 0.39931498702720547\n",
      "Progress: 14.9% ... Training loss: 0.221 ... Validation loss: 0.399iteration: 1494\n",
      "train_loss: 0.221625082807506\n",
      "val_loss: 0.39967171816641145\n",
      "Progress: 14.9% ... Training loss: 0.221 ... Validation loss: 0.399iteration: 1495\n",
      "train_loss: 0.22150859038052706\n",
      "val_loss: 0.39924328567893574\n",
      "Progress: 15.0% ... Training loss: 0.226 ... Validation loss: 0.407iteration: 1496\n",
      "train_loss: 0.226048734714466\n",
      "val_loss: 0.4073273810179738\n",
      "Progress: 15.0% ... Training loss: 0.225 ... Validation loss: 0.412iteration: 1497\n",
      "train_loss: 0.2258098331474962\n",
      "val_loss: 0.4125608453131637\n",
      "Progress: 15.0% ... Training loss: 0.232 ... Validation loss: 0.406iteration: 1498\n",
      "train_loss: 0.23242370183239033\n",
      "val_loss: 0.4068201571951704\n",
      "Progress: 15.0% ... Training loss: 0.236 ... Validation loss: 0.434iteration: 1499\n",
      "train_loss: 0.23683284271538063\n",
      "val_loss: 0.43414993594112966\n",
      "Progress: 15.0% ... Training loss: 0.232 ... Validation loss: 0.402iteration: 1500\n",
      "train_loss: 0.23219539215067206\n",
      "val_loss: 0.4028117109530041\n",
      "Progress: 15.0% ... Training loss: 0.225 ... Validation loss: 0.414iteration: 1501\n",
      "train_loss: 0.22568207487392072\n",
      "val_loss: 0.4144706663535377\n",
      "Progress: 15.0% ... Training loss: 0.220 ... Validation loss: 0.402iteration: 1502\n",
      "train_loss: 0.22046014121263616\n",
      "val_loss: 0.40299284795743673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 15.0% ... Training loss: 0.221 ... Validation loss: 0.407iteration: 1503\n",
      "train_loss: 0.22160412530211868\n",
      "val_loss: 0.4073263070758364\n",
      "Progress: 15.0% ... Training loss: 0.219 ... Validation loss: 0.400iteration: 1504\n",
      "train_loss: 0.21947862450534808\n",
      "val_loss: 0.40009192723469394\n",
      "Progress: 15.1% ... Training loss: 0.223 ... Validation loss: 0.402iteration: 1505\n",
      "train_loss: 0.2233743998444528\n",
      "val_loss: 0.402128441114676\n",
      "Progress: 15.1% ... Training loss: 0.226 ... Validation loss: 0.415iteration: 1506\n",
      "train_loss: 0.22608758483772673\n",
      "val_loss: 0.4156092294808642\n",
      "Progress: 15.1% ... Training loss: 0.218 ... Validation loss: 0.405iteration: 1507\n",
      "train_loss: 0.21817528007976852\n",
      "val_loss: 0.40553556745888775\n",
      "Progress: 15.1% ... Training loss: 0.238 ... Validation loss: 0.423iteration: 1508\n",
      "train_loss: 0.2389036741179387\n",
      "val_loss: 0.42306293891125246\n",
      "Progress: 15.1% ... Training loss: 0.227 ... Validation loss: 0.414iteration: 1509\n",
      "train_loss: 0.2277148439352758\n",
      "val_loss: 0.41447760078486656\n",
      "Progress: 15.1% ... Training loss: 0.225 ... Validation loss: 0.409iteration: 1510\n",
      "train_loss: 0.22511222655511895\n",
      "val_loss: 0.409159366635097\n",
      "Progress: 15.1% ... Training loss: 0.227 ... Validation loss: 0.414iteration: 1511\n",
      "train_loss: 0.22778065511855283\n",
      "val_loss: 0.41410786586856985\n",
      "Progress: 15.1% ... Training loss: 0.222 ... Validation loss: 0.408iteration: 1512\n",
      "train_loss: 0.22238858176395981\n",
      "val_loss: 0.408253828605081\n",
      "Progress: 15.1% ... Training loss: 0.219 ... Validation loss: 0.413iteration: 1513\n",
      "train_loss: 0.2194377795628582\n",
      "val_loss: 0.4132682752720186\n",
      "Progress: 15.1% ... Training loss: 0.236 ... Validation loss: 0.420iteration: 1514\n",
      "train_loss: 0.23672710514696987\n",
      "val_loss: 0.4206951962170184\n",
      "Progress: 15.2% ... Training loss: 0.222 ... Validation loss: 0.418iteration: 1515\n",
      "train_loss: 0.22281678270827343\n",
      "val_loss: 0.41875461675318804\n",
      "Progress: 15.2% ... Training loss: 0.218 ... Validation loss: 0.406iteration: 1516\n",
      "train_loss: 0.21804395853740222\n",
      "val_loss: 0.40650687046390294\n",
      "Progress: 15.2% ... Training loss: 0.223 ... Validation loss: 0.404iteration: 1517\n",
      "train_loss: 0.22339674625117173\n",
      "val_loss: 0.4041846388584163\n",
      "Progress: 15.2% ... Training loss: 0.222 ... Validation loss: 0.410iteration: 1518\n",
      "train_loss: 0.22221506717402867\n",
      "val_loss: 0.4108807631633678\n",
      "Progress: 15.2% ... Training loss: 0.217 ... Validation loss: 0.409iteration: 1519\n",
      "train_loss: 0.2178889300291709\n",
      "val_loss: 0.4092358299947245\n",
      "Progress: 15.2% ... Training loss: 0.217 ... Validation loss: 0.409iteration: 1520\n",
      "train_loss: 0.2170904044158053\n",
      "val_loss: 0.40913041872562866\n",
      "Progress: 15.2% ... Training loss: 0.220 ... Validation loss: 0.414iteration: 1521\n",
      "train_loss: 0.22068171117642105\n",
      "val_loss: 0.41490499680087806\n",
      "Progress: 15.2% ... Training loss: 0.218 ... Validation loss: 0.414iteration: 1522\n",
      "train_loss: 0.21808332799097024\n",
      "val_loss: 0.4146132222937127\n",
      "Progress: 15.2% ... Training loss: 0.221 ... Validation loss: 0.412iteration: 1523\n",
      "train_loss: 0.22184474028459977\n",
      "val_loss: 0.41288095229455213\n",
      "Progress: 15.2% ... Training loss: 0.218 ... Validation loss: 0.420iteration: 1524\n",
      "train_loss: 0.21895266279575062\n",
      "val_loss: 0.4202974829604915\n",
      "Progress: 15.2% ... Training loss: 0.217 ... Validation loss: 0.409iteration: 1525\n",
      "train_loss: 0.21767349278592785\n",
      "val_loss: 0.4096331816403062\n",
      "Progress: 15.3% ... Training loss: 0.216 ... Validation loss: 0.409iteration: 1526\n",
      "train_loss: 0.21645095103088485\n",
      "val_loss: 0.4099253390976286\n",
      "Progress: 15.3% ... Training loss: 0.218 ... Validation loss: 0.412iteration: 1527\n",
      "train_loss: 0.21885753120317977\n",
      "val_loss: 0.4126993086153571\n",
      "Progress: 15.3% ... Training loss: 0.216 ... Validation loss: 0.402iteration: 1528\n",
      "train_loss: 0.21650883900047735\n",
      "val_loss: 0.4021187168516745\n",
      "Progress: 15.3% ... Training loss: 0.217 ... Validation loss: 0.399iteration: 1529\n",
      "train_loss: 0.21734209540329186\n",
      "val_loss: 0.3999574319054227\n",
      "Progress: 15.3% ... Training loss: 0.217 ... Validation loss: 0.396iteration: 1530\n",
      "train_loss: 0.21739946761019643\n",
      "val_loss: 0.3963693696542139\n",
      "Progress: 15.3% ... Training loss: 0.218 ... Validation loss: 0.396iteration: 1531\n",
      "train_loss: 0.21812953795732684\n",
      "val_loss: 0.3960416675406899\n",
      "Progress: 15.3% ... Training loss: 0.218 ... Validation loss: 0.397iteration: 1532\n",
      "train_loss: 0.21845275708333844\n",
      "val_loss: 0.3972881877810197\n",
      "Progress: 15.3% ... Training loss: 0.218 ... Validation loss: 0.402iteration: 1533\n",
      "train_loss: 0.2182759508717213\n",
      "val_loss: 0.40280089520975537\n",
      "Progress: 15.3% ... Training loss: 0.216 ... Validation loss: 0.411iteration: 1534\n",
      "train_loss: 0.21610284039887565\n",
      "val_loss: 0.4112893622151636\n",
      "Progress: 15.3% ... Training loss: 0.215 ... Validation loss: 0.402iteration: 1535\n",
      "train_loss: 0.21508368166330752\n",
      "val_loss: 0.4025724270353921\n",
      "Progress: 15.4% ... Training loss: 0.214 ... Validation loss: 0.401iteration: 1536\n",
      "train_loss: 0.2144877153260756\n",
      "val_loss: 0.4013020170121757\n",
      "Progress: 15.4% ... Training loss: 0.222 ... Validation loss: 0.411iteration: 1537\n",
      "train_loss: 0.22236738432252118\n",
      "val_loss: 0.4114000478543392\n",
      "Progress: 15.4% ... Training loss: 0.219 ... Validation loss: 0.402iteration: 1538\n",
      "train_loss: 0.21934798853376813\n",
      "val_loss: 0.4022774304644937\n",
      "Progress: 15.4% ... Training loss: 0.216 ... Validation loss: 0.414iteration: 1539\n",
      "train_loss: 0.21656360980555095\n",
      "val_loss: 0.4140840988578577\n",
      "Progress: 15.4% ... Training loss: 0.232 ... Validation loss: 0.409iteration: 1540\n",
      "train_loss: 0.2321466166421648\n",
      "val_loss: 0.40913068996499913\n",
      "Progress: 15.4% ... Training loss: 0.216 ... Validation loss: 0.410iteration: 1541\n",
      "train_loss: 0.2162694240532701\n",
      "val_loss: 0.41098837776448655\n",
      "Progress: 15.4% ... Training loss: 0.215 ... Validation loss: 0.402iteration: 1542\n",
      "train_loss: 0.215822141657706\n",
      "val_loss: 0.40240058556787156\n",
      "Progress: 15.4% ... Training loss: 0.215 ... Validation loss: 0.401iteration: 1543\n",
      "train_loss: 0.21558849263753038\n",
      "val_loss: 0.401929026304149\n",
      "Progress: 15.4% ... Training loss: 0.220 ... Validation loss: 0.405iteration: 1544\n",
      "train_loss: 0.22028274474198611\n",
      "val_loss: 0.4054233711685755\n",
      "Progress: 15.4% ... Training loss: 0.235 ... Validation loss: 0.412iteration: 1545\n",
      "train_loss: 0.23554839075776146\n",
      "val_loss: 0.4120750317440801\n",
      "Progress: 15.5% ... Training loss: 0.221 ... Validation loss: 0.422iteration: 1546\n",
      "train_loss: 0.22197508642775846\n",
      "val_loss: 0.4228806998371988\n",
      "Progress: 15.5% ... Training loss: 0.226 ... Validation loss: 0.409iteration: 1547\n",
      "train_loss: 0.22624023891480183\n",
      "val_loss: 0.4090615809165205\n",
      "Progress: 15.5% ... Training loss: 0.215 ... Validation loss: 0.411iteration: 1548\n",
      "train_loss: 0.21543172115357126\n",
      "val_loss: 0.41140523613340013\n",
      "Progress: 15.5% ... Training loss: 0.221 ... Validation loss: 0.407iteration: 1549\n",
      "train_loss: 0.22113476669077167\n",
      "val_loss: 0.4079983548769737\n",
      "Progress: 15.5% ... Training loss: 0.215 ... Validation loss: 0.406iteration: 1550\n",
      "train_loss: 0.21507395283214162\n",
      "val_loss: 0.40611301155483137\n",
      "Progress: 15.5% ... Training loss: 0.214 ... Validation loss: 0.400iteration: 1551\n",
      "train_loss: 0.2140612099953558\n",
      "val_loss: 0.400055477263383\n",
      "Progress: 15.5% ... Training loss: 0.214 ... Validation loss: 0.406iteration: 1552\n",
      "train_loss: 0.21465871247349466\n",
      "val_loss: 0.40659685208242485\n",
      "Progress: 15.5% ... Training loss: 0.226 ... Validation loss: 0.409iteration: 1553\n",
      "train_loss: 0.2263036547897894\n",
      "val_loss: 0.4098971188138961\n",
      "Progress: 15.5% ... Training loss: 0.213 ... Validation loss: 0.403iteration: 1554\n",
      "train_loss: 0.21317354175488162\n",
      "val_loss: 0.40306444687343973\n",
      "Progress: 15.6% ... Training loss: 0.214 ... Validation loss: 0.398iteration: 1555\n",
      "train_loss: 0.21498205847757584\n",
      "val_loss: 0.3981472638148353\n",
      "Progress: 15.6% ... Training loss: 0.213 ... Validation loss: 0.401iteration: 1556\n",
      "train_loss: 0.21379967237494785\n",
      "val_loss: 0.4015411731903456\n",
      "Progress: 15.6% ... Training loss: 0.218 ... Validation loss: 0.403iteration: 1557\n",
      "train_loss: 0.21834031054195052\n",
      "val_loss: 0.4031283772473209\n",
      "Progress: 15.6% ... Training loss: 0.220 ... Validation loss: 0.417iteration: 1558\n",
      "train_loss: 0.22044151801196896\n",
      "val_loss: 0.4176591437255962\n",
      "Progress: 15.6% ... Training loss: 0.213 ... Validation loss: 0.404iteration: 1559\n",
      "train_loss: 0.21365207338650558\n",
      "val_loss: 0.4048051618622654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 15.6% ... Training loss: 0.212 ... Validation loss: 0.400iteration: 1560\n",
      "train_loss: 0.21291917824942544\n",
      "val_loss: 0.4001617459442288\n",
      "Progress: 15.6% ... Training loss: 0.212 ... Validation loss: 0.402iteration: 1561\n",
      "train_loss: 0.21238391167075626\n",
      "val_loss: 0.4021118336447079\n",
      "Progress: 15.6% ... Training loss: 0.212 ... Validation loss: 0.398iteration: 1562\n",
      "train_loss: 0.21208203381435584\n",
      "val_loss: 0.39850333807800364\n",
      "Progress: 15.6% ... Training loss: 0.212 ... Validation loss: 0.400iteration: 1563\n",
      "train_loss: 0.21276271622402004\n",
      "val_loss: 0.40083688845492355\n",
      "Progress: 15.6% ... Training loss: 0.218 ... Validation loss: 0.411iteration: 1564\n",
      "train_loss: 0.21872371757212553\n",
      "val_loss: 0.4117750297334474\n",
      "Progress: 15.7% ... Training loss: 0.212 ... Validation loss: 0.392iteration: 1565\n",
      "train_loss: 0.21274878370442832\n",
      "val_loss: 0.3920672052719676\n",
      "Progress: 15.7% ... Training loss: 0.215 ... Validation loss: 0.401iteration: 1566\n",
      "train_loss: 0.21554935119942026\n",
      "val_loss: 0.4019020853009547\n",
      "Progress: 15.7% ... Training loss: 0.215 ... Validation loss: 0.391iteration: 1567\n",
      "train_loss: 0.21565520374833033\n",
      "val_loss: 0.39199683425418863\n",
      "Progress: 15.7% ... Training loss: 0.212 ... Validation loss: 0.395iteration: 1568\n",
      "train_loss: 0.2125823828711762\n",
      "val_loss: 0.395958577328734\n",
      "Progress: 15.7% ... Training loss: 0.212 ... Validation loss: 0.401iteration: 1569\n",
      "train_loss: 0.21236683427437278\n",
      "val_loss: 0.40153245292604556\n",
      "Progress: 15.7% ... Training loss: 0.220 ... Validation loss: 0.400iteration: 1570\n",
      "train_loss: 0.22003754191443872\n",
      "val_loss: 0.4001172168829492\n",
      "Progress: 15.7% ... Training loss: 0.212 ... Validation loss: 0.396iteration: 1571\n",
      "train_loss: 0.21296664733720327\n",
      "val_loss: 0.39655024520702553\n",
      "Progress: 15.7% ... Training loss: 0.229 ... Validation loss: 0.407iteration: 1572\n",
      "train_loss: 0.2297917100429024\n",
      "val_loss: 0.4073193130968427\n",
      "Progress: 15.7% ... Training loss: 0.211 ... Validation loss: 0.396iteration: 1573\n",
      "train_loss: 0.2117210140328971\n",
      "val_loss: 0.3966863176510671\n",
      "Progress: 15.7% ... Training loss: 0.215 ... Validation loss: 0.398iteration: 1574\n",
      "train_loss: 0.21532616512977004\n",
      "val_loss: 0.39837600561251546\n",
      "Progress: 15.8% ... Training loss: 0.211 ... Validation loss: 0.393iteration: 1575\n",
      "train_loss: 0.21134315025354702\n",
      "val_loss: 0.3938921905449236\n",
      "Progress: 15.8% ... Training loss: 0.218 ... Validation loss: 0.399iteration: 1576\n",
      "train_loss: 0.21872424685725628\n",
      "val_loss: 0.3996581801036279\n",
      "Progress: 15.8% ... Training loss: 0.213 ... Validation loss: 0.395iteration: 1577\n",
      "train_loss: 0.21302333123851105\n",
      "val_loss: 0.395365966006065\n",
      "Progress: 15.8% ... Training loss: 0.211 ... Validation loss: 0.395iteration: 1578\n",
      "train_loss: 0.21185181952090335\n",
      "val_loss: 0.3959476901838165\n",
      "Progress: 15.8% ... Training loss: 0.210 ... Validation loss: 0.390iteration: 1579\n",
      "train_loss: 0.21075288496742237\n",
      "val_loss: 0.39004197941070484\n",
      "Progress: 15.8% ... Training loss: 0.210 ... Validation loss: 0.391iteration: 1580\n",
      "train_loss: 0.2100616199074555\n",
      "val_loss: 0.39105167319126277\n",
      "Progress: 15.8% ... Training loss: 0.210 ... Validation loss: 0.391iteration: 1581\n",
      "train_loss: 0.2102791186131093\n",
      "val_loss: 0.3916960677234175\n",
      "Progress: 15.8% ... Training loss: 0.210 ... Validation loss: 0.389iteration: 1582\n",
      "train_loss: 0.2102165266561153\n",
      "val_loss: 0.3891389108901612\n",
      "Progress: 15.8% ... Training loss: 0.212 ... Validation loss: 0.388iteration: 1583\n",
      "train_loss: 0.21231561580717875\n",
      "val_loss: 0.38856341774321945\n",
      "Progress: 15.8% ... Training loss: 0.221 ... Validation loss: 0.397iteration: 1584\n",
      "train_loss: 0.22142546251969925\n",
      "val_loss: 0.3974486608155523\n",
      "Progress: 15.8% ... Training loss: 0.210 ... Validation loss: 0.387iteration: 1585\n",
      "train_loss: 0.2102348903820943\n",
      "val_loss: 0.38799257595076503\n",
      "Progress: 15.9% ... Training loss: 0.220 ... Validation loss: 0.397iteration: 1586\n",
      "train_loss: 0.2207032857140989\n",
      "val_loss: 0.39781079775478884\n",
      "Progress: 15.9% ... Training loss: 0.221 ... Validation loss: 0.412iteration: 1587\n",
      "train_loss: 0.22124253846677036\n",
      "val_loss: 0.41219471558701876\n",
      "Progress: 15.9% ... Training loss: 0.225 ... Validation loss: 0.401iteration: 1588\n",
      "train_loss: 0.22500654749338203\n",
      "val_loss: 0.4014287866544143\n",
      "Progress: 15.9% ... Training loss: 0.212 ... Validation loss: 0.402iteration: 1589\n",
      "train_loss: 0.2124413059524632\n",
      "val_loss: 0.40242551471336174\n",
      "Progress: 15.9% ... Training loss: 0.215 ... Validation loss: 0.413iteration: 1590\n",
      "train_loss: 0.2157544639147778\n",
      "val_loss: 0.41304143255736564\n",
      "Progress: 15.9% ... Training loss: 0.211 ... Validation loss: 0.396iteration: 1591\n",
      "train_loss: 0.21183651828928857\n",
      "val_loss: 0.39626656646099434\n",
      "Progress: 15.9% ... Training loss: 0.211 ... Validation loss: 0.396iteration: 1592\n",
      "train_loss: 0.21120558074896167\n",
      "val_loss: 0.39605766608115134\n",
      "Progress: 15.9% ... Training loss: 0.214 ... Validation loss: 0.398iteration: 1593\n",
      "train_loss: 0.21496230584227433\n",
      "val_loss: 0.39885388580374886\n",
      "Progress: 15.9% ... Training loss: 0.208 ... Validation loss: 0.391iteration: 1594\n",
      "train_loss: 0.2087722512438048\n",
      "val_loss: 0.39150461973547235\n",
      "Progress: 15.9% ... Training loss: 0.233 ... Validation loss: 0.415iteration: 1595\n",
      "train_loss: 0.23373882743265686\n",
      "val_loss: 0.41534194751411746\n",
      "Progress: 16.0% ... Training loss: 0.209 ... Validation loss: 0.383iteration: 1596\n",
      "train_loss: 0.20943479859697509\n",
      "val_loss: 0.3832873796857912\n",
      "Progress: 16.0% ... Training loss: 0.222 ... Validation loss: 0.405iteration: 1597\n",
      "train_loss: 0.22217842549755812\n",
      "val_loss: 0.405657962369895\n",
      "Progress: 16.0% ... Training loss: 0.218 ... Validation loss: 0.382iteration: 1598\n",
      "train_loss: 0.2185173628926445\n",
      "val_loss: 0.3827203470062233\n",
      "Progress: 16.0% ... Training loss: 0.222 ... Validation loss: 0.427iteration: 1599\n",
      "train_loss: 0.22253672049290255\n",
      "val_loss: 0.4274433604964401\n",
      "Progress: 16.0% ... Training loss: 0.209 ... Validation loss: 0.384iteration: 1600\n",
      "train_loss: 0.20918908217651871\n",
      "val_loss: 0.38488052554220836\n",
      "Progress: 16.0% ... Training loss: 0.209 ... Validation loss: 0.385iteration: 1601\n",
      "train_loss: 0.20969197686526822\n",
      "val_loss: 0.38508356414195377\n",
      "Progress: 16.0% ... Training loss: 0.212 ... Validation loss: 0.393iteration: 1602\n",
      "train_loss: 0.21218631760150508\n",
      "val_loss: 0.39318715187787484\n",
      "Progress: 16.0% ... Training loss: 0.210 ... Validation loss: 0.384iteration: 1603\n",
      "train_loss: 0.21030716805318467\n",
      "val_loss: 0.3841397140089109\n",
      "Progress: 16.0% ... Training loss: 0.212 ... Validation loss: 0.377iteration: 1604\n",
      "train_loss: 0.21291028989226465\n",
      "val_loss: 0.3772565654364932\n",
      "Progress: 16.1% ... Training loss: 0.218 ... Validation loss: 0.403iteration: 1605\n",
      "train_loss: 0.21820368454614492\n",
      "val_loss: 0.4033476342660265\n",
      "Progress: 16.1% ... Training loss: 0.220 ... Validation loss: 0.379iteration: 1606\n",
      "train_loss: 0.22060006773221924\n",
      "val_loss: 0.3792801516786159\n",
      "Progress: 16.1% ... Training loss: 0.209 ... Validation loss: 0.391iteration: 1607\n",
      "train_loss: 0.20909800623366612\n",
      "val_loss: 0.39167203614171403\n",
      "Progress: 16.1% ... Training loss: 0.211 ... Validation loss: 0.381iteration: 1608\n",
      "train_loss: 0.21109250159586848\n",
      "val_loss: 0.3815147074855106\n",
      "Progress: 16.1% ... Training loss: 0.209 ... Validation loss: 0.394iteration: 1609\n",
      "train_loss: 0.2091059705663004\n",
      "val_loss: 0.3943812932986388\n",
      "Progress: 16.1% ... Training loss: 0.210 ... Validation loss: 0.396iteration: 1610\n",
      "train_loss: 0.2108685052663985\n",
      "val_loss: 0.3966815162799717\n",
      "Progress: 16.1% ... Training loss: 0.230 ... Validation loss: 0.384iteration: 1611\n",
      "train_loss: 0.23064188739075428\n",
      "val_loss: 0.38487399843875275\n",
      "Progress: 16.1% ... Training loss: 0.209 ... Validation loss: 0.395iteration: 1612\n",
      "train_loss: 0.2091794191453484\n",
      "val_loss: 0.3954415927922335\n",
      "Progress: 16.1% ... Training loss: 0.209 ... Validation loss: 0.382iteration: 1613\n",
      "train_loss: 0.20903702842408498\n",
      "val_loss: 0.3829803430072917\n",
      "Progress: 16.1% ... Training loss: 0.233 ... Validation loss: 0.404iteration: 1614\n",
      "train_loss: 0.2336980572385101\n",
      "val_loss: 0.40475207074035535\n",
      "Progress: 16.1% ... Training loss: 0.217 ... Validation loss: 0.376iteration: 1615\n",
      "train_loss: 0.21737963174482736\n",
      "val_loss: 0.3768128714962416\n",
      "Progress: 16.2% ... Training loss: 0.210 ... Validation loss: 0.385iteration: 1616\n",
      "train_loss: 0.21049876265609233\n",
      "val_loss: 0.3850468192641157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 16.2% ... Training loss: 0.220 ... Validation loss: 0.414iteration: 1617\n",
      "train_loss: 0.22060986643805247\n",
      "val_loss: 0.41481142926296594\n",
      "Progress: 16.2% ... Training loss: 0.206 ... Validation loss: 0.380iteration: 1618\n",
      "train_loss: 0.2068472035615224\n",
      "val_loss: 0.38069860638237857\n",
      "Progress: 16.2% ... Training loss: 0.206 ... Validation loss: 0.380iteration: 1619\n",
      "train_loss: 0.20690856962786802\n",
      "val_loss: 0.3801608563253048\n",
      "Progress: 16.2% ... Training loss: 0.240 ... Validation loss: 0.443iteration: 1620\n",
      "train_loss: 0.24045637508562215\n",
      "val_loss: 0.4431750180157255\n",
      "Progress: 16.2% ... Training loss: 0.207 ... Validation loss: 0.376iteration: 1621\n",
      "train_loss: 0.20735415981272834\n",
      "val_loss: 0.3768417580212099\n",
      "Progress: 16.2% ... Training loss: 0.207 ... Validation loss: 0.382iteration: 1622\n",
      "train_loss: 0.2075993962031747\n",
      "val_loss: 0.3820942223037872\n",
      "Progress: 16.2% ... Training loss: 0.205 ... Validation loss: 0.378iteration: 1623\n",
      "train_loss: 0.20582759020770247\n",
      "val_loss: 0.37804438100662485\n",
      "Progress: 16.2% ... Training loss: 0.205 ... Validation loss: 0.384iteration: 1624\n",
      "train_loss: 0.20539878712419438\n",
      "val_loss: 0.3845201854861216\n",
      "Progress: 16.2% ... Training loss: 0.207 ... Validation loss: 0.390iteration: 1625\n",
      "train_loss: 0.20759645413276076\n",
      "val_loss: 0.3909263735045764\n",
      "Progress: 16.3% ... Training loss: 0.208 ... Validation loss: 0.377iteration: 1626\n",
      "train_loss: 0.20879941429034377\n",
      "val_loss: 0.3770249408263098\n",
      "Progress: 16.3% ... Training loss: 0.214 ... Validation loss: 0.401iteration: 1627\n",
      "train_loss: 0.21416606151512277\n",
      "val_loss: 0.4017857072729195\n",
      "Progress: 16.3% ... Training loss: 0.206 ... Validation loss: 0.379iteration: 1628\n",
      "train_loss: 0.20642429548992283\n",
      "val_loss: 0.3798270778623384\n",
      "Progress: 16.3% ... Training loss: 0.232 ... Validation loss: 0.393iteration: 1629\n",
      "train_loss: 0.23275360384995158\n",
      "val_loss: 0.3933139263579384\n",
      "Progress: 16.3% ... Training loss: 0.208 ... Validation loss: 0.399iteration: 1630\n",
      "train_loss: 0.20818014928504971\n",
      "val_loss: 0.3990736585176729\n",
      "Progress: 16.3% ... Training loss: 0.205 ... Validation loss: 0.379iteration: 1631\n",
      "train_loss: 0.20558428490718633\n",
      "val_loss: 0.3792450814853212\n",
      "Progress: 16.3% ... Training loss: 0.204 ... Validation loss: 0.382iteration: 1632\n",
      "train_loss: 0.20452787845054085\n",
      "val_loss: 0.38253549303656453\n",
      "Progress: 16.3% ... Training loss: 0.205 ... Validation loss: 0.386iteration: 1633\n",
      "train_loss: 0.20576378933322217\n",
      "val_loss: 0.38601742734606714\n",
      "Progress: 16.3% ... Training loss: 0.207 ... Validation loss: 0.394iteration: 1634\n",
      "train_loss: 0.2076309654717911\n",
      "val_loss: 0.39463354419170776\n",
      "Progress: 16.4% ... Training loss: 0.204 ... Validation loss: 0.382iteration: 1635\n",
      "train_loss: 0.20410491005186399\n",
      "val_loss: 0.3823607350315504\n",
      "Progress: 16.4% ... Training loss: 0.208 ... Validation loss: 0.397iteration: 1636\n",
      "train_loss: 0.20877667350085258\n",
      "val_loss: 0.39734956070658395\n",
      "Progress: 16.4% ... Training loss: 0.211 ... Validation loss: 0.379iteration: 1637\n",
      "train_loss: 0.21138606524277154\n",
      "val_loss: 0.37998718905224016\n",
      "Progress: 16.4% ... Training loss: 0.206 ... Validation loss: 0.391iteration: 1638\n",
      "train_loss: 0.2060886303916887\n",
      "val_loss: 0.3913401260292378\n",
      "Progress: 16.4% ... Training loss: 0.207 ... Validation loss: 0.382iteration: 1639\n",
      "train_loss: 0.20757089475782353\n",
      "val_loss: 0.3821451803960605\n",
      "Progress: 16.4% ... Training loss: 0.208 ... Validation loss: 0.397iteration: 1640\n",
      "train_loss: 0.2081377004083524\n",
      "val_loss: 0.3973139476471909\n",
      "Progress: 16.4% ... Training loss: 0.203 ... Validation loss: 0.387iteration: 1641\n",
      "train_loss: 0.2039800343244411\n",
      "val_loss: 0.3878799921286347\n",
      "Progress: 16.4% ... Training loss: 0.208 ... Validation loss: 0.378iteration: 1642\n",
      "train_loss: 0.20882483322234505\n",
      "val_loss: 0.37889472296637117\n",
      "Progress: 16.4% ... Training loss: 0.204 ... Validation loss: 0.391iteration: 1643\n",
      "train_loss: 0.20475839719980996\n",
      "val_loss: 0.3912002955645791\n",
      "Progress: 16.4% ... Training loss: 0.203 ... Validation loss: 0.383iteration: 1644\n",
      "train_loss: 0.20389202634269263\n",
      "val_loss: 0.38312191576969057\n",
      "Progress: 16.4% ... Training loss: 0.209 ... Validation loss: 0.396iteration: 1645\n",
      "train_loss: 0.2093988727022957\n",
      "val_loss: 0.3967761085482159\n",
      "Progress: 16.5% ... Training loss: 0.203 ... Validation loss: 0.373iteration: 1646\n",
      "train_loss: 0.2038788173202062\n",
      "val_loss: 0.37345338747838813\n",
      "Progress: 16.5% ... Training loss: 0.205 ... Validation loss: 0.371iteration: 1647\n",
      "train_loss: 0.2056496593838114\n",
      "val_loss: 0.371779432481893\n",
      "Progress: 16.5% ... Training loss: 0.209 ... Validation loss: 0.385iteration: 1648\n",
      "train_loss: 0.2092445136057329\n",
      "val_loss: 0.38532407169462335\n",
      "Progress: 16.5% ... Training loss: 0.218 ... Validation loss: 0.385iteration: 1649\n",
      "train_loss: 0.218981728928477\n",
      "val_loss: 0.3852216362664619\n",
      "Progress: 16.5% ... Training loss: 0.203 ... Validation loss: 0.383iteration: 1650\n",
      "train_loss: 0.2036253450187838\n",
      "val_loss: 0.38354765847765915\n",
      "Progress: 16.5% ... Training loss: 0.203 ... Validation loss: 0.376iteration: 1651\n",
      "train_loss: 0.2030754762090315\n",
      "val_loss: 0.3767462577988396\n",
      "Progress: 16.5% ... Training loss: 0.205 ... Validation loss: 0.373iteration: 1652\n",
      "train_loss: 0.20523969493710995\n",
      "val_loss: 0.373164741661337\n",
      "Progress: 16.5% ... Training loss: 0.204 ... Validation loss: 0.378iteration: 1653\n",
      "train_loss: 0.2041243079654008\n",
      "val_loss: 0.37813776841100327\n",
      "Progress: 16.5% ... Training loss: 0.214 ... Validation loss: 0.383iteration: 1654\n",
      "train_loss: 0.21462455702462938\n",
      "val_loss: 0.38338569571098763\n",
      "Progress: 16.6% ... Training loss: 0.203 ... Validation loss: 0.392iteration: 1655\n",
      "train_loss: 0.20366831670761112\n",
      "val_loss: 0.3928694641548715\n",
      "Progress: 16.6% ... Training loss: 0.201 ... Validation loss: 0.382iteration: 1656\n",
      "train_loss: 0.20187234998775946\n",
      "val_loss: 0.3825441217416299\n",
      "Progress: 16.6% ... Training loss: 0.202 ... Validation loss: 0.380iteration: 1657\n",
      "train_loss: 0.2020101923459097\n",
      "val_loss: 0.3800873963908762\n",
      "Progress: 16.6% ... Training loss: 0.202 ... Validation loss: 0.388iteration: 1658\n",
      "train_loss: 0.20296476005744826\n",
      "val_loss: 0.3882750933766796\n",
      "Progress: 16.6% ... Training loss: 0.204 ... Validation loss: 0.386iteration: 1659\n",
      "train_loss: 0.20433033558142705\n",
      "val_loss: 0.386020487109281\n",
      "Progress: 16.6% ... Training loss: 0.201 ... Validation loss: 0.378iteration: 1660\n",
      "train_loss: 0.20119777309154654\n",
      "val_loss: 0.37872217518903606\n",
      "Progress: 16.6% ... Training loss: 0.201 ... Validation loss: 0.374iteration: 1661\n",
      "train_loss: 0.20155880599392947\n",
      "val_loss: 0.37431768731287984\n",
      "Progress: 16.6% ... Training loss: 0.212 ... Validation loss: 0.377iteration: 1662\n",
      "train_loss: 0.21283039204534235\n",
      "val_loss: 0.377799867247391\n",
      "Progress: 16.6% ... Training loss: 0.203 ... Validation loss: 0.376iteration: 1663\n",
      "train_loss: 0.20335672648271852\n",
      "val_loss: 0.3766983180223618\n",
      "Progress: 16.6% ... Training loss: 0.201 ... Validation loss: 0.375iteration: 1664\n",
      "train_loss: 0.2015082046082616\n",
      "val_loss: 0.37541418305115437\n",
      "Progress: 16.6% ... Training loss: 0.201 ... Validation loss: 0.373iteration: 1665\n",
      "train_loss: 0.20156961768637263\n",
      "val_loss: 0.3739359988951606\n",
      "Progress: 16.7% ... Training loss: 0.203 ... Validation loss: 0.381iteration: 1666\n",
      "train_loss: 0.20361386643441598\n",
      "val_loss: 0.3810151773103003\n",
      "Progress: 16.7% ... Training loss: 0.205 ... Validation loss: 0.384iteration: 1667\n",
      "train_loss: 0.2053498958194283\n",
      "val_loss: 0.3849335117344361\n",
      "Progress: 16.7% ... Training loss: 0.205 ... Validation loss: 0.363iteration: 1668\n",
      "train_loss: 0.2052860490793758\n",
      "val_loss: 0.36375761877532714\n",
      "Progress: 16.7% ... Training loss: 0.201 ... Validation loss: 0.379iteration: 1669\n",
      "train_loss: 0.201656956967617\n",
      "val_loss: 0.3793764047093802\n",
      "Progress: 16.7% ... Training loss: 0.204 ... Validation loss: 0.397iteration: 1670\n",
      "train_loss: 0.20492658743333794\n",
      "val_loss: 0.397199851384127\n",
      "Progress: 16.7% ... Training loss: 0.201 ... Validation loss: 0.386iteration: 1671\n",
      "train_loss: 0.20165344321271228\n",
      "val_loss: 0.38692706184335734\n",
      "Progress: 16.7% ... Training loss: 0.201 ... Validation loss: 0.379iteration: 1672\n",
      "train_loss: 0.20117419255262406\n",
      "val_loss: 0.3792127639923438\n",
      "Progress: 16.7% ... Training loss: 0.206 ... Validation loss: 0.394iteration: 1673\n",
      "train_loss: 0.20622438065454646\n",
      "val_loss: 0.3946492933043229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 16.7% ... Training loss: 0.203 ... Validation loss: 0.375iteration: 1674\n",
      "train_loss: 0.20357009400888124\n",
      "val_loss: 0.37541314053956404\n",
      "Progress: 16.8% ... Training loss: 0.208 ... Validation loss: 0.375iteration: 1675\n",
      "train_loss: 0.20838697144246227\n",
      "val_loss: 0.3754013719610447\n",
      "Progress: 16.8% ... Training loss: 0.205 ... Validation loss: 0.409iteration: 1676\n",
      "train_loss: 0.2054038250740902\n",
      "val_loss: 0.4096737455609246\n",
      "Progress: 16.8% ... Training loss: 0.202 ... Validation loss: 0.377iteration: 1677\n",
      "train_loss: 0.20251444756332374\n",
      "val_loss: 0.37723549436386256\n",
      "Progress: 16.8% ... Training loss: 0.202 ... Validation loss: 0.377iteration: 1678\n",
      "train_loss: 0.20267064251428157\n",
      "val_loss: 0.3771572184656153\n",
      "Progress: 16.8% ... Training loss: 0.198 ... Validation loss: 0.378iteration: 1679\n",
      "train_loss: 0.19898816099359773\n",
      "val_loss: 0.3787447496580316\n",
      "Progress: 16.8% ... Training loss: 0.203 ... Validation loss: 0.388iteration: 1680\n",
      "train_loss: 0.20304840665201665\n",
      "val_loss: 0.3885017525415429\n",
      "Progress: 16.8% ... Training loss: 0.198 ... Validation loss: 0.378iteration: 1681\n",
      "train_loss: 0.19872950683715462\n",
      "val_loss: 0.37896126618130593\n",
      "Progress: 16.8% ... Training loss: 0.201 ... Validation loss: 0.372iteration: 1682\n",
      "train_loss: 0.20111732645121624\n",
      "val_loss: 0.3721742064404365\n",
      "Progress: 16.8% ... Training loss: 0.210 ... Validation loss: 0.395iteration: 1683\n",
      "train_loss: 0.2100746899528642\n",
      "val_loss: 0.3952741670542527\n",
      "Progress: 16.8% ... Training loss: 0.199 ... Validation loss: 0.369iteration: 1684\n",
      "train_loss: 0.19975050316445894\n",
      "val_loss: 0.3698143922200286\n",
      "Progress: 16.9% ... Training loss: 0.200 ... Validation loss: 0.380iteration: 1685\n",
      "train_loss: 0.20070265703102064\n",
      "val_loss: 0.38029196981870383\n",
      "Progress: 16.9% ... Training loss: 0.202 ... Validation loss: 0.380iteration: 1686\n",
      "train_loss: 0.20275913639741058\n",
      "val_loss: 0.380264150049238\n",
      "Progress: 16.9% ... Training loss: 0.205 ... Validation loss: 0.377iteration: 1687\n",
      "train_loss: 0.2053738681608002\n",
      "val_loss: 0.3772292894500514\n",
      "Progress: 16.9% ... Training loss: 0.201 ... Validation loss: 0.381iteration: 1688\n",
      "train_loss: 0.20102501655696783\n",
      "val_loss: 0.38177202735027954\n",
      "Progress: 16.9% ... Training loss: 0.198 ... Validation loss: 0.375iteration: 1689\n",
      "train_loss: 0.19817199849686526\n",
      "val_loss: 0.3754351256640681\n",
      "Progress: 16.9% ... Training loss: 0.203 ... Validation loss: 0.392iteration: 1690\n",
      "train_loss: 0.2034407423674602\n",
      "val_loss: 0.3920374912479676\n",
      "Progress: 16.9% ... Training loss: 0.199 ... Validation loss: 0.375iteration: 1691\n",
      "train_loss: 0.19974842159194875\n",
      "val_loss: 0.37587745981698323\n",
      "Progress: 16.9% ... Training loss: 0.212 ... Validation loss: 0.377iteration: 1692\n",
      "train_loss: 0.21292709096728057\n",
      "val_loss: 0.3774738486993131\n",
      "Progress: 16.9% ... Training loss: 0.199 ... Validation loss: 0.378iteration: 1693\n",
      "train_loss: 0.19909596253486295\n",
      "val_loss: 0.3785400510849027\n",
      "Progress: 16.9% ... Training loss: 0.204 ... Validation loss: 0.367iteration: 1694\n",
      "train_loss: 0.2040567423520165\n",
      "val_loss: 0.3679160844454997\n",
      "Progress: 16.9% ... Training loss: 0.201 ... Validation loss: 0.371iteration: 1695\n",
      "train_loss: 0.20163347633800702\n",
      "val_loss: 0.37175833318936047\n",
      "Progress: 17.0% ... Training loss: 0.198 ... Validation loss: 0.378iteration: 1696\n",
      "train_loss: 0.19883909363915594\n",
      "val_loss: 0.37802097290144066\n",
      "Progress: 17.0% ... Training loss: 0.209 ... Validation loss: 0.390iteration: 1697\n",
      "train_loss: 0.2095774889032307\n",
      "val_loss: 0.39093051277497276\n",
      "Progress: 17.0% ... Training loss: 0.216 ... Validation loss: 0.368iteration: 1698\n",
      "train_loss: 0.21640281293488847\n",
      "val_loss: 0.36862775031521877\n",
      "Progress: 17.0% ... Training loss: 0.201 ... Validation loss: 0.367iteration: 1699\n",
      "train_loss: 0.2016868935896683\n",
      "val_loss: 0.3670058598882736\n",
      "Progress: 17.0% ... Training loss: 0.198 ... Validation loss: 0.367iteration: 1700\n",
      "train_loss: 0.19863264497457378\n",
      "val_loss: 0.3670768855046458\n",
      "Progress: 17.0% ... Training loss: 0.200 ... Validation loss: 0.379iteration: 1701\n",
      "train_loss: 0.20038793721600912\n",
      "val_loss: 0.37918568135188885\n",
      "Progress: 17.0% ... Training loss: 0.209 ... Validation loss: 0.390iteration: 1702\n",
      "train_loss: 0.2094525105604244\n",
      "val_loss: 0.39080573960900566\n",
      "Progress: 17.0% ... Training loss: 0.200 ... Validation loss: 0.370iteration: 1703\n",
      "train_loss: 0.20018950232408042\n",
      "val_loss: 0.37030702044468866\n",
      "Progress: 17.0% ... Training loss: 0.200 ... Validation loss: 0.371iteration: 1704\n",
      "train_loss: 0.20001645987284689\n",
      "val_loss: 0.37160057707670446\n",
      "Progress: 17.1% ... Training loss: 0.200 ... Validation loss: 0.367iteration: 1705\n",
      "train_loss: 0.2002786744843903\n",
      "val_loss: 0.3673984236792433\n",
      "Progress: 17.1% ... Training loss: 0.201 ... Validation loss: 0.374iteration: 1706\n",
      "train_loss: 0.20170653563247978\n",
      "val_loss: 0.3740876317797144\n",
      "Progress: 17.1% ... Training loss: 0.205 ... Validation loss: 0.371iteration: 1707\n",
      "train_loss: 0.2056414986090982\n",
      "val_loss: 0.3717667436438926\n",
      "Progress: 17.1% ... Training loss: 0.196 ... Validation loss: 0.360iteration: 1708\n",
      "train_loss: 0.19665551268411985\n",
      "val_loss: 0.36091893624246646\n",
      "Progress: 17.1% ... Training loss: 0.206 ... Validation loss: 0.376iteration: 1709\n",
      "train_loss: 0.2062608289950107\n",
      "val_loss: 0.37640014526834337\n",
      "Progress: 17.1% ... Training loss: 0.213 ... Validation loss: 0.386iteration: 1710\n",
      "train_loss: 0.21380291294217632\n",
      "val_loss: 0.3869798840617881\n",
      "Progress: 17.1% ... Training loss: 0.197 ... Validation loss: 0.363iteration: 1711\n",
      "train_loss: 0.19772842131204985\n",
      "val_loss: 0.36369225486682444\n",
      "Progress: 17.1% ... Training loss: 0.198 ... Validation loss: 0.363iteration: 1712\n",
      "train_loss: 0.1981660933470368\n",
      "val_loss: 0.3632802735029404\n",
      "Progress: 17.1% ... Training loss: 0.201 ... Validation loss: 0.370iteration: 1713\n",
      "train_loss: 0.20126798052845934\n",
      "val_loss: 0.3702408457166612\n",
      "Progress: 17.1% ... Training loss: 0.198 ... Validation loss: 0.373iteration: 1714\n",
      "train_loss: 0.19865926723250843\n",
      "val_loss: 0.3736510773349006\n",
      "Progress: 17.1% ... Training loss: 0.196 ... Validation loss: 0.368iteration: 1715\n",
      "train_loss: 0.1968427908709198\n",
      "val_loss: 0.36853840896353735\n",
      "Progress: 17.2% ... Training loss: 0.195 ... Validation loss: 0.372iteration: 1716\n",
      "train_loss: 0.19576489847561568\n",
      "val_loss: 0.3727673860259467\n",
      "Progress: 17.2% ... Training loss: 0.196 ... Validation loss: 0.373iteration: 1717\n",
      "train_loss: 0.19630246792513753\n",
      "val_loss: 0.37321019600396554\n",
      "Progress: 17.2% ... Training loss: 0.199 ... Validation loss: 0.374iteration: 1718\n",
      "train_loss: 0.1997251087619621\n",
      "val_loss: 0.3745707264822231\n",
      "Progress: 17.2% ... Training loss: 0.223 ... Validation loss: 0.391iteration: 1719\n",
      "train_loss: 0.22394790462521105\n",
      "val_loss: 0.39169822939010496\n",
      "Progress: 17.2% ... Training loss: 0.201 ... Validation loss: 0.367iteration: 1720\n",
      "train_loss: 0.2018071823004494\n",
      "val_loss: 0.36776339488115745\n",
      "Progress: 17.2% ... Training loss: 0.194 ... Validation loss: 0.366iteration: 1721\n",
      "train_loss: 0.19458381396232452\n",
      "val_loss: 0.3667239499908913\n",
      "Progress: 17.2% ... Training loss: 0.194 ... Validation loss: 0.365iteration: 1722\n",
      "train_loss: 0.1940248172728184\n",
      "val_loss: 0.36557380512633875\n",
      "Progress: 17.2% ... Training loss: 0.201 ... Validation loss: 0.370iteration: 1723\n",
      "train_loss: 0.20160887106848033\n",
      "val_loss: 0.3707499119465584\n",
      "Progress: 17.2% ... Training loss: 0.197 ... Validation loss: 0.368iteration: 1724\n",
      "train_loss: 0.19726366520223282\n",
      "val_loss: 0.3684372113132604\n",
      "Progress: 17.2% ... Training loss: 0.196 ... Validation loss: 0.362iteration: 1725\n",
      "train_loss: 0.1968442395420393\n",
      "val_loss: 0.3627787488834464\n",
      "Progress: 17.3% ... Training loss: 0.193 ... Validation loss: 0.370iteration: 1726\n",
      "train_loss: 0.19329262875124886\n",
      "val_loss: 0.37094951340978527\n",
      "Progress: 17.3% ... Training loss: 0.194 ... Validation loss: 0.376iteration: 1727\n",
      "train_loss: 0.1946053053316796\n",
      "val_loss: 0.37668651637335926\n",
      "Progress: 17.3% ... Training loss: 0.201 ... Validation loss: 0.369iteration: 1728\n",
      "train_loss: 0.20128522533650853\n",
      "val_loss: 0.3697914175207553\n",
      "Progress: 17.3% ... Training loss: 0.193 ... Validation loss: 0.368iteration: 1729\n",
      "train_loss: 0.1937187343514319\n",
      "val_loss: 0.3689279249008929\n",
      "Progress: 17.3% ... Training loss: 0.196 ... Validation loss: 0.367iteration: 1730\n",
      "train_loss: 0.19688592140851238\n",
      "val_loss: 0.3679822323716354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 17.3% ... Training loss: 0.193 ... Validation loss: 0.369iteration: 1731\n",
      "train_loss: 0.1932663608908522\n",
      "val_loss: 0.36942698376092464\n",
      "Progress: 17.3% ... Training loss: 0.194 ... Validation loss: 0.364iteration: 1732\n",
      "train_loss: 0.19416671506648195\n",
      "val_loss: 0.36416193538928315\n",
      "Progress: 17.3% ... Training loss: 0.196 ... Validation loss: 0.363iteration: 1733\n",
      "train_loss: 0.1965426867922636\n",
      "val_loss: 0.36347723911487856\n",
      "Progress: 17.3% ... Training loss: 0.194 ... Validation loss: 0.365iteration: 1734\n",
      "train_loss: 0.19404921613406992\n",
      "val_loss: 0.36515268658464556\n",
      "Progress: 17.4% ... Training loss: 0.193 ... Validation loss: 0.363iteration: 1735\n",
      "train_loss: 0.19384621343646524\n",
      "val_loss: 0.36302937636335886\n",
      "Progress: 17.4% ... Training loss: 0.199 ... Validation loss: 0.381iteration: 1736\n",
      "train_loss: 0.1992190470871822\n",
      "val_loss: 0.38161047336466225\n",
      "Progress: 17.4% ... Training loss: 0.202 ... Validation loss: 0.375iteration: 1737\n",
      "train_loss: 0.2026802252924643\n",
      "val_loss: 0.37574595963746843\n",
      "Progress: 17.4% ... Training loss: 0.220 ... Validation loss: 0.404iteration: 1738\n",
      "train_loss: 0.22034815733216492\n",
      "val_loss: 0.40484642304360663\n",
      "Progress: 17.4% ... Training loss: 0.202 ... Validation loss: 0.367iteration: 1739\n",
      "train_loss: 0.2023464180105745\n",
      "val_loss: 0.3677714512816234\n",
      "Progress: 17.4% ... Training loss: 0.191 ... Validation loss: 0.368iteration: 1740\n",
      "train_loss: 0.19199904664510972\n",
      "val_loss: 0.3686366909609561\n",
      "Progress: 17.4% ... Training loss: 0.196 ... Validation loss: 0.366iteration: 1741\n",
      "train_loss: 0.19611146263830212\n",
      "val_loss: 0.3662783667190319\n",
      "Progress: 17.4% ... Training loss: 0.191 ... Validation loss: 0.370iteration: 1742\n",
      "train_loss: 0.19180650707524424\n",
      "val_loss: 0.37032885034301916\n",
      "Progress: 17.4% ... Training loss: 0.193 ... Validation loss: 0.372iteration: 1743\n",
      "train_loss: 0.19378340836534874\n",
      "val_loss: 0.3725011105418624\n",
      "Progress: 17.4% ... Training loss: 0.193 ... Validation loss: 0.372iteration: 1744\n",
      "train_loss: 0.19353508582183046\n",
      "val_loss: 0.3729239907516957\n",
      "Progress: 17.4% ... Training loss: 0.193 ... Validation loss: 0.372iteration: 1745\n",
      "train_loss: 0.19337800415450979\n",
      "val_loss: 0.3724315159060771\n",
      "Progress: 17.5% ... Training loss: 0.192 ... Validation loss: 0.362iteration: 1746\n",
      "train_loss: 0.19247800919628136\n",
      "val_loss: 0.36285366998067303\n",
      "Progress: 17.5% ... Training loss: 0.193 ... Validation loss: 0.361iteration: 1747\n",
      "train_loss: 0.19345621678647604\n",
      "val_loss: 0.3615016207293355\n",
      "Progress: 17.5% ... Training loss: 0.191 ... Validation loss: 0.360iteration: 1748\n",
      "train_loss: 0.19133798266041455\n",
      "val_loss: 0.36018731075040333\n",
      "Progress: 17.5% ... Training loss: 0.191 ... Validation loss: 0.362iteration: 1749\n",
      "train_loss: 0.19142939514955198\n",
      "val_loss: 0.3621506852240599\n",
      "Progress: 17.5% ... Training loss: 0.191 ... Validation loss: 0.364iteration: 1750\n",
      "train_loss: 0.1917014138133326\n",
      "val_loss: 0.3649447782168468\n",
      "Progress: 17.5% ... Training loss: 0.191 ... Validation loss: 0.358iteration: 1751\n",
      "train_loss: 0.1910106203617887\n",
      "val_loss: 0.35865505079484206\n",
      "Progress: 17.5% ... Training loss: 0.192 ... Validation loss: 0.359iteration: 1752\n",
      "train_loss: 0.19252622818858167\n",
      "val_loss: 0.3596239431491335\n",
      "Progress: 17.5% ... Training loss: 0.195 ... Validation loss: 0.364iteration: 1753\n",
      "train_loss: 0.19548910941797654\n",
      "val_loss: 0.3642144848039755\n",
      "Progress: 17.5% ... Training loss: 0.191 ... Validation loss: 0.361iteration: 1754\n",
      "train_loss: 0.19169949611010753\n",
      "val_loss: 0.3613796185258793\n",
      "Progress: 17.6% ... Training loss: 0.200 ... Validation loss: 0.387iteration: 1755\n",
      "train_loss: 0.20069319570954264\n",
      "val_loss: 0.3879989223798816\n",
      "Progress: 17.6% ... Training loss: 0.195 ... Validation loss: 0.362iteration: 1756\n",
      "train_loss: 0.19540987783797079\n",
      "val_loss: 0.36203802445297256\n",
      "Progress: 17.6% ... Training loss: 0.191 ... Validation loss: 0.369iteration: 1757\n",
      "train_loss: 0.19143633455087095\n",
      "val_loss: 0.36927203469346126\n",
      "Progress: 17.6% ... Training loss: 0.211 ... Validation loss: 0.407iteration: 1758\n",
      "train_loss: 0.2115311943450574\n",
      "val_loss: 0.4073543986713746\n",
      "Progress: 17.6% ... Training loss: 0.197 ... Validation loss: 0.357iteration: 1759\n",
      "train_loss: 0.19786698657541776\n",
      "val_loss: 0.35712069513715977\n",
      "Progress: 17.6% ... Training loss: 0.190 ... Validation loss: 0.362iteration: 1760\n",
      "train_loss: 0.19035361058405886\n",
      "val_loss: 0.36214544737656107\n",
      "Progress: 17.6% ... Training loss: 0.190 ... Validation loss: 0.359iteration: 1761\n",
      "train_loss: 0.19070728849930946\n",
      "val_loss: 0.35939363639592137\n",
      "Progress: 17.6% ... Training loss: 0.191 ... Validation loss: 0.358iteration: 1762\n",
      "train_loss: 0.19174462276033752\n",
      "val_loss: 0.3583223869337631\n",
      "Progress: 17.6% ... Training loss: 0.193 ... Validation loss: 0.352iteration: 1763\n",
      "train_loss: 0.19378673402185911\n",
      "val_loss: 0.35297848791108655\n",
      "Progress: 17.6% ... Training loss: 0.197 ... Validation loss: 0.364iteration: 1764\n",
      "train_loss: 0.19732409347707527\n",
      "val_loss: 0.3643169473637864\n",
      "Progress: 17.6% ... Training loss: 0.192 ... Validation loss: 0.353iteration: 1765\n",
      "train_loss: 0.19266995243766952\n",
      "val_loss: 0.35356387041685694\n",
      "Progress: 17.7% ... Training loss: 0.190 ... Validation loss: 0.360iteration: 1766\n",
      "train_loss: 0.1908982565091078\n",
      "val_loss: 0.36055448600434814\n",
      "Progress: 17.7% ... Training loss: 0.190 ... Validation loss: 0.353iteration: 1767\n",
      "train_loss: 0.19010849244939035\n",
      "val_loss: 0.3536598044403634\n",
      "Progress: 17.7% ... Training loss: 0.191 ... Validation loss: 0.352iteration: 1768\n",
      "train_loss: 0.19163693405344281\n",
      "val_loss: 0.35208910971038826\n",
      "Progress: 17.7% ... Training loss: 0.192 ... Validation loss: 0.353iteration: 1769\n",
      "train_loss: 0.19224031718953222\n",
      "val_loss: 0.35325267774817326\n",
      "Progress: 17.7% ... Training loss: 0.189 ... Validation loss: 0.364iteration: 1770\n",
      "train_loss: 0.18991405032757366\n",
      "val_loss: 0.3640236326130926\n",
      "Progress: 17.7% ... Training loss: 0.193 ... Validation loss: 0.355iteration: 1771\n",
      "train_loss: 0.19359824441179402\n",
      "val_loss: 0.3553237768330959\n",
      "Progress: 17.7% ... Training loss: 0.195 ... Validation loss: 0.367iteration: 1772\n",
      "train_loss: 0.19556285725495157\n",
      "val_loss: 0.3672440910192177\n",
      "Progress: 17.7% ... Training loss: 0.190 ... Validation loss: 0.353iteration: 1773\n",
      "train_loss: 0.19057223110687657\n",
      "val_loss: 0.3531426874898579\n",
      "Progress: 17.7% ... Training loss: 0.192 ... Validation loss: 0.375iteration: 1774\n",
      "train_loss: 0.19299915528324718\n",
      "val_loss: 0.37587393327004925\n",
      "Progress: 17.8% ... Training loss: 0.190 ... Validation loss: 0.352iteration: 1775\n",
      "train_loss: 0.1908886892239844\n",
      "val_loss: 0.3528645871988193\n",
      "Progress: 17.8% ... Training loss: 0.188 ... Validation loss: 0.364iteration: 1776\n",
      "train_loss: 0.18891424075814156\n",
      "val_loss: 0.364025368244732\n",
      "Progress: 17.8% ... Training loss: 0.189 ... Validation loss: 0.358iteration: 1777\n",
      "train_loss: 0.18935159918968328\n",
      "val_loss: 0.35879602951879896\n",
      "Progress: 17.8% ... Training loss: 0.204 ... Validation loss: 0.389iteration: 1778\n",
      "train_loss: 0.2043071602871359\n",
      "val_loss: 0.3895903987403061\n",
      "Progress: 17.8% ... Training loss: 0.199 ... Validation loss: 0.358iteration: 1779\n",
      "train_loss: 0.19954801528148647\n",
      "val_loss: 0.35858662337116853\n",
      "Progress: 17.8% ... Training loss: 0.188 ... Validation loss: 0.362iteration: 1780\n",
      "train_loss: 0.18884256868309093\n",
      "val_loss: 0.36286328264785755\n",
      "Progress: 17.8% ... Training loss: 0.204 ... Validation loss: 0.371iteration: 1781\n",
      "train_loss: 0.2043299544873387\n",
      "val_loss: 0.37133629349345953\n",
      "Progress: 17.8% ... Training loss: 0.192 ... Validation loss: 0.347iteration: 1782\n",
      "train_loss: 0.1922013393952766\n",
      "val_loss: 0.34787434578248677\n",
      "Progress: 17.8% ... Training loss: 0.190 ... Validation loss: 0.350iteration: 1783\n",
      "train_loss: 0.19085650188560954\n",
      "val_loss: 0.3508436193626219\n",
      "Progress: 17.8% ... Training loss: 0.187 ... Validation loss: 0.351iteration: 1784\n",
      "train_loss: 0.18716104563598934\n",
      "val_loss: 0.351931851782316\n",
      "Progress: 17.9% ... Training loss: 0.188 ... Validation loss: 0.357iteration: 1785\n",
      "train_loss: 0.18837887517301335\n",
      "val_loss: 0.3574591309069966\n",
      "Progress: 17.9% ... Training loss: 0.188 ... Validation loss: 0.350iteration: 1786\n",
      "train_loss: 0.1884101938997399\n",
      "val_loss: 0.35072966348287016\n",
      "Progress: 17.9% ... Training loss: 0.188 ... Validation loss: 0.360iteration: 1787\n",
      "train_loss: 0.1887084028807668\n",
      "val_loss: 0.3608372440717439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 17.9% ... Training loss: 0.190 ... Validation loss: 0.351iteration: 1788\n",
      "train_loss: 0.1903275348514495\n",
      "val_loss: 0.3510103038040899\n",
      "Progress: 17.9% ... Training loss: 0.188 ... Validation loss: 0.351iteration: 1789\n",
      "train_loss: 0.18859071128216529\n",
      "val_loss: 0.3518110980043393\n",
      "Progress: 17.9% ... Training loss: 0.188 ... Validation loss: 0.346iteration: 1790\n",
      "train_loss: 0.188967979698572\n",
      "val_loss: 0.34630910967405315\n",
      "Progress: 17.9% ... Training loss: 0.187 ... Validation loss: 0.364iteration: 1791\n",
      "train_loss: 0.18794538811556635\n",
      "val_loss: 0.3645817886852087\n",
      "Progress: 17.9% ... Training loss: 0.189 ... Validation loss: 0.355iteration: 1792\n",
      "train_loss: 0.18947836533735943\n",
      "val_loss: 0.35510904473894916\n",
      "Progress: 17.9% ... Training loss: 0.192 ... Validation loss: 0.377iteration: 1793\n",
      "train_loss: 0.19238833721740278\n",
      "val_loss: 0.3776525707475751\n",
      "Progress: 17.9% ... Training loss: 0.194 ... Validation loss: 0.353iteration: 1794\n",
      "train_loss: 0.19453942371505997\n",
      "val_loss: 0.353192244304106\n",
      "Progress: 17.9% ... Training loss: 0.188 ... Validation loss: 0.364iteration: 1795\n",
      "train_loss: 0.18808645325045864\n",
      "val_loss: 0.36484087285017597\n",
      "Progress: 18.0% ... Training loss: 0.187 ... Validation loss: 0.360iteration: 1796\n",
      "train_loss: 0.18788751392253453\n",
      "val_loss: 0.3606932740625742\n",
      "Progress: 18.0% ... Training loss: 0.188 ... Validation loss: 0.363iteration: 1797\n",
      "train_loss: 0.18825339878121342\n",
      "val_loss: 0.3638968078058288\n",
      "Progress: 18.0% ... Training loss: 0.187 ... Validation loss: 0.362iteration: 1798\n",
      "train_loss: 0.1870957658761314\n",
      "val_loss: 0.3620605406695193\n",
      "Progress: 18.0% ... Training loss: 0.187 ... Validation loss: 0.365iteration: 1799\n",
      "train_loss: 0.18700571392789545\n",
      "val_loss: 0.36587996047167165\n",
      "Progress: 18.0% ... Training loss: 0.191 ... Validation loss: 0.377iteration: 1800\n",
      "train_loss: 0.1910585993485572\n",
      "val_loss: 0.3775607020674219\n",
      "Progress: 18.0% ... Training loss: 0.187 ... Validation loss: 0.346iteration: 1801\n",
      "train_loss: 0.18723629627027438\n",
      "val_loss: 0.34693406391802956\n",
      "Progress: 18.0% ... Training loss: 0.189 ... Validation loss: 0.352iteration: 1802\n",
      "train_loss: 0.18911505673115378\n",
      "val_loss: 0.3529743053586515\n",
      "Progress: 18.0% ... Training loss: 0.188 ... Validation loss: 0.343iteration: 1803\n",
      "train_loss: 0.18810624298030765\n",
      "val_loss: 0.34370944952654126\n",
      "Progress: 18.0% ... Training loss: 0.187 ... Validation loss: 0.344iteration: 1804\n",
      "train_loss: 0.1874939692655409\n",
      "val_loss: 0.3446846428801714\n",
      "Progress: 18.1% ... Training loss: 0.188 ... Validation loss: 0.356iteration: 1805\n",
      "train_loss: 0.18863078113819834\n",
      "val_loss: 0.3564260763372685\n",
      "Progress: 18.1% ... Training loss: 0.188 ... Validation loss: 0.352iteration: 1806\n",
      "train_loss: 0.18831643376122725\n",
      "val_loss: 0.35293681470932803\n",
      "Progress: 18.1% ... Training loss: 0.186 ... Validation loss: 0.359iteration: 1807\n",
      "train_loss: 0.18676953739568905\n",
      "val_loss: 0.3594697762364655\n",
      "Progress: 18.1% ... Training loss: 0.187 ... Validation loss: 0.353iteration: 1808\n",
      "train_loss: 0.18737480612883264\n",
      "val_loss: 0.35375925721282697\n",
      "Progress: 18.1% ... Training loss: 0.188 ... Validation loss: 0.358iteration: 1809\n",
      "train_loss: 0.18813391995407\n",
      "val_loss: 0.35819611006401875\n",
      "Progress: 18.1% ... Training loss: 0.188 ... Validation loss: 0.356iteration: 1810\n",
      "train_loss: 0.18831477547757608\n",
      "val_loss: 0.35605655328793906\n",
      "Progress: 18.1% ... Training loss: 0.186 ... Validation loss: 0.353iteration: 1811\n",
      "train_loss: 0.1864078840920077\n",
      "val_loss: 0.35360246430017983\n",
      "Progress: 18.1% ... Training loss: 0.188 ... Validation loss: 0.356iteration: 1812\n",
      "train_loss: 0.1883702353273325\n",
      "val_loss: 0.3565693989291674\n",
      "Progress: 18.1% ... Training loss: 0.184 ... Validation loss: 0.354iteration: 1813\n",
      "train_loss: 0.18425458788981555\n",
      "val_loss: 0.35421214249446464\n",
      "Progress: 18.1% ... Training loss: 0.185 ... Validation loss: 0.346iteration: 1814\n",
      "train_loss: 0.18504796082115355\n",
      "val_loss: 0.34669504357140624\n",
      "Progress: 18.1% ... Training loss: 0.185 ... Validation loss: 0.350iteration: 1815\n",
      "train_loss: 0.1857320194451407\n",
      "val_loss: 0.3500553920511435\n",
      "Progress: 18.2% ... Training loss: 0.186 ... Validation loss: 0.344iteration: 1816\n",
      "train_loss: 0.18661150762601592\n",
      "val_loss: 0.34406449619225243\n",
      "Progress: 18.2% ... Training loss: 0.191 ... Validation loss: 0.341iteration: 1817\n",
      "train_loss: 0.19123096776137322\n",
      "val_loss: 0.34158058609678515\n",
      "Progress: 18.2% ... Training loss: 0.217 ... Validation loss: 0.367iteration: 1818\n",
      "train_loss: 0.2178315946293879\n",
      "val_loss: 0.3679246806320181\n",
      "Progress: 18.2% ... Training loss: 0.214 ... Validation loss: 0.356iteration: 1819\n",
      "train_loss: 0.21497327608060054\n",
      "val_loss: 0.35627781914098344\n",
      "Progress: 18.2% ... Training loss: 0.191 ... Validation loss: 0.369iteration: 1820\n",
      "train_loss: 0.1918948991685391\n",
      "val_loss: 0.3690080339066419\n",
      "Progress: 18.2% ... Training loss: 0.189 ... Validation loss: 0.346iteration: 1821\n",
      "train_loss: 0.1898415760182018\n",
      "val_loss: 0.3467582848552874\n",
      "Progress: 18.2% ... Training loss: 0.184 ... Validation loss: 0.347iteration: 1822\n",
      "train_loss: 0.18474746177870918\n",
      "val_loss: 0.3478190747741247\n",
      "Progress: 18.2% ... Training loss: 0.186 ... Validation loss: 0.356iteration: 1823\n",
      "train_loss: 0.18670673083570546\n",
      "val_loss: 0.3564874181983121\n",
      "Progress: 18.2% ... Training loss: 0.191 ... Validation loss: 0.349iteration: 1824\n",
      "train_loss: 0.19198454183791264\n",
      "val_loss: 0.3493940457642229\n",
      "Progress: 18.2% ... Training loss: 0.185 ... Validation loss: 0.356iteration: 1825\n",
      "train_loss: 0.1856486776583444\n",
      "val_loss: 0.3563934083682711\n",
      "Progress: 18.3% ... Training loss: 0.184 ... Validation loss: 0.348iteration: 1826\n",
      "train_loss: 0.1840477917625877\n",
      "val_loss: 0.3484181304813665\n",
      "Progress: 18.3% ... Training loss: 0.184 ... Validation loss: 0.341iteration: 1827\n",
      "train_loss: 0.18448630754108616\n",
      "val_loss: 0.34192754488049987\n",
      "Progress: 18.3% ... Training loss: 0.185 ... Validation loss: 0.347iteration: 1828\n",
      "train_loss: 0.18566954618658954\n",
      "val_loss: 0.3477861865147002\n",
      "Progress: 18.3% ... Training loss: 0.182 ... Validation loss: 0.339iteration: 1829\n",
      "train_loss: 0.18253793443142272\n",
      "val_loss: 0.33950224295677356\n",
      "Progress: 18.3% ... Training loss: 0.208 ... Validation loss: 0.371iteration: 1830\n",
      "train_loss: 0.2083950766719659\n",
      "val_loss: 0.3717205964708998\n",
      "Progress: 18.3% ... Training loss: 0.192 ... Validation loss: 0.341iteration: 1831\n",
      "train_loss: 0.1929636973838761\n",
      "val_loss: 0.3414617786332055\n",
      "Progress: 18.3% ... Training loss: 0.187 ... Validation loss: 0.340iteration: 1832\n",
      "train_loss: 0.18791209176898596\n",
      "val_loss: 0.340804329829809\n",
      "Progress: 18.3% ... Training loss: 0.182 ... Validation loss: 0.343iteration: 1833\n",
      "train_loss: 0.18226593439271116\n",
      "val_loss: 0.34382321905839297\n",
      "Progress: 18.3% ... Training loss: 0.188 ... Validation loss: 0.341iteration: 1834\n",
      "train_loss: 0.18819110671855305\n",
      "val_loss: 0.3411761042058309\n",
      "Progress: 18.4% ... Training loss: 0.203 ... Validation loss: 0.368iteration: 1835\n",
      "train_loss: 0.20322053839972648\n",
      "val_loss: 0.36852721999111837\n",
      "Progress: 18.4% ... Training loss: 0.186 ... Validation loss: 0.344iteration: 1836\n",
      "train_loss: 0.18657079224285517\n",
      "val_loss: 0.3442430917966256\n",
      "Progress: 18.4% ... Training loss: 0.182 ... Validation loss: 0.338iteration: 1837\n",
      "train_loss: 0.18289611956903398\n",
      "val_loss: 0.33868142997231787\n",
      "Progress: 18.4% ... Training loss: 0.194 ... Validation loss: 0.345iteration: 1838\n",
      "train_loss: 0.19437876638949317\n",
      "val_loss: 0.34599027443369745\n",
      "Progress: 18.4% ... Training loss: 0.184 ... Validation loss: 0.344iteration: 1839\n",
      "train_loss: 0.18448171033318797\n",
      "val_loss: 0.34465883124217545\n",
      "Progress: 18.4% ... Training loss: 0.192 ... Validation loss: 0.342iteration: 1840\n",
      "train_loss: 0.19284266475243358\n",
      "val_loss: 0.34248335454016815\n",
      "Progress: 18.4% ... Training loss: 0.187 ... Validation loss: 0.352iteration: 1841\n",
      "train_loss: 0.18740270530892117\n",
      "val_loss: 0.3524187454084564\n",
      "Progress: 18.4% ... Training loss: 0.181 ... Validation loss: 0.344iteration: 1842\n",
      "train_loss: 0.1819487393618879\n",
      "val_loss: 0.34427501050384446\n",
      "Progress: 18.4% ... Training loss: 0.186 ... Validation loss: 0.351iteration: 1843\n",
      "train_loss: 0.18654872088658375\n",
      "val_loss: 0.351176125102296\n",
      "Progress: 18.4% ... Training loss: 0.183 ... Validation loss: 0.355iteration: 1844\n",
      "train_loss: 0.1837778075141996\n",
      "val_loss: 0.35537018645742646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 18.4% ... Training loss: 0.181 ... Validation loss: 0.353iteration: 1845\n",
      "train_loss: 0.18177747751455417\n",
      "val_loss: 0.3533867042814272\n",
      "Progress: 18.5% ... Training loss: 0.181 ... Validation loss: 0.347iteration: 1846\n",
      "train_loss: 0.18135554358395245\n",
      "val_loss: 0.34757087895215344\n",
      "Progress: 18.5% ... Training loss: 0.182 ... Validation loss: 0.348iteration: 1847\n",
      "train_loss: 0.18264678686434682\n",
      "val_loss: 0.3483567425121854\n",
      "Progress: 18.5% ... Training loss: 0.192 ... Validation loss: 0.359iteration: 1848\n",
      "train_loss: 0.19258858638516516\n",
      "val_loss: 0.3599904411287476\n",
      "Progress: 18.5% ... Training loss: 0.192 ... Validation loss: 0.339iteration: 1849\n",
      "train_loss: 0.1928547743321635\n",
      "val_loss: 0.3399376981809925\n",
      "Progress: 18.5% ... Training loss: 0.182 ... Validation loss: 0.345iteration: 1850\n",
      "train_loss: 0.18204598913412068\n",
      "val_loss: 0.3457815321844189\n",
      "Progress: 18.5% ... Training loss: 0.182 ... Validation loss: 0.335iteration: 1851\n",
      "train_loss: 0.1823498944717483\n",
      "val_loss: 0.3352728479522637\n",
      "Progress: 18.5% ... Training loss: 0.181 ... Validation loss: 0.344iteration: 1852\n",
      "train_loss: 0.1810714042488313\n",
      "val_loss: 0.3446328871799916\n",
      "Progress: 18.5% ... Training loss: 0.194 ... Validation loss: 0.338iteration: 1853\n",
      "train_loss: 0.19427305773859457\n",
      "val_loss: 0.33818985938878626\n",
      "Progress: 18.5% ... Training loss: 0.187 ... Validation loss: 0.353iteration: 1854\n",
      "train_loss: 0.187882120759621\n",
      "val_loss: 0.35350535751496465\n",
      "Progress: 18.6% ... Training loss: 0.184 ... Validation loss: 0.336iteration: 1855\n",
      "train_loss: 0.1840995882461399\n",
      "val_loss: 0.3366770149816985\n",
      "Progress: 18.6% ... Training loss: 0.180 ... Validation loss: 0.349iteration: 1856\n",
      "train_loss: 0.18085578301562008\n",
      "val_loss: 0.3494117185840736\n",
      "Progress: 18.6% ... Training loss: 0.180 ... Validation loss: 0.345iteration: 1857\n",
      "train_loss: 0.1809261842012848\n",
      "val_loss: 0.34536329220098066\n",
      "Progress: 18.6% ... Training loss: 0.184 ... Validation loss: 0.338iteration: 1858\n",
      "train_loss: 0.1845801176925642\n",
      "val_loss: 0.3380141749754234\n",
      "Progress: 18.6% ... Training loss: 0.180 ... Validation loss: 0.346iteration: 1859\n",
      "train_loss: 0.18096521619458708\n",
      "val_loss: 0.3469659562069509\n",
      "Progress: 18.6% ... Training loss: 0.189 ... Validation loss: 0.342iteration: 1860\n",
      "train_loss: 0.1897832652542413\n",
      "val_loss: 0.3423858373533041\n",
      "Progress: 18.6% ... Training loss: 0.180 ... Validation loss: 0.345iteration: 1861\n",
      "train_loss: 0.18016289611110758\n",
      "val_loss: 0.34554721133044153\n",
      "Progress: 18.6% ... Training loss: 0.180 ... Validation loss: 0.346iteration: 1862\n",
      "train_loss: 0.18060881902214918\n",
      "val_loss: 0.34645465432216643\n",
      "Progress: 18.6% ... Training loss: 0.182 ... Validation loss: 0.357iteration: 1863\n",
      "train_loss: 0.18219863698663724\n",
      "val_loss: 0.3573599102529324\n",
      "Progress: 18.6% ... Training loss: 0.180 ... Validation loss: 0.342iteration: 1864\n",
      "train_loss: 0.1807416385230523\n",
      "val_loss: 0.3426214674772968\n",
      "Progress: 18.6% ... Training loss: 0.181 ... Validation loss: 0.350iteration: 1865\n",
      "train_loss: 0.18148805993938275\n",
      "val_loss: 0.35093796770461577\n",
      "Progress: 18.7% ... Training loss: 0.185 ... Validation loss: 0.335iteration: 1866\n",
      "train_loss: 0.1856527022348765\n",
      "val_loss: 0.3357737391261223\n",
      "Progress: 18.7% ... Training loss: 0.178 ... Validation loss: 0.336iteration: 1867\n",
      "train_loss: 0.1785611995975828\n",
      "val_loss: 0.33691647939758546\n",
      "Progress: 18.7% ... Training loss: 0.178 ... Validation loss: 0.337iteration: 1868\n",
      "train_loss: 0.17877860771584592\n",
      "val_loss: 0.33797997437625643\n",
      "Progress: 18.7% ... Training loss: 0.179 ... Validation loss: 0.339iteration: 1869\n",
      "train_loss: 0.17917854796307056\n",
      "val_loss: 0.33908228950283265\n",
      "Progress: 18.7% ... Training loss: 0.179 ... Validation loss: 0.341iteration: 1870\n",
      "train_loss: 0.17907235292953783\n",
      "val_loss: 0.3410411764917434\n",
      "Progress: 18.7% ... Training loss: 0.187 ... Validation loss: 0.354iteration: 1871\n",
      "train_loss: 0.18706323642102\n",
      "val_loss: 0.35422068835901016\n",
      "Progress: 18.7% ... Training loss: 0.186 ... Validation loss: 0.340iteration: 1872\n",
      "train_loss: 0.18649510487216772\n",
      "val_loss: 0.3406172622552866\n",
      "Progress: 18.7% ... Training loss: 0.179 ... Validation loss: 0.348iteration: 1873\n",
      "train_loss: 0.17976483587989425\n",
      "val_loss: 0.34819682268769414\n",
      "Progress: 18.7% ... Training loss: 0.178 ... Validation loss: 0.342iteration: 1874\n",
      "train_loss: 0.17820989125978598\n",
      "val_loss: 0.34232007752060883\n",
      "Progress: 18.8% ... Training loss: 0.185 ... Validation loss: 0.353iteration: 1875\n",
      "train_loss: 0.18531030907230717\n",
      "val_loss: 0.353850309072813\n",
      "Progress: 18.8% ... Training loss: 0.178 ... Validation loss: 0.332iteration: 1876\n",
      "train_loss: 0.1784729185481186\n",
      "val_loss: 0.3321811644788646\n",
      "Progress: 18.8% ... Training loss: 0.181 ... Validation loss: 0.350iteration: 1877\n",
      "train_loss: 0.18131137155559743\n",
      "val_loss: 0.3506043907154201\n",
      "Progress: 18.8% ... Training loss: 0.178 ... Validation loss: 0.337iteration: 1878\n",
      "train_loss: 0.17846965229774717\n",
      "val_loss: 0.33734731632404513\n",
      "Progress: 18.8% ... Training loss: 0.179 ... Validation loss: 0.329iteration: 1879\n",
      "train_loss: 0.17933280408308114\n",
      "val_loss: 0.3298386641250005\n",
      "Progress: 18.8% ... Training loss: 0.179 ... Validation loss: 0.332iteration: 1880\n",
      "train_loss: 0.17921162092957646\n",
      "val_loss: 0.3325592805975719\n",
      "Progress: 18.8% ... Training loss: 0.188 ... Validation loss: 0.337iteration: 1881\n",
      "train_loss: 0.18822420772548684\n",
      "val_loss: 0.337501632007206\n",
      "Progress: 18.8% ... Training loss: 0.178 ... Validation loss: 0.334iteration: 1882\n",
      "train_loss: 0.17835963349992523\n",
      "val_loss: 0.33408399319565013\n",
      "Progress: 18.8% ... Training loss: 0.177 ... Validation loss: 0.331iteration: 1883\n",
      "train_loss: 0.1771904925724184\n",
      "val_loss: 0.33113265325578084\n",
      "Progress: 18.8% ... Training loss: 0.186 ... Validation loss: 0.344iteration: 1884\n",
      "train_loss: 0.18619928430144314\n",
      "val_loss: 0.3447851833072476\n",
      "Progress: 18.9% ... Training loss: 0.177 ... Validation loss: 0.330iteration: 1885\n",
      "train_loss: 0.17742471589239903\n",
      "val_loss: 0.3307325470397149\n",
      "Progress: 18.9% ... Training loss: 0.177 ... Validation loss: 0.332iteration: 1886\n",
      "train_loss: 0.17737529700856453\n",
      "val_loss: 0.3328413543261631\n",
      "Progress: 18.9% ... Training loss: 0.178 ... Validation loss: 0.335iteration: 1887\n",
      "train_loss: 0.17823435362554807\n",
      "val_loss: 0.33543680377258656\n",
      "Progress: 18.9% ... Training loss: 0.181 ... Validation loss: 0.335iteration: 1888\n",
      "train_loss: 0.18178330232883497\n",
      "val_loss: 0.3350493596270301\n",
      "Progress: 18.9% ... Training loss: 0.183 ... Validation loss: 0.337iteration: 1889\n",
      "train_loss: 0.18388529035217405\n",
      "val_loss: 0.3370729050081686\n",
      "Progress: 18.9% ... Training loss: 0.181 ... Validation loss: 0.340iteration: 1890\n",
      "train_loss: 0.1811170077336734\n",
      "val_loss: 0.34086464405043926\n",
      "Progress: 18.9% ... Training loss: 0.177 ... Validation loss: 0.336iteration: 1891\n",
      "train_loss: 0.17788893931114888\n",
      "val_loss: 0.33647846266861614\n",
      "Progress: 18.9% ... Training loss: 0.177 ... Validation loss: 0.344iteration: 1892\n",
      "train_loss: 0.17761020330158206\n",
      "val_loss: 0.3441973020922032\n",
      "Progress: 18.9% ... Training loss: 0.176 ... Validation loss: 0.334iteration: 1893\n",
      "train_loss: 0.17654394754276248\n",
      "val_loss: 0.3340121474488126\n",
      "Progress: 18.9% ... Training loss: 0.177 ... Validation loss: 0.335iteration: 1894\n",
      "train_loss: 0.17775235033093276\n",
      "val_loss: 0.33535517210656085\n",
      "Progress: 18.9% ... Training loss: 0.181 ... Validation loss: 0.332iteration: 1895\n",
      "train_loss: 0.18115884384205677\n",
      "val_loss: 0.3322365408993039\n",
      "Progress: 19.0% ... Training loss: 0.176 ... Validation loss: 0.337iteration: 1896\n",
      "train_loss: 0.17642125737250172\n",
      "val_loss: 0.33730706847930797\n",
      "Progress: 19.0% ... Training loss: 0.177 ... Validation loss: 0.339iteration: 1897\n",
      "train_loss: 0.17755757158767146\n",
      "val_loss: 0.3390034651804058\n",
      "Progress: 19.0% ... Training loss: 0.177 ... Validation loss: 0.331iteration: 1898\n",
      "train_loss: 0.17731217595497997\n",
      "val_loss: 0.33127628070737436\n",
      "Progress: 19.0% ... Training loss: 0.177 ... Validation loss: 0.333iteration: 1899\n",
      "train_loss: 0.177641990249957\n",
      "val_loss: 0.3334525560080622\n",
      "Progress: 19.0% ... Training loss: 0.177 ... Validation loss: 0.336iteration: 1900\n",
      "train_loss: 0.17756334299590734\n",
      "val_loss: 0.33671977486660387\n",
      "Progress: 19.0% ... Training loss: 0.175 ... Validation loss: 0.336iteration: 1901\n",
      "train_loss: 0.17593175644370707\n",
      "val_loss: 0.33690316422927796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 19.0% ... Training loss: 0.181 ... Validation loss: 0.340iteration: 1902\n",
      "train_loss: 0.18103426571463532\n",
      "val_loss: 0.3408034469871493\n",
      "Progress: 19.0% ... Training loss: 0.186 ... Validation loss: 0.338iteration: 1903\n",
      "train_loss: 0.18696953739597433\n",
      "val_loss: 0.33836853247236365\n",
      "Progress: 19.0% ... Training loss: 0.178 ... Validation loss: 0.335iteration: 1904\n",
      "train_loss: 0.17876925503428287\n",
      "val_loss: 0.3352087434123027\n",
      "Progress: 19.1% ... Training loss: 0.177 ... Validation loss: 0.332iteration: 1905\n",
      "train_loss: 0.1772700141127901\n",
      "val_loss: 0.3327091684935823\n",
      "Progress: 19.1% ... Training loss: 0.176 ... Validation loss: 0.332iteration: 1906\n",
      "train_loss: 0.17639364718586545\n",
      "val_loss: 0.3321838904816737\n",
      "Progress: 19.1% ... Training loss: 0.177 ... Validation loss: 0.333iteration: 1907\n",
      "train_loss: 0.17783546546585252\n",
      "val_loss: 0.3336574787843636\n",
      "Progress: 19.1% ... Training loss: 0.175 ... Validation loss: 0.326iteration: 1908\n",
      "train_loss: 0.17558260700308173\n",
      "val_loss: 0.32659962311452295\n",
      "Progress: 19.1% ... Training loss: 0.177 ... Validation loss: 0.328iteration: 1909\n",
      "train_loss: 0.17729086978174555\n",
      "val_loss: 0.32826793676436766\n",
      "Progress: 19.1% ... Training loss: 0.175 ... Validation loss: 0.324iteration: 1910\n",
      "train_loss: 0.17511312654767008\n",
      "val_loss: 0.32492273801622396\n",
      "Progress: 19.1% ... Training loss: 0.183 ... Validation loss: 0.330iteration: 1911\n",
      "train_loss: 0.1839029544333194\n",
      "val_loss: 0.33061271150987614\n",
      "Progress: 19.1% ... Training loss: 0.176 ... Validation loss: 0.331iteration: 1912\n",
      "train_loss: 0.1761738386142403\n",
      "val_loss: 0.33145115360846567\n",
      "Progress: 19.1% ... Training loss: 0.174 ... Validation loss: 0.328iteration: 1913\n",
      "train_loss: 0.17433985717581457\n",
      "val_loss: 0.32850537462528084\n",
      "Progress: 19.1% ... Training loss: 0.174 ... Validation loss: 0.324iteration: 1914\n",
      "train_loss: 0.17464412494914663\n",
      "val_loss: 0.3248178157675898\n",
      "Progress: 19.1% ... Training loss: 0.175 ... Validation loss: 0.325iteration: 1915\n",
      "train_loss: 0.17551447535767745\n",
      "val_loss: 0.32566907714498416\n",
      "Progress: 19.2% ... Training loss: 0.176 ... Validation loss: 0.335iteration: 1916\n",
      "train_loss: 0.17692141350443596\n",
      "val_loss: 0.3358379049883373\n",
      "Progress: 19.2% ... Training loss: 0.173 ... Validation loss: 0.326iteration: 1917\n",
      "train_loss: 0.17391404168445437\n",
      "val_loss: 0.3263021016047501\n",
      "Progress: 19.2% ... Training loss: 0.173 ... Validation loss: 0.326iteration: 1918\n",
      "train_loss: 0.17378356109665424\n",
      "val_loss: 0.3262382353245324\n",
      "Progress: 19.2% ... Training loss: 0.176 ... Validation loss: 0.323iteration: 1919\n",
      "train_loss: 0.1762667014685422\n",
      "val_loss: 0.32369868818517333\n",
      "Progress: 19.2% ... Training loss: 0.174 ... Validation loss: 0.324iteration: 1920\n",
      "train_loss: 0.1743781907981093\n",
      "val_loss: 0.3246919993923848\n",
      "Progress: 19.2% ... Training loss: 0.176 ... Validation loss: 0.326iteration: 1921\n",
      "train_loss: 0.17679903147109677\n",
      "val_loss: 0.32695430268766124\n",
      "Progress: 19.2% ... Training loss: 0.175 ... Validation loss: 0.323iteration: 1922\n",
      "train_loss: 0.17544862304086367\n",
      "val_loss: 0.32329260660478626\n",
      "Progress: 19.2% ... Training loss: 0.173 ... Validation loss: 0.324iteration: 1923\n",
      "train_loss: 0.17342106426846712\n",
      "val_loss: 0.32492567696204744\n",
      "Progress: 19.2% ... Training loss: 0.173 ... Validation loss: 0.320iteration: 1924\n",
      "train_loss: 0.17375122284635008\n",
      "val_loss: 0.3205694451000302\n",
      "Progress: 19.2% ... Training loss: 0.179 ... Validation loss: 0.341iteration: 1925\n",
      "train_loss: 0.17977940761537142\n",
      "val_loss: 0.3415287770702924\n",
      "Progress: 19.3% ... Training loss: 0.173 ... Validation loss: 0.321iteration: 1926\n",
      "train_loss: 0.17360090963255886\n",
      "val_loss: 0.32126920826116256\n",
      "Progress: 19.3% ... Training loss: 0.173 ... Validation loss: 0.320iteration: 1927\n",
      "train_loss: 0.17342070281211538\n",
      "val_loss: 0.32011769340172835\n",
      "Progress: 19.3% ... Training loss: 0.175 ... Validation loss: 0.323iteration: 1928\n",
      "train_loss: 0.17549135412158867\n",
      "val_loss: 0.32361032992251165\n",
      "Progress: 19.3% ... Training loss: 0.173 ... Validation loss: 0.325iteration: 1929\n",
      "train_loss: 0.17366820436255848\n",
      "val_loss: 0.32563282739810007\n",
      "Progress: 19.3% ... Training loss: 0.172 ... Validation loss: 0.316iteration: 1930\n",
      "train_loss: 0.17277728551888724\n",
      "val_loss: 0.31672251177294697\n",
      "Progress: 19.3% ... Training loss: 0.178 ... Validation loss: 0.322iteration: 1931\n",
      "train_loss: 0.1783482107004812\n",
      "val_loss: 0.3222556619071838\n",
      "Progress: 19.3% ... Training loss: 0.187 ... Validation loss: 0.336iteration: 1932\n",
      "train_loss: 0.18727986146090278\n",
      "val_loss: 0.3360855602248341\n",
      "Progress: 19.3% ... Training loss: 0.176 ... Validation loss: 0.326iteration: 1933\n",
      "train_loss: 0.17655550135565168\n",
      "val_loss: 0.3261735955548982\n",
      "Progress: 19.3% ... Training loss: 0.175 ... Validation loss: 0.328iteration: 1934\n",
      "train_loss: 0.17503674804349087\n",
      "val_loss: 0.32883683237037437\n",
      "Progress: 19.4% ... Training loss: 0.178 ... Validation loss: 0.330iteration: 1935\n",
      "train_loss: 0.17880907365644713\n",
      "val_loss: 0.3303162276261926\n",
      "Progress: 19.4% ... Training loss: 0.180 ... Validation loss: 0.329iteration: 1936\n",
      "train_loss: 0.18033941910468976\n",
      "val_loss: 0.3299074225065564\n",
      "Progress: 19.4% ... Training loss: 0.177 ... Validation loss: 0.324iteration: 1937\n",
      "train_loss: 0.17793294392209663\n",
      "val_loss: 0.3247655178724681\n",
      "Progress: 19.4% ... Training loss: 0.175 ... Validation loss: 0.325iteration: 1938\n",
      "train_loss: 0.17577455840441292\n",
      "val_loss: 0.3255268751438267\n",
      "Progress: 19.4% ... Training loss: 0.175 ... Validation loss: 0.329iteration: 1939\n",
      "train_loss: 0.17504917858668265\n",
      "val_loss: 0.32929459271418415\n",
      "Progress: 19.4% ... Training loss: 0.175 ... Validation loss: 0.325iteration: 1940\n",
      "train_loss: 0.17599489152071993\n",
      "val_loss: 0.3259474321173918\n",
      "Progress: 19.4% ... Training loss: 0.176 ... Validation loss: 0.331iteration: 1941\n",
      "train_loss: 0.1768335867728018\n",
      "val_loss: 0.33113664755964745\n",
      "Progress: 19.4% ... Training loss: 0.172 ... Validation loss: 0.324iteration: 1942\n",
      "train_loss: 0.17249745259687815\n",
      "val_loss: 0.3243470198351209\n",
      "Progress: 19.4% ... Training loss: 0.177 ... Validation loss: 0.325iteration: 1943\n",
      "train_loss: 0.17731924829844858\n",
      "val_loss: 0.32539908817522367\n",
      "Progress: 19.4% ... Training loss: 0.171 ... Validation loss: 0.320iteration: 1944\n",
      "train_loss: 0.1711459732343232\n",
      "val_loss: 0.3205730591301467\n",
      "Progress: 19.4% ... Training loss: 0.171 ... Validation loss: 0.319iteration: 1945\n",
      "train_loss: 0.1712846017912915\n",
      "val_loss: 0.3193506793445601\n",
      "Progress: 19.5% ... Training loss: 0.170 ... Validation loss: 0.318iteration: 1946\n",
      "train_loss: 0.17091519825970483\n",
      "val_loss: 0.3184508309959781\n",
      "Progress: 19.5% ... Training loss: 0.172 ... Validation loss: 0.316iteration: 1947\n",
      "train_loss: 0.17215847040284907\n",
      "val_loss: 0.31625469236481313\n",
      "Progress: 19.5% ... Training loss: 0.174 ... Validation loss: 0.325iteration: 1948\n",
      "train_loss: 0.17424429488470627\n",
      "val_loss: 0.3258305583143974\n",
      "Progress: 19.5% ... Training loss: 0.171 ... Validation loss: 0.315iteration: 1949\n",
      "train_loss: 0.17157279952429302\n",
      "val_loss: 0.315856547983048\n",
      "Progress: 19.5% ... Training loss: 0.173 ... Validation loss: 0.317iteration: 1950\n",
      "train_loss: 0.17399918226421524\n",
      "val_loss: 0.31709995163241095\n",
      "Progress: 19.5% ... Training loss: 0.180 ... Validation loss: 0.319iteration: 1951\n",
      "train_loss: 0.18073489353125785\n",
      "val_loss: 0.31966241977060805\n",
      "Progress: 19.5% ... Training loss: 0.172 ... Validation loss: 0.309iteration: 1952\n",
      "train_loss: 0.17270206796559043\n",
      "val_loss: 0.3098213719015313\n",
      "Progress: 19.5% ... Training loss: 0.176 ... Validation loss: 0.315iteration: 1953\n",
      "train_loss: 0.1762447248144884\n",
      "val_loss: 0.31509265176055695\n",
      "Progress: 19.5% ... Training loss: 0.180 ... Validation loss: 0.317iteration: 1954\n",
      "train_loss: 0.18015613087677598\n",
      "val_loss: 0.31758877117436846\n",
      "Progress: 19.6% ... Training loss: 0.174 ... Validation loss: 0.309iteration: 1955\n",
      "train_loss: 0.17419810396137728\n",
      "val_loss: 0.3090538374912684\n",
      "Progress: 19.6% ... Training loss: 0.171 ... Validation loss: 0.311iteration: 1956\n",
      "train_loss: 0.1719417206287662\n",
      "val_loss: 0.3112273349031235\n",
      "Progress: 19.6% ... Training loss: 0.170 ... Validation loss: 0.314iteration: 1957\n",
      "train_loss: 0.17041656538423883\n",
      "val_loss: 0.314446209031417\n",
      "Progress: 19.6% ... Training loss: 0.170 ... Validation loss: 0.313iteration: 1958\n",
      "train_loss: 0.17059499279476698\n",
      "val_loss: 0.31377871926937195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 19.6% ... Training loss: 0.171 ... Validation loss: 0.325iteration: 1959\n",
      "train_loss: 0.17173465264138021\n",
      "val_loss: 0.3251987340683827\n",
      "Progress: 19.6% ... Training loss: 0.172 ... Validation loss: 0.323iteration: 1960\n",
      "train_loss: 0.17292860238395508\n",
      "val_loss: 0.3230090895276909\n",
      "Progress: 19.6% ... Training loss: 0.172 ... Validation loss: 0.311iteration: 1961\n",
      "train_loss: 0.17246029170160646\n",
      "val_loss: 0.3117980419279951\n",
      "Progress: 19.6% ... Training loss: 0.170 ... Validation loss: 0.322iteration: 1962\n",
      "train_loss: 0.17055770933982772\n",
      "val_loss: 0.3221154926093111\n",
      "Progress: 19.6% ... Training loss: 0.171 ... Validation loss: 0.316iteration: 1963\n",
      "train_loss: 0.17181343957817588\n",
      "val_loss: 0.3167152024535759\n",
      "Progress: 19.6% ... Training loss: 0.169 ... Validation loss: 0.310iteration: 1964\n",
      "train_loss: 0.16946260906577126\n",
      "val_loss: 0.3108648642368825\n",
      "Progress: 19.6% ... Training loss: 0.171 ... Validation loss: 0.314iteration: 1965\n",
      "train_loss: 0.17162500771752137\n",
      "val_loss: 0.31489791634490977\n",
      "Progress: 19.7% ... Training loss: 0.171 ... Validation loss: 0.308iteration: 1966\n",
      "train_loss: 0.1714341967372286\n",
      "val_loss: 0.30817774508510454\n",
      "Progress: 19.7% ... Training loss: 0.169 ... Validation loss: 0.315iteration: 1967\n",
      "train_loss: 0.16917759075975514\n",
      "val_loss: 0.31538317129500804\n",
      "Progress: 19.7% ... Training loss: 0.170 ... Validation loss: 0.311iteration: 1968\n",
      "train_loss: 0.17073457381398516\n",
      "val_loss: 0.3115793967049796\n",
      "Progress: 19.7% ... Training loss: 0.170 ... Validation loss: 0.318iteration: 1969\n",
      "train_loss: 0.17022833155558142\n",
      "val_loss: 0.3181379088966339\n",
      "Progress: 19.7% ... Training loss: 0.169 ... Validation loss: 0.323iteration: 1970\n",
      "train_loss: 0.16927870404000592\n",
      "val_loss: 0.3231012646859358\n",
      "Progress: 19.7% ... Training loss: 0.179 ... Validation loss: 0.326iteration: 1971\n",
      "train_loss: 0.17902923346304972\n",
      "val_loss: 0.32698818488047676\n",
      "Progress: 19.7% ... Training loss: 0.179 ... Validation loss: 0.312iteration: 1972\n",
      "train_loss: 0.17988215630387036\n",
      "val_loss: 0.3125196834483789\n",
      "Progress: 19.7% ... Training loss: 0.170 ... Validation loss: 0.315iteration: 1973\n",
      "train_loss: 0.17052267474542984\n",
      "val_loss: 0.315746683909741\n",
      "Progress: 19.7% ... Training loss: 0.171 ... Validation loss: 0.327iteration: 1974\n",
      "train_loss: 0.1715390405805754\n",
      "val_loss: 0.3270118674731415\n",
      "Progress: 19.8% ... Training loss: 0.169 ... Validation loss: 0.319iteration: 1975\n",
      "train_loss: 0.1697373265096112\n",
      "val_loss: 0.31915724183021765\n",
      "Progress: 19.8% ... Training loss: 0.173 ... Validation loss: 0.309iteration: 1976\n",
      "train_loss: 0.1732802936948105\n",
      "val_loss: 0.3090819811168379\n",
      "Progress: 19.8% ... Training loss: 0.180 ... Validation loss: 0.322iteration: 1977\n",
      "train_loss: 0.18081481694865661\n",
      "val_loss: 0.3227235739942256\n",
      "Progress: 19.8% ... Training loss: 0.175 ... Validation loss: 0.310iteration: 1978\n",
      "train_loss: 0.17512157926887031\n",
      "val_loss: 0.3103074588462249\n",
      "Progress: 19.8% ... Training loss: 0.167 ... Validation loss: 0.315iteration: 1979\n",
      "train_loss: 0.16746599383493466\n",
      "val_loss: 0.3155508344857889\n",
      "Progress: 19.8% ... Training loss: 0.171 ... Validation loss: 0.327iteration: 1980\n",
      "train_loss: 0.17191310965297243\n",
      "val_loss: 0.32732998810906294\n",
      "Progress: 19.8% ... Training loss: 0.169 ... Validation loss: 0.309iteration: 1981\n",
      "train_loss: 0.16901734157140624\n",
      "val_loss: 0.30982519527399993\n",
      "Progress: 19.8% ... Training loss: 0.174 ... Validation loss: 0.330iteration: 1982\n",
      "train_loss: 0.1746522160550358\n",
      "val_loss: 0.33063223814382914\n",
      "Progress: 19.8% ... Training loss: 0.177 ... Validation loss: 0.311iteration: 1983\n",
      "train_loss: 0.1774373400295855\n",
      "val_loss: 0.31173341660596054\n",
      "Progress: 19.8% ... Training loss: 0.178 ... Validation loss: 0.341iteration: 1984\n",
      "train_loss: 0.17897579467627908\n",
      "val_loss: 0.3410648671857417\n",
      "Progress: 19.9% ... Training loss: 0.167 ... Validation loss: 0.309iteration: 1985\n",
      "train_loss: 0.16782892341526065\n",
      "val_loss: 0.3098895005485072\n",
      "Progress: 19.9% ... Training loss: 0.168 ... Validation loss: 0.324iteration: 1986\n",
      "train_loss: 0.16885650974390465\n",
      "val_loss: 0.3241177769901595\n",
      "Progress: 19.9% ... Training loss: 0.167 ... Validation loss: 0.311iteration: 1987\n",
      "train_loss: 0.16713989506232035\n",
      "val_loss: 0.3116353077510728\n",
      "Progress: 19.9% ... Training loss: 0.168 ... Validation loss: 0.320iteration: 1988\n",
      "train_loss: 0.1680909265317043\n",
      "val_loss: 0.32040415882087553\n",
      "Progress: 19.9% ... Training loss: 0.171 ... Validation loss: 0.308iteration: 1989\n",
      "train_loss: 0.1711312919756908\n",
      "val_loss: 0.3089632329478608\n",
      "Progress: 19.9% ... Training loss: 0.167 ... Validation loss: 0.314iteration: 1990\n",
      "train_loss: 0.1677136529511525\n",
      "val_loss: 0.31493130273483494\n",
      "Progress: 19.9% ... Training loss: 0.167 ... Validation loss: 0.314iteration: 1991\n",
      "train_loss: 0.16716384424378875\n",
      "val_loss: 0.31439100633400896\n",
      "Progress: 19.9% ... Training loss: 0.175 ... Validation loss: 0.334iteration: 1992\n",
      "train_loss: 0.17597606597090584\n",
      "val_loss: 0.3346728887148423\n",
      "Progress: 19.9% ... Training loss: 0.168 ... Validation loss: 0.316iteration: 1993\n",
      "train_loss: 0.1682608186685139\n",
      "val_loss: 0.3160934981080376\n",
      "Progress: 19.9% ... Training loss: 0.168 ... Validation loss: 0.311iteration: 1994\n",
      "train_loss: 0.16801647371303216\n",
      "val_loss: 0.3116866963542149\n",
      "Progress: 19.9% ... Training loss: 0.167 ... Validation loss: 0.312iteration: 1995\n",
      "train_loss: 0.16751827570938774\n",
      "val_loss: 0.31204908572668794\n",
      "Progress: 20.0% ... Training loss: 0.165 ... Validation loss: 0.311iteration: 1996\n",
      "train_loss: 0.165907773323089\n",
      "val_loss: 0.31152763285127777\n",
      "Progress: 20.0% ... Training loss: 0.166 ... Validation loss: 0.312iteration: 1997\n",
      "train_loss: 0.16692304773760092\n",
      "val_loss: 0.3129304145322554\n",
      "Progress: 20.0% ... Training loss: 0.167 ... Validation loss: 0.312iteration: 1998\n",
      "train_loss: 0.16775968726599247\n",
      "val_loss: 0.3126648884356504\n",
      "Progress: 20.0% ... Training loss: 0.169 ... Validation loss: 0.322iteration: 1999\n",
      "train_loss: 0.1690001767121967\n",
      "val_loss: 0.3221972727575019\n",
      "Progress: 20.0% ... Training loss: 0.168 ... Validation loss: 0.308iteration: 2000\n",
      "train_loss: 0.168575210536433\n",
      "val_loss: 0.3086532730132897\n",
      "Progress: 20.0% ... Training loss: 0.183 ... Validation loss: 0.332iteration: 2001\n",
      "train_loss: 0.18301637247980343\n",
      "val_loss: 0.3325332173848691\n",
      "Progress: 20.0% ... Training loss: 0.165 ... Validation loss: 0.306iteration: 2002\n",
      "train_loss: 0.16588062008388388\n",
      "val_loss: 0.30671506153125566\n",
      "Progress: 20.0% ... Training loss: 0.166 ... Validation loss: 0.311iteration: 2003\n",
      "train_loss: 0.16691406364443184\n",
      "val_loss: 0.3116195409492671\n",
      "Progress: 20.0% ... Training loss: 0.170 ... Validation loss: 0.331iteration: 2004\n",
      "train_loss: 0.17033238995831076\n",
      "val_loss: 0.3319387637832276\n",
      "Progress: 20.1% ... Training loss: 0.167 ... Validation loss: 0.315iteration: 2005\n",
      "train_loss: 0.1671530930133468\n",
      "val_loss: 0.31565467652652657\n",
      "Progress: 20.1% ... Training loss: 0.172 ... Validation loss: 0.330iteration: 2006\n",
      "train_loss: 0.17242678207667875\n",
      "val_loss: 0.330467796494116\n",
      "Progress: 20.1% ... Training loss: 0.181 ... Validation loss: 0.311iteration: 2007\n",
      "train_loss: 0.18160320953461775\n",
      "val_loss: 0.3117885038381204\n",
      "Progress: 20.1% ... Training loss: 0.167 ... Validation loss: 0.305iteration: 2008\n",
      "train_loss: 0.16707614932808318\n",
      "val_loss: 0.30574203115651166\n",
      "Progress: 20.1% ... Training loss: 0.166 ... Validation loss: 0.311iteration: 2009\n",
      "train_loss: 0.16682248292282006\n",
      "val_loss: 0.311978030192101\n",
      "Progress: 20.1% ... Training loss: 0.168 ... Validation loss: 0.308iteration: 2010\n",
      "train_loss: 0.16831974127784885\n",
      "val_loss: 0.30877121670373275\n",
      "Progress: 20.1% ... Training loss: 0.165 ... Validation loss: 0.320iteration: 2011\n",
      "train_loss: 0.16582598276925012\n",
      "val_loss: 0.32009259228925924\n",
      "Progress: 20.1% ... Training loss: 0.166 ... Validation loss: 0.310iteration: 2012\n",
      "train_loss: 0.16634114327607247\n",
      "val_loss: 0.31094456922889524\n",
      "Progress: 20.1% ... Training loss: 0.165 ... Validation loss: 0.317iteration: 2013\n",
      "train_loss: 0.1651726578936393\n",
      "val_loss: 0.317815730568864\n",
      "Progress: 20.1% ... Training loss: 0.166 ... Validation loss: 0.308iteration: 2014\n",
      "train_loss: 0.16681005354976372\n",
      "val_loss: 0.30805976403093\n",
      "Progress: 20.1% ... Training loss: 0.168 ... Validation loss: 0.330iteration: 2015\n",
      "train_loss: 0.16853093626125878\n",
      "val_loss: 0.33027980865783046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 20.2% ... Training loss: 0.166 ... Validation loss: 0.316iteration: 2016\n",
      "train_loss: 0.16696452353265606\n",
      "val_loss: 0.3161125706704283\n",
      "Progress: 20.2% ... Training loss: 0.167 ... Validation loss: 0.315iteration: 2017\n",
      "train_loss: 0.1674204230687514\n",
      "val_loss: 0.31590000768618587\n",
      "Progress: 20.2% ... Training loss: 0.179 ... Validation loss: 0.311iteration: 2018\n",
      "train_loss: 0.17916035588952445\n",
      "val_loss: 0.31117672616934167\n",
      "Progress: 20.2% ... Training loss: 0.175 ... Validation loss: 0.352iteration: 2019\n",
      "train_loss: 0.17574200633099743\n",
      "val_loss: 0.3524093399689606\n",
      "Progress: 20.2% ... Training loss: 0.167 ... Validation loss: 0.314iteration: 2020\n",
      "train_loss: 0.16722312147530508\n",
      "val_loss: 0.31460388281533114\n",
      "Progress: 20.2% ... Training loss: 0.165 ... Validation loss: 0.310iteration: 2021\n",
      "train_loss: 0.16503709492849034\n",
      "val_loss: 0.31000649905716277\n",
      "Progress: 20.2% ... Training loss: 0.164 ... Validation loss: 0.319iteration: 2022\n",
      "train_loss: 0.1647941190782188\n",
      "val_loss: 0.3192992996381686\n",
      "Progress: 20.2% ... Training loss: 0.167 ... Validation loss: 0.304iteration: 2023\n",
      "train_loss: 0.16700509567774274\n",
      "val_loss: 0.3040095442616366\n",
      "Progress: 20.2% ... Training loss: 0.182 ... Validation loss: 0.350iteration: 2024\n",
      "train_loss: 0.1823263802960718\n",
      "val_loss: 0.3506591315800263\n",
      "Progress: 20.2% ... Training loss: 0.165 ... Validation loss: 0.310iteration: 2025\n",
      "train_loss: 0.1653471004757945\n",
      "val_loss: 0.3107253728888352\n",
      "Progress: 20.3% ... Training loss: 0.165 ... Validation loss: 0.312iteration: 2026\n",
      "train_loss: 0.16530106122545485\n",
      "val_loss: 0.3123650428833045\n",
      "Progress: 20.3% ... Training loss: 0.169 ... Validation loss: 0.329iteration: 2027\n",
      "train_loss: 0.16930031298589718\n",
      "val_loss: 0.32940883479476274\n",
      "Progress: 20.3% ... Training loss: 0.168 ... Validation loss: 0.311iteration: 2028\n",
      "train_loss: 0.1681201049183462\n",
      "val_loss: 0.31137413278248594\n",
      "Progress: 20.3% ... Training loss: 0.186 ... Validation loss: 0.367iteration: 2029\n",
      "train_loss: 0.18606734693016524\n",
      "val_loss: 0.3676649508612445\n",
      "Progress: 20.3% ... Training loss: 0.178 ... Validation loss: 0.309iteration: 2030\n",
      "train_loss: 0.17802422498478482\n",
      "val_loss: 0.3096183874325763\n",
      "Progress: 20.3% ... Training loss: 0.184 ... Validation loss: 0.362iteration: 2031\n",
      "train_loss: 0.18458803667160456\n",
      "val_loss: 0.3624622153152736\n",
      "Progress: 20.3% ... Training loss: 0.173 ... Validation loss: 0.305iteration: 2032\n",
      "train_loss: 0.1736355302053248\n",
      "val_loss: 0.30523750408803607\n",
      "Progress: 20.3% ... Training loss: 0.174 ... Validation loss: 0.347iteration: 2033\n",
      "train_loss: 0.1740425654153121\n",
      "val_loss: 0.3476089111865824\n",
      "Progress: 20.3% ... Training loss: 0.163 ... Validation loss: 0.313iteration: 2034\n",
      "train_loss: 0.16371540255574613\n",
      "val_loss: 0.31384419890421145\n",
      "Progress: 20.4% ... Training loss: 0.163 ... Validation loss: 0.314iteration: 2035\n",
      "train_loss: 0.16368132768811489\n",
      "val_loss: 0.3140349982464038\n",
      "Progress: 20.4% ... Training loss: 0.162 ... Validation loss: 0.309iteration: 2036\n",
      "train_loss: 0.16292421862777934\n",
      "val_loss: 0.3093099590464826\n",
      "Progress: 20.4% ... Training loss: 0.164 ... Validation loss: 0.306iteration: 2037\n",
      "train_loss: 0.16417546240270015\n",
      "val_loss: 0.30611420141372303\n",
      "Progress: 20.4% ... Training loss: 0.166 ... Validation loss: 0.302iteration: 2038\n",
      "train_loss: 0.16619044733677527\n",
      "val_loss: 0.30213370883867313\n",
      "Progress: 20.4% ... Training loss: 0.165 ... Validation loss: 0.307iteration: 2039\n",
      "train_loss: 0.16509124973159928\n",
      "val_loss: 0.307039711600592\n",
      "Progress: 20.4% ... Training loss: 0.172 ... Validation loss: 0.299iteration: 2040\n",
      "train_loss: 0.17265106085373444\n",
      "val_loss: 0.29901129310990016\n",
      "Progress: 20.4% ... Training loss: 0.163 ... Validation loss: 0.314iteration: 2041\n",
      "train_loss: 0.16324951997470852\n",
      "val_loss: 0.31414767561104884\n",
      "Progress: 20.4% ... Training loss: 0.164 ... Validation loss: 0.310iteration: 2042\n",
      "train_loss: 0.16439214127059668\n",
      "val_loss: 0.31057222753655755\n",
      "Progress: 20.4% ... Training loss: 0.175 ... Validation loss: 0.312iteration: 2043\n",
      "train_loss: 0.1759323280723513\n",
      "val_loss: 0.3127456889940273\n",
      "Progress: 20.4% ... Training loss: 0.168 ... Validation loss: 0.299iteration: 2044\n",
      "train_loss: 0.16886654735123788\n",
      "val_loss: 0.2998609269657912\n",
      "Progress: 20.4% ... Training loss: 0.169 ... Validation loss: 0.320iteration: 2045\n",
      "train_loss: 0.16912948525111732\n",
      "val_loss: 0.3206680059499937\n",
      "Progress: 20.5% ... Training loss: 0.164 ... Validation loss: 0.301iteration: 2046\n",
      "train_loss: 0.16417136660041837\n",
      "val_loss: 0.3014761593256912\n",
      "Progress: 20.5% ... Training loss: 0.163 ... Validation loss: 0.303iteration: 2047\n",
      "train_loss: 0.1639645648505616\n",
      "val_loss: 0.3037458019739805\n",
      "Progress: 20.5% ... Training loss: 0.162 ... Validation loss: 0.297iteration: 2048\n",
      "train_loss: 0.16221561442574337\n",
      "val_loss: 0.2976104058209335\n",
      "Progress: 20.5% ... Training loss: 0.161 ... Validation loss: 0.300iteration: 2049\n",
      "train_loss: 0.16120546944230282\n",
      "val_loss: 0.30054403992870093\n",
      "Progress: 20.5% ... Training loss: 0.161 ... Validation loss: 0.299iteration: 2050\n",
      "train_loss: 0.16140032898912252\n",
      "val_loss: 0.29903501613414357\n",
      "Progress: 20.5% ... Training loss: 0.163 ... Validation loss: 0.298iteration: 2051\n",
      "train_loss: 0.1630068244002231\n",
      "val_loss: 0.2983523133087962\n",
      "Progress: 20.5% ... Training loss: 0.166 ... Validation loss: 0.314iteration: 2052\n",
      "train_loss: 0.16608457350933234\n",
      "val_loss: 0.3142814003732267\n",
      "Progress: 20.5% ... Training loss: 0.161 ... Validation loss: 0.297iteration: 2053\n",
      "train_loss: 0.16179204834425484\n",
      "val_loss: 0.2977239808755957\n",
      "Progress: 20.5% ... Training loss: 0.161 ... Validation loss: 0.308iteration: 2054\n",
      "train_loss: 0.1612432213202961\n",
      "val_loss: 0.3088179609629559\n",
      "Progress: 20.6% ... Training loss: 0.163 ... Validation loss: 0.320iteration: 2055\n",
      "train_loss: 0.1632416349857303\n",
      "val_loss: 0.32033737842537036\n",
      "Progress: 20.6% ... Training loss: 0.160 ... Validation loss: 0.313iteration: 2056\n",
      "train_loss: 0.16073330311098144\n",
      "val_loss: 0.3135001920762501\n",
      "Progress: 20.6% ... Training loss: 0.161 ... Validation loss: 0.315iteration: 2057\n",
      "train_loss: 0.16192780783657493\n",
      "val_loss: 0.31594917573125997\n",
      "Progress: 20.6% ... Training loss: 0.164 ... Validation loss: 0.297iteration: 2058\n",
      "train_loss: 0.164042742327714\n",
      "val_loss: 0.2971207830226132\n",
      "Progress: 20.6% ... Training loss: 0.160 ... Validation loss: 0.302iteration: 2059\n",
      "train_loss: 0.16076040352119658\n",
      "val_loss: 0.3021237505572814\n",
      "Progress: 20.6% ... Training loss: 0.162 ... Validation loss: 0.306iteration: 2060\n",
      "train_loss: 0.16237820211472004\n",
      "val_loss: 0.3062926066684106\n",
      "Progress: 20.6% ... Training loss: 0.162 ... Validation loss: 0.320iteration: 2061\n",
      "train_loss: 0.16209924068452936\n",
      "val_loss: 0.3203909009328476\n",
      "Progress: 20.6% ... Training loss: 0.166 ... Validation loss: 0.334iteration: 2062\n",
      "train_loss: 0.16646427508669348\n",
      "val_loss: 0.33490041490836364\n",
      "Progress: 20.6% ... Training loss: 0.160 ... Validation loss: 0.309iteration: 2063\n",
      "train_loss: 0.16024650461304307\n",
      "val_loss: 0.30922623182456954\n",
      "Progress: 20.6% ... Training loss: 0.161 ... Validation loss: 0.309iteration: 2064\n",
      "train_loss: 0.16110960825475218\n",
      "val_loss: 0.30956854093088276\n",
      "Progress: 20.6% ... Training loss: 0.162 ... Validation loss: 0.318iteration: 2065\n",
      "train_loss: 0.16251827376910374\n",
      "val_loss: 0.31852343301141234\n",
      "Progress: 20.7% ... Training loss: 0.161 ... Validation loss: 0.313iteration: 2066\n",
      "train_loss: 0.16153280059040842\n",
      "val_loss: 0.31365895083689505\n",
      "Progress: 20.7% ... Training loss: 0.163 ... Validation loss: 0.302iteration: 2067\n",
      "train_loss: 0.1639860027075516\n",
      "val_loss: 0.30201759621462315\n",
      "Progress: 20.7% ... Training loss: 0.163 ... Validation loss: 0.306iteration: 2068\n",
      "train_loss: 0.16362007238551945\n",
      "val_loss: 0.3067338562710715\n",
      "Progress: 20.7% ... Training loss: 0.168 ... Validation loss: 0.294iteration: 2069\n",
      "train_loss: 0.16879360664292095\n",
      "val_loss: 0.29437666457169526\n",
      "Progress: 20.7% ... Training loss: 0.177 ... Validation loss: 0.336iteration: 2070\n",
      "train_loss: 0.17754276452124726\n",
      "val_loss: 0.33621290437034707\n",
      "Progress: 20.7% ... Training loss: 0.162 ... Validation loss: 0.292iteration: 2071\n",
      "train_loss: 0.16226125187598903\n",
      "val_loss: 0.29269928006471463\n",
      "Progress: 20.7% ... Training loss: 0.161 ... Validation loss: 0.296iteration: 2072\n",
      "train_loss: 0.16163781486761075\n",
      "val_loss: 0.29657983593034193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 20.7% ... Training loss: 0.164 ... Validation loss: 0.301iteration: 2073\n",
      "train_loss: 0.16439543522054392\n",
      "val_loss: 0.30100863647352966\n",
      "Progress: 20.7% ... Training loss: 0.160 ... Validation loss: 0.297iteration: 2074\n",
      "train_loss: 0.1602083869821433\n",
      "val_loss: 0.29765048812269396\n",
      "Progress: 20.8% ... Training loss: 0.159 ... Validation loss: 0.308iteration: 2075\n",
      "train_loss: 0.15986446421662207\n",
      "val_loss: 0.30880800267649405\n",
      "Progress: 20.8% ... Training loss: 0.160 ... Validation loss: 0.317iteration: 2076\n",
      "train_loss: 0.16071705428573171\n",
      "val_loss: 0.3170710128892915\n",
      "Progress: 20.8% ... Training loss: 0.160 ... Validation loss: 0.310iteration: 2077\n",
      "train_loss: 0.160439952005997\n",
      "val_loss: 0.3105514893208001\n",
      "Progress: 20.8% ... Training loss: 0.159 ... Validation loss: 0.300iteration: 2078\n",
      "train_loss: 0.15947097630002152\n",
      "val_loss: 0.30089875055993975\n",
      "Progress: 20.8% ... Training loss: 0.159 ... Validation loss: 0.301iteration: 2079\n",
      "train_loss: 0.1590334492243808\n",
      "val_loss: 0.3011362351371969\n",
      "Progress: 20.8% ... Training loss: 0.169 ... Validation loss: 0.318iteration: 2080\n",
      "train_loss: 0.1696806385382521\n",
      "val_loss: 0.3189161810187037\n",
      "Progress: 20.8% ... Training loss: 0.159 ... Validation loss: 0.300iteration: 2081\n",
      "train_loss: 0.15938616134517491\n",
      "val_loss: 0.3005043080656573\n",
      "Progress: 20.8% ... Training loss: 0.158 ... Validation loss: 0.298iteration: 2082\n",
      "train_loss: 0.15882584250448695\n",
      "val_loss: 0.2989843296525817\n",
      "Progress: 20.8% ... Training loss: 0.159 ... Validation loss: 0.312iteration: 2083\n",
      "train_loss: 0.15967241896682796\n",
      "val_loss: 0.3126540555506693\n",
      "Progress: 20.8% ... Training loss: 0.163 ... Validation loss: 0.299iteration: 2084\n",
      "train_loss: 0.16335244015775677\n",
      "val_loss: 0.2990487145124299\n",
      "Progress: 20.9% ... Training loss: 0.161 ... Validation loss: 0.306iteration: 2085\n",
      "train_loss: 0.16151016514140326\n",
      "val_loss: 0.30627245375791257\n",
      "Progress: 20.9% ... Training loss: 0.178 ... Validation loss: 0.356iteration: 2086\n",
      "train_loss: 0.1787505268493364\n",
      "val_loss: 0.35689298291877897\n",
      "Progress: 20.9% ... Training loss: 0.158 ... Validation loss: 0.295iteration: 2087\n",
      "train_loss: 0.15895153317534463\n",
      "val_loss: 0.29557049868051294\n",
      "Progress: 20.9% ... Training loss: 0.160 ... Validation loss: 0.297iteration: 2088\n",
      "train_loss: 0.16050917736608342\n",
      "val_loss: 0.2979649379275458\n",
      "Progress: 20.9% ... Training loss: 0.159 ... Validation loss: 0.301iteration: 2089\n",
      "train_loss: 0.15929667464814953\n",
      "val_loss: 0.3014286774854826\n",
      "Progress: 20.9% ... Training loss: 0.158 ... Validation loss: 0.298iteration: 2090\n",
      "train_loss: 0.15868961670131723\n",
      "val_loss: 0.29853305183150286\n",
      "Progress: 20.9% ... Training loss: 0.158 ... Validation loss: 0.299iteration: 2091\n",
      "train_loss: 0.1580536219873353\n",
      "val_loss: 0.29924225972051727\n",
      "Progress: 20.9% ... Training loss: 0.158 ... Validation loss: 0.305iteration: 2092\n",
      "train_loss: 0.15886494503104692\n",
      "val_loss: 0.30581008552400063\n",
      "Progress: 20.9% ... Training loss: 0.165 ... Validation loss: 0.317iteration: 2093\n",
      "train_loss: 0.16547683503424548\n",
      "val_loss: 0.31793407122043954\n",
      "Progress: 20.9% ... Training loss: 0.157 ... Validation loss: 0.293iteration: 2094\n",
      "train_loss: 0.15785546237177453\n",
      "val_loss: 0.2934197104653344\n",
      "Progress: 20.9% ... Training loss: 0.167 ... Validation loss: 0.305iteration: 2095\n",
      "train_loss: 0.16751988349887895\n",
      "val_loss: 0.30582812269261084\n",
      "Progress: 21.0% ... Training loss: 0.170 ... Validation loss: 0.292iteration: 2096\n",
      "train_loss: 0.17056752387215493\n",
      "val_loss: 0.29250917090338685\n",
      "Progress: 21.0% ... Training loss: 0.170 ... Validation loss: 0.316iteration: 2097\n",
      "train_loss: 0.17033451899415433\n",
      "val_loss: 0.3166684078814683\n",
      "Progress: 21.0% ... Training loss: 0.158 ... Validation loss: 0.299iteration: 2098\n",
      "train_loss: 0.15858130217145924\n",
      "val_loss: 0.2990861192412803\n",
      "Progress: 21.0% ... Training loss: 0.157 ... Validation loss: 0.288iteration: 2099\n",
      "train_loss: 0.15742149071918296\n",
      "val_loss: 0.2882652144396022\n",
      "Progress: 21.0% ... Training loss: 0.156 ... Validation loss: 0.291iteration: 2100\n",
      "train_loss: 0.15678125497745926\n",
      "val_loss: 0.2918961168909254\n",
      "Progress: 21.0% ... Training loss: 0.167 ... Validation loss: 0.294iteration: 2101\n",
      "train_loss: 0.16727833779223306\n",
      "val_loss: 0.2940986029389154\n",
      "Progress: 21.0% ... Training loss: 0.158 ... Validation loss: 0.308iteration: 2102\n",
      "train_loss: 0.15838954789943635\n",
      "val_loss: 0.3085193350323784\n",
      "Progress: 21.0% ... Training loss: 0.159 ... Validation loss: 0.293iteration: 2103\n",
      "train_loss: 0.15994461551041433\n",
      "val_loss: 0.2939550953954129\n",
      "Progress: 21.0% ... Training loss: 0.156 ... Validation loss: 0.295iteration: 2104\n",
      "train_loss: 0.15626439634651187\n",
      "val_loss: 0.2952270689635406\n",
      "Progress: 21.1% ... Training loss: 0.156 ... Validation loss: 0.299iteration: 2105\n",
      "train_loss: 0.15659807817511867\n",
      "val_loss: 0.29918830128701857\n",
      "Progress: 21.1% ... Training loss: 0.159 ... Validation loss: 0.299iteration: 2106\n",
      "train_loss: 0.15975709195916193\n",
      "val_loss: 0.29932828901967523\n",
      "Progress: 21.1% ... Training loss: 0.168 ... Validation loss: 0.311iteration: 2107\n",
      "train_loss: 0.16873802802920437\n",
      "val_loss: 0.31153023771740807\n",
      "Progress: 21.1% ... Training loss: 0.158 ... Validation loss: 0.294iteration: 2108\n",
      "train_loss: 0.1583469174801761\n",
      "val_loss: 0.2942127050416143\n",
      "Progress: 21.1% ... Training loss: 0.157 ... Validation loss: 0.304iteration: 2109\n",
      "train_loss: 0.1571892128275747\n",
      "val_loss: 0.3040088675758658\n",
      "Progress: 21.1% ... Training loss: 0.177 ... Validation loss: 0.305iteration: 2110\n",
      "train_loss: 0.17770296697965485\n",
      "val_loss: 0.30570517102817907\n",
      "Progress: 21.1% ... Training loss: 0.161 ... Validation loss: 0.314iteration: 2111\n",
      "train_loss: 0.1614114123047071\n",
      "val_loss: 0.3142902683182577\n",
      "Progress: 21.1% ... Training loss: 0.155 ... Validation loss: 0.298iteration: 2112\n",
      "train_loss: 0.15575476062478488\n",
      "val_loss: 0.29819138379026155\n",
      "Progress: 21.1% ... Training loss: 0.156 ... Validation loss: 0.293iteration: 2113\n",
      "train_loss: 0.15648714451782908\n",
      "val_loss: 0.293395872352356\n",
      "Progress: 21.1% ... Training loss: 0.171 ... Validation loss: 0.301iteration: 2114\n",
      "train_loss: 0.1713505455934288\n",
      "val_loss: 0.3018827121285774\n",
      "Progress: 21.1% ... Training loss: 0.167 ... Validation loss: 0.317iteration: 2115\n",
      "train_loss: 0.1676506144438044\n",
      "val_loss: 0.3179325733731541\n",
      "Progress: 21.2% ... Training loss: 0.156 ... Validation loss: 0.295iteration: 2116\n",
      "train_loss: 0.15627338882623293\n",
      "val_loss: 0.2955081165662812\n",
      "Progress: 21.2% ... Training loss: 0.159 ... Validation loss: 0.304iteration: 2117\n",
      "train_loss: 0.15962488962806615\n",
      "val_loss: 0.30430100876659005\n",
      "Progress: 21.2% ... Training loss: 0.160 ... Validation loss: 0.301iteration: 2118\n",
      "train_loss: 0.16043074852986058\n",
      "val_loss: 0.3013963994266041\n",
      "Progress: 21.2% ... Training loss: 0.178 ... Validation loss: 0.305iteration: 2119\n",
      "train_loss: 0.178625593167079\n",
      "val_loss: 0.30575427922429615\n",
      "Progress: 21.2% ... Training loss: 0.160 ... Validation loss: 0.318iteration: 2120\n",
      "train_loss: 0.16047473499671241\n",
      "val_loss: 0.31895030765727783\n",
      "Progress: 21.2% ... Training loss: 0.160 ... Validation loss: 0.296iteration: 2121\n",
      "train_loss: 0.16021900431509023\n",
      "val_loss: 0.29653596969802154\n",
      "Progress: 21.2% ... Training loss: 0.156 ... Validation loss: 0.295iteration: 2122\n",
      "train_loss: 0.1565470150090746\n",
      "val_loss: 0.29580886283965974\n",
      "Progress: 21.2% ... Training loss: 0.157 ... Validation loss: 0.295iteration: 2123\n",
      "train_loss: 0.15739487212305547\n",
      "val_loss: 0.29577812963424194\n",
      "Progress: 21.2% ... Training loss: 0.154 ... Validation loss: 0.297iteration: 2124\n",
      "train_loss: 0.15483056233659204\n",
      "val_loss: 0.29787745284729594\n",
      "Progress: 21.2% ... Training loss: 0.156 ... Validation loss: 0.291iteration: 2125\n",
      "train_loss: 0.15674404979144183\n",
      "val_loss: 0.2912770084336106\n",
      "Progress: 21.3% ... Training loss: 0.157 ... Validation loss: 0.292iteration: 2126\n",
      "train_loss: 0.1579813371432197\n",
      "val_loss: 0.29214036955060824\n",
      "Progress: 21.3% ... Training loss: 0.160 ... Validation loss: 0.308iteration: 2127\n",
      "train_loss: 0.16042696866268294\n",
      "val_loss: 0.3082913991303346\n",
      "Progress: 21.3% ... Training loss: 0.155 ... Validation loss: 0.294iteration: 2128\n",
      "train_loss: 0.15574229431132233\n",
      "val_loss: 0.2945200881083501\n",
      "Progress: 21.3% ... Training loss: 0.163 ... Validation loss: 0.323iteration: 2129\n",
      "train_loss: 0.16355617552739832\n",
      "val_loss: 0.3232643530719758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 21.3% ... Training loss: 0.159 ... Validation loss: 0.290iteration: 2130\n",
      "train_loss: 0.1596553222230748\n",
      "val_loss: 0.2907019046382257\n",
      "Progress: 21.3% ... Training loss: 0.159 ... Validation loss: 0.317iteration: 2131\n",
      "train_loss: 0.1596748290881091\n",
      "val_loss: 0.3171183196716576\n",
      "Progress: 21.3% ... Training loss: 0.155 ... Validation loss: 0.290iteration: 2132\n",
      "train_loss: 0.15528363664042757\n",
      "val_loss: 0.29068408277850843\n",
      "Progress: 21.3% ... Training loss: 0.154 ... Validation loss: 0.302iteration: 2133\n",
      "train_loss: 0.15431744959676572\n",
      "val_loss: 0.30229639818851456\n",
      "Progress: 21.3% ... Training loss: 0.154 ... Validation loss: 0.298iteration: 2134\n",
      "train_loss: 0.15468657536688088\n",
      "val_loss: 0.29877395671610424\n",
      "Progress: 21.4% ... Training loss: 0.161 ... Validation loss: 0.320iteration: 2135\n",
      "train_loss: 0.16125970521733446\n",
      "val_loss: 0.32075282175884284\n",
      "Progress: 21.4% ... Training loss: 0.154 ... Validation loss: 0.294iteration: 2136\n",
      "train_loss: 0.1541630295503987\n",
      "val_loss: 0.2947505625637851\n",
      "Progress: 21.4% ... Training loss: 0.161 ... Validation loss: 0.320iteration: 2137\n",
      "train_loss: 0.1613142120313542\n",
      "val_loss: 0.3209606088832647\n",
      "Progress: 21.4% ... Training loss: 0.154 ... Validation loss: 0.296iteration: 2138\n",
      "train_loss: 0.15443292486631546\n",
      "val_loss: 0.29684322026831667\n",
      "Progress: 21.4% ... Training loss: 0.159 ... Validation loss: 0.289iteration: 2139\n",
      "train_loss: 0.15972750383083606\n",
      "val_loss: 0.28903440880800735\n",
      "Progress: 21.4% ... Training loss: 0.153 ... Validation loss: 0.301iteration: 2140\n",
      "train_loss: 0.1538016520990458\n",
      "val_loss: 0.3019576312296048\n",
      "Progress: 21.4% ... Training loss: 0.162 ... Validation loss: 0.308iteration: 2141\n",
      "train_loss: 0.16222823602900804\n",
      "val_loss: 0.308662109995569\n",
      "Progress: 21.4% ... Training loss: 0.154 ... Validation loss: 0.296iteration: 2142\n",
      "train_loss: 0.15410357825989707\n",
      "val_loss: 0.2965717158014489\n",
      "Progress: 21.4% ... Training loss: 0.161 ... Validation loss: 0.289iteration: 2143\n",
      "train_loss: 0.16136144823675305\n",
      "val_loss: 0.28916910262421586\n",
      "Progress: 21.4% ... Training loss: 0.155 ... Validation loss: 0.308iteration: 2144\n",
      "train_loss: 0.15587048033920392\n",
      "val_loss: 0.30832078130414214\n",
      "Progress: 21.4% ... Training loss: 0.179 ... Validation loss: 0.295iteration: 2145\n",
      "train_loss: 0.17910566103451894\n",
      "val_loss: 0.29589067191189233\n",
      "Progress: 21.5% ... Training loss: 0.172 ... Validation loss: 0.348iteration: 2146\n",
      "train_loss: 0.1724943714298608\n",
      "val_loss: 0.3486773604098962\n",
      "Progress: 21.5% ... Training loss: 0.173 ... Validation loss: 0.295iteration: 2147\n",
      "train_loss: 0.17366116526039965\n",
      "val_loss: 0.2958076139711125\n",
      "Progress: 21.5% ... Training loss: 0.164 ... Validation loss: 0.331iteration: 2148\n",
      "train_loss: 0.1642410813760848\n",
      "val_loss: 0.3319180003819871\n",
      "Progress: 21.5% ... Training loss: 0.156 ... Validation loss: 0.290iteration: 2149\n",
      "train_loss: 0.15602817492213736\n",
      "val_loss: 0.2906949816816643\n",
      "Progress: 21.5% ... Training loss: 0.153 ... Validation loss: 0.301iteration: 2150\n",
      "train_loss: 0.15369578761382172\n",
      "val_loss: 0.3013801062061593\n",
      "Progress: 21.5% ... Training loss: 0.153 ... Validation loss: 0.290iteration: 2151\n",
      "train_loss: 0.15378281074687317\n",
      "val_loss: 0.2905089291211515\n",
      "Progress: 21.5% ... Training loss: 0.152 ... Validation loss: 0.291iteration: 2152\n",
      "train_loss: 0.15251026724336872\n",
      "val_loss: 0.2916026717323748\n",
      "Progress: 21.5% ... Training loss: 0.154 ... Validation loss: 0.287iteration: 2153\n",
      "train_loss: 0.15496354433102022\n",
      "val_loss: 0.2875507205819643\n",
      "Progress: 21.5% ... Training loss: 0.157 ... Validation loss: 0.301iteration: 2154\n",
      "train_loss: 0.1577042793301059\n",
      "val_loss: 0.3016144954290221\n",
      "Progress: 21.6% ... Training loss: 0.153 ... Validation loss: 0.289iteration: 2155\n",
      "train_loss: 0.15343804286726256\n",
      "val_loss: 0.2898782205036219\n",
      "Progress: 21.6% ... Training loss: 0.152 ... Validation loss: 0.294iteration: 2156\n",
      "train_loss: 0.15214976159069504\n",
      "val_loss: 0.294498467629583\n",
      "Progress: 21.6% ... Training loss: 0.153 ... Validation loss: 0.306iteration: 2157\n",
      "train_loss: 0.15312246637761323\n",
      "val_loss: 0.3062695440957459\n",
      "Progress: 21.6% ... Training loss: 0.157 ... Validation loss: 0.293iteration: 2158\n",
      "train_loss: 0.15761284647964882\n",
      "val_loss: 0.29323093007638396\n",
      "Progress: 21.6% ... Training loss: 0.162 ... Validation loss: 0.324iteration: 2159\n",
      "train_loss: 0.1627568691826714\n",
      "val_loss: 0.3249367808782128\n",
      "Progress: 21.6% ... Training loss: 0.151 ... Validation loss: 0.302iteration: 2160\n",
      "train_loss: 0.151747056460744\n",
      "val_loss: 0.302135651642592\n",
      "Progress: 21.6% ... Training loss: 0.156 ... Validation loss: 0.291iteration: 2161\n",
      "train_loss: 0.15674576129248993\n",
      "val_loss: 0.29187249842664614\n",
      "Progress: 21.6% ... Training loss: 0.151 ... Validation loss: 0.296iteration: 2162\n",
      "train_loss: 0.15129992443005003\n",
      "val_loss: 0.2962590350191627\n",
      "Progress: 21.6% ... Training loss: 0.151 ... Validation loss: 0.296iteration: 2163\n",
      "train_loss: 0.15134483149329844\n",
      "val_loss: 0.29657070594206353\n",
      "Progress: 21.6% ... Training loss: 0.153 ... Validation loss: 0.302iteration: 2164\n",
      "train_loss: 0.15332393461086025\n",
      "val_loss: 0.30281425436339793\n",
      "Progress: 21.6% ... Training loss: 0.151 ... Validation loss: 0.295iteration: 2165\n",
      "train_loss: 0.15173962114670705\n",
      "val_loss: 0.29548928406953817\n",
      "Progress: 21.7% ... Training loss: 0.151 ... Validation loss: 0.300iteration: 2166\n",
      "train_loss: 0.1518464261489226\n",
      "val_loss: 0.3000150818802495\n",
      "Progress: 21.7% ... Training loss: 0.158 ... Validation loss: 0.290iteration: 2167\n",
      "train_loss: 0.15877609611315144\n",
      "val_loss: 0.29072507222336375\n",
      "Progress: 21.7% ... Training loss: 0.164 ... Validation loss: 0.318iteration: 2168\n",
      "train_loss: 0.16454051836376432\n",
      "val_loss: 0.3183123680123097\n",
      "Progress: 21.7% ... Training loss: 0.155 ... Validation loss: 0.279iteration: 2169\n",
      "train_loss: 0.1553196331574118\n",
      "val_loss: 0.27985278459401075\n",
      "Progress: 21.7% ... Training loss: 0.152 ... Validation loss: 0.295iteration: 2170\n",
      "train_loss: 0.15247819079201705\n",
      "val_loss: 0.29559667159106306\n",
      "Progress: 21.7% ... Training loss: 0.152 ... Validation loss: 0.281iteration: 2171\n",
      "train_loss: 0.15201700295443335\n",
      "val_loss: 0.28139118186445117\n",
      "Progress: 21.7% ... Training loss: 0.151 ... Validation loss: 0.291iteration: 2172\n",
      "train_loss: 0.1512789520677803\n",
      "val_loss: 0.29131760420481656\n",
      "Progress: 21.7% ... Training loss: 0.158 ... Validation loss: 0.287iteration: 2173\n",
      "train_loss: 0.15838972008161878\n",
      "val_loss: 0.28760319130427936\n",
      "Progress: 21.7% ... Training loss: 0.155 ... Validation loss: 0.314iteration: 2174\n",
      "train_loss: 0.1556310065463346\n",
      "val_loss: 0.31479025004792016\n",
      "Progress: 21.8% ... Training loss: 0.150 ... Validation loss: 0.291iteration: 2175\n",
      "train_loss: 0.15093952138704558\n",
      "val_loss: 0.29177715387224035\n",
      "Progress: 21.8% ... Training loss: 0.149 ... Validation loss: 0.297iteration: 2176\n",
      "train_loss: 0.14992376343657962\n",
      "val_loss: 0.2971978438234443\n",
      "Progress: 21.8% ... Training loss: 0.150 ... Validation loss: 0.293iteration: 2177\n",
      "train_loss: 0.15021407183995408\n",
      "val_loss: 0.29379767754823555\n",
      "Progress: 21.8% ... Training loss: 0.149 ... Validation loss: 0.290iteration: 2178\n",
      "train_loss: 0.14992109392606492\n",
      "val_loss: 0.29004823295293786\n",
      "Progress: 21.8% ... Training loss: 0.150 ... Validation loss: 0.298iteration: 2179\n",
      "train_loss: 0.15049158246167615\n",
      "val_loss: 0.29862812158699925\n",
      "Progress: 21.8% ... Training loss: 0.149 ... Validation loss: 0.292iteration: 2180\n",
      "train_loss: 0.14969096118735795\n",
      "val_loss: 0.2926067800668758\n",
      "Progress: 21.8% ... Training loss: 0.157 ... Validation loss: 0.302iteration: 2181\n",
      "train_loss: 0.15717823589392552\n",
      "val_loss: 0.30249325344806216\n",
      "Progress: 21.8% ... Training loss: 0.154 ... Validation loss: 0.281iteration: 2182\n",
      "train_loss: 0.15454407841144738\n",
      "val_loss: 0.28118271725044985\n",
      "Progress: 21.8% ... Training loss: 0.152 ... Validation loss: 0.304iteration: 2183\n",
      "train_loss: 0.15282384212903305\n",
      "val_loss: 0.3045677423417401\n",
      "Progress: 21.8% ... Training loss: 0.149 ... Validation loss: 0.286iteration: 2184\n",
      "train_loss: 0.14983440718324118\n",
      "val_loss: 0.2864609874834108\n",
      "Progress: 21.9% ... Training loss: 0.149 ... Validation loss: 0.287iteration: 2185\n",
      "train_loss: 0.14957297483864077\n",
      "val_loss: 0.2870976886510309\n",
      "Progress: 21.9% ... Training loss: 0.150 ... Validation loss: 0.278iteration: 2186\n",
      "train_loss: 0.15057347950206412\n",
      "val_loss: 0.2785052236030871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 21.9% ... Training loss: 0.150 ... Validation loss: 0.279iteration: 2187\n",
      "train_loss: 0.15003265330326662\n",
      "val_loss: 0.2791280209355625\n",
      "Progress: 21.9% ... Training loss: 0.149 ... Validation loss: 0.279iteration: 2188\n",
      "train_loss: 0.1496255656955763\n",
      "val_loss: 0.2795271779220438\n",
      "Progress: 21.9% ... Training loss: 0.151 ... Validation loss: 0.278iteration: 2189\n",
      "train_loss: 0.15139960876897382\n",
      "val_loss: 0.27869931995519476\n",
      "Progress: 21.9% ... Training loss: 0.150 ... Validation loss: 0.287iteration: 2190\n",
      "train_loss: 0.15026782409157835\n",
      "val_loss: 0.28765105724210427\n",
      "Progress: 21.9% ... Training loss: 0.150 ... Validation loss: 0.282iteration: 2191\n",
      "train_loss: 0.15035838153847766\n",
      "val_loss: 0.2826095072436879\n",
      "Progress: 21.9% ... Training loss: 0.150 ... Validation loss: 0.285iteration: 2192\n",
      "train_loss: 0.15057576475382717\n",
      "val_loss: 0.2858620240181021\n",
      "Progress: 21.9% ... Training loss: 0.152 ... Validation loss: 0.278iteration: 2193\n",
      "train_loss: 0.15280841245125903\n",
      "val_loss: 0.2780246158571501\n",
      "Progress: 21.9% ... Training loss: 0.151 ... Validation loss: 0.291iteration: 2194\n",
      "train_loss: 0.1513193172433905\n",
      "val_loss: 0.2910692926452841\n",
      "Progress: 21.9% ... Training loss: 0.152 ... Validation loss: 0.282iteration: 2195\n",
      "train_loss: 0.1527846803989643\n",
      "val_loss: 0.2826210215930741\n",
      "Progress: 22.0% ... Training loss: 0.154 ... Validation loss: 0.298iteration: 2196\n",
      "train_loss: 0.1548939923955799\n",
      "val_loss: 0.29825125796924334\n",
      "Progress: 22.0% ... Training loss: 0.149 ... Validation loss: 0.281iteration: 2197\n",
      "train_loss: 0.14942666073377486\n",
      "val_loss: 0.28176445569088276\n",
      "Progress: 22.0% ... Training loss: 0.147 ... Validation loss: 0.279iteration: 2198\n",
      "train_loss: 0.1478594066515966\n",
      "val_loss: 0.2794579461773546\n",
      "Progress: 22.0% ... Training loss: 0.149 ... Validation loss: 0.281iteration: 2199\n",
      "train_loss: 0.14914778710102522\n",
      "val_loss: 0.2815140522492137\n",
      "Progress: 22.0% ... Training loss: 0.150 ... Validation loss: 0.286iteration: 2200\n",
      "train_loss: 0.15062739180486712\n",
      "val_loss: 0.28626369622035824\n",
      "Progress: 22.0% ... Training loss: 0.148 ... Validation loss: 0.280iteration: 2201\n",
      "train_loss: 0.14854760541440204\n",
      "val_loss: 0.2801252826127809\n",
      "Progress: 22.0% ... Training loss: 0.149 ... Validation loss: 0.283iteration: 2202\n",
      "train_loss: 0.14950776519989806\n",
      "val_loss: 0.2839144337194479\n",
      "Progress: 22.0% ... Training loss: 0.149 ... Validation loss: 0.280iteration: 2203\n",
      "train_loss: 0.1499584408875035\n",
      "val_loss: 0.2800651062027289\n",
      "Progress: 22.0% ... Training loss: 0.149 ... Validation loss: 0.271iteration: 2204\n",
      "train_loss: 0.14957777149779494\n",
      "val_loss: 0.2717951462537824\n",
      "Progress: 22.1% ... Training loss: 0.150 ... Validation loss: 0.272iteration: 2205\n",
      "train_loss: 0.15050169654957146\n",
      "val_loss: 0.27253083503995457\n",
      "Progress: 22.1% ... Training loss: 0.148 ... Validation loss: 0.290iteration: 2206\n",
      "train_loss: 0.1487389868558437\n",
      "val_loss: 0.2908164620968365\n",
      "Progress: 22.1% ... Training loss: 0.161 ... Validation loss: 0.327iteration: 2207\n",
      "train_loss: 0.161459957288746\n",
      "val_loss: 0.32724064742170805\n",
      "Progress: 22.1% ... Training loss: 0.148 ... Validation loss: 0.278iteration: 2208\n",
      "train_loss: 0.1488806343139103\n",
      "val_loss: 0.2781350845760883\n",
      "Progress: 22.1% ... Training loss: 0.147 ... Validation loss: 0.282iteration: 2209\n",
      "train_loss: 0.14734674362992853\n",
      "val_loss: 0.2828621619816588\n",
      "Progress: 22.1% ... Training loss: 0.148 ... Validation loss: 0.278iteration: 2210\n",
      "train_loss: 0.1483453892898642\n",
      "val_loss: 0.2780139752966299\n",
      "Progress: 22.1% ... Training loss: 0.148 ... Validation loss: 0.273iteration: 2211\n",
      "train_loss: 0.14845000220677207\n",
      "val_loss: 0.27364488294784733\n",
      "Progress: 22.1% ... Training loss: 0.147 ... Validation loss: 0.277iteration: 2212\n",
      "train_loss: 0.14755361987636245\n",
      "val_loss: 0.2775706839401681\n",
      "Progress: 22.1% ... Training loss: 0.146 ... Validation loss: 0.275iteration: 2213\n",
      "train_loss: 0.14692808305612445\n",
      "val_loss: 0.27513816649248357\n",
      "Progress: 22.1% ... Training loss: 0.148 ... Validation loss: 0.274iteration: 2214\n",
      "train_loss: 0.14879252701783652\n",
      "val_loss: 0.2740706257362918\n",
      "Progress: 22.1% ... Training loss: 0.149 ... Validation loss: 0.285iteration: 2215\n",
      "train_loss: 0.1498197346714976\n",
      "val_loss: 0.2858027728936802\n",
      "Progress: 22.2% ... Training loss: 0.147 ... Validation loss: 0.285iteration: 2216\n",
      "train_loss: 0.1470966873186275\n",
      "val_loss: 0.28598359465315815\n",
      "Progress: 22.2% ... Training loss: 0.147 ... Validation loss: 0.276iteration: 2217\n",
      "train_loss: 0.14707628921742974\n",
      "val_loss: 0.27682878091884794\n",
      "Progress: 22.2% ... Training loss: 0.150 ... Validation loss: 0.277iteration: 2218\n",
      "train_loss: 0.15054070198868447\n",
      "val_loss: 0.27775124306848986\n",
      "Progress: 22.2% ... Training loss: 0.154 ... Validation loss: 0.299iteration: 2219\n",
      "train_loss: 0.15457949799749462\n",
      "val_loss: 0.2995883871029466\n",
      "Progress: 22.2% ... Training loss: 0.146 ... Validation loss: 0.274iteration: 2220\n",
      "train_loss: 0.14624086094451585\n",
      "val_loss: 0.2748856782722045\n",
      "Progress: 22.2% ... Training loss: 0.146 ... Validation loss: 0.282iteration: 2221\n",
      "train_loss: 0.14647570812817493\n",
      "val_loss: 0.282695196728885\n",
      "Progress: 22.2% ... Training loss: 0.156 ... Validation loss: 0.308iteration: 2222\n",
      "train_loss: 0.1562030346163222\n",
      "val_loss: 0.308344031202992\n",
      "Progress: 22.2% ... Training loss: 0.147 ... Validation loss: 0.286iteration: 2223\n",
      "train_loss: 0.14716161300295527\n",
      "val_loss: 0.2862233753230241\n",
      "Progress: 22.2% ... Training loss: 0.150 ... Validation loss: 0.278iteration: 2224\n",
      "train_loss: 0.15073744414091256\n",
      "val_loss: 0.27883510856974336\n",
      "Progress: 22.2% ... Training loss: 0.146 ... Validation loss: 0.275iteration: 2225\n",
      "train_loss: 0.14685503567290548\n",
      "val_loss: 0.27543800721550266\n",
      "Progress: 22.3% ... Training loss: 0.149 ... Validation loss: 0.273iteration: 2226\n",
      "train_loss: 0.14932528191788588\n",
      "val_loss: 0.2737804306858083\n",
      "Progress: 22.3% ... Training loss: 0.152 ... Validation loss: 0.286iteration: 2227\n",
      "train_loss: 0.15245090945155207\n",
      "val_loss: 0.2866711107292204\n",
      "Progress: 22.3% ... Training loss: 0.146 ... Validation loss: 0.269iteration: 2228\n",
      "train_loss: 0.14654613468621855\n",
      "val_loss: 0.26924997086142627\n",
      "Progress: 22.3% ... Training loss: 0.149 ... Validation loss: 0.282iteration: 2229\n",
      "train_loss: 0.1491161905332007\n",
      "val_loss: 0.2821027789912528\n",
      "Progress: 22.3% ... Training loss: 0.149 ... Validation loss: 0.269iteration: 2230\n",
      "train_loss: 0.14976301448345136\n",
      "val_loss: 0.26947367671322847\n",
      "Progress: 22.3% ... Training loss: 0.164 ... Validation loss: 0.313iteration: 2231\n",
      "train_loss: 0.16426522091899753\n",
      "val_loss: 0.31350738789272864\n",
      "Progress: 22.3% ... Training loss: 0.147 ... Validation loss: 0.272iteration: 2232\n",
      "train_loss: 0.147426935141863\n",
      "val_loss: 0.27263043920341595\n",
      "Progress: 22.3% ... Training loss: 0.160 ... Validation loss: 0.305iteration: 2233\n",
      "train_loss: 0.16066655676895447\n",
      "val_loss: 0.30538827333592655\n",
      "Progress: 22.3% ... Training loss: 0.145 ... Validation loss: 0.275iteration: 2234\n",
      "train_loss: 0.14543504291391554\n",
      "val_loss: 0.2754785962695451\n",
      "Progress: 22.4% ... Training loss: 0.146 ... Validation loss: 0.268iteration: 2235\n",
      "train_loss: 0.1462746471483184\n",
      "val_loss: 0.2687966744839181\n",
      "Progress: 22.4% ... Training loss: 0.145 ... Validation loss: 0.278iteration: 2236\n",
      "train_loss: 0.1455287299193813\n",
      "val_loss: 0.2785619932768449\n",
      "Progress: 22.4% ... Training loss: 0.145 ... Validation loss: 0.273iteration: 2237\n",
      "train_loss: 0.1457372422324375\n",
      "val_loss: 0.27304918978581455\n",
      "Progress: 22.4% ... Training loss: 0.152 ... Validation loss: 0.273iteration: 2238\n",
      "train_loss: 0.1524502430588094\n",
      "val_loss: 0.2738719521783772\n",
      "Progress: 22.4% ... Training loss: 0.146 ... Validation loss: 0.276iteration: 2239\n",
      "train_loss: 0.14648923989483517\n",
      "val_loss: 0.2760198620825213\n",
      "Progress: 22.4% ... Training loss: 0.146 ... Validation loss: 0.273iteration: 2240\n",
      "train_loss: 0.14683316151381576\n",
      "val_loss: 0.27393160070725464\n",
      "Progress: 22.4% ... Training loss: 0.145 ... Validation loss: 0.267iteration: 2241\n",
      "train_loss: 0.14584325421240335\n",
      "val_loss: 0.26758644324016306\n",
      "Progress: 22.4% ... Training loss: 0.145 ... Validation loss: 0.266iteration: 2242\n",
      "train_loss: 0.14562966065074193\n",
      "val_loss: 0.26666655842917664\n",
      "Progress: 22.4% ... Training loss: 0.146 ... Validation loss: 0.266iteration: 2243\n",
      "train_loss: 0.14611125850948095\n",
      "val_loss: 0.2665878909505315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 22.4% ... Training loss: 0.144 ... Validation loss: 0.267iteration: 2244\n",
      "train_loss: 0.1441220819206156\n",
      "val_loss: 0.2676170224874778\n",
      "Progress: 22.4% ... Training loss: 0.144 ... Validation loss: 0.271iteration: 2245\n",
      "train_loss: 0.1447691313644616\n",
      "val_loss: 0.27184012909085625\n",
      "Progress: 22.5% ... Training loss: 0.151 ... Validation loss: 0.286iteration: 2246\n",
      "train_loss: 0.15153742803571763\n",
      "val_loss: 0.28617482096466246\n",
      "Progress: 22.5% ... Training loss: 0.145 ... Validation loss: 0.267iteration: 2247\n",
      "train_loss: 0.1454187709124903\n",
      "val_loss: 0.2670961614141693\n",
      "Progress: 22.5% ... Training loss: 0.151 ... Validation loss: 0.273iteration: 2248\n",
      "train_loss: 0.1512138608691357\n",
      "val_loss: 0.2736115303173214\n",
      "Progress: 22.5% ... Training loss: 0.144 ... Validation loss: 0.274iteration: 2249\n",
      "train_loss: 0.14499701632103698\n",
      "val_loss: 0.2743097702718165\n",
      "Progress: 22.5% ... Training loss: 0.143 ... Validation loss: 0.273iteration: 2250\n",
      "train_loss: 0.14359904613999433\n",
      "val_loss: 0.273089265810482\n",
      "Progress: 22.5% ... Training loss: 0.155 ... Validation loss: 0.295iteration: 2251\n",
      "train_loss: 0.1550686342940607\n",
      "val_loss: 0.29546844524627164\n",
      "Progress: 22.5% ... Training loss: 0.156 ... Validation loss: 0.275iteration: 2252\n",
      "train_loss: 0.15647384478116694\n",
      "val_loss: 0.27527066512880394\n",
      "Progress: 22.5% ... Training loss: 0.145 ... Validation loss: 0.276iteration: 2253\n",
      "train_loss: 0.1458294458331827\n",
      "val_loss: 0.27617834483436154\n",
      "Progress: 22.5% ... Training loss: 0.145 ... Validation loss: 0.280iteration: 2254\n",
      "train_loss: 0.14572350524508046\n",
      "val_loss: 0.2808969753997976\n",
      "Progress: 22.6% ... Training loss: 0.150 ... Validation loss: 0.275iteration: 2255\n",
      "train_loss: 0.15085555813202928\n",
      "val_loss: 0.2759970096156135\n",
      "Progress: 22.6% ... Training loss: 0.144 ... Validation loss: 0.279iteration: 2256\n",
      "train_loss: 0.14406909696332218\n",
      "val_loss: 0.2795846254597116\n",
      "Progress: 22.6% ... Training loss: 0.143 ... Validation loss: 0.271iteration: 2257\n",
      "train_loss: 0.1439771285939708\n",
      "val_loss: 0.27113549401775305\n",
      "Progress: 22.6% ... Training loss: 0.143 ... Validation loss: 0.274iteration: 2258\n",
      "train_loss: 0.14320863509025655\n",
      "val_loss: 0.2746990094723225\n",
      "Progress: 22.6% ... Training loss: 0.144 ... Validation loss: 0.277iteration: 2259\n",
      "train_loss: 0.144127062325521\n",
      "val_loss: 0.27712794655827727\n",
      "Progress: 22.6% ... Training loss: 0.154 ... Validation loss: 0.297iteration: 2260\n",
      "train_loss: 0.15425678004100107\n",
      "val_loss: 0.2975374164516305\n",
      "Progress: 22.6% ... Training loss: 0.146 ... Validation loss: 0.275iteration: 2261\n",
      "train_loss: 0.14653654131846608\n",
      "val_loss: 0.2753616550706343\n",
      "Progress: 22.6% ... Training loss: 0.171 ... Validation loss: 0.321iteration: 2262\n",
      "train_loss: 0.17145420717388993\n",
      "val_loss: 0.321198825785942\n",
      "Progress: 22.6% ... Training loss: 0.148 ... Validation loss: 0.268iteration: 2263\n",
      "train_loss: 0.14852071408924797\n",
      "val_loss: 0.2685625538375522\n",
      "Progress: 22.6% ... Training loss: 0.148 ... Validation loss: 0.286iteration: 2264\n",
      "train_loss: 0.14810164575295534\n",
      "val_loss: 0.2863472475149112\n",
      "Progress: 22.6% ... Training loss: 0.143 ... Validation loss: 0.269iteration: 2265\n",
      "train_loss: 0.14351632734178046\n",
      "val_loss: 0.2695206113999775\n",
      "Progress: 22.7% ... Training loss: 0.144 ... Validation loss: 0.281iteration: 2266\n",
      "train_loss: 0.14464896906320873\n",
      "val_loss: 0.2819943246515059\n",
      "Progress: 22.7% ... Training loss: 0.149 ... Validation loss: 0.266iteration: 2267\n",
      "train_loss: 0.14951443499158068\n",
      "val_loss: 0.2663213237280438\n",
      "Progress: 22.7% ... Training loss: 0.154 ... Validation loss: 0.294iteration: 2268\n",
      "train_loss: 0.1545457844068509\n",
      "val_loss: 0.2940210326399765\n",
      "Progress: 22.7% ... Training loss: 0.143 ... Validation loss: 0.266iteration: 2269\n",
      "train_loss: 0.14386185460187786\n",
      "val_loss: 0.2660497981883859\n",
      "Progress: 22.7% ... Training loss: 0.148 ... Validation loss: 0.261iteration: 2270\n",
      "train_loss: 0.14802552672416333\n",
      "val_loss: 0.26194820657654483\n",
      "Progress: 22.7% ... Training loss: 0.141 ... Validation loss: 0.268iteration: 2271\n",
      "train_loss: 0.1419622970833364\n",
      "val_loss: 0.26869597860475253\n",
      "Progress: 22.7% ... Training loss: 0.154 ... Validation loss: 0.295iteration: 2272\n",
      "train_loss: 0.1546960405256444\n",
      "val_loss: 0.29561787414182183\n",
      "Progress: 22.7% ... Training loss: 0.142 ... Validation loss: 0.265iteration: 2273\n",
      "train_loss: 0.14267934761498652\n",
      "val_loss: 0.26578348382477324\n",
      "Progress: 22.7% ... Training loss: 0.143 ... Validation loss: 0.268iteration: 2274\n",
      "train_loss: 0.1432539889148492\n",
      "val_loss: 0.2681385478225132\n",
      "Progress: 22.8% ... Training loss: 0.147 ... Validation loss: 0.278iteration: 2275\n",
      "train_loss: 0.1477729176299819\n",
      "val_loss: 0.2783408934960589\n",
      "Progress: 22.8% ... Training loss: 0.143 ... Validation loss: 0.267iteration: 2276\n",
      "train_loss: 0.14367022352323058\n",
      "val_loss: 0.26751577495346157\n",
      "Progress: 22.8% ... Training loss: 0.148 ... Validation loss: 0.286iteration: 2277\n",
      "train_loss: 0.14841824352646904\n",
      "val_loss: 0.2868323022977082\n",
      "Progress: 22.8% ... Training loss: 0.144 ... Validation loss: 0.276iteration: 2278\n",
      "train_loss: 0.144746242524667\n",
      "val_loss: 0.2762889142993421\n",
      "Progress: 22.8% ... Training loss: 0.143 ... Validation loss: 0.265iteration: 2279\n",
      "train_loss: 0.1437825672551728\n",
      "val_loss: 0.2655737909890548\n",
      "Progress: 22.8% ... Training loss: 0.145 ... Validation loss: 0.262iteration: 2280\n",
      "train_loss: 0.1453043218118504\n",
      "val_loss: 0.26275210679856575\n",
      "Progress: 22.8% ... Training loss: 0.153 ... Validation loss: 0.284iteration: 2281\n",
      "train_loss: 0.15347849009477627\n",
      "val_loss: 0.2846157203666313\n",
      "Progress: 22.8% ... Training loss: 0.143 ... Validation loss: 0.266iteration: 2282\n",
      "train_loss: 0.14372496587354913\n",
      "val_loss: 0.2666309176126118\n",
      "Progress: 22.8% ... Training loss: 0.147 ... Validation loss: 0.269iteration: 2283\n",
      "train_loss: 0.14748445995672846\n",
      "val_loss: 0.2696537762167001\n",
      "Progress: 22.8% ... Training loss: 0.144 ... Validation loss: 0.283iteration: 2284\n",
      "train_loss: 0.14470177699394804\n",
      "val_loss: 0.28317611380077773\n",
      "Progress: 22.9% ... Training loss: 0.143 ... Validation loss: 0.276iteration: 2285\n",
      "train_loss: 0.14377893852562174\n",
      "val_loss: 0.27679405846335214\n",
      "Progress: 22.9% ... Training loss: 0.160 ... Validation loss: 0.275iteration: 2286\n",
      "train_loss: 0.16067919811876843\n",
      "val_loss: 0.2753012807193629\n",
      "Progress: 22.9% ... Training loss: 0.145 ... Validation loss: 0.274iteration: 2287\n",
      "train_loss: 0.14529777943360617\n",
      "val_loss: 0.27406998945218225\n",
      "Progress: 22.9% ... Training loss: 0.146 ... Validation loss: 0.267iteration: 2288\n",
      "train_loss: 0.14682336341569624\n",
      "val_loss: 0.2671824589649867\n",
      "Progress: 22.9% ... Training loss: 0.140 ... Validation loss: 0.263iteration: 2289\n",
      "train_loss: 0.14089480888757816\n",
      "val_loss: 0.2638855648593367\n",
      "Progress: 22.9% ... Training loss: 0.142 ... Validation loss: 0.271iteration: 2290\n",
      "train_loss: 0.14224771275810488\n",
      "val_loss: 0.2712902236110625\n",
      "Progress: 22.9% ... Training loss: 0.142 ... Validation loss: 0.272iteration: 2291\n",
      "train_loss: 0.14205992128241487\n",
      "val_loss: 0.27238428408865456\n",
      "Progress: 22.9% ... Training loss: 0.140 ... Validation loss: 0.258iteration: 2292\n",
      "train_loss: 0.14092191120531333\n",
      "val_loss: 0.25858022994693364\n",
      "Progress: 22.9% ... Training loss: 0.141 ... Validation loss: 0.258iteration: 2293\n",
      "train_loss: 0.1412391788107066\n",
      "val_loss: 0.2582343950347164\n",
      "Progress: 22.9% ... Training loss: 0.140 ... Validation loss: 0.258iteration: 2294\n",
      "train_loss: 0.14075096804434056\n",
      "val_loss: 0.2584516807519075\n",
      "Progress: 22.9% ... Training loss: 0.141 ... Validation loss: 0.259iteration: 2295\n",
      "train_loss: 0.14141502311415322\n",
      "val_loss: 0.25954857355210953\n",
      "Progress: 23.0% ... Training loss: 0.140 ... Validation loss: 0.259iteration: 2296\n",
      "train_loss: 0.1405496041227313\n",
      "val_loss: 0.2594950845915976\n",
      "Progress: 23.0% ... Training loss: 0.142 ... Validation loss: 0.261iteration: 2297\n",
      "train_loss: 0.1429674375218082\n",
      "val_loss: 0.26163609327832743\n",
      "Progress: 23.0% ... Training loss: 0.141 ... Validation loss: 0.261iteration: 2298\n",
      "train_loss: 0.14175174801872306\n",
      "val_loss: 0.2614104538961498\n",
      "Progress: 23.0% ... Training loss: 0.140 ... Validation loss: 0.261iteration: 2299\n",
      "train_loss: 0.1405601414740471\n",
      "val_loss: 0.26100840335434905\n",
      "Progress: 23.0% ... Training loss: 0.141 ... Validation loss: 0.261iteration: 2300\n",
      "train_loss: 0.14153419047044455\n",
      "val_loss: 0.26185747878448046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 23.0% ... Training loss: 0.141 ... Validation loss: 0.263iteration: 2301\n",
      "train_loss: 0.14102740469508976\n",
      "val_loss: 0.26390401501192545\n",
      "Progress: 23.0% ... Training loss: 0.140 ... Validation loss: 0.273iteration: 2302\n",
      "train_loss: 0.14072685826342635\n",
      "val_loss: 0.2730314892041237\n",
      "Progress: 23.0% ... Training loss: 0.140 ... Validation loss: 0.272iteration: 2303\n",
      "train_loss: 0.14082487683390696\n",
      "val_loss: 0.2723807284254844\n",
      "Progress: 23.0% ... Training loss: 0.145 ... Validation loss: 0.264iteration: 2304\n",
      "train_loss: 0.14543892945179535\n",
      "val_loss: 0.26496625790231\n",
      "Progress: 23.1% ... Training loss: 0.140 ... Validation loss: 0.271iteration: 2305\n",
      "train_loss: 0.140271951887633\n",
      "val_loss: 0.27146409360427165\n",
      "Progress: 23.1% ... Training loss: 0.139 ... Validation loss: 0.265iteration: 2306\n",
      "train_loss: 0.13976619221763256\n",
      "val_loss: 0.2655452038236197\n",
      "Progress: 23.1% ... Training loss: 0.140 ... Validation loss: 0.273iteration: 2307\n",
      "train_loss: 0.1402416520740013\n",
      "val_loss: 0.27393629957809834\n",
      "Progress: 23.1% ... Training loss: 0.140 ... Validation loss: 0.268iteration: 2308\n",
      "train_loss: 0.14003298573410541\n",
      "val_loss: 0.26893586170360656\n",
      "Progress: 23.1% ... Training loss: 0.143 ... Validation loss: 0.266iteration: 2309\n",
      "train_loss: 0.14389004682938974\n",
      "val_loss: 0.2660340230511448\n",
      "Progress: 23.1% ... Training loss: 0.143 ... Validation loss: 0.269iteration: 2310\n",
      "train_loss: 0.14395044177150332\n",
      "val_loss: 0.2690365211783734\n",
      "Progress: 23.1% ... Training loss: 0.147 ... Validation loss: 0.266iteration: 2311\n",
      "train_loss: 0.1471743639120529\n",
      "val_loss: 0.26625324645025716\n",
      "Progress: 23.1% ... Training loss: 0.140 ... Validation loss: 0.267iteration: 2312\n",
      "train_loss: 0.14045393138716097\n",
      "val_loss: 0.26776069013336135\n",
      "Progress: 23.1% ... Training loss: 0.140 ... Validation loss: 0.271iteration: 2313\n",
      "train_loss: 0.14038204367834922\n",
      "val_loss: 0.27197336932168925\n",
      "Progress: 23.1% ... Training loss: 0.139 ... Validation loss: 0.269iteration: 2314\n",
      "train_loss: 0.1397333275914424\n",
      "val_loss: 0.2692049906380127\n",
      "Progress: 23.1% ... Training loss: 0.144 ... Validation loss: 0.274iteration: 2315\n",
      "train_loss: 0.14467026460485385\n",
      "val_loss: 0.2744326903207461\n",
      "Progress: 23.2% ... Training loss: 0.139 ... Validation loss: 0.263iteration: 2316\n",
      "train_loss: 0.13906353740019706\n",
      "val_loss: 0.2630061519919576\n",
      "Progress: 23.2% ... Training loss: 0.139 ... Validation loss: 0.265iteration: 2317\n",
      "train_loss: 0.13918841759151449\n",
      "val_loss: 0.2655573827885661\n",
      "Progress: 23.2% ... Training loss: 0.143 ... Validation loss: 0.263iteration: 2318\n",
      "train_loss: 0.14347628207007881\n",
      "val_loss: 0.2633286719620584\n",
      "Progress: 23.2% ... Training loss: 0.138 ... Validation loss: 0.262iteration: 2319\n",
      "train_loss: 0.13894140497696153\n",
      "val_loss: 0.26224128793240903\n",
      "Progress: 23.2% ... Training loss: 0.138 ... Validation loss: 0.263iteration: 2320\n",
      "train_loss: 0.13858827918592498\n",
      "val_loss: 0.26314467825812876\n",
      "Progress: 23.2% ... Training loss: 0.142 ... Validation loss: 0.258iteration: 2321\n",
      "train_loss: 0.14241723361395855\n",
      "val_loss: 0.2587789808195704\n",
      "Progress: 23.2% ... Training loss: 0.155 ... Validation loss: 0.314iteration: 2322\n",
      "train_loss: 0.1557275192821639\n",
      "val_loss: 0.31457336080274295\n",
      "Progress: 23.2% ... Training loss: 0.154 ... Validation loss: 0.268iteration: 2323\n",
      "train_loss: 0.15478235360476536\n",
      "val_loss: 0.26895374064421834\n",
      "Progress: 23.2% ... Training loss: 0.139 ... Validation loss: 0.271iteration: 2324\n",
      "train_loss: 0.1395799290745882\n",
      "val_loss: 0.27126416698475875\n",
      "Progress: 23.2% ... Training loss: 0.138 ... Validation loss: 0.271iteration: 2325\n",
      "train_loss: 0.1380056159988801\n",
      "val_loss: 0.2711691191412022\n",
      "Progress: 23.3% ... Training loss: 0.138 ... Validation loss: 0.267iteration: 2326\n",
      "train_loss: 0.13852135891209752\n",
      "val_loss: 0.2674144424183895\n",
      "Progress: 23.3% ... Training loss: 0.138 ... Validation loss: 0.277iteration: 2327\n",
      "train_loss: 0.13895648741655792\n",
      "val_loss: 0.2776081503774362\n",
      "Progress: 23.3% ... Training loss: 0.139 ... Validation loss: 0.281iteration: 2328\n",
      "train_loss: 0.13953970673981267\n",
      "val_loss: 0.2813200360317368\n",
      "Progress: 23.3% ... Training loss: 0.138 ... Validation loss: 0.276iteration: 2329\n",
      "train_loss: 0.13854266974312512\n",
      "val_loss: 0.27688583134451783\n",
      "Progress: 23.3% ... Training loss: 0.138 ... Validation loss: 0.275iteration: 2330\n",
      "train_loss: 0.13883528754700974\n",
      "val_loss: 0.27580526182694837\n",
      "Progress: 23.3% ... Training loss: 0.139 ... Validation loss: 0.269iteration: 2331\n",
      "train_loss: 0.13931763622015333\n",
      "val_loss: 0.26920095326079263\n",
      "Progress: 23.3% ... Training loss: 0.143 ... Validation loss: 0.287iteration: 2332\n",
      "train_loss: 0.14384525755750588\n",
      "val_loss: 0.28716231259908204\n",
      "Progress: 23.3% ... Training loss: 0.136 ... Validation loss: 0.265iteration: 2333\n",
      "train_loss: 0.13687120043897832\n",
      "val_loss: 0.2653889816983257\n",
      "Progress: 23.3% ... Training loss: 0.138 ... Validation loss: 0.265iteration: 2334\n",
      "train_loss: 0.13812416467280522\n",
      "val_loss: 0.26549394356909944\n",
      "Progress: 23.4% ... Training loss: 0.139 ... Validation loss: 0.269iteration: 2335\n",
      "train_loss: 0.13971123463793567\n",
      "val_loss: 0.26979932999809614\n",
      "Progress: 23.4% ... Training loss: 0.142 ... Validation loss: 0.267iteration: 2336\n",
      "train_loss: 0.14223259239830705\n",
      "val_loss: 0.26750355088475053\n",
      "Progress: 23.4% ... Training loss: 0.146 ... Validation loss: 0.284iteration: 2337\n",
      "train_loss: 0.146431280046881\n",
      "val_loss: 0.28484785754596076\n",
      "Progress: 23.4% ... Training loss: 0.146 ... Validation loss: 0.268iteration: 2338\n",
      "train_loss: 0.14677557270013916\n",
      "val_loss: 0.26801939615689757\n",
      "Progress: 23.4% ... Training loss: 0.141 ... Validation loss: 0.287iteration: 2339\n",
      "train_loss: 0.14118888033814003\n",
      "val_loss: 0.28708735307396005\n",
      "Progress: 23.4% ... Training loss: 0.136 ... Validation loss: 0.265iteration: 2340\n",
      "train_loss: 0.1363682549434142\n",
      "val_loss: 0.265244245772333\n",
      "Progress: 23.4% ... Training loss: 0.137 ... Validation loss: 0.277iteration: 2341\n",
      "train_loss: 0.13771925318268557\n",
      "val_loss: 0.2770240356566507\n",
      "Progress: 23.4% ... Training loss: 0.155 ... Validation loss: 0.270iteration: 2342\n",
      "train_loss: 0.15503130088224748\n",
      "val_loss: 0.27078886064061786\n",
      "Progress: 23.4% ... Training loss: 0.139 ... Validation loss: 0.282iteration: 2343\n",
      "train_loss: 0.13956271609094595\n",
      "val_loss: 0.28269708069140737\n",
      "Progress: 23.4% ... Training loss: 0.137 ... Validation loss: 0.269iteration: 2344\n",
      "train_loss: 0.13722643690155134\n",
      "val_loss: 0.26954582752100453\n",
      "Progress: 23.4% ... Training loss: 0.138 ... Validation loss: 0.261iteration: 2345\n",
      "train_loss: 0.13871961394076798\n",
      "val_loss: 0.2614443772425556\n",
      "Progress: 23.5% ... Training loss: 0.136 ... Validation loss: 0.265iteration: 2346\n",
      "train_loss: 0.13642042883935354\n",
      "val_loss: 0.2651577980501758\n",
      "Progress: 23.5% ... Training loss: 0.135 ... Validation loss: 0.262iteration: 2347\n",
      "train_loss: 0.13588681808358058\n",
      "val_loss: 0.26245951480150237\n",
      "Progress: 23.5% ... Training loss: 0.138 ... Validation loss: 0.264iteration: 2348\n",
      "train_loss: 0.1386494660934304\n",
      "val_loss: 0.26477604450344044\n",
      "Progress: 23.5% ... Training loss: 0.138 ... Validation loss: 0.263iteration: 2349\n",
      "train_loss: 0.13842902120421857\n",
      "val_loss: 0.263433715281715\n",
      "Progress: 23.5% ... Training loss: 0.135 ... Validation loss: 0.264iteration: 2350\n",
      "train_loss: 0.1352929816871801\n",
      "val_loss: 0.2643427926104254\n",
      "Progress: 23.5% ... Training loss: 0.135 ... Validation loss: 0.263iteration: 2351\n",
      "train_loss: 0.1350536463352317\n",
      "val_loss: 0.26330603663394103\n",
      "Progress: 23.5% ... Training loss: 0.136 ... Validation loss: 0.261iteration: 2352\n",
      "train_loss: 0.13632581112559758\n",
      "val_loss: 0.2619652700677876\n",
      "Progress: 23.5% ... Training loss: 0.135 ... Validation loss: 0.261iteration: 2353\n",
      "train_loss: 0.13528613871765155\n",
      "val_loss: 0.26189248958523365\n",
      "Progress: 23.5% ... Training loss: 0.138 ... Validation loss: 0.273iteration: 2354\n",
      "train_loss: 0.13895942535500108\n",
      "val_loss: 0.2734012752147815\n",
      "Progress: 23.6% ... Training loss: 0.138 ... Validation loss: 0.260iteration: 2355\n",
      "train_loss: 0.13878375647227606\n",
      "val_loss: 0.26083764117450087\n",
      "Progress: 23.6% ... Training loss: 0.135 ... Validation loss: 0.264iteration: 2356\n",
      "train_loss: 0.135668610035006\n",
      "val_loss: 0.26486545003165796\n",
      "Progress: 23.6% ... Training loss: 0.137 ... Validation loss: 0.262iteration: 2357\n",
      "train_loss: 0.13752405144952773\n",
      "val_loss: 0.26239059474287085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 23.6% ... Training loss: 0.148 ... Validation loss: 0.264iteration: 2358\n",
      "train_loss: 0.14868885524665723\n",
      "val_loss: 0.26475726312757225\n",
      "Progress: 23.6% ... Training loss: 0.137 ... Validation loss: 0.264iteration: 2359\n",
      "train_loss: 0.1379067406720981\n",
      "val_loss: 0.26433474309074356\n",
      "Progress: 23.6% ... Training loss: 0.135 ... Validation loss: 0.270iteration: 2360\n",
      "train_loss: 0.13565249061668616\n",
      "val_loss: 0.27053317106811814\n",
      "Progress: 23.6% ... Training loss: 0.141 ... Validation loss: 0.283iteration: 2361\n",
      "train_loss: 0.14128954314416609\n",
      "val_loss: 0.2832083202834093\n",
      "Progress: 23.6% ... Training loss: 0.141 ... Validation loss: 0.261iteration: 2362\n",
      "train_loss: 0.14190797027924423\n",
      "val_loss: 0.26188912967620115\n",
      "Progress: 23.6% ... Training loss: 0.137 ... Validation loss: 0.265iteration: 2363\n",
      "train_loss: 0.13793256162917517\n",
      "val_loss: 0.26530231860503345\n",
      "Progress: 23.6% ... Training loss: 0.137 ... Validation loss: 0.265iteration: 2364\n",
      "train_loss: 0.13752338980403742\n",
      "val_loss: 0.26570031268564087\n",
      "Progress: 23.6% ... Training loss: 0.136 ... Validation loss: 0.272iteration: 2365\n",
      "train_loss: 0.1360380332145257\n",
      "val_loss: 0.272128133245668\n",
      "Progress: 23.7% ... Training loss: 0.136 ... Validation loss: 0.272iteration: 2366\n",
      "train_loss: 0.13662374139876024\n",
      "val_loss: 0.2727362489343065\n",
      "Progress: 23.7% ... Training loss: 0.136 ... Validation loss: 0.267iteration: 2367\n",
      "train_loss: 0.13620361059914882\n",
      "val_loss: 0.26791772565231337\n",
      "Progress: 23.7% ... Training loss: 0.142 ... Validation loss: 0.260iteration: 2368\n",
      "train_loss: 0.14244238754434763\n",
      "val_loss: 0.2608236325126488\n",
      "Progress: 23.7% ... Training loss: 0.137 ... Validation loss: 0.280iteration: 2369\n",
      "train_loss: 0.13700229932187644\n",
      "val_loss: 0.28038071259335234\n",
      "Progress: 23.7% ... Training loss: 0.139 ... Validation loss: 0.284iteration: 2370\n",
      "train_loss: 0.1391070150667103\n",
      "val_loss: 0.28449283274248827\n",
      "Progress: 23.7% ... Training loss: 0.134 ... Validation loss: 0.258iteration: 2371\n",
      "train_loss: 0.13442544386869223\n",
      "val_loss: 0.2589821780877932\n",
      "Progress: 23.7% ... Training loss: 0.136 ... Validation loss: 0.267iteration: 2372\n",
      "train_loss: 0.13662620405918124\n",
      "val_loss: 0.2675364724401331\n",
      "Progress: 23.7% ... Training loss: 0.136 ... Validation loss: 0.267iteration: 2373\n",
      "train_loss: 0.13642421629498983\n",
      "val_loss: 0.26784676860371354\n",
      "Progress: 23.7% ... Training loss: 0.134 ... Validation loss: 0.255iteration: 2374\n",
      "train_loss: 0.13490265786363267\n",
      "val_loss: 0.2555192029438803\n",
      "Progress: 23.8% ... Training loss: 0.136 ... Validation loss: 0.253iteration: 2375\n",
      "train_loss: 0.13617053656870548\n",
      "val_loss: 0.25373599470703695\n",
      "Progress: 23.8% ... Training loss: 0.141 ... Validation loss: 0.276iteration: 2376\n",
      "train_loss: 0.14161053341829452\n",
      "val_loss: 0.2769744164220542\n",
      "Progress: 23.8% ... Training loss: 0.135 ... Validation loss: 0.252iteration: 2377\n",
      "train_loss: 0.13587164887946485\n",
      "val_loss: 0.25272176654461626\n",
      "Progress: 23.8% ... Training loss: 0.134 ... Validation loss: 0.258iteration: 2378\n",
      "train_loss: 0.13466916815254656\n",
      "val_loss: 0.2585418531680378\n",
      "Progress: 23.8% ... Training loss: 0.135 ... Validation loss: 0.267iteration: 2379\n",
      "train_loss: 0.13592234806567371\n",
      "val_loss: 0.26726051118819055\n",
      "Progress: 23.8% ... Training loss: 0.134 ... Validation loss: 0.257iteration: 2380\n",
      "train_loss: 0.13413220360789535\n",
      "val_loss: 0.2573696783603596\n",
      "Progress: 23.8% ... Training loss: 0.142 ... Validation loss: 0.282iteration: 2381\n",
      "train_loss: 0.14242664386371257\n",
      "val_loss: 0.2820659939455533\n",
      "Progress: 23.8% ... Training loss: 0.145 ... Validation loss: 0.251iteration: 2382\n",
      "train_loss: 0.145030256078352\n",
      "val_loss: 0.25190460747613386\n",
      "Progress: 23.8% ... Training loss: 0.134 ... Validation loss: 0.257iteration: 2383\n",
      "train_loss: 0.13477445024788867\n",
      "val_loss: 0.25709068505870186\n",
      "Progress: 23.8% ... Training loss: 0.146 ... Validation loss: 0.253iteration: 2384\n",
      "train_loss: 0.14629816749892352\n",
      "val_loss: 0.2538062955345259\n",
      "Progress: 23.9% ... Training loss: 0.144 ... Validation loss: 0.297iteration: 2385\n",
      "train_loss: 0.14420808218512846\n",
      "val_loss: 0.29793414147986974\n",
      "Progress: 23.9% ... Training loss: 0.144 ... Validation loss: 0.258iteration: 2386\n",
      "train_loss: 0.14402039230284894\n",
      "val_loss: 0.2586018064997945\n",
      "Progress: 23.9% ... Training loss: 0.142 ... Validation loss: 0.290iteration: 2387\n",
      "train_loss: 0.14274783517409742\n",
      "val_loss: 0.2901797747931682\n",
      "Progress: 23.9% ... Training loss: 0.134 ... Validation loss: 0.259iteration: 2388\n",
      "train_loss: 0.13418130317993657\n",
      "val_loss: 0.25949941473835125\n",
      "Progress: 23.9% ... Training loss: 0.136 ... Validation loss: 0.256iteration: 2389\n",
      "train_loss: 0.13617472874746805\n",
      "val_loss: 0.25691757513305324\n",
      "Progress: 23.9% ... Training loss: 0.133 ... Validation loss: 0.264iteration: 2390\n",
      "train_loss: 0.13304202258065917\n",
      "val_loss: 0.26424458990756344\n",
      "Progress: 23.9% ... Training loss: 0.133 ... Validation loss: 0.265iteration: 2391\n",
      "train_loss: 0.13317022263980524\n",
      "val_loss: 0.26517174716400815\n",
      "Progress: 23.9% ... Training loss: 0.132 ... Validation loss: 0.262iteration: 2392\n",
      "train_loss: 0.13284571879771692\n",
      "val_loss: 0.2622432855466836\n",
      "Progress: 23.9% ... Training loss: 0.133 ... Validation loss: 0.266iteration: 2393\n",
      "train_loss: 0.1339755523133939\n",
      "val_loss: 0.26609122136623903\n",
      "Progress: 23.9% ... Training loss: 0.132 ... Validation loss: 0.252iteration: 2394\n",
      "train_loss: 0.13286993377401804\n",
      "val_loss: 0.2527256301014785\n",
      "Progress: 23.9% ... Training loss: 0.135 ... Validation loss: 0.257iteration: 2395\n",
      "train_loss: 0.13556708660530872\n",
      "val_loss: 0.25799716319295196\n",
      "Progress: 24.0% ... Training loss: 0.133 ... Validation loss: 0.255iteration: 2396\n",
      "train_loss: 0.1332666966708865\n",
      "val_loss: 0.2558903231553427\n",
      "Progress: 24.0% ... Training loss: 0.135 ... Validation loss: 0.251iteration: 2397\n",
      "train_loss: 0.1359447748930509\n",
      "val_loss: 0.2519060517452464\n",
      "Progress: 24.0% ... Training loss: 0.133 ... Validation loss: 0.258iteration: 2398\n",
      "train_loss: 0.13300543719695662\n",
      "val_loss: 0.25812621753078346\n",
      "Progress: 24.0% ... Training loss: 0.143 ... Validation loss: 0.273iteration: 2399\n",
      "train_loss: 0.1431309200928294\n",
      "val_loss: 0.2735947513307565\n",
      "Progress: 24.0% ... Training loss: 0.133 ... Validation loss: 0.249iteration: 2400\n",
      "train_loss: 0.1331849045096229\n",
      "val_loss: 0.2493799861405953\n",
      "Progress: 24.0% ... Training loss: 0.134 ... Validation loss: 0.249iteration: 2401\n",
      "train_loss: 0.13466409826276163\n",
      "val_loss: 0.2495075673676896\n",
      "Progress: 24.0% ... Training loss: 0.136 ... Validation loss: 0.254iteration: 2402\n",
      "train_loss: 0.13624342605389625\n",
      "val_loss: 0.2547016870001638\n",
      "Progress: 24.0% ... Training loss: 0.135 ... Validation loss: 0.278iteration: 2403\n",
      "train_loss: 0.13565876722181427\n",
      "val_loss: 0.2786933858346856\n",
      "Progress: 24.0% ... Training loss: 0.133 ... Validation loss: 0.264iteration: 2404\n",
      "train_loss: 0.1331676165779881\n",
      "val_loss: 0.26401214988455785\n",
      "Progress: 24.1% ... Training loss: 0.134 ... Validation loss: 0.278iteration: 2405\n",
      "train_loss: 0.13476463171286854\n",
      "val_loss: 0.2785738927869249\n",
      "Progress: 24.1% ... Training loss: 0.135 ... Validation loss: 0.258iteration: 2406\n",
      "train_loss: 0.13530474571085224\n",
      "val_loss: 0.2587886427716986\n",
      "Progress: 24.1% ... Training loss: 0.133 ... Validation loss: 0.264iteration: 2407\n",
      "train_loss: 0.13362390768590185\n",
      "val_loss: 0.26413390547797877\n",
      "Progress: 24.1% ... Training loss: 0.132 ... Validation loss: 0.256iteration: 2408\n",
      "train_loss: 0.13254595060237762\n",
      "val_loss: 0.2569092972432633\n",
      "Progress: 24.1% ... Training loss: 0.131 ... Validation loss: 0.256iteration: 2409\n",
      "train_loss: 0.13141531365170622\n",
      "val_loss: 0.2564522889576325\n",
      "Progress: 24.1% ... Training loss: 0.141 ... Validation loss: 0.258iteration: 2410\n",
      "train_loss: 0.141376041993073\n",
      "val_loss: 0.25811766459794094\n",
      "Progress: 24.1% ... Training loss: 0.134 ... Validation loss: 0.275iteration: 2411\n",
      "train_loss: 0.13417910558313192\n",
      "val_loss: 0.27593054280561946\n",
      "Progress: 24.1% ... Training loss: 0.133 ... Validation loss: 0.263iteration: 2412\n",
      "train_loss: 0.13342105453806052\n",
      "val_loss: 0.2637114096545821\n",
      "Progress: 24.1% ... Training loss: 0.131 ... Validation loss: 0.255iteration: 2413\n",
      "train_loss: 0.13121680299660957\n",
      "val_loss: 0.2554695406591084\n",
      "Progress: 24.1% ... Training loss: 0.142 ... Validation loss: 0.254iteration: 2414\n",
      "train_loss: 0.14205360389005842\n",
      "val_loss: 0.2549376568184315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 24.1% ... Training loss: 0.131 ... Validation loss: 0.260iteration: 2415\n",
      "train_loss: 0.13153379776679666\n",
      "val_loss: 0.26051662095647593\n",
      "Progress: 24.2% ... Training loss: 0.134 ... Validation loss: 0.264iteration: 2416\n",
      "train_loss: 0.13428426027700174\n",
      "val_loss: 0.26442765504684385\n",
      "Progress: 24.2% ... Training loss: 0.133 ... Validation loss: 0.251iteration: 2417\n",
      "train_loss: 0.1333357125297916\n",
      "val_loss: 0.2514201864743525\n",
      "Progress: 24.2% ... Training loss: 0.131 ... Validation loss: 0.254iteration: 2418\n",
      "train_loss: 0.13158829239803227\n",
      "val_loss: 0.25433835072803634\n",
      "Progress: 24.2% ... Training loss: 0.136 ... Validation loss: 0.254iteration: 2419\n",
      "train_loss: 0.1360569185025706\n",
      "val_loss: 0.2545786616242693\n",
      "Progress: 24.2% ... Training loss: 0.131 ... Validation loss: 0.256iteration: 2420\n",
      "train_loss: 0.1319573077779118\n",
      "val_loss: 0.25674853639454537\n",
      "Progress: 24.2% ... Training loss: 0.135 ... Validation loss: 0.249iteration: 2421\n",
      "train_loss: 0.13555973629580734\n",
      "val_loss: 0.24951769468145757\n",
      "Progress: 24.2% ... Training loss: 0.135 ... Validation loss: 0.260iteration: 2422\n",
      "train_loss: 0.13519891314730006\n",
      "val_loss: 0.26056422786025296\n",
      "Progress: 24.2% ... Training loss: 0.132 ... Validation loss: 0.250iteration: 2423\n",
      "train_loss: 0.13245326600253213\n",
      "val_loss: 0.2504178404467297\n",
      "Progress: 24.2% ... Training loss: 0.138 ... Validation loss: 0.255iteration: 2424\n",
      "train_loss: 0.13815390094258717\n",
      "val_loss: 0.2553898780623808\n",
      "Progress: 24.2% ... Training loss: 0.131 ... Validation loss: 0.253iteration: 2425\n",
      "train_loss: 0.13134972659387031\n",
      "val_loss: 0.25358612532971153\n",
      "Progress: 24.3% ... Training loss: 0.132 ... Validation loss: 0.260iteration: 2426\n",
      "train_loss: 0.13264996830122278\n",
      "val_loss: 0.2604197970476621\n",
      "Progress: 24.3% ... Training loss: 0.131 ... Validation loss: 0.250iteration: 2427\n",
      "train_loss: 0.13100363833303724\n",
      "val_loss: 0.25067054257358784\n",
      "Progress: 24.3% ... Training loss: 0.132 ... Validation loss: 0.250iteration: 2428\n",
      "train_loss: 0.13238159396487809\n",
      "val_loss: 0.25013834136941837\n",
      "Progress: 24.3% ... Training loss: 0.136 ... Validation loss: 0.268iteration: 2429\n",
      "train_loss: 0.13608504330738613\n",
      "val_loss: 0.26860105413656665\n",
      "Progress: 24.3% ... Training loss: 0.147 ... Validation loss: 0.259iteration: 2430\n",
      "train_loss: 0.1474401440775506\n",
      "val_loss: 0.25917337390081935\n",
      "Progress: 24.3% ... Training loss: 0.141 ... Validation loss: 0.283iteration: 2431\n",
      "train_loss: 0.14127395649577862\n",
      "val_loss: 0.2830084312120748\n",
      "Progress: 24.3% ... Training loss: 0.136 ... Validation loss: 0.273iteration: 2432\n",
      "train_loss: 0.13686104217878614\n",
      "val_loss: 0.27321006782523877\n",
      "Progress: 24.3% ... Training loss: 0.131 ... Validation loss: 0.266iteration: 2433\n",
      "train_loss: 0.13186206299456135\n",
      "val_loss: 0.26652583689928866\n",
      "Progress: 24.3% ... Training loss: 0.131 ... Validation loss: 0.258iteration: 2434\n",
      "train_loss: 0.13101118736618197\n",
      "val_loss: 0.2585886554430998\n",
      "Progress: 24.4% ... Training loss: 0.130 ... Validation loss: 0.253iteration: 2435\n",
      "train_loss: 0.1306400080358061\n",
      "val_loss: 0.25335947362921296\n",
      "Progress: 24.4% ... Training loss: 0.131 ... Validation loss: 0.249iteration: 2436\n",
      "train_loss: 0.13131910644421443\n",
      "val_loss: 0.24967166690860126\n",
      "Progress: 24.4% ... Training loss: 0.130 ... Validation loss: 0.246iteration: 2437\n",
      "train_loss: 0.13050631635751425\n",
      "val_loss: 0.24644340500417042\n",
      "Progress: 24.4% ... Training loss: 0.130 ... Validation loss: 0.248iteration: 2438\n",
      "train_loss: 0.13025787374416387\n",
      "val_loss: 0.24857016357255565\n",
      "Progress: 24.4% ... Training loss: 0.132 ... Validation loss: 0.247iteration: 2439\n",
      "train_loss: 0.13274673299597886\n",
      "val_loss: 0.24713392398500128\n",
      "Progress: 24.4% ... Training loss: 0.129 ... Validation loss: 0.247iteration: 2440\n",
      "train_loss: 0.1298781014620739\n",
      "val_loss: 0.2479073322271854\n",
      "Progress: 24.4% ... Training loss: 0.137 ... Validation loss: 0.250iteration: 2441\n",
      "train_loss: 0.13777536932668658\n",
      "val_loss: 0.25025499028328996\n",
      "Progress: 24.4% ... Training loss: 0.130 ... Validation loss: 0.252iteration: 2442\n",
      "train_loss: 0.13062005792281944\n",
      "val_loss: 0.2522598720130566\n",
      "Progress: 24.4% ... Training loss: 0.131 ... Validation loss: 0.254iteration: 2443\n",
      "train_loss: 0.13173768794825869\n",
      "val_loss: 0.2546019464136037\n",
      "Progress: 24.4% ... Training loss: 0.129 ... Validation loss: 0.248iteration: 2444\n",
      "train_loss: 0.12939620852596292\n",
      "val_loss: 0.24854372716575768\n",
      "Progress: 24.4% ... Training loss: 0.129 ... Validation loss: 0.244iteration: 2445\n",
      "train_loss: 0.1294996422413615\n",
      "val_loss: 0.24498255825095525\n",
      "Progress: 24.5% ... Training loss: 0.129 ... Validation loss: 0.246iteration: 2446\n",
      "train_loss: 0.12995389167373722\n",
      "val_loss: 0.24674550166757148\n",
      "Progress: 24.5% ... Training loss: 0.131 ... Validation loss: 0.242iteration: 2447\n",
      "train_loss: 0.13134608503123302\n",
      "val_loss: 0.24272320702002975\n",
      "Progress: 24.5% ... Training loss: 0.130 ... Validation loss: 0.245iteration: 2448\n",
      "train_loss: 0.13097925273352573\n",
      "val_loss: 0.24595050896655693\n",
      "Progress: 24.5% ... Training loss: 0.131 ... Validation loss: 0.247iteration: 2449\n",
      "train_loss: 0.13148531133718752\n",
      "val_loss: 0.24774257148612705\n",
      "Progress: 24.5% ... Training loss: 0.134 ... Validation loss: 0.246iteration: 2450\n",
      "train_loss: 0.13442025167100102\n",
      "val_loss: 0.24656642420472577\n",
      "Progress: 24.5% ... Training loss: 0.139 ... Validation loss: 0.253iteration: 2451\n",
      "train_loss: 0.13928783184960694\n",
      "val_loss: 0.25384104047222383\n",
      "Progress: 24.5% ... Training loss: 0.139 ... Validation loss: 0.252iteration: 2452\n",
      "train_loss: 0.13912770273888106\n",
      "val_loss: 0.25269234362197796\n",
      "Progress: 24.5% ... Training loss: 0.129 ... Validation loss: 0.254iteration: 2453\n",
      "train_loss: 0.12987604455418073\n",
      "val_loss: 0.2547898368823236\n",
      "Progress: 24.5% ... Training loss: 0.129 ... Validation loss: 0.252iteration: 2454\n",
      "train_loss: 0.12921268762718077\n",
      "val_loss: 0.25249537748022616\n",
      "Progress: 24.6% ... Training loss: 0.130 ... Validation loss: 0.253iteration: 2455\n",
      "train_loss: 0.1304226180551639\n",
      "val_loss: 0.2535344145402394\n",
      "Progress: 24.6% ... Training loss: 0.132 ... Validation loss: 0.244iteration: 2456\n",
      "train_loss: 0.13204179204834376\n",
      "val_loss: 0.2446901607058685\n",
      "Progress: 24.6% ... Training loss: 0.135 ... Validation loss: 0.256iteration: 2457\n",
      "train_loss: 0.13574459983822632\n",
      "val_loss: 0.25657936792534813\n",
      "Progress: 24.6% ... Training loss: 0.146 ... Validation loss: 0.257iteration: 2458\n",
      "train_loss: 0.1466177884336626\n",
      "val_loss: 0.2577810257905441\n",
      "Progress: 24.6% ... Training loss: 0.135 ... Validation loss: 0.253iteration: 2459\n",
      "train_loss: 0.13529696054343093\n",
      "val_loss: 0.253329009366492\n",
      "Progress: 24.6% ... Training loss: 0.144 ... Validation loss: 0.256iteration: 2460\n",
      "train_loss: 0.1446554759360657\n",
      "val_loss: 0.2564255901770709\n",
      "Progress: 24.6% ... Training loss: 0.146 ... Validation loss: 0.258iteration: 2461\n",
      "train_loss: 0.14623315389705005\n",
      "val_loss: 0.2582348314621781\n",
      "Progress: 24.6% ... Training loss: 0.137 ... Validation loss: 0.256iteration: 2462\n",
      "train_loss: 0.13732178988862065\n",
      "val_loss: 0.25639822631897197\n",
      "Progress: 24.6% ... Training loss: 0.131 ... Validation loss: 0.247iteration: 2463\n",
      "train_loss: 0.13157308845709081\n",
      "val_loss: 0.24732314021563045\n",
      "Progress: 24.6% ... Training loss: 0.129 ... Validation loss: 0.250iteration: 2464\n",
      "train_loss: 0.12955610650036434\n",
      "val_loss: 0.2505268255072646\n",
      "Progress: 24.6% ... Training loss: 0.131 ... Validation loss: 0.255iteration: 2465\n",
      "train_loss: 0.13179506347963996\n",
      "val_loss: 0.2557693843481241\n",
      "Progress: 24.7% ... Training loss: 0.129 ... Validation loss: 0.250iteration: 2466\n",
      "train_loss: 0.12916802603466765\n",
      "val_loss: 0.2503223837733021\n",
      "Progress: 24.7% ... Training loss: 0.132 ... Validation loss: 0.258iteration: 2467\n",
      "train_loss: 0.13207197768364573\n",
      "val_loss: 0.2589995373336256\n",
      "Progress: 24.7% ... Training loss: 0.128 ... Validation loss: 0.252iteration: 2468\n",
      "train_loss: 0.1289680225270793\n",
      "val_loss: 0.25203859788683225\n",
      "Progress: 24.7% ... Training loss: 0.130 ... Validation loss: 0.258iteration: 2469\n",
      "train_loss: 0.1306109917599501\n",
      "val_loss: 0.25867360903109093\n",
      "Progress: 24.7% ... Training loss: 0.143 ... Validation loss: 0.257iteration: 2470\n",
      "train_loss: 0.1437327990129709\n",
      "val_loss: 0.2575100375570132\n",
      "Progress: 24.7% ... Training loss: 0.130 ... Validation loss: 0.253iteration: 2471\n",
      "train_loss: 0.13017739915489962\n",
      "val_loss: 0.2531065451589045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 24.7% ... Training loss: 0.127 ... Validation loss: 0.242iteration: 2472\n",
      "train_loss: 0.12776396223450245\n",
      "val_loss: 0.2423913577305338\n",
      "Progress: 24.7% ... Training loss: 0.126 ... Validation loss: 0.244iteration: 2473\n",
      "train_loss: 0.12699310062604238\n",
      "val_loss: 0.2448299168195457\n",
      "Progress: 24.7% ... Training loss: 0.139 ... Validation loss: 0.252iteration: 2474\n",
      "train_loss: 0.13980715850699937\n",
      "val_loss: 0.2526246411180225\n",
      "Progress: 24.8% ... Training loss: 0.140 ... Validation loss: 0.276iteration: 2475\n",
      "train_loss: 0.1400448140456513\n",
      "val_loss: 0.2766900964020699\n",
      "Progress: 24.8% ... Training loss: 0.127 ... Validation loss: 0.251iteration: 2476\n",
      "train_loss: 0.1273391601455256\n",
      "val_loss: 0.2510297396987161\n",
      "Progress: 24.8% ... Training loss: 0.128 ... Validation loss: 0.255iteration: 2477\n",
      "train_loss: 0.12829734251263286\n",
      "val_loss: 0.25533596786163093\n",
      "Progress: 24.8% ... Training loss: 0.130 ... Validation loss: 0.261iteration: 2478\n",
      "train_loss: 0.1307556006208511\n",
      "val_loss: 0.2612720126383522\n",
      "Progress: 24.8% ... Training loss: 0.135 ... Validation loss: 0.253iteration: 2479\n",
      "train_loss: 0.13546849382763393\n",
      "val_loss: 0.2537489749283151\n",
      "Progress: 24.8% ... Training loss: 0.129 ... Validation loss: 0.245iteration: 2480\n",
      "train_loss: 0.12937237894794293\n",
      "val_loss: 0.2450797137710296\n",
      "Progress: 24.8% ... Training loss: 0.127 ... Validation loss: 0.245iteration: 2481\n",
      "train_loss: 0.1274073830535178\n",
      "val_loss: 0.24571607209631832\n",
      "Progress: 24.8% ... Training loss: 0.129 ... Validation loss: 0.243iteration: 2482\n",
      "train_loss: 0.1291408082740496\n",
      "val_loss: 0.2436455604379342\n",
      "Progress: 24.8% ... Training loss: 0.127 ... Validation loss: 0.243iteration: 2483\n",
      "train_loss: 0.12710605752687903\n",
      "val_loss: 0.24311395647854306\n",
      "Progress: 24.8% ... Training loss: 0.129 ... Validation loss: 0.249iteration: 2484\n",
      "train_loss: 0.12999898845453317\n",
      "val_loss: 0.24960886049980596\n",
      "Progress: 24.9% ... Training loss: 0.135 ... Validation loss: 0.243iteration: 2485\n",
      "train_loss: 0.13509385328151285\n",
      "val_loss: 0.2436060608352181\n",
      "Progress: 24.9% ... Training loss: 0.137 ... Validation loss: 0.271iteration: 2486\n",
      "train_loss: 0.13766986058924324\n",
      "val_loss: 0.27118775664025635\n",
      "Progress: 24.9% ... Training loss: 0.133 ... Validation loss: 0.249iteration: 2487\n",
      "train_loss: 0.1337310597237635\n",
      "val_loss: 0.2491455010499316\n",
      "Progress: 24.9% ... Training loss: 0.127 ... Validation loss: 0.246iteration: 2488\n",
      "train_loss: 0.12735323023445982\n",
      "val_loss: 0.2466603055263611\n",
      "Progress: 24.9% ... Training loss: 0.129 ... Validation loss: 0.259iteration: 2489\n",
      "train_loss: 0.1291276126597913\n",
      "val_loss: 0.2590904619474529\n",
      "Progress: 24.9% ... Training loss: 0.128 ... Validation loss: 0.252iteration: 2490\n",
      "train_loss: 0.12883735362823112\n",
      "val_loss: 0.25243951507358\n",
      "Progress: 24.9% ... Training loss: 0.131 ... Validation loss: 0.246iteration: 2491\n",
      "train_loss: 0.13188212131494953\n",
      "val_loss: 0.24609450644667483\n",
      "Progress: 24.9% ... Training loss: 0.144 ... Validation loss: 0.262iteration: 2492\n",
      "train_loss: 0.14420117648409825\n",
      "val_loss: 0.26234311517993464\n",
      "Progress: 24.9% ... Training loss: 0.158 ... Validation loss: 0.265iteration: 2493\n",
      "train_loss: 0.158738281119181\n",
      "val_loss: 0.2655117950281494\n",
      "Progress: 24.9% ... Training loss: 0.130 ... Validation loss: 0.252iteration: 2494\n",
      "train_loss: 0.13044076855237027\n",
      "val_loss: 0.25225224187849726\n",
      "Progress: 24.9% ... Training loss: 0.136 ... Validation loss: 0.249iteration: 2495\n",
      "train_loss: 0.13629400907609143\n",
      "val_loss: 0.2497737695004984\n",
      "Progress: 25.0% ... Training loss: 0.141 ... Validation loss: 0.265iteration: 2496\n",
      "train_loss: 0.14176025985382265\n",
      "val_loss: 0.2657458959668381\n",
      "Progress: 25.0% ... Training loss: 0.157 ... Validation loss: 0.269iteration: 2497\n",
      "train_loss: 0.15706893945389663\n",
      "val_loss: 0.2691678634906201\n",
      "Progress: 25.0% ... Training loss: 0.128 ... Validation loss: 0.251iteration: 2498\n",
      "train_loss: 0.12853412045342907\n",
      "val_loss: 0.2511759471224926\n",
      "Progress: 25.0% ... Training loss: 0.126 ... Validation loss: 0.244iteration: 2499\n",
      "train_loss: 0.12642318685988463\n",
      "val_loss: 0.24485527363617357\n",
      "Progress: 25.0% ... Training loss: 0.126 ... Validation loss: 0.244iteration: 2500\n",
      "train_loss: 0.12632093990011872\n",
      "val_loss: 0.24404154334532258\n",
      "Progress: 25.0% ... Training loss: 0.125 ... Validation loss: 0.244iteration: 2501\n",
      "train_loss: 0.12594111594669655\n",
      "val_loss: 0.24415880491344216\n",
      "Progress: 25.0% ... Training loss: 0.127 ... Validation loss: 0.242iteration: 2502\n",
      "train_loss: 0.1276803326532845\n",
      "val_loss: 0.24289164129127222\n",
      "Progress: 25.0% ... Training loss: 0.127 ... Validation loss: 0.250iteration: 2503\n",
      "train_loss: 0.12704135483344695\n",
      "val_loss: 0.2504084214580383\n",
      "Progress: 25.0% ... Training loss: 0.127 ... Validation loss: 0.239iteration: 2504\n",
      "train_loss: 0.1270215861501768\n",
      "val_loss: 0.23911029643725537\n",
      "Progress: 25.1% ... Training loss: 0.125 ... Validation loss: 0.237iteration: 2505\n",
      "train_loss: 0.12580686173412794\n",
      "val_loss: 0.23791097238386416\n",
      "Progress: 25.1% ... Training loss: 0.128 ... Validation loss: 0.239iteration: 2506\n",
      "train_loss: 0.12831896261515444\n",
      "val_loss: 0.23937513665636623\n",
      "Progress: 25.1% ... Training loss: 0.129 ... Validation loss: 0.248iteration: 2507\n",
      "train_loss: 0.12928889445973613\n",
      "val_loss: 0.24810792472003698\n",
      "Progress: 25.1% ... Training loss: 0.126 ... Validation loss: 0.240iteration: 2508\n",
      "train_loss: 0.12630768783870994\n",
      "val_loss: 0.24057257440180838\n",
      "Progress: 25.1% ... Training loss: 0.127 ... Validation loss: 0.244iteration: 2509\n",
      "train_loss: 0.12729749489463696\n",
      "val_loss: 0.24400902893599585\n",
      "Progress: 25.1% ... Training loss: 0.125 ... Validation loss: 0.248iteration: 2510\n",
      "train_loss: 0.12583795220506774\n",
      "val_loss: 0.2480793104180609\n",
      "Progress: 25.1% ... Training loss: 0.126 ... Validation loss: 0.247iteration: 2511\n",
      "train_loss: 0.12699804983397978\n",
      "val_loss: 0.24789633558465038\n",
      "Progress: 25.1% ... Training loss: 0.126 ... Validation loss: 0.246iteration: 2512\n",
      "train_loss: 0.1268269034788432\n",
      "val_loss: 0.24676807541547016\n",
      "Progress: 25.1% ... Training loss: 0.125 ... Validation loss: 0.252iteration: 2513\n",
      "train_loss: 0.12561617318443935\n",
      "val_loss: 0.25229105736255075\n",
      "Progress: 25.1% ... Training loss: 0.124 ... Validation loss: 0.245iteration: 2514\n",
      "train_loss: 0.12472905189786346\n",
      "val_loss: 0.24550544220096163\n",
      "Progress: 25.1% ... Training loss: 0.125 ... Validation loss: 0.247iteration: 2515\n",
      "train_loss: 0.12529153390814154\n",
      "val_loss: 0.24769562388660107\n",
      "Progress: 25.2% ... Training loss: 0.125 ... Validation loss: 0.240iteration: 2516\n",
      "train_loss: 0.1258598373380517\n",
      "val_loss: 0.24057625744566644\n",
      "Progress: 25.2% ... Training loss: 0.130 ... Validation loss: 0.241iteration: 2517\n",
      "train_loss: 0.13089053601639716\n",
      "val_loss: 0.24178453486904108\n",
      "Progress: 25.2% ... Training loss: 0.126 ... Validation loss: 0.250iteration: 2518\n",
      "train_loss: 0.1266513361285333\n",
      "val_loss: 0.2508110730573716\n",
      "Progress: 25.2% ... Training loss: 0.126 ... Validation loss: 0.241iteration: 2519\n",
      "train_loss: 0.1263046986014018\n",
      "val_loss: 0.24197818208273278\n",
      "Progress: 25.2% ... Training loss: 0.125 ... Validation loss: 0.252iteration: 2520\n",
      "train_loss: 0.1253246835325071\n",
      "val_loss: 0.2520983301525913\n",
      "Progress: 25.2% ... Training loss: 0.124 ... Validation loss: 0.249iteration: 2521\n",
      "train_loss: 0.12448629886163837\n",
      "val_loss: 0.24972288007864066\n",
      "Progress: 25.2% ... Training loss: 0.126 ... Validation loss: 0.240iteration: 2522\n",
      "train_loss: 0.1262144907837065\n",
      "val_loss: 0.24095533662617566\n",
      "Progress: 25.2% ... Training loss: 0.129 ... Validation loss: 0.265iteration: 2523\n",
      "train_loss: 0.12945978651467172\n",
      "val_loss: 0.26534174149757933\n",
      "Progress: 25.2% ... Training loss: 0.124 ... Validation loss: 0.247iteration: 2524\n",
      "train_loss: 0.12448321101999162\n",
      "val_loss: 0.24731102552036408\n",
      "Progress: 25.2% ... Training loss: 0.134 ... Validation loss: 0.271iteration: 2525\n",
      "train_loss: 0.13489619750157458\n",
      "val_loss: 0.27134838138637346\n",
      "Progress: 25.3% ... Training loss: 0.126 ... Validation loss: 0.241iteration: 2526\n",
      "train_loss: 0.12689772520116277\n",
      "val_loss: 0.24181010820750587\n",
      "Progress: 25.3% ... Training loss: 0.129 ... Validation loss: 0.258iteration: 2527\n",
      "train_loss: 0.129692690681483\n",
      "val_loss: 0.258852419457479\n",
      "Progress: 25.3% ... Training loss: 0.137 ... Validation loss: 0.244iteration: 2528\n",
      "train_loss: 0.1377331632173146\n",
      "val_loss: 0.2444325182481911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 25.3% ... Training loss: 0.132 ... Validation loss: 0.274iteration: 2529\n",
      "train_loss: 0.1328420143233499\n",
      "val_loss: 0.27457548735114157\n",
      "Progress: 25.3% ... Training loss: 0.138 ... Validation loss: 0.245iteration: 2530\n",
      "train_loss: 0.1384128380369354\n",
      "val_loss: 0.24527885223412327\n",
      "Progress: 25.3% ... Training loss: 0.130 ... Validation loss: 0.269iteration: 2531\n",
      "train_loss: 0.13000632241022161\n",
      "val_loss: 0.2697372335053367\n",
      "Progress: 25.3% ... Training loss: 0.124 ... Validation loss: 0.247iteration: 2532\n",
      "train_loss: 0.12454455095279923\n",
      "val_loss: 0.24753410517587504\n",
      "Progress: 25.3% ... Training loss: 0.123 ... Validation loss: 0.239iteration: 2533\n",
      "train_loss: 0.1233093509826509\n",
      "val_loss: 0.23911022777658072\n",
      "Progress: 25.3% ... Training loss: 0.125 ... Validation loss: 0.251iteration: 2534\n",
      "train_loss: 0.12550083396053516\n",
      "val_loss: 0.25144145724987627\n",
      "Progress: 25.4% ... Training loss: 0.124 ... Validation loss: 0.247iteration: 2535\n",
      "train_loss: 0.12491762747950962\n",
      "val_loss: 0.24713555738254145\n",
      "Progress: 25.4% ... Training loss: 0.128 ... Validation loss: 0.248iteration: 2536\n",
      "train_loss: 0.1281777440061938\n",
      "val_loss: 0.24838907261053172\n",
      "Progress: 25.4% ... Training loss: 0.124 ... Validation loss: 0.234iteration: 2537\n",
      "train_loss: 0.12452934386102162\n",
      "val_loss: 0.23439044660365288\n",
      "Progress: 25.4% ... Training loss: 0.130 ... Validation loss: 0.254iteration: 2538\n",
      "train_loss: 0.13090826752209478\n",
      "val_loss: 0.25444516304627474\n",
      "Progress: 25.4% ... Training loss: 0.123 ... Validation loss: 0.241iteration: 2539\n",
      "train_loss: 0.12311537537709044\n",
      "val_loss: 0.24123616000553041\n",
      "Progress: 25.4% ... Training loss: 0.122 ... Validation loss: 0.240iteration: 2540\n",
      "train_loss: 0.12256939017813302\n",
      "val_loss: 0.24087654410919213\n",
      "Progress: 25.4% ... Training loss: 0.126 ... Validation loss: 0.235iteration: 2541\n",
      "train_loss: 0.12653075284413182\n",
      "val_loss: 0.23505867884039391\n",
      "Progress: 25.4% ... Training loss: 0.123 ... Validation loss: 0.239iteration: 2542\n",
      "train_loss: 0.12354759920943158\n",
      "val_loss: 0.2392502366093508\n",
      "Progress: 25.4% ... Training loss: 0.123 ... Validation loss: 0.236iteration: 2543\n",
      "train_loss: 0.12318852492817818\n",
      "val_loss: 0.23681723388704448\n",
      "Progress: 25.4% ... Training loss: 0.133 ... Validation loss: 0.238iteration: 2544\n",
      "train_loss: 0.13370712261804943\n",
      "val_loss: 0.23853448585020987\n",
      "Progress: 25.4% ... Training loss: 0.128 ... Validation loss: 0.257iteration: 2545\n",
      "train_loss: 0.1287165157314493\n",
      "val_loss: 0.2578047716690699\n",
      "Progress: 25.5% ... Training loss: 0.133 ... Validation loss: 0.239iteration: 2546\n",
      "train_loss: 0.1332155671992631\n",
      "val_loss: 0.2396360825454265\n",
      "Progress: 25.5% ... Training loss: 0.127 ... Validation loss: 0.260iteration: 2547\n",
      "train_loss: 0.12775138376143538\n",
      "val_loss: 0.26079716557369303\n",
      "Progress: 25.5% ... Training loss: 0.123 ... Validation loss: 0.235iteration: 2548\n",
      "train_loss: 0.12326541283522119\n",
      "val_loss: 0.23572384100876478\n",
      "Progress: 25.5% ... Training loss: 0.123 ... Validation loss: 0.235iteration: 2549\n",
      "train_loss: 0.123767117134099\n",
      "val_loss: 0.23548024632252587\n",
      "Progress: 25.5% ... Training loss: 0.127 ... Validation loss: 0.236iteration: 2550\n",
      "train_loss: 0.12776138217941713\n",
      "val_loss: 0.23664266681319812\n",
      "Progress: 25.5% ... Training loss: 0.123 ... Validation loss: 0.241iteration: 2551\n",
      "train_loss: 0.12341509610044331\n",
      "val_loss: 0.24164464190201368\n",
      "Progress: 25.5% ... Training loss: 0.122 ... Validation loss: 0.237iteration: 2552\n",
      "train_loss: 0.12220904307843121\n",
      "val_loss: 0.23736037452777053\n",
      "Progress: 25.5% ... Training loss: 0.122 ... Validation loss: 0.240iteration: 2553\n",
      "train_loss: 0.12212297372045376\n",
      "val_loss: 0.2400276980542304\n",
      "Progress: 25.5% ... Training loss: 0.123 ... Validation loss: 0.241iteration: 2554\n",
      "train_loss: 0.12312363611556265\n",
      "val_loss: 0.24126220811035579\n",
      "Progress: 25.6% ... Training loss: 0.124 ... Validation loss: 0.245iteration: 2555\n",
      "train_loss: 0.12468608647365288\n",
      "val_loss: 0.2459229853880514\n",
      "Progress: 25.6% ... Training loss: 0.126 ... Validation loss: 0.235iteration: 2556\n",
      "train_loss: 0.12644008115515962\n",
      "val_loss: 0.23546812693172697\n",
      "Progress: 25.6% ... Training loss: 0.123 ... Validation loss: 0.234iteration: 2557\n",
      "train_loss: 0.12301547411572342\n",
      "val_loss: 0.23444201881400994\n",
      "Progress: 25.6% ... Training loss: 0.122 ... Validation loss: 0.244iteration: 2558\n",
      "train_loss: 0.12216711684921126\n",
      "val_loss: 0.2443828709083877\n",
      "Progress: 25.6% ... Training loss: 0.136 ... Validation loss: 0.241iteration: 2559\n",
      "train_loss: 0.13600831834593377\n",
      "val_loss: 0.24116676239336413\n",
      "Progress: 25.6% ... Training loss: 0.151 ... Validation loss: 0.282iteration: 2560\n",
      "train_loss: 0.15134359303682784\n",
      "val_loss: 0.2821363040195185\n",
      "Progress: 25.6% ... Training loss: 0.139 ... Validation loss: 0.245iteration: 2561\n",
      "train_loss: 0.13953332245122935\n",
      "val_loss: 0.24504768490805395\n",
      "Progress: 25.6% ... Training loss: 0.123 ... Validation loss: 0.249iteration: 2562\n",
      "train_loss: 0.12387263177110121\n",
      "val_loss: 0.24998934473479426\n",
      "Progress: 25.6% ... Training loss: 0.126 ... Validation loss: 0.241iteration: 2563\n",
      "train_loss: 0.12681452301800106\n",
      "val_loss: 0.24126324075573824\n",
      "Progress: 25.6% ... Training loss: 0.134 ... Validation loss: 0.264iteration: 2564\n",
      "train_loss: 0.13470716663707621\n",
      "val_loss: 0.26476338988594816\n",
      "Progress: 25.6% ... Training loss: 0.122 ... Validation loss: 0.242iteration: 2565\n",
      "train_loss: 0.12227041594654176\n",
      "val_loss: 0.2424718771103273\n",
      "Progress: 25.7% ... Training loss: 0.123 ... Validation loss: 0.236iteration: 2566\n",
      "train_loss: 0.12326215569323012\n",
      "val_loss: 0.23639499007732093\n",
      "Progress: 25.7% ... Training loss: 0.122 ... Validation loss: 0.242iteration: 2567\n",
      "train_loss: 0.12280375484751649\n",
      "val_loss: 0.24272128630194587\n",
      "Progress: 25.7% ... Training loss: 0.132 ... Validation loss: 0.240iteration: 2568\n",
      "train_loss: 0.13296646172296384\n",
      "val_loss: 0.24062351241489952\n",
      "Progress: 25.7% ... Training loss: 0.126 ... Validation loss: 0.245iteration: 2569\n",
      "train_loss: 0.12651839720436675\n",
      "val_loss: 0.24548169978377343\n",
      "Progress: 25.7% ... Training loss: 0.131 ... Validation loss: 0.235iteration: 2570\n",
      "train_loss: 0.13144828712167986\n",
      "val_loss: 0.23571854768731823\n",
      "Progress: 25.7% ... Training loss: 0.125 ... Validation loss: 0.259iteration: 2571\n",
      "train_loss: 0.12595756450473683\n",
      "val_loss: 0.2592826914998493\n",
      "Progress: 25.7% ... Training loss: 0.123 ... Validation loss: 0.248iteration: 2572\n",
      "train_loss: 0.12337605254123969\n",
      "val_loss: 0.24866779912834783\n",
      "Progress: 25.7% ... Training loss: 0.124 ... Validation loss: 0.236iteration: 2573\n",
      "train_loss: 0.12403623057566887\n",
      "val_loss: 0.2367055407640297\n",
      "Progress: 25.7% ... Training loss: 0.138 ... Validation loss: 0.282iteration: 2574\n",
      "train_loss: 0.13812104624823418\n",
      "val_loss: 0.2822859519815311\n",
      "Progress: 25.8% ... Training loss: 0.137 ... Validation loss: 0.239iteration: 2575\n",
      "train_loss: 0.13719906225891196\n",
      "val_loss: 0.23984589667554274\n",
      "Progress: 25.8% ... Training loss: 0.131 ... Validation loss: 0.270iteration: 2576\n",
      "train_loss: 0.1311393638240846\n",
      "val_loss: 0.2702232264672154\n",
      "Progress: 25.8% ... Training loss: 0.125 ... Validation loss: 0.233iteration: 2577\n",
      "train_loss: 0.12546753458071586\n",
      "val_loss: 0.23388178935823503\n",
      "Progress: 25.8% ... Training loss: 0.121 ... Validation loss: 0.242iteration: 2578\n",
      "train_loss: 0.12132767910736252\n",
      "val_loss: 0.24253220625373306\n",
      "Progress: 25.8% ... Training loss: 0.132 ... Validation loss: 0.235iteration: 2579\n",
      "train_loss: 0.13226478037464545\n",
      "val_loss: 0.23553736512567805\n",
      "Progress: 25.8% ... Training loss: 0.121 ... Validation loss: 0.244iteration: 2580\n",
      "train_loss: 0.12195195743893812\n",
      "val_loss: 0.24416969105586497\n",
      "Progress: 25.8% ... Training loss: 0.122 ... Validation loss: 0.235iteration: 2581\n",
      "train_loss: 0.12241254086011201\n",
      "val_loss: 0.23552040589001383\n",
      "Progress: 25.8% ... Training loss: 0.120 ... Validation loss: 0.236iteration: 2582\n",
      "train_loss: 0.12069227118965734\n",
      "val_loss: 0.23650862089491262\n",
      "Progress: 25.8% ... Training loss: 0.120 ... Validation loss: 0.231iteration: 2583\n",
      "train_loss: 0.12039006120121268\n",
      "val_loss: 0.23126176207076854\n",
      "Progress: 25.8% ... Training loss: 0.126 ... Validation loss: 0.238iteration: 2584\n",
      "train_loss: 0.12612366251597992\n",
      "val_loss: 0.23886890370219088\n",
      "Progress: 25.9% ... Training loss: 0.123 ... Validation loss: 0.227iteration: 2585\n",
      "train_loss: 0.12321469811984716\n",
      "val_loss: 0.22742861655634622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 25.9% ... Training loss: 0.122 ... Validation loss: 0.237iteration: 2586\n",
      "train_loss: 0.12259584678734377\n",
      "val_loss: 0.23757350140060834\n",
      "Progress: 25.9% ... Training loss: 0.120 ... Validation loss: 0.229iteration: 2587\n",
      "train_loss: 0.12063193562866698\n",
      "val_loss: 0.2293959860836835\n",
      "Progress: 25.9% ... Training loss: 0.125 ... Validation loss: 0.227iteration: 2588\n",
      "train_loss: 0.12550291276919628\n",
      "val_loss: 0.22793710374795195\n",
      "Progress: 25.9% ... Training loss: 0.124 ... Validation loss: 0.222iteration: 2589\n",
      "train_loss: 0.12490937400772974\n",
      "val_loss: 0.22279819287917704\n",
      "Progress: 25.9% ... Training loss: 0.127 ... Validation loss: 0.228iteration: 2590\n",
      "train_loss: 0.12781669864841488\n",
      "val_loss: 0.22861522685052074\n",
      "Progress: 25.9% ... Training loss: 0.123 ... Validation loss: 0.237iteration: 2591\n",
      "train_loss: 0.12348097423694385\n",
      "val_loss: 0.237485546529684\n",
      "Progress: 25.9% ... Training loss: 0.124 ... Validation loss: 0.241iteration: 2592\n",
      "train_loss: 0.1245961630677506\n",
      "val_loss: 0.24168230074896185\n",
      "Progress: 25.9% ... Training loss: 0.121 ... Validation loss: 0.223iteration: 2593\n",
      "train_loss: 0.12102078236191673\n",
      "val_loss: 0.22359223893113378\n",
      "Progress: 25.9% ... Training loss: 0.121 ... Validation loss: 0.224iteration: 2594\n",
      "train_loss: 0.12121620097507511\n",
      "val_loss: 0.22492707291703481\n",
      "Progress: 25.9% ... Training loss: 0.120 ... Validation loss: 0.227iteration: 2595\n",
      "train_loss: 0.12023170462667145\n",
      "val_loss: 0.22718124478788246\n",
      "Progress: 26.0% ... Training loss: 0.120 ... Validation loss: 0.231iteration: 2596\n",
      "train_loss: 0.1204493094297959\n",
      "val_loss: 0.23105424309027073\n",
      "Progress: 26.0% ... Training loss: 0.124 ... Validation loss: 0.232iteration: 2597\n",
      "train_loss: 0.1240994622214927\n",
      "val_loss: 0.23237375458850065\n",
      "Progress: 26.0% ... Training loss: 0.121 ... Validation loss: 0.229iteration: 2598\n",
      "train_loss: 0.12104832766560955\n",
      "val_loss: 0.22901802143547484\n",
      "Progress: 26.0% ... Training loss: 0.119 ... Validation loss: 0.234iteration: 2599\n",
      "train_loss: 0.11963979932471497\n",
      "val_loss: 0.23492732502377522\n",
      "Progress: 26.0% ... Training loss: 0.120 ... Validation loss: 0.240iteration: 2600\n",
      "train_loss: 0.12020523709701253\n",
      "val_loss: 0.24034865677421074\n",
      "Progress: 26.0% ... Training loss: 0.134 ... Validation loss: 0.253iteration: 2601\n",
      "train_loss: 0.1340662996297802\n",
      "val_loss: 0.25331755547585033\n",
      "Progress: 26.0% ... Training loss: 0.132 ... Validation loss: 0.233iteration: 2602\n",
      "train_loss: 0.13297257020967\n",
      "val_loss: 0.2334402702394738\n",
      "Progress: 26.0% ... Training loss: 0.119 ... Validation loss: 0.233iteration: 2603\n",
      "train_loss: 0.11938732776238938\n",
      "val_loss: 0.23323963599521857\n",
      "Progress: 26.0% ... Training loss: 0.119 ... Validation loss: 0.225iteration: 2604\n",
      "train_loss: 0.1192958866482085\n",
      "val_loss: 0.22572536903009102\n",
      "Progress: 26.1% ... Training loss: 0.121 ... Validation loss: 0.231iteration: 2605\n",
      "train_loss: 0.12143970886555287\n",
      "val_loss: 0.2312546820742908\n",
      "Progress: 26.1% ... Training loss: 0.120 ... Validation loss: 0.242iteration: 2606\n",
      "train_loss: 0.1203475928231331\n",
      "val_loss: 0.24243049845984999\n",
      "Progress: 26.1% ... Training loss: 0.118 ... Validation loss: 0.232iteration: 2607\n",
      "train_loss: 0.11856992597966068\n",
      "val_loss: 0.23283223745953363\n",
      "Progress: 26.1% ... Training loss: 0.118 ... Validation loss: 0.227iteration: 2608\n",
      "train_loss: 0.11853367044982796\n",
      "val_loss: 0.22760808424597176\n",
      "Progress: 26.1% ... Training loss: 0.121 ... Validation loss: 0.231iteration: 2609\n",
      "train_loss: 0.12157920469921703\n",
      "val_loss: 0.23167769805730568\n",
      "Progress: 26.1% ... Training loss: 0.128 ... Validation loss: 0.233iteration: 2610\n",
      "train_loss: 0.12895836378329362\n",
      "val_loss: 0.2338439540450503\n",
      "Progress: 26.1% ... Training loss: 0.122 ... Validation loss: 0.243iteration: 2611\n",
      "train_loss: 0.12202762878995713\n",
      "val_loss: 0.24388471163996883\n",
      "Progress: 26.1% ... Training loss: 0.122 ... Validation loss: 0.229iteration: 2612\n",
      "train_loss: 0.12242204399460635\n",
      "val_loss: 0.22917838688867107\n",
      "Progress: 26.1% ... Training loss: 0.119 ... Validation loss: 0.232iteration: 2613\n",
      "train_loss: 0.11926228576135807\n",
      "val_loss: 0.2323150384176487\n",
      "Progress: 26.1% ... Training loss: 0.119 ... Validation loss: 0.231iteration: 2614\n",
      "train_loss: 0.11985341141626585\n",
      "val_loss: 0.2312173385237845\n",
      "Progress: 26.1% ... Training loss: 0.118 ... Validation loss: 0.230iteration: 2615\n",
      "train_loss: 0.11865205658844817\n",
      "val_loss: 0.2302883444516603\n",
      "Progress: 26.2% ... Training loss: 0.124 ... Validation loss: 0.227iteration: 2616\n",
      "train_loss: 0.12471530680821587\n",
      "val_loss: 0.22758292471661726\n",
      "Progress: 26.2% ... Training loss: 0.129 ... Validation loss: 0.255iteration: 2617\n",
      "train_loss: 0.12958680923351218\n",
      "val_loss: 0.25528111800229514\n",
      "Progress: 26.2% ... Training loss: 0.124 ... Validation loss: 0.225iteration: 2618\n",
      "train_loss: 0.12483218890942899\n",
      "val_loss: 0.22549962104901908\n",
      "Progress: 26.2% ... Training loss: 0.122 ... Validation loss: 0.240iteration: 2619\n",
      "train_loss: 0.12291508970440516\n",
      "val_loss: 0.24060199260506288\n",
      "Progress: 26.2% ... Training loss: 0.123 ... Validation loss: 0.233iteration: 2620\n",
      "train_loss: 0.12397160189433247\n",
      "val_loss: 0.23380789429123902\n",
      "Progress: 26.2% ... Training loss: 0.121 ... Validation loss: 0.249iteration: 2621\n",
      "train_loss: 0.1211493352506092\n",
      "val_loss: 0.249511545522638\n",
      "Progress: 26.2% ... Training loss: 0.137 ... Validation loss: 0.239iteration: 2622\n",
      "train_loss: 0.1377967198156421\n",
      "val_loss: 0.2392537732210748\n",
      "Progress: 26.2% ... Training loss: 0.148 ... Validation loss: 0.274iteration: 2623\n",
      "train_loss: 0.14851940180442097\n",
      "val_loss: 0.27489357182184754\n",
      "Progress: 26.2% ... Training loss: 0.134 ... Validation loss: 0.234iteration: 2624\n",
      "train_loss: 0.1343212921843278\n",
      "val_loss: 0.23416620643163813\n",
      "Progress: 26.2% ... Training loss: 0.136 ... Validation loss: 0.263iteration: 2625\n",
      "train_loss: 0.1367727447980613\n",
      "val_loss: 0.2632124593313559\n",
      "Progress: 26.3% ... Training loss: 0.118 ... Validation loss: 0.227iteration: 2626\n",
      "train_loss: 0.11840316398118722\n",
      "val_loss: 0.227804960555593\n",
      "Progress: 26.3% ... Training loss: 0.120 ... Validation loss: 0.243iteration: 2627\n",
      "train_loss: 0.12059549531870317\n",
      "val_loss: 0.24310736690571796\n",
      "Progress: 26.3% ... Training loss: 0.119 ... Validation loss: 0.236iteration: 2628\n",
      "train_loss: 0.1191172079348999\n",
      "val_loss: 0.23650072609299183\n",
      "Progress: 26.3% ... Training loss: 0.121 ... Validation loss: 0.226iteration: 2629\n",
      "train_loss: 0.12168709539143138\n",
      "val_loss: 0.22617762486422774\n",
      "Progress: 26.3% ... Training loss: 0.122 ... Validation loss: 0.245iteration: 2630\n",
      "train_loss: 0.12228042209663396\n",
      "val_loss: 0.24506562994159056\n",
      "Progress: 26.3% ... Training loss: 0.141 ... Validation loss: 0.236iteration: 2631\n",
      "train_loss: 0.14130267461284204\n",
      "val_loss: 0.23620401775275562\n",
      "Progress: 26.3% ... Training loss: 0.126 ... Validation loss: 0.255iteration: 2632\n",
      "train_loss: 0.1264664549897202\n",
      "val_loss: 0.2557524908089297\n",
      "Progress: 26.3% ... Training loss: 0.130 ... Validation loss: 0.227iteration: 2633\n",
      "train_loss: 0.13003300834041837\n",
      "val_loss: 0.2274205193708084\n",
      "Progress: 26.3% ... Training loss: 0.121 ... Validation loss: 0.242iteration: 2634\n",
      "train_loss: 0.12161390277775505\n",
      "val_loss: 0.24205422034843693\n",
      "Progress: 26.4% ... Training loss: 0.118 ... Validation loss: 0.220iteration: 2635\n",
      "train_loss: 0.11871609033890622\n",
      "val_loss: 0.22070778028789237\n",
      "Progress: 26.4% ... Training loss: 0.131 ... Validation loss: 0.258iteration: 2636\n",
      "train_loss: 0.13174948656438454\n",
      "val_loss: 0.2581278843676435\n",
      "Progress: 26.4% ... Training loss: 0.131 ... Validation loss: 0.227iteration: 2637\n",
      "train_loss: 0.1312825868296421\n",
      "val_loss: 0.22741920098792764\n",
      "Progress: 26.4% ... Training loss: 0.119 ... Validation loss: 0.245iteration: 2638\n",
      "train_loss: 0.11935239077844405\n",
      "val_loss: 0.2455549577756601\n",
      "Progress: 26.4% ... Training loss: 0.117 ... Validation loss: 0.230iteration: 2639\n",
      "train_loss: 0.11752013034687568\n",
      "val_loss: 0.23005204853007458\n",
      "Progress: 26.4% ... Training loss: 0.118 ... Validation loss: 0.228iteration: 2640\n",
      "train_loss: 0.11822601060674773\n",
      "val_loss: 0.22827164306077982\n",
      "Progress: 26.4% ... Training loss: 0.117 ... Validation loss: 0.231iteration: 2641\n",
      "train_loss: 0.11728754208584274\n",
      "val_loss: 0.23114333935240675\n",
      "Progress: 26.4% ... Training loss: 0.125 ... Validation loss: 0.250iteration: 2642\n",
      "train_loss: 0.12547944565147942\n",
      "val_loss: 0.2509279996279921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 26.4% ... Training loss: 0.119 ... Validation loss: 0.229iteration: 2643\n",
      "train_loss: 0.11989482382603082\n",
      "val_loss: 0.2290031224898163\n",
      "Progress: 26.4% ... Training loss: 0.117 ... Validation loss: 0.231iteration: 2644\n",
      "train_loss: 0.11777217615363078\n",
      "val_loss: 0.23113423355305163\n",
      "Progress: 26.4% ... Training loss: 0.127 ... Validation loss: 0.224iteration: 2645\n",
      "train_loss: 0.12712795433767413\n",
      "val_loss: 0.2243182699053887\n",
      "Progress: 26.5% ... Training loss: 0.127 ... Validation loss: 0.240iteration: 2646\n",
      "train_loss: 0.12761804307339644\n",
      "val_loss: 0.24007091539126302\n",
      "Progress: 26.5% ... Training loss: 0.122 ... Validation loss: 0.223iteration: 2647\n",
      "train_loss: 0.12210944260164976\n",
      "val_loss: 0.2233475922497495\n",
      "Progress: 26.5% ... Training loss: 0.120 ... Validation loss: 0.230iteration: 2648\n",
      "train_loss: 0.12063035311797458\n",
      "val_loss: 0.23017912676121802\n",
      "Progress: 26.5% ... Training loss: 0.118 ... Validation loss: 0.223iteration: 2649\n",
      "train_loss: 0.11853557217826138\n",
      "val_loss: 0.22367733337458284\n",
      "Progress: 26.5% ... Training loss: 0.116 ... Validation loss: 0.231iteration: 2650\n",
      "train_loss: 0.11682792239180644\n",
      "val_loss: 0.2310875718584972\n",
      "Progress: 26.5% ... Training loss: 0.119 ... Validation loss: 0.242iteration: 2651\n",
      "train_loss: 0.11910269707602389\n",
      "val_loss: 0.24238477147980633\n",
      "Progress: 26.5% ... Training loss: 0.116 ... Validation loss: 0.229iteration: 2652\n",
      "train_loss: 0.11681036745938976\n",
      "val_loss: 0.2295513130245772\n",
      "Progress: 26.5% ... Training loss: 0.124 ... Validation loss: 0.228iteration: 2653\n",
      "train_loss: 0.12492751514276297\n",
      "val_loss: 0.22841767884918432\n",
      "Progress: 26.5% ... Training loss: 0.119 ... Validation loss: 0.233iteration: 2654\n",
      "train_loss: 0.1193571421444766\n",
      "val_loss: 0.23300561472145165\n",
      "Progress: 26.6% ... Training loss: 0.117 ... Validation loss: 0.240iteration: 2655\n",
      "train_loss: 0.11750822358501478\n",
      "val_loss: 0.24038647365065383\n",
      "Progress: 26.6% ... Training loss: 0.116 ... Validation loss: 0.234iteration: 2656\n",
      "train_loss: 0.11673710004281306\n",
      "val_loss: 0.23476122603999255\n",
      "Progress: 26.6% ... Training loss: 0.116 ... Validation loss: 0.229iteration: 2657\n",
      "train_loss: 0.11615615079826237\n",
      "val_loss: 0.22985302379200176\n",
      "Progress: 26.6% ... Training loss: 0.116 ... Validation loss: 0.236iteration: 2658\n",
      "train_loss: 0.11661595535824232\n",
      "val_loss: 0.2367825447113618\n",
      "Progress: 26.6% ... Training loss: 0.117 ... Validation loss: 0.226iteration: 2659\n",
      "train_loss: 0.11703687098824049\n",
      "val_loss: 0.22663997879203515\n",
      "Progress: 26.6% ... Training loss: 0.129 ... Validation loss: 0.225iteration: 2660\n",
      "train_loss: 0.12916993679185704\n",
      "val_loss: 0.22569774310692334\n",
      "Progress: 26.6% ... Training loss: 0.117 ... Validation loss: 0.242iteration: 2661\n",
      "train_loss: 0.11787986208749682\n",
      "val_loss: 0.242928055484684\n",
      "Progress: 26.6% ... Training loss: 0.117 ... Validation loss: 0.225iteration: 2662\n",
      "train_loss: 0.11781596990817683\n",
      "val_loss: 0.22594089434411088\n",
      "Progress: 26.6% ... Training loss: 0.116 ... Validation loss: 0.227iteration: 2663\n",
      "train_loss: 0.11642304794515543\n",
      "val_loss: 0.22790509939703466\n",
      "Progress: 26.6% ... Training loss: 0.116 ... Validation loss: 0.232iteration: 2664\n",
      "train_loss: 0.116528815210225\n",
      "val_loss: 0.23270498492056002\n",
      "Progress: 26.6% ... Training loss: 0.119 ... Validation loss: 0.242iteration: 2665\n",
      "train_loss: 0.11900573913770313\n",
      "val_loss: 0.2424941702021785\n",
      "Progress: 26.7% ... Training loss: 0.119 ... Validation loss: 0.225iteration: 2666\n",
      "train_loss: 0.11992751079929113\n",
      "val_loss: 0.2251933908830388\n",
      "Progress: 26.7% ... Training loss: 0.117 ... Validation loss: 0.229iteration: 2667\n",
      "train_loss: 0.11715537273005695\n",
      "val_loss: 0.2297610069126412\n",
      "Progress: 26.7% ... Training loss: 0.117 ... Validation loss: 0.234iteration: 2668\n",
      "train_loss: 0.11715687847585299\n",
      "val_loss: 0.23433569974519897\n",
      "Progress: 26.7% ... Training loss: 0.115 ... Validation loss: 0.231iteration: 2669\n",
      "train_loss: 0.11566173566530825\n",
      "val_loss: 0.23127084506494644\n",
      "Progress: 26.7% ... Training loss: 0.117 ... Validation loss: 0.221iteration: 2670\n",
      "train_loss: 0.11779800001733312\n",
      "val_loss: 0.22166197798199974\n",
      "Progress: 26.7% ... Training loss: 0.118 ... Validation loss: 0.228iteration: 2671\n",
      "train_loss: 0.1182258912168331\n",
      "val_loss: 0.2282213618355948\n",
      "Progress: 26.7% ... Training loss: 0.122 ... Validation loss: 0.226iteration: 2672\n",
      "train_loss: 0.12217167482872182\n",
      "val_loss: 0.22602210256957753\n",
      "Progress: 26.7% ... Training loss: 0.116 ... Validation loss: 0.234iteration: 2673\n",
      "train_loss: 0.11656719826968424\n",
      "val_loss: 0.23418506098847341\n",
      "Progress: 26.7% ... Training loss: 0.115 ... Validation loss: 0.232iteration: 2674\n",
      "train_loss: 0.11527522537214512\n",
      "val_loss: 0.23255457962108553\n",
      "Progress: 26.8% ... Training loss: 0.114 ... Validation loss: 0.225iteration: 2675\n",
      "train_loss: 0.11490200454769434\n",
      "val_loss: 0.22537409404602565\n",
      "Progress: 26.8% ... Training loss: 0.115 ... Validation loss: 0.230iteration: 2676\n",
      "train_loss: 0.11547244492990691\n",
      "val_loss: 0.2302481341901947\n",
      "Progress: 26.8% ... Training loss: 0.125 ... Validation loss: 0.263iteration: 2677\n",
      "train_loss: 0.12540323420671684\n",
      "val_loss: 0.263194618647962\n",
      "Progress: 26.8% ... Training loss: 0.118 ... Validation loss: 0.226iteration: 2678\n",
      "train_loss: 0.11802962172420461\n",
      "val_loss: 0.22660092869500295\n",
      "Progress: 26.8% ... Training loss: 0.119 ... Validation loss: 0.225iteration: 2679\n",
      "train_loss: 0.11974943159585837\n",
      "val_loss: 0.22593272644733567\n",
      "Progress: 26.8% ... Training loss: 0.119 ... Validation loss: 0.250iteration: 2680\n",
      "train_loss: 0.11972512484142024\n",
      "val_loss: 0.2503913965086968\n",
      "Progress: 26.8% ... Training loss: 0.114 ... Validation loss: 0.226iteration: 2681\n",
      "train_loss: 0.11466528129542437\n",
      "val_loss: 0.2266030476980026\n",
      "Progress: 26.8% ... Training loss: 0.116 ... Validation loss: 0.218iteration: 2682\n",
      "train_loss: 0.11621893577661885\n",
      "val_loss: 0.21810788444630183\n",
      "Progress: 26.8% ... Training loss: 0.120 ... Validation loss: 0.244iteration: 2683\n",
      "train_loss: 0.12016857490619567\n",
      "val_loss: 0.24446052544662572\n",
      "Progress: 26.8% ... Training loss: 0.119 ... Validation loss: 0.220iteration: 2684\n",
      "train_loss: 0.11961265063765561\n",
      "val_loss: 0.22031461556314227\n",
      "Progress: 26.9% ... Training loss: 0.115 ... Validation loss: 0.229iteration: 2685\n",
      "train_loss: 0.11534095587306485\n",
      "val_loss: 0.22922517876817747\n",
      "Progress: 26.9% ... Training loss: 0.116 ... Validation loss: 0.242iteration: 2686\n",
      "train_loss: 0.11657847253375159\n",
      "val_loss: 0.2422563349292644\n",
      "Progress: 26.9% ... Training loss: 0.116 ... Validation loss: 0.219iteration: 2687\n",
      "train_loss: 0.11671167767869087\n",
      "val_loss: 0.21983686042922942\n",
      "Progress: 26.9% ... Training loss: 0.117 ... Validation loss: 0.231iteration: 2688\n",
      "train_loss: 0.11707715818188782\n",
      "val_loss: 0.2317521221972226\n",
      "Progress: 26.9% ... Training loss: 0.121 ... Validation loss: 0.222iteration: 2689\n",
      "train_loss: 0.12186627911353201\n",
      "val_loss: 0.22241797995411852\n",
      "Progress: 26.9% ... Training loss: 0.139 ... Validation loss: 0.284iteration: 2690\n",
      "train_loss: 0.13907888970256382\n",
      "val_loss: 0.28425427690403715\n",
      "Progress: 26.9% ... Training loss: 0.128 ... Validation loss: 0.225iteration: 2691\n",
      "train_loss: 0.12851484872005645\n",
      "val_loss: 0.22520323643725754\n",
      "Progress: 26.9% ... Training loss: 0.125 ... Validation loss: 0.259iteration: 2692\n",
      "train_loss: 0.12554049833257397\n",
      "val_loss: 0.25942453728509257\n",
      "Progress: 26.9% ... Training loss: 0.117 ... Validation loss: 0.224iteration: 2693\n",
      "train_loss: 0.11780243224560555\n",
      "val_loss: 0.22481231957768935\n",
      "Progress: 26.9% ... Training loss: 0.122 ... Validation loss: 0.259iteration: 2694\n",
      "train_loss: 0.12270373364803222\n",
      "val_loss: 0.259414543796649\n",
      "Progress: 26.9% ... Training loss: 0.117 ... Validation loss: 0.224iteration: 2695\n",
      "train_loss: 0.11790620240119376\n",
      "val_loss: 0.22451194424282386\n",
      "Progress: 27.0% ... Training loss: 0.116 ... Validation loss: 0.239iteration: 2696\n",
      "train_loss: 0.11611206273779696\n",
      "val_loss: 0.23961096138035146\n",
      "Progress: 27.0% ... Training loss: 0.113 ... Validation loss: 0.225iteration: 2697\n",
      "train_loss: 0.11351547492799999\n",
      "val_loss: 0.22555520766713572\n",
      "Progress: 27.0% ... Training loss: 0.117 ... Validation loss: 0.226iteration: 2698\n",
      "train_loss: 0.11756805678699501\n",
      "val_loss: 0.22669193792863213\n",
      "Progress: 27.0% ... Training loss: 0.114 ... Validation loss: 0.236iteration: 2699\n",
      "train_loss: 0.11497919324240734\n",
      "val_loss: 0.23601638447148107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 27.0% ... Training loss: 0.115 ... Validation loss: 0.223iteration: 2700\n",
      "train_loss: 0.11591256445783872\n",
      "val_loss: 0.223558333651896\n",
      "Progress: 27.0% ... Training loss: 0.125 ... Validation loss: 0.255iteration: 2701\n",
      "train_loss: 0.12551843916740854\n",
      "val_loss: 0.2559717440410518\n",
      "Progress: 27.0% ... Training loss: 0.121 ... Validation loss: 0.219iteration: 2702\n",
      "train_loss: 0.1215907071028038\n",
      "val_loss: 0.21979278874120145\n",
      "Progress: 27.0% ... Training loss: 0.119 ... Validation loss: 0.244iteration: 2703\n",
      "train_loss: 0.11910220975198113\n",
      "val_loss: 0.2447163250351595\n",
      "Progress: 27.0% ... Training loss: 0.118 ... Validation loss: 0.221iteration: 2704\n",
      "train_loss: 0.11878225744438495\n",
      "val_loss: 0.22153514631061724\n",
      "Progress: 27.1% ... Training loss: 0.114 ... Validation loss: 0.222iteration: 2705\n",
      "train_loss: 0.11401576043930788\n",
      "val_loss: 0.22296788546889665\n",
      "Progress: 27.1% ... Training loss: 0.114 ... Validation loss: 0.236iteration: 2706\n",
      "train_loss: 0.11447856589751627\n",
      "val_loss: 0.23612558693916033\n",
      "Progress: 27.1% ... Training loss: 0.124 ... Validation loss: 0.222iteration: 2707\n",
      "train_loss: 0.12435359266994245\n",
      "val_loss: 0.22299461184982414\n",
      "Progress: 27.1% ... Training loss: 0.114 ... Validation loss: 0.230iteration: 2708\n",
      "train_loss: 0.11468047624102844\n",
      "val_loss: 0.23001482308512303\n",
      "Progress: 27.1% ... Training loss: 0.114 ... Validation loss: 0.225iteration: 2709\n",
      "train_loss: 0.11423722919434919\n",
      "val_loss: 0.22599202576541183\n",
      "Progress: 27.1% ... Training loss: 0.114 ... Validation loss: 0.224iteration: 2710\n",
      "train_loss: 0.11422192601947394\n",
      "val_loss: 0.2241619199402584\n",
      "Progress: 27.1% ... Training loss: 0.113 ... Validation loss: 0.231iteration: 2711\n",
      "train_loss: 0.11374369915663803\n",
      "val_loss: 0.23171126564852706\n",
      "Progress: 27.1% ... Training loss: 0.118 ... Validation loss: 0.223iteration: 2712\n",
      "train_loss: 0.11881919335219117\n",
      "val_loss: 0.22365033390781552\n",
      "Progress: 27.1% ... Training loss: 0.114 ... Validation loss: 0.233iteration: 2713\n",
      "train_loss: 0.11422841950574517\n",
      "val_loss: 0.23386018101280825\n",
      "Progress: 27.1% ... Training loss: 0.113 ... Validation loss: 0.221iteration: 2714\n",
      "train_loss: 0.11363863528963149\n",
      "val_loss: 0.22195425274719946\n",
      "Progress: 27.1% ... Training loss: 0.118 ... Validation loss: 0.218iteration: 2715\n",
      "train_loss: 0.11829484137296412\n",
      "val_loss: 0.21878900665238007\n",
      "Progress: 27.2% ... Training loss: 0.115 ... Validation loss: 0.224iteration: 2716\n",
      "train_loss: 0.11503032352891822\n",
      "val_loss: 0.2244449064861888\n",
      "Progress: 27.2% ... Training loss: 0.115 ... Validation loss: 0.215iteration: 2717\n",
      "train_loss: 0.11558815986939663\n",
      "val_loss: 0.21591767875486445\n",
      "Progress: 27.2% ... Training loss: 0.112 ... Validation loss: 0.227iteration: 2718\n",
      "train_loss: 0.11286653918826385\n",
      "val_loss: 0.22723339937458684\n",
      "Progress: 27.2% ... Training loss: 0.113 ... Validation loss: 0.232iteration: 2719\n",
      "train_loss: 0.1134600601050067\n",
      "val_loss: 0.2323913389587349\n",
      "Progress: 27.2% ... Training loss: 0.112 ... Validation loss: 0.219iteration: 2720\n",
      "train_loss: 0.11281802047087758\n",
      "val_loss: 0.21939999546060515\n",
      "Progress: 27.2% ... Training loss: 0.117 ... Validation loss: 0.220iteration: 2721\n",
      "train_loss: 0.11716524086288821\n",
      "val_loss: 0.2200448474379622\n",
      "Progress: 27.2% ... Training loss: 0.117 ... Validation loss: 0.239iteration: 2722\n",
      "train_loss: 0.11739309988434221\n",
      "val_loss: 0.23929071964983634\n",
      "Progress: 27.2% ... Training loss: 0.114 ... Validation loss: 0.228iteration: 2723\n",
      "train_loss: 0.11426363404749194\n",
      "val_loss: 0.22849355956907375\n",
      "Progress: 27.2% ... Training loss: 0.114 ... Validation loss: 0.217iteration: 2724\n",
      "train_loss: 0.11487002166778992\n",
      "val_loss: 0.21795496746086002\n",
      "Progress: 27.2% ... Training loss: 0.118 ... Validation loss: 0.220iteration: 2725\n",
      "train_loss: 0.1180034444938289\n",
      "val_loss: 0.22021879482324785\n",
      "Progress: 27.3% ... Training loss: 0.115 ... Validation loss: 0.238iteration: 2726\n",
      "train_loss: 0.11507872228405261\n",
      "val_loss: 0.23824743475768145\n",
      "Progress: 27.3% ... Training loss: 0.112 ... Validation loss: 0.218iteration: 2727\n",
      "train_loss: 0.11229556693271386\n",
      "val_loss: 0.21878141542816315\n",
      "Progress: 27.3% ... Training loss: 0.112 ... Validation loss: 0.219iteration: 2728\n",
      "train_loss: 0.11200651178831957\n",
      "val_loss: 0.2196953467524056\n",
      "Progress: 27.3% ... Training loss: 0.112 ... Validation loss: 0.227iteration: 2729\n",
      "train_loss: 0.11278805147020891\n",
      "val_loss: 0.22780287550934952\n",
      "Progress: 27.3% ... Training loss: 0.112 ... Validation loss: 0.218iteration: 2730\n",
      "train_loss: 0.11211887012807263\n",
      "val_loss: 0.21839089450822421\n",
      "Progress: 27.3% ... Training loss: 0.112 ... Validation loss: 0.220iteration: 2731\n",
      "train_loss: 0.11288742409286998\n",
      "val_loss: 0.2209595796056474\n",
      "Progress: 27.3% ... Training loss: 0.113 ... Validation loss: 0.214iteration: 2732\n",
      "train_loss: 0.11302257543014485\n",
      "val_loss: 0.21488510285196258\n",
      "Progress: 27.3% ... Training loss: 0.114 ... Validation loss: 0.223iteration: 2733\n",
      "train_loss: 0.11449399543219219\n",
      "val_loss: 0.22310077002909207\n",
      "Progress: 27.3% ... Training loss: 0.117 ... Validation loss: 0.214iteration: 2734\n",
      "train_loss: 0.11729175075848491\n",
      "val_loss: 0.21461847773223566\n",
      "Progress: 27.4% ... Training loss: 0.113 ... Validation loss: 0.229iteration: 2735\n",
      "train_loss: 0.11306907949443629\n",
      "val_loss: 0.22909636000422928\n",
      "Progress: 27.4% ... Training loss: 0.127 ... Validation loss: 0.220iteration: 2736\n",
      "train_loss: 0.12770436012183456\n",
      "val_loss: 0.2206688310168128\n",
      "Progress: 27.4% ... Training loss: 0.117 ... Validation loss: 0.238iteration: 2737\n",
      "train_loss: 0.11765466510085809\n",
      "val_loss: 0.23865233840200695\n",
      "Progress: 27.4% ... Training loss: 0.112 ... Validation loss: 0.225iteration: 2738\n",
      "train_loss: 0.11279489640004339\n",
      "val_loss: 0.22554568756345725\n",
      "Progress: 27.4% ... Training loss: 0.112 ... Validation loss: 0.224iteration: 2739\n",
      "train_loss: 0.11215111873091309\n",
      "val_loss: 0.2245335033526668\n",
      "Progress: 27.4% ... Training loss: 0.111 ... Validation loss: 0.219iteration: 2740\n",
      "train_loss: 0.11193554717104147\n",
      "val_loss: 0.2199003042954467\n",
      "Progress: 27.4% ... Training loss: 0.113 ... Validation loss: 0.237iteration: 2741\n",
      "train_loss: 0.11390703979558252\n",
      "val_loss: 0.23759883735017048\n",
      "Progress: 27.4% ... Training loss: 0.114 ... Validation loss: 0.215iteration: 2742\n",
      "train_loss: 0.11451311541992465\n",
      "val_loss: 0.21591270860960862\n",
      "Progress: 27.4% ... Training loss: 0.114 ... Validation loss: 0.233iteration: 2743\n",
      "train_loss: 0.11476809046725799\n",
      "val_loss: 0.23363170870869104\n",
      "Progress: 27.4% ... Training loss: 0.113 ... Validation loss: 0.225iteration: 2744\n",
      "train_loss: 0.11322867715085162\n",
      "val_loss: 0.22540996615074566\n",
      "Progress: 27.4% ... Training loss: 0.115 ... Validation loss: 0.238iteration: 2745\n",
      "train_loss: 0.11564689148569413\n",
      "val_loss: 0.23887385497064975\n",
      "Progress: 27.5% ... Training loss: 0.111 ... Validation loss: 0.218iteration: 2746\n",
      "train_loss: 0.11194083566731922\n",
      "val_loss: 0.21811285284291024\n",
      "Progress: 27.5% ... Training loss: 0.113 ... Validation loss: 0.218iteration: 2747\n",
      "train_loss: 0.11360240414482528\n",
      "val_loss: 0.2187217351045475\n",
      "Progress: 27.5% ... Training loss: 0.119 ... Validation loss: 0.229iteration: 2748\n",
      "train_loss: 0.11914489107833098\n",
      "val_loss: 0.2291147971053046\n",
      "Progress: 27.5% ... Training loss: 0.112 ... Validation loss: 0.217iteration: 2749\n",
      "train_loss: 0.11274878518422676\n",
      "val_loss: 0.2171467116232058\n",
      "Progress: 27.5% ... Training loss: 0.113 ... Validation loss: 0.223iteration: 2750\n",
      "train_loss: 0.11346382472583867\n",
      "val_loss: 0.2238526230946613\n",
      "Progress: 27.5% ... Training loss: 0.111 ... Validation loss: 0.215iteration: 2751\n",
      "train_loss: 0.11152038248302862\n",
      "val_loss: 0.21561331879842227\n",
      "Progress: 27.5% ... Training loss: 0.112 ... Validation loss: 0.223iteration: 2752\n",
      "train_loss: 0.11212780513842867\n",
      "val_loss: 0.22339751883496936\n",
      "Progress: 27.5% ... Training loss: 0.116 ... Validation loss: 0.215iteration: 2753\n",
      "train_loss: 0.11632800565025266\n",
      "val_loss: 0.21595256119251915\n",
      "Progress: 27.5% ... Training loss: 0.111 ... Validation loss: 0.221iteration: 2754\n",
      "train_loss: 0.11108011695212897\n",
      "val_loss: 0.2217188937907514\n",
      "Progress: 27.6% ... Training loss: 0.114 ... Validation loss: 0.214iteration: 2755\n",
      "train_loss: 0.11422361744578897\n",
      "val_loss: 0.2149697347595063\n",
      "Progress: 27.6% ... Training loss: 0.118 ... Validation loss: 0.237iteration: 2756\n",
      "train_loss: 0.11874135457317905\n",
      "val_loss: 0.23715474102950873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 27.6% ... Training loss: 0.116 ... Validation loss: 0.217iteration: 2757\n",
      "train_loss: 0.11634523290558342\n",
      "val_loss: 0.2171601353663164\n",
      "Progress: 27.6% ... Training loss: 0.111 ... Validation loss: 0.224iteration: 2758\n",
      "train_loss: 0.1115724973638545\n",
      "val_loss: 0.2247866420129755\n",
      "Progress: 27.6% ... Training loss: 0.123 ... Validation loss: 0.222iteration: 2759\n",
      "train_loss: 0.12384859599453177\n",
      "val_loss: 0.22274960502537122\n",
      "Progress: 27.6% ... Training loss: 0.113 ... Validation loss: 0.222iteration: 2760\n",
      "train_loss: 0.11385314849614994\n",
      "val_loss: 0.22218501134800098\n",
      "Progress: 27.6% ... Training loss: 0.110 ... Validation loss: 0.216iteration: 2761\n",
      "train_loss: 0.11023634731312937\n",
      "val_loss: 0.21629670362590037\n",
      "Progress: 27.6% ... Training loss: 0.122 ... Validation loss: 0.225iteration: 2762\n",
      "train_loss: 0.12233333222486871\n",
      "val_loss: 0.22535611254074614\n",
      "Progress: 27.6% ... Training loss: 0.111 ... Validation loss: 0.221iteration: 2763\n",
      "train_loss: 0.11115721092782102\n",
      "val_loss: 0.22165545803702777\n",
      "Progress: 27.6% ... Training loss: 0.112 ... Validation loss: 0.222iteration: 2764\n",
      "train_loss: 0.11267448901898841\n",
      "val_loss: 0.22294338384707568\n",
      "Progress: 27.6% ... Training loss: 0.113 ... Validation loss: 0.230iteration: 2765\n",
      "train_loss: 0.11319061333153842\n",
      "val_loss: 0.2305094193489736\n",
      "Progress: 27.7% ... Training loss: 0.110 ... Validation loss: 0.215iteration: 2766\n",
      "train_loss: 0.11000638544284708\n",
      "val_loss: 0.21551832942273919\n",
      "Progress: 27.7% ... Training loss: 0.111 ... Validation loss: 0.214iteration: 2767\n",
      "train_loss: 0.11156322172440974\n",
      "val_loss: 0.21447954748426526\n",
      "Progress: 27.7% ... Training loss: 0.112 ... Validation loss: 0.222iteration: 2768\n",
      "train_loss: 0.1128850324833174\n",
      "val_loss: 0.22241382601296517\n",
      "Progress: 27.7% ... Training loss: 0.110 ... Validation loss: 0.212iteration: 2769\n",
      "train_loss: 0.11051540987961535\n",
      "val_loss: 0.21202549114856312\n",
      "Progress: 27.7% ... Training loss: 0.110 ... Validation loss: 0.216iteration: 2770\n",
      "train_loss: 0.11086254160857552\n",
      "val_loss: 0.2167971633664697\n",
      "Progress: 27.7% ... Training loss: 0.114 ... Validation loss: 0.229iteration: 2771\n",
      "train_loss: 0.11410678470232939\n",
      "val_loss: 0.22960037081347842\n",
      "Progress: 27.7% ... Training loss: 0.112 ... Validation loss: 0.215iteration: 2772\n",
      "train_loss: 0.11234332425112507\n",
      "val_loss: 0.21540317654879107\n",
      "Progress: 27.7% ... Training loss: 0.120 ... Validation loss: 0.237iteration: 2773\n",
      "train_loss: 0.12057893692037322\n",
      "val_loss: 0.23771382505857422\n",
      "Progress: 27.7% ... Training loss: 0.114 ... Validation loss: 0.210iteration: 2774\n",
      "train_loss: 0.11455021650131846\n",
      "val_loss: 0.21088441906195482\n",
      "Progress: 27.8% ... Training loss: 0.113 ... Validation loss: 0.213iteration: 2775\n",
      "train_loss: 0.113258915782347\n",
      "val_loss: 0.2134069328180497\n",
      "Progress: 27.8% ... Training loss: 0.110 ... Validation loss: 0.210iteration: 2776\n",
      "train_loss: 0.11045392854802265\n",
      "val_loss: 0.21007417221150484\n",
      "Progress: 27.8% ... Training loss: 0.110 ... Validation loss: 0.213iteration: 2777\n",
      "train_loss: 0.11037596038258948\n",
      "val_loss: 0.21382791798397713\n",
      "Progress: 27.8% ... Training loss: 0.109 ... Validation loss: 0.216iteration: 2778\n",
      "train_loss: 0.10941062156398033\n",
      "val_loss: 0.21606706039122286\n",
      "Progress: 27.8% ... Training loss: 0.111 ... Validation loss: 0.222iteration: 2779\n",
      "train_loss: 0.11199259379607923\n",
      "val_loss: 0.2225476474901268\n",
      "Progress: 27.8% ... Training loss: 0.114 ... Validation loss: 0.212iteration: 2780\n",
      "train_loss: 0.11485906772561826\n",
      "val_loss: 0.2122005908355631\n",
      "Progress: 27.8% ... Training loss: 0.110 ... Validation loss: 0.216iteration: 2781\n",
      "train_loss: 0.11011729077927108\n",
      "val_loss: 0.21620246463856835\n",
      "Progress: 27.8% ... Training loss: 0.117 ... Validation loss: 0.214iteration: 2782\n",
      "train_loss: 0.11781180380864398\n",
      "val_loss: 0.2143655332955241\n",
      "Progress: 27.8% ... Training loss: 0.115 ... Validation loss: 0.221iteration: 2783\n",
      "train_loss: 0.11501915401948143\n",
      "val_loss: 0.22105180209259032\n",
      "Progress: 27.8% ... Training loss: 0.111 ... Validation loss: 0.210iteration: 2784\n",
      "train_loss: 0.11120851148752205\n",
      "val_loss: 0.21082135998078858\n",
      "Progress: 27.9% ... Training loss: 0.110 ... Validation loss: 0.212iteration: 2785\n",
      "train_loss: 0.11025172762986848\n",
      "val_loss: 0.21230196693415593\n",
      "Progress: 27.9% ... Training loss: 0.113 ... Validation loss: 0.212iteration: 2786\n",
      "train_loss: 0.11346362207901084\n",
      "val_loss: 0.2121066488301071\n",
      "Progress: 27.9% ... Training loss: 0.112 ... Validation loss: 0.216iteration: 2787\n",
      "train_loss: 0.11247720245736849\n",
      "val_loss: 0.21639549497847874\n",
      "Progress: 27.9% ... Training loss: 0.109 ... Validation loss: 0.210iteration: 2788\n",
      "train_loss: 0.10901482754651298\n",
      "val_loss: 0.2105625796263184\n",
      "Progress: 27.9% ... Training loss: 0.110 ... Validation loss: 0.212iteration: 2789\n",
      "train_loss: 0.110711696697909\n",
      "val_loss: 0.21201321236873458\n",
      "Progress: 27.9% ... Training loss: 0.118 ... Validation loss: 0.226iteration: 2790\n",
      "train_loss: 0.11876302625368029\n",
      "val_loss: 0.22627227525607227\n",
      "Progress: 27.9% ... Training loss: 0.110 ... Validation loss: 0.209iteration: 2791\n",
      "train_loss: 0.11079044420980257\n",
      "val_loss: 0.2095817309814626\n",
      "Progress: 27.9% ... Training loss: 0.109 ... Validation loss: 0.217iteration: 2792\n",
      "train_loss: 0.10990742927205473\n",
      "val_loss: 0.21727449973241586\n",
      "Progress: 27.9% ... Training loss: 0.109 ... Validation loss: 0.214iteration: 2793\n",
      "train_loss: 0.10971292812613614\n",
      "val_loss: 0.21428108870042933\n",
      "Progress: 27.9% ... Training loss: 0.110 ... Validation loss: 0.209iteration: 2794\n",
      "train_loss: 0.11003506185499123\n",
      "val_loss: 0.20976323471275277\n",
      "Progress: 27.9% ... Training loss: 0.109 ... Validation loss: 0.212iteration: 2795\n",
      "train_loss: 0.10989021045920622\n",
      "val_loss: 0.21259710281552902\n",
      "Progress: 28.0% ... Training loss: 0.111 ... Validation loss: 0.219iteration: 2796\n",
      "train_loss: 0.11173921379647128\n",
      "val_loss: 0.21900178792899577\n",
      "Progress: 28.0% ... Training loss: 0.111 ... Validation loss: 0.210iteration: 2797\n",
      "train_loss: 0.11170669658402699\n",
      "val_loss: 0.21099950144701093\n",
      "Progress: 28.0% ... Training loss: 0.109 ... Validation loss: 0.212iteration: 2798\n",
      "train_loss: 0.10907397525170444\n",
      "val_loss: 0.21229554752857835\n",
      "Progress: 28.0% ... Training loss: 0.110 ... Validation loss: 0.212iteration: 2799\n",
      "train_loss: 0.11001182846918228\n",
      "val_loss: 0.21267209832541864\n",
      "Progress: 28.0% ... Training loss: 0.109 ... Validation loss: 0.218iteration: 2800\n",
      "train_loss: 0.10950426826573569\n",
      "val_loss: 0.2187357421534515\n",
      "Progress: 28.0% ... Training loss: 0.109 ... Validation loss: 0.218iteration: 2801\n",
      "train_loss: 0.10921062016973761\n",
      "val_loss: 0.21823780900256456\n",
      "Progress: 28.0% ... Training loss: 0.110 ... Validation loss: 0.225iteration: 2802\n",
      "train_loss: 0.11082094299810202\n",
      "val_loss: 0.22539888839635747\n",
      "Progress: 28.0% ... Training loss: 0.109 ... Validation loss: 0.211iteration: 2803\n",
      "train_loss: 0.10995053856971176\n",
      "val_loss: 0.21150445916643415\n",
      "Progress: 28.0% ... Training loss: 0.109 ... Validation loss: 0.217iteration: 2804\n",
      "train_loss: 0.109752038584288\n",
      "val_loss: 0.21740534125537717\n",
      "Progress: 28.1% ... Training loss: 0.109 ... Validation loss: 0.214iteration: 2805\n",
      "train_loss: 0.1090562395985541\n",
      "val_loss: 0.21428032861355004\n",
      "Progress: 28.1% ... Training loss: 0.131 ... Validation loss: 0.231iteration: 2806\n",
      "train_loss: 0.13197247079350874\n",
      "val_loss: 0.2318117608807353\n",
      "Progress: 28.1% ... Training loss: 0.163 ... Validation loss: 0.244iteration: 2807\n",
      "train_loss: 0.16387910004629608\n",
      "val_loss: 0.2444776369249578\n",
      "Progress: 28.1% ... Training loss: 0.174 ... Validation loss: 0.297iteration: 2808\n",
      "train_loss: 0.17421034561920815\n",
      "val_loss: 0.2973499532583374\n",
      "Progress: 28.1% ... Training loss: 0.146 ... Validation loss: 0.226iteration: 2809\n",
      "train_loss: 0.14687593945717936\n",
      "val_loss: 0.226714837932985\n",
      "Progress: 28.1% ... Training loss: 0.126 ... Validation loss: 0.241iteration: 2810\n",
      "train_loss: 0.12681191938440137\n",
      "val_loss: 0.24125491021562845\n",
      "Progress: 28.1% ... Training loss: 0.120 ... Validation loss: 0.207iteration: 2811\n",
      "train_loss: 0.12047642532605156\n",
      "val_loss: 0.2079580503819172\n",
      "Progress: 28.1% ... Training loss: 0.122 ... Validation loss: 0.237iteration: 2812\n",
      "train_loss: 0.12298485545679728\n",
      "val_loss: 0.23748768846275412\n",
      "Progress: 28.1% ... Training loss: 0.115 ... Validation loss: 0.207iteration: 2813\n",
      "train_loss: 0.11595875960121253\n",
      "val_loss: 0.207702936547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 28.1% ... Training loss: 0.110 ... Validation loss: 0.211iteration: 2814\n",
      "train_loss: 0.11000593740780178\n",
      "val_loss: 0.2112388031653211\n",
      "Progress: 28.1% ... Training loss: 0.109 ... Validation loss: 0.207iteration: 2815\n",
      "train_loss: 0.1093628123602997\n",
      "val_loss: 0.2070894145939576\n",
      "Progress: 28.2% ... Training loss: 0.109 ... Validation loss: 0.217iteration: 2816\n",
      "train_loss: 0.10971296402174513\n",
      "val_loss: 0.21796148521265196\n",
      "Progress: 28.2% ... Training loss: 0.108 ... Validation loss: 0.216iteration: 2817\n",
      "train_loss: 0.10817935829700949\n",
      "val_loss: 0.2160780692178467\n",
      "Progress: 28.2% ... Training loss: 0.108 ... Validation loss: 0.210iteration: 2818\n",
      "train_loss: 0.10837212685386798\n",
      "val_loss: 0.2102945668501156\n",
      "Progress: 28.2% ... Training loss: 0.109 ... Validation loss: 0.216iteration: 2819\n",
      "train_loss: 0.10909142383661445\n",
      "val_loss: 0.21632163514497682\n",
      "Progress: 28.2% ... Training loss: 0.109 ... Validation loss: 0.223iteration: 2820\n",
      "train_loss: 0.10940299094267052\n",
      "val_loss: 0.2230439774731838\n",
      "Progress: 28.2% ... Training loss: 0.108 ... Validation loss: 0.212iteration: 2821\n",
      "train_loss: 0.10893292358064377\n",
      "val_loss: 0.21293977755745225\n",
      "Progress: 28.2% ... Training loss: 0.113 ... Validation loss: 0.233iteration: 2822\n",
      "train_loss: 0.11388884635192131\n",
      "val_loss: 0.23399653927036515\n",
      "Progress: 28.2% ... Training loss: 0.112 ... Validation loss: 0.215iteration: 2823\n",
      "train_loss: 0.1122636455730804\n",
      "val_loss: 0.21594128303390303\n",
      "Progress: 28.2% ... Training loss: 0.108 ... Validation loss: 0.225iteration: 2824\n",
      "train_loss: 0.1081911052513331\n",
      "val_loss: 0.22560136914502743\n",
      "Progress: 28.2% ... Training loss: 0.108 ... Validation loss: 0.217iteration: 2825\n",
      "train_loss: 0.1085203009083861\n",
      "val_loss: 0.21760688072185674\n",
      "Progress: 28.3% ... Training loss: 0.110 ... Validation loss: 0.236iteration: 2826\n",
      "train_loss: 0.11015654799734054\n",
      "val_loss: 0.2360647061681237\n",
      "Progress: 28.3% ... Training loss: 0.122 ... Validation loss: 0.212iteration: 2827\n",
      "train_loss: 0.12202465104976479\n",
      "val_loss: 0.21279446269379912\n",
      "Progress: 28.3% ... Training loss: 0.111 ... Validation loss: 0.220iteration: 2828\n",
      "train_loss: 0.11144233794122042\n",
      "val_loss: 0.2200527083491005\n",
      "Progress: 28.3% ... Training loss: 0.107 ... Validation loss: 0.207iteration: 2829\n",
      "train_loss: 0.1077854155287148\n",
      "val_loss: 0.20760758977835814\n",
      "Progress: 28.3% ... Training loss: 0.109 ... Validation loss: 0.204iteration: 2830\n",
      "train_loss: 0.10916974270016698\n",
      "val_loss: 0.20408588905907038\n",
      "Progress: 28.3% ... Training loss: 0.110 ... Validation loss: 0.209iteration: 2831\n",
      "train_loss: 0.11069159402158063\n",
      "val_loss: 0.20920919197258228\n",
      "Progress: 28.3% ... Training loss: 0.109 ... Validation loss: 0.208iteration: 2832\n",
      "train_loss: 0.10911566241052002\n",
      "val_loss: 0.20812148672062208\n",
      "Progress: 28.3% ... Training loss: 0.107 ... Validation loss: 0.212iteration: 2833\n",
      "train_loss: 0.10745024265369146\n",
      "val_loss: 0.21292506451282997\n",
      "Progress: 28.3% ... Training loss: 0.108 ... Validation loss: 0.219iteration: 2834\n",
      "train_loss: 0.10808201172775911\n",
      "val_loss: 0.2199110974907239\n",
      "Progress: 28.4% ... Training loss: 0.110 ... Validation loss: 0.226iteration: 2835\n",
      "train_loss: 0.1101478381477867\n",
      "val_loss: 0.22663677597298135\n",
      "Progress: 28.4% ... Training loss: 0.112 ... Validation loss: 0.234iteration: 2836\n",
      "train_loss: 0.1129060040689765\n",
      "val_loss: 0.23400491279378602\n",
      "Progress: 28.4% ... Training loss: 0.108 ... Validation loss: 0.211iteration: 2837\n",
      "train_loss: 0.10865504640679506\n",
      "val_loss: 0.21169168108952546\n",
      "Progress: 28.4% ... Training loss: 0.106 ... Validation loss: 0.213iteration: 2838\n",
      "train_loss: 0.10695618042168506\n",
      "val_loss: 0.2138075764256831\n",
      "Progress: 28.4% ... Training loss: 0.107 ... Validation loss: 0.214iteration: 2839\n",
      "train_loss: 0.10728180422332384\n",
      "val_loss: 0.21451463181649863\n",
      "Progress: 28.4% ... Training loss: 0.107 ... Validation loss: 0.210iteration: 2840\n",
      "train_loss: 0.10771572067981598\n",
      "val_loss: 0.21006503374740987\n",
      "Progress: 28.4% ... Training loss: 0.109 ... Validation loss: 0.217iteration: 2841\n",
      "train_loss: 0.10945387812276841\n",
      "val_loss: 0.2178985886160117\n",
      "Progress: 28.4% ... Training loss: 0.111 ... Validation loss: 0.216iteration: 2842\n",
      "train_loss: 0.11137604385328649\n",
      "val_loss: 0.21683032762682267\n",
      "Progress: 28.4% ... Training loss: 0.107 ... Validation loss: 0.214iteration: 2843\n",
      "train_loss: 0.10709349793924981\n",
      "val_loss: 0.2145432968945827\n",
      "Progress: 28.4% ... Training loss: 0.107 ... Validation loss: 0.222iteration: 2844\n",
      "train_loss: 0.10770437237521908\n",
      "val_loss: 0.22246804017793045\n",
      "Progress: 28.4% ... Training loss: 0.106 ... Validation loss: 0.211iteration: 2845\n",
      "train_loss: 0.10645751996384381\n",
      "val_loss: 0.21162474046740368\n",
      "Progress: 28.5% ... Training loss: 0.109 ... Validation loss: 0.208iteration: 2846\n",
      "train_loss: 0.10922129535308704\n",
      "val_loss: 0.20820663209954762\n",
      "Progress: 28.5% ... Training loss: 0.114 ... Validation loss: 0.214iteration: 2847\n",
      "train_loss: 0.11442140771749511\n",
      "val_loss: 0.21466991344171132\n",
      "Progress: 28.5% ... Training loss: 0.131 ... Validation loss: 0.249iteration: 2848\n",
      "train_loss: 0.13116859433184413\n",
      "val_loss: 0.2495572691045148\n",
      "Progress: 28.5% ... Training loss: 0.110 ... Validation loss: 0.211iteration: 2849\n",
      "train_loss: 0.11013548079958393\n",
      "val_loss: 0.2113981938063452\n",
      "Progress: 28.5% ... Training loss: 0.107 ... Validation loss: 0.215iteration: 2850\n",
      "train_loss: 0.10719686320009363\n",
      "val_loss: 0.21577906122684382\n",
      "Progress: 28.5% ... Training loss: 0.112 ... Validation loss: 0.222iteration: 2851\n",
      "train_loss: 0.11272309803122804\n",
      "val_loss: 0.22270416313978214\n",
      "Progress: 28.5% ... Training loss: 0.106 ... Validation loss: 0.211iteration: 2852\n",
      "train_loss: 0.10673707840517774\n",
      "val_loss: 0.21150253925846912\n",
      "Progress: 28.5% ... Training loss: 0.107 ... Validation loss: 0.212iteration: 2853\n",
      "train_loss: 0.10749371367110634\n",
      "val_loss: 0.2122306256155488\n",
      "Progress: 28.5% ... Training loss: 0.111 ... Validation loss: 0.217iteration: 2854\n",
      "train_loss: 0.11182134261246679\n",
      "val_loss: 0.21772346326293482\n",
      "Progress: 28.6% ... Training loss: 0.107 ... Validation loss: 0.206iteration: 2855\n",
      "train_loss: 0.10712816612665294\n",
      "val_loss: 0.20616275314945856\n",
      "Progress: 28.6% ... Training loss: 0.107 ... Validation loss: 0.214iteration: 2856\n",
      "train_loss: 0.10782295523831202\n",
      "val_loss: 0.21480594225556\n",
      "Progress: 28.6% ... Training loss: 0.107 ... Validation loss: 0.220iteration: 2857\n",
      "train_loss: 0.10722893350504456\n",
      "val_loss: 0.2204331899188649\n",
      "Progress: 28.6% ... Training loss: 0.109 ... Validation loss: 0.217iteration: 2858\n",
      "train_loss: 0.1090231048073767\n",
      "val_loss: 0.21745673559939036\n",
      "Progress: 28.6% ... Training loss: 0.112 ... Validation loss: 0.204iteration: 2859\n",
      "train_loss: 0.11265267648998671\n",
      "val_loss: 0.2049508496905923\n",
      "Progress: 28.6% ... Training loss: 0.105 ... Validation loss: 0.209iteration: 2860\n",
      "train_loss: 0.10591443894422783\n",
      "val_loss: 0.20929569053521468\n",
      "Progress: 28.6% ... Training loss: 0.109 ... Validation loss: 0.204iteration: 2861\n",
      "train_loss: 0.10906826236278823\n",
      "val_loss: 0.20410720063516022\n",
      "Progress: 28.6% ... Training loss: 0.130 ... Validation loss: 0.260iteration: 2862\n",
      "train_loss: 0.13082089430394628\n",
      "val_loss: 0.2605281041772326\n",
      "Progress: 28.6% ... Training loss: 0.112 ... Validation loss: 0.203iteration: 2863\n",
      "train_loss: 0.11268574558433422\n",
      "val_loss: 0.20352818081673366\n",
      "Progress: 28.6% ... Training loss: 0.108 ... Validation loss: 0.211iteration: 2864\n",
      "train_loss: 0.10879672027581057\n",
      "val_loss: 0.21154828147090762\n",
      "Progress: 28.6% ... Training loss: 0.107 ... Validation loss: 0.221iteration: 2865\n",
      "train_loss: 0.10722109578208501\n",
      "val_loss: 0.221592006711058\n",
      "Progress: 28.7% ... Training loss: 0.105 ... Validation loss: 0.206iteration: 2866\n",
      "train_loss: 0.10570167481420067\n",
      "val_loss: 0.20671735900273744\n",
      "Progress: 28.7% ... Training loss: 0.105 ... Validation loss: 0.210iteration: 2867\n",
      "train_loss: 0.10541775542942937\n",
      "val_loss: 0.21041040497043137\n",
      "Progress: 28.7% ... Training loss: 0.113 ... Validation loss: 0.237iteration: 2868\n",
      "train_loss: 0.11311149234327947\n",
      "val_loss: 0.23766089025657683\n",
      "Progress: 28.7% ... Training loss: 0.110 ... Validation loss: 0.206iteration: 2869\n",
      "train_loss: 0.11051475495011095\n",
      "val_loss: 0.20608502452857852\n",
      "Progress: 28.7% ... Training loss: 0.107 ... Validation loss: 0.214iteration: 2870\n",
      "train_loss: 0.10781903041912344\n",
      "val_loss: 0.21460971653089625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 28.7% ... Training loss: 0.107 ... Validation loss: 0.202iteration: 2871\n",
      "train_loss: 0.10751360351843005\n",
      "val_loss: 0.20226686981413328\n",
      "Progress: 28.7% ... Training loss: 0.105 ... Validation loss: 0.201iteration: 2872\n",
      "train_loss: 0.105530544174673\n",
      "val_loss: 0.20180995899349455\n",
      "Progress: 28.7% ... Training loss: 0.109 ... Validation loss: 0.214iteration: 2873\n",
      "train_loss: 0.10944109414817438\n",
      "val_loss: 0.21435556893561475\n",
      "Progress: 28.7% ... Training loss: 0.106 ... Validation loss: 0.211iteration: 2874\n",
      "train_loss: 0.10681484805198176\n",
      "val_loss: 0.21192511179457824\n",
      "Progress: 28.8% ... Training loss: 0.107 ... Validation loss: 0.204iteration: 2875\n",
      "train_loss: 0.1078418672104808\n",
      "val_loss: 0.20470670948713934\n",
      "Progress: 28.8% ... Training loss: 0.110 ... Validation loss: 0.224iteration: 2876\n",
      "train_loss: 0.11070459232778894\n",
      "val_loss: 0.22458440151494294\n",
      "Progress: 28.8% ... Training loss: 0.106 ... Validation loss: 0.204iteration: 2877\n",
      "train_loss: 0.10646563243144579\n",
      "val_loss: 0.20430210853647265\n",
      "Progress: 28.8% ... Training loss: 0.122 ... Validation loss: 0.225iteration: 2878\n",
      "train_loss: 0.12248261732035373\n",
      "val_loss: 0.2250438223493634\n",
      "Progress: 28.8% ... Training loss: 0.115 ... Validation loss: 0.209iteration: 2879\n",
      "train_loss: 0.1158045200411261\n",
      "val_loss: 0.2091594769442545\n",
      "Progress: 28.8% ... Training loss: 0.113 ... Validation loss: 0.224iteration: 2880\n",
      "train_loss: 0.11359028395286495\n",
      "val_loss: 0.2243997159451603\n",
      "Progress: 28.8% ... Training loss: 0.111 ... Validation loss: 0.212iteration: 2881\n",
      "train_loss: 0.11159037315789058\n",
      "val_loss: 0.21215744477014434\n",
      "Progress: 28.8% ... Training loss: 0.114 ... Validation loss: 0.237iteration: 2882\n",
      "train_loss: 0.11473285962679895\n",
      "val_loss: 0.2371357698054921\n",
      "Progress: 28.8% ... Training loss: 0.116 ... Validation loss: 0.210iteration: 2883\n",
      "train_loss: 0.11671841014680137\n",
      "val_loss: 0.2104435768152564\n",
      "Progress: 28.8% ... Training loss: 0.109 ... Validation loss: 0.226iteration: 2884\n",
      "train_loss: 0.10914622998187286\n",
      "val_loss: 0.22683562244463706\n",
      "Progress: 28.9% ... Training loss: 0.114 ... Validation loss: 0.209iteration: 2885\n",
      "train_loss: 0.11452436426917055\n",
      "val_loss: 0.20913189415079161\n",
      "Progress: 28.9% ... Training loss: 0.104 ... Validation loss: 0.214iteration: 2886\n",
      "train_loss: 0.10470325102193136\n",
      "val_loss: 0.2149354006905708\n",
      "Progress: 28.9% ... Training loss: 0.106 ... Validation loss: 0.225iteration: 2887\n",
      "train_loss: 0.10673594103377318\n",
      "val_loss: 0.22555627641944306\n",
      "Progress: 28.9% ... Training loss: 0.115 ... Validation loss: 0.216iteration: 2888\n",
      "train_loss: 0.11595083745394513\n",
      "val_loss: 0.21650312964365598\n",
      "Progress: 28.9% ... Training loss: 0.122 ... Validation loss: 0.261iteration: 2889\n",
      "train_loss: 0.12282363564424109\n",
      "val_loss: 0.2610094312213395\n",
      "Progress: 28.9% ... Training loss: 0.108 ... Validation loss: 0.213iteration: 2890\n",
      "train_loss: 0.10880456713102991\n",
      "val_loss: 0.2131213808223317\n",
      "Progress: 28.9% ... Training loss: 0.112 ... Validation loss: 0.240iteration: 2891\n",
      "train_loss: 0.11242323181038678\n",
      "val_loss: 0.24023563250774455\n",
      "Progress: 28.9% ... Training loss: 0.106 ... Validation loss: 0.210iteration: 2892\n",
      "train_loss: 0.10622195015672156\n",
      "val_loss: 0.21083345459968858\n",
      "Progress: 28.9% ... Training loss: 0.108 ... Validation loss: 0.226iteration: 2893\n",
      "train_loss: 0.10868152834063029\n",
      "val_loss: 0.22631818567726789\n",
      "Progress: 28.9% ... Training loss: 0.111 ... Validation loss: 0.211iteration: 2894\n",
      "train_loss: 0.11117695654630536\n",
      "val_loss: 0.21145717909362977\n",
      "Progress: 28.9% ... Training loss: 0.105 ... Validation loss: 0.218iteration: 2895\n",
      "train_loss: 0.10571755734866502\n",
      "val_loss: 0.21898533346683738\n",
      "Progress: 29.0% ... Training loss: 0.112 ... Validation loss: 0.213iteration: 2896\n",
      "train_loss: 0.11255239168583427\n",
      "val_loss: 0.21301715619895362\n",
      "Progress: 29.0% ... Training loss: 0.113 ... Validation loss: 0.243iteration: 2897\n",
      "train_loss: 0.11300874202364165\n",
      "val_loss: 0.24381367707610843\n",
      "Progress: 29.0% ... Training loss: 0.113 ... Validation loss: 0.212iteration: 2898\n",
      "train_loss: 0.11363493747113843\n",
      "val_loss: 0.21237219416541464\n",
      "Progress: 29.0% ... Training loss: 0.106 ... Validation loss: 0.224iteration: 2899\n",
      "train_loss: 0.10693235041441698\n",
      "val_loss: 0.22422193280775943\n",
      "Progress: 29.0% ... Training loss: 0.109 ... Validation loss: 0.210iteration: 2900\n",
      "train_loss: 0.10913116219022334\n",
      "val_loss: 0.2104525339481366\n",
      "Progress: 29.0% ... Training loss: 0.105 ... Validation loss: 0.208iteration: 2901\n",
      "train_loss: 0.1051039698431554\n",
      "val_loss: 0.20860506505113985\n",
      "Progress: 29.0% ... Training loss: 0.106 ... Validation loss: 0.213iteration: 2902\n",
      "train_loss: 0.10645918130518314\n",
      "val_loss: 0.21388301832970297\n",
      "Progress: 29.0% ... Training loss: 0.109 ... Validation loss: 0.206iteration: 2903\n",
      "train_loss: 0.10980379392379926\n",
      "val_loss: 0.20695559595889906\n",
      "Progress: 29.0% ... Training loss: 0.107 ... Validation loss: 0.218iteration: 2904\n",
      "train_loss: 0.10757388497829083\n",
      "val_loss: 0.21873827942482263\n",
      "Progress: 29.1% ... Training loss: 0.107 ... Validation loss: 0.203iteration: 2905\n",
      "train_loss: 0.10769681438293979\n",
      "val_loss: 0.20344294740418414\n",
      "Progress: 29.1% ... Training loss: 0.106 ... Validation loss: 0.221iteration: 2906\n",
      "train_loss: 0.10616228800618373\n",
      "val_loss: 0.22131278189463666\n",
      "Progress: 29.1% ... Training loss: 0.113 ... Validation loss: 0.207iteration: 2907\n",
      "train_loss: 0.11306432100159265\n",
      "val_loss: 0.20708224692775798\n",
      "Progress: 29.1% ... Training loss: 0.103 ... Validation loss: 0.211iteration: 2908\n",
      "train_loss: 0.1038744598617791\n",
      "val_loss: 0.21118051490351408\n",
      "Progress: 29.1% ... Training loss: 0.105 ... Validation loss: 0.216iteration: 2909\n",
      "train_loss: 0.10503032127661173\n",
      "val_loss: 0.21671653641294272\n",
      "Progress: 29.1% ... Training loss: 0.104 ... Validation loss: 0.205iteration: 2910\n",
      "train_loss: 0.10447406511347308\n",
      "val_loss: 0.2059970579498696\n",
      "Progress: 29.1% ... Training loss: 0.105 ... Validation loss: 0.215iteration: 2911\n",
      "train_loss: 0.10560993274842656\n",
      "val_loss: 0.21562731475333005\n",
      "Progress: 29.1% ... Training loss: 0.105 ... Validation loss: 0.205iteration: 2912\n",
      "train_loss: 0.10502327255502143\n",
      "val_loss: 0.20560392121198795\n",
      "Progress: 29.1% ... Training loss: 0.104 ... Validation loss: 0.207iteration: 2913\n",
      "train_loss: 0.10450154378609502\n",
      "val_loss: 0.2070662299790487\n",
      "Progress: 29.1% ... Training loss: 0.103 ... Validation loss: 0.209iteration: 2914\n",
      "train_loss: 0.10340125418310081\n",
      "val_loss: 0.2092408637092361\n",
      "Progress: 29.1% ... Training loss: 0.104 ... Validation loss: 0.208iteration: 2915\n",
      "train_loss: 0.10477651134481065\n",
      "val_loss: 0.2087328904378498\n",
      "Progress: 29.2% ... Training loss: 0.105 ... Validation loss: 0.220iteration: 2916\n",
      "train_loss: 0.10577961931949183\n",
      "val_loss: 0.22093483037821737\n",
      "Progress: 29.2% ... Training loss: 0.112 ... Validation loss: 0.207iteration: 2917\n",
      "train_loss: 0.11242599982694611\n",
      "val_loss: 0.20798553970086683\n",
      "Progress: 29.2% ... Training loss: 0.115 ... Validation loss: 0.241iteration: 2918\n",
      "train_loss: 0.11543981907115065\n",
      "val_loss: 0.24163154498144557\n",
      "Progress: 29.2% ... Training loss: 0.125 ... Validation loss: 0.210iteration: 2919\n",
      "train_loss: 0.1258178474260802\n",
      "val_loss: 0.2108377062559618\n",
      "Progress: 29.2% ... Training loss: 0.116 ... Validation loss: 0.262iteration: 2920\n",
      "train_loss: 0.11664272437805856\n",
      "val_loss: 0.2620449470726832\n",
      "Progress: 29.2% ... Training loss: 0.104 ... Validation loss: 0.210iteration: 2921\n",
      "train_loss: 0.10484063276284931\n",
      "val_loss: 0.21086401171788738\n",
      "Progress: 29.2% ... Training loss: 0.108 ... Validation loss: 0.218iteration: 2922\n",
      "train_loss: 0.1081986113736275\n",
      "val_loss: 0.21852655814393135\n",
      "Progress: 29.2% ... Training loss: 0.107 ... Validation loss: 0.226iteration: 2923\n",
      "train_loss: 0.10747062439227978\n",
      "val_loss: 0.22602182622321515\n",
      "Progress: 29.2% ... Training loss: 0.105 ... Validation loss: 0.203iteration: 2924\n",
      "train_loss: 0.10581533810888265\n",
      "val_loss: 0.20320349108878905\n",
      "Progress: 29.2% ... Training loss: 0.108 ... Validation loss: 0.221iteration: 2925\n",
      "train_loss: 0.10893806481156593\n",
      "val_loss: 0.2215053849500929\n",
      "Progress: 29.3% ... Training loss: 0.108 ... Validation loss: 0.199iteration: 2926\n",
      "train_loss: 0.10844991415443055\n",
      "val_loss: 0.19923206024912324\n",
      "Progress: 29.3% ... Training loss: 0.111 ... Validation loss: 0.217iteration: 2927\n",
      "train_loss: 0.11127940060119997\n",
      "val_loss: 0.21700666824076464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 29.3% ... Training loss: 0.108 ... Validation loss: 0.203iteration: 2928\n",
      "train_loss: 0.10854521480991063\n",
      "val_loss: 0.20310626854263852\n",
      "Progress: 29.3% ... Training loss: 0.114 ... Validation loss: 0.227iteration: 2929\n",
      "train_loss: 0.1144846930633356\n",
      "val_loss: 0.22793919574879817\n",
      "Progress: 29.3% ... Training loss: 0.109 ... Validation loss: 0.202iteration: 2930\n",
      "train_loss: 0.10975509223229958\n",
      "val_loss: 0.2025712672728901\n",
      "Progress: 29.3% ... Training loss: 0.104 ... Validation loss: 0.219iteration: 2931\n",
      "train_loss: 0.10486681379917656\n",
      "val_loss: 0.21910403321075086\n",
      "Progress: 29.3% ... Training loss: 0.113 ... Validation loss: 0.206iteration: 2932\n",
      "train_loss: 0.11312918488314476\n",
      "val_loss: 0.20671646240413963\n",
      "Progress: 29.3% ... Training loss: 0.106 ... Validation loss: 0.222iteration: 2933\n",
      "train_loss: 0.10619425750502091\n",
      "val_loss: 0.22231318967907532\n",
      "Progress: 29.3% ... Training loss: 0.105 ... Validation loss: 0.206iteration: 2934\n",
      "train_loss: 0.10529348853306424\n",
      "val_loss: 0.2069373477838904\n",
      "Progress: 29.4% ... Training loss: 0.104 ... Validation loss: 0.218iteration: 2935\n",
      "train_loss: 0.10487888764392467\n",
      "val_loss: 0.2184199636771658\n",
      "Progress: 29.4% ... Training loss: 0.107 ... Validation loss: 0.223iteration: 2936\n",
      "train_loss: 0.10709323915537233\n",
      "val_loss: 0.2232571197402056\n",
      "Progress: 29.4% ... Training loss: 0.103 ... Validation loss: 0.215iteration: 2937\n",
      "train_loss: 0.10387124570785619\n",
      "val_loss: 0.2151488399553825\n",
      "Progress: 29.4% ... Training loss: 0.105 ... Validation loss: 0.212iteration: 2938\n",
      "train_loss: 0.10597048878383404\n",
      "val_loss: 0.21206576623692058\n",
      "Progress: 29.4% ... Training loss: 0.110 ... Validation loss: 0.205iteration: 2939\n",
      "train_loss: 0.11006967969156821\n",
      "val_loss: 0.20500333746431265\n",
      "Progress: 29.4% ... Training loss: 0.105 ... Validation loss: 0.206iteration: 2940\n",
      "train_loss: 0.10504290207762394\n",
      "val_loss: 0.20689635164870343\n",
      "Progress: 29.4% ... Training loss: 0.103 ... Validation loss: 0.215iteration: 2941\n",
      "train_loss: 0.10368646743245216\n",
      "val_loss: 0.21525843487523527\n",
      "Progress: 29.4% ... Training loss: 0.102 ... Validation loss: 0.206iteration: 2942\n",
      "train_loss: 0.10295769031107528\n",
      "val_loss: 0.20690388421479144\n",
      "Progress: 29.4% ... Training loss: 0.103 ... Validation loss: 0.215iteration: 2943\n",
      "train_loss: 0.10314988030615348\n",
      "val_loss: 0.21517743876032883\n",
      "Progress: 29.4% ... Training loss: 0.102 ... Validation loss: 0.212iteration: 2944\n",
      "train_loss: 0.10288286472588899\n",
      "val_loss: 0.21210383066565752\n",
      "Progress: 29.4% ... Training loss: 0.107 ... Validation loss: 0.220iteration: 2945\n",
      "train_loss: 0.10708415374680284\n",
      "val_loss: 0.22019659125185131\n",
      "Progress: 29.5% ... Training loss: 0.106 ... Validation loss: 0.203iteration: 2946\n",
      "train_loss: 0.10616504879259868\n",
      "val_loss: 0.2036596763972587\n",
      "Progress: 29.5% ... Training loss: 0.109 ... Validation loss: 0.210iteration: 2947\n",
      "train_loss: 0.10920334313068594\n",
      "val_loss: 0.21075567693098007\n",
      "Progress: 29.5% ... Training loss: 0.103 ... Validation loss: 0.201iteration: 2948\n",
      "train_loss: 0.10392430807226039\n",
      "val_loss: 0.20162167469161646\n",
      "Progress: 29.5% ... Training loss: 0.111 ... Validation loss: 0.216iteration: 2949\n",
      "train_loss: 0.11127761485484598\n",
      "val_loss: 0.21602748639615715\n",
      "Progress: 29.5% ... Training loss: 0.107 ... Validation loss: 0.196iteration: 2950\n",
      "train_loss: 0.10741700406499065\n",
      "val_loss: 0.1969881487120195\n",
      "Progress: 29.5% ... Training loss: 0.106 ... Validation loss: 0.211iteration: 2951\n",
      "train_loss: 0.10695718720106535\n",
      "val_loss: 0.21190085376922307\n",
      "Progress: 29.5% ... Training loss: 0.105 ... Validation loss: 0.207iteration: 2952\n",
      "train_loss: 0.10524066161700135\n",
      "val_loss: 0.20759781689748713\n",
      "Progress: 29.5% ... Training loss: 0.103 ... Validation loss: 0.208iteration: 2953\n",
      "train_loss: 0.10396087150465479\n",
      "val_loss: 0.20867932813003803\n",
      "Progress: 29.5% ... Training loss: 0.104 ... Validation loss: 0.203iteration: 2954\n",
      "train_loss: 0.10461913997577049\n",
      "val_loss: 0.20380435482704426\n",
      "Progress: 29.6% ... Training loss: 0.106 ... Validation loss: 0.208iteration: 2955\n",
      "train_loss: 0.10608324373344818\n",
      "val_loss: 0.2081670212070153\n",
      "Progress: 29.6% ... Training loss: 0.105 ... Validation loss: 0.198iteration: 2956\n",
      "train_loss: 0.10588460988020566\n",
      "val_loss: 0.19836242544143312\n",
      "Progress: 29.6% ... Training loss: 0.103 ... Validation loss: 0.204iteration: 2957\n",
      "train_loss: 0.10316368049528543\n",
      "val_loss: 0.20474034547529513\n",
      "Progress: 29.6% ... Training loss: 0.106 ... Validation loss: 0.209iteration: 2958\n",
      "train_loss: 0.10677242020595837\n",
      "val_loss: 0.2092195202144676\n",
      "Progress: 29.6% ... Training loss: 0.102 ... Validation loss: 0.199iteration: 2959\n",
      "train_loss: 0.10281102804509701\n",
      "val_loss: 0.19966690414261978\n",
      "Progress: 29.6% ... Training loss: 0.103 ... Validation loss: 0.202iteration: 2960\n",
      "train_loss: 0.10341018068509847\n",
      "val_loss: 0.20263955941849718\n",
      "Progress: 29.6% ... Training loss: 0.110 ... Validation loss: 0.195iteration: 2961\n",
      "train_loss: 0.11079901484809736\n",
      "val_loss: 0.19516858217556857\n",
      "Progress: 29.6% ... Training loss: 0.105 ... Validation loss: 0.203iteration: 2962\n",
      "train_loss: 0.10523453561871152\n",
      "val_loss: 0.20368630951137456\n",
      "Progress: 29.6% ... Training loss: 0.103 ... Validation loss: 0.197iteration: 2963\n",
      "train_loss: 0.10327473908958303\n",
      "val_loss: 0.1973634446899825\n",
      "Progress: 29.6% ... Training loss: 0.103 ... Validation loss: 0.198iteration: 2964\n",
      "train_loss: 0.10304097830020492\n",
      "val_loss: 0.19815950171811358\n",
      "Progress: 29.6% ... Training loss: 0.109 ... Validation loss: 0.210iteration: 2965\n",
      "train_loss: 0.10993108325743173\n",
      "val_loss: 0.21009652838647608\n",
      "Progress: 29.7% ... Training loss: 0.109 ... Validation loss: 0.194iteration: 2966\n",
      "train_loss: 0.10964286159311394\n",
      "val_loss: 0.19429910434597947\n",
      "Progress: 29.7% ... Training loss: 0.112 ... Validation loss: 0.210iteration: 2967\n",
      "train_loss: 0.11213503350155933\n",
      "val_loss: 0.21052019856552648\n",
      "Progress: 29.7% ... Training loss: 0.114 ... Validation loss: 0.201iteration: 2968\n",
      "train_loss: 0.11493469560444075\n",
      "val_loss: 0.20122283035475605\n",
      "Progress: 29.7% ... Training loss: 0.105 ... Validation loss: 0.214iteration: 2969\n",
      "train_loss: 0.10531563082812545\n",
      "val_loss: 0.21467069564815272\n",
      "Progress: 29.7% ... Training loss: 0.112 ... Validation loss: 0.200iteration: 2970\n",
      "train_loss: 0.11272937923202168\n",
      "val_loss: 0.20056263115873976\n",
      "Progress: 29.7% ... Training loss: 0.130 ... Validation loss: 0.234iteration: 2971\n",
      "train_loss: 0.130164154487585\n",
      "val_loss: 0.23400200957053915\n",
      "Progress: 29.7% ... Training loss: 0.144 ... Validation loss: 0.214iteration: 2972\n",
      "train_loss: 0.14408703288624378\n",
      "val_loss: 0.21466551155457492\n",
      "Progress: 29.7% ... Training loss: 0.156 ... Validation loss: 0.273iteration: 2973\n",
      "train_loss: 0.15600346041635907\n",
      "val_loss: 0.2733946981049613\n",
      "Progress: 29.7% ... Training loss: 0.113 ... Validation loss: 0.196iteration: 2974\n",
      "train_loss: 0.11320541103093942\n",
      "val_loss: 0.1969982849528657\n",
      "Progress: 29.8% ... Training loss: 0.112 ... Validation loss: 0.232iteration: 2975\n",
      "train_loss: 0.11224484918692915\n",
      "val_loss: 0.2324973393235776\n",
      "Progress: 29.8% ... Training loss: 0.101 ... Validation loss: 0.202iteration: 2976\n",
      "train_loss: 0.10183328015667686\n",
      "val_loss: 0.20244941036345443\n",
      "Progress: 29.8% ... Training loss: 0.101 ... Validation loss: 0.203iteration: 2977\n",
      "train_loss: 0.10168221311158415\n",
      "val_loss: 0.20371548652976135\n",
      "Progress: 29.8% ... Training loss: 0.101 ... Validation loss: 0.205iteration: 2978\n",
      "train_loss: 0.10175905827357047\n",
      "val_loss: 0.20539586000007354\n",
      "Progress: 29.8% ... Training loss: 0.102 ... Validation loss: 0.209iteration: 2979\n",
      "train_loss: 0.1028075003217489\n",
      "val_loss: 0.20900230925074037\n",
      "Progress: 29.8% ... Training loss: 0.104 ... Validation loss: 0.195iteration: 2980\n",
      "train_loss: 0.10431933086314282\n",
      "val_loss: 0.19592664320000466\n",
      "Progress: 29.8% ... Training loss: 0.101 ... Validation loss: 0.204iteration: 2981\n",
      "train_loss: 0.10136358998925452\n",
      "val_loss: 0.204312980910139\n",
      "Progress: 29.8% ... Training loss: 0.101 ... Validation loss: 0.212iteration: 2982\n",
      "train_loss: 0.10144313105855009\n",
      "val_loss: 0.212106047036156\n",
      "Progress: 29.8% ... Training loss: 0.101 ... Validation loss: 0.201iteration: 2983\n",
      "train_loss: 0.10169514071323082\n",
      "val_loss: 0.20196228676551123\n",
      "Progress: 29.8% ... Training loss: 0.116 ... Validation loss: 0.237iteration: 2984\n",
      "train_loss: 0.11625634858226737\n",
      "val_loss: 0.23791800730814935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 29.9% ... Training loss: 0.107 ... Validation loss: 0.196iteration: 2985\n",
      "train_loss: 0.10761992968752107\n",
      "val_loss: 0.196865748159149\n",
      "Progress: 29.9% ... Training loss: 0.128 ... Validation loss: 0.243iteration: 2986\n",
      "train_loss: 0.12871580948215555\n",
      "val_loss: 0.2439980692140823\n",
      "Progress: 29.9% ... Training loss: 0.110 ... Validation loss: 0.198iteration: 2987\n",
      "train_loss: 0.11058787580610899\n",
      "val_loss: 0.19893618779453215\n",
      "Progress: 29.9% ... Training loss: 0.102 ... Validation loss: 0.213iteration: 2988\n",
      "train_loss: 0.1029993336289211\n",
      "val_loss: 0.21387442977230647\n",
      "Progress: 29.9% ... Training loss: 0.102 ... Validation loss: 0.206iteration: 2989\n",
      "train_loss: 0.10254295389567838\n",
      "val_loss: 0.2064567043572227\n",
      "Progress: 29.9% ... Training loss: 0.101 ... Validation loss: 0.203iteration: 2990\n",
      "train_loss: 0.10185413591335066\n",
      "val_loss: 0.20351763793405872\n",
      "Progress: 29.9% ... Training loss: 0.102 ... Validation loss: 0.209iteration: 2991\n",
      "train_loss: 0.10217040239956546\n",
      "val_loss: 0.20984641009687685\n",
      "Progress: 29.9% ... Training loss: 0.103 ... Validation loss: 0.215iteration: 2992\n",
      "train_loss: 0.10324932714682425\n",
      "val_loss: 0.21543219541662417\n",
      "Progress: 29.9% ... Training loss: 0.101 ... Validation loss: 0.205iteration: 2993\n",
      "train_loss: 0.10190894358367447\n",
      "val_loss: 0.20539383456628066\n",
      "Progress: 29.9% ... Training loss: 0.108 ... Validation loss: 0.238iteration: 2994\n",
      "train_loss: 0.10811352224678795\n",
      "val_loss: 0.23813790948005903\n",
      "Progress: 29.9% ... Training loss: 0.102 ... Validation loss: 0.206iteration: 2995\n",
      "train_loss: 0.10207779068325834\n",
      "val_loss: 0.20670673911498819\n",
      "Progress: 30.0% ... Training loss: 0.103 ... Validation loss: 0.199iteration: 2996\n",
      "train_loss: 0.10333487909638796\n",
      "val_loss: 0.1996502394847198\n",
      "Progress: 30.0% ... Training loss: 0.101 ... Validation loss: 0.209iteration: 2997\n",
      "train_loss: 0.1019849353897567\n",
      "val_loss: 0.2091287267772986\n",
      "Progress: 30.0% ... Training loss: 0.100 ... Validation loss: 0.193iteration: 2998\n",
      "train_loss: 0.10087985060096456\n",
      "val_loss: 0.19342236515938108\n",
      "Progress: 30.0% ... Training loss: 0.107 ... Validation loss: 0.191iteration: 2999\n",
      "train_loss: 0.10798464958438679\n",
      "val_loss: 0.1911609026949168\n",
      "Progress: 30.0% ... Training loss: 0.101 ... Validation loss: 0.196iteration: 3000\n",
      "train_loss: 0.10182754736935276\n",
      "val_loss: 0.1961841225735702\n",
      "Progress: 30.0% ... Training loss: 0.101 ... Validation loss: 0.200iteration: 3001\n",
      "train_loss: 0.10146323526866498\n",
      "val_loss: 0.20074617500028719\n",
      "Progress: 30.0% ... Training loss: 0.101 ... Validation loss: 0.195iteration: 3002\n",
      "train_loss: 0.10157190368939384\n",
      "val_loss: 0.19501294627171742\n",
      "Progress: 30.0% ... Training loss: 0.110 ... Validation loss: 0.228iteration: 3003\n",
      "train_loss: 0.11098413368068925\n",
      "val_loss: 0.22874213589392633\n",
      "Progress: 30.0% ... Training loss: 0.100 ... Validation loss: 0.199iteration: 3004\n",
      "train_loss: 0.10074375673579847\n",
      "val_loss: 0.19919897081255938\n",
      "Progress: 30.1% ... Training loss: 0.101 ... Validation loss: 0.195iteration: 3005\n",
      "train_loss: 0.10170495861317223\n",
      "val_loss: 0.19502134379848599\n",
      "Progress: 30.1% ... Training loss: 0.102 ... Validation loss: 0.193iteration: 3006\n",
      "train_loss: 0.10232547407515862\n",
      "val_loss: 0.19383677701461674\n",
      "Progress: 30.1% ... Training loss: 0.103 ... Validation loss: 0.193iteration: 3007\n",
      "train_loss: 0.10315540617876405\n",
      "val_loss: 0.19340237130315502\n",
      "Progress: 30.1% ... Training loss: 0.100 ... Validation loss: 0.206iteration: 3008\n",
      "train_loss: 0.10081136039190643\n",
      "val_loss: 0.206342009860885\n",
      "Progress: 30.1% ... Training loss: 0.100 ... Validation loss: 0.197iteration: 3009\n",
      "train_loss: 0.10038268348048718\n",
      "val_loss: 0.19789215263962773\n",
      "Progress: 30.1% ... Training loss: 0.107 ... Validation loss: 0.200iteration: 3010\n",
      "train_loss: 0.10776214528851173\n",
      "val_loss: 0.2003349353156144\n",
      "Progress: 30.1% ... Training loss: 0.107 ... Validation loss: 0.224iteration: 3011\n",
      "train_loss: 0.10764467931764557\n",
      "val_loss: 0.22434543976340368\n",
      "Progress: 30.1% ... Training loss: 0.101 ... Validation loss: 0.201iteration: 3012\n",
      "train_loss: 0.10105040422969516\n",
      "val_loss: 0.2011948864163658\n",
      "Progress: 30.1% ... Training loss: 0.104 ... Validation loss: 0.210iteration: 3013\n",
      "train_loss: 0.10406332705380468\n",
      "val_loss: 0.2103197202538582\n",
      "Progress: 30.1% ... Training loss: 0.103 ... Validation loss: 0.193iteration: 3014\n",
      "train_loss: 0.1034045898794363\n",
      "val_loss: 0.193963136586837\n",
      "Progress: 30.1% ... Training loss: 0.105 ... Validation loss: 0.201iteration: 3015\n",
      "train_loss: 0.10565229656312386\n",
      "val_loss: 0.2019934210474345\n",
      "Progress: 30.2% ... Training loss: 0.106 ... Validation loss: 0.196iteration: 3016\n",
      "train_loss: 0.10690144411903199\n",
      "val_loss: 0.19691738614822726\n",
      "Progress: 30.2% ... Training loss: 0.112 ... Validation loss: 0.214iteration: 3017\n",
      "train_loss: 0.11294725798285092\n",
      "val_loss: 0.2147935036429797\n",
      "Progress: 30.2% ... Training loss: 0.100 ... Validation loss: 0.191iteration: 3018\n",
      "train_loss: 0.100019433924769\n",
      "val_loss: 0.1913921066275615\n",
      "Progress: 30.2% ... Training loss: 0.100 ... Validation loss: 0.189iteration: 3019\n",
      "train_loss: 0.10087924888482692\n",
      "val_loss: 0.1899372251805957\n",
      "Progress: 30.2% ... Training loss: 0.100 ... Validation loss: 0.195iteration: 3020\n",
      "train_loss: 0.10094942555979684\n",
      "val_loss: 0.19587827478511055\n",
      "Progress: 30.2% ... Training loss: 0.104 ... Validation loss: 0.196iteration: 3021\n",
      "train_loss: 0.10429802992586024\n",
      "val_loss: 0.19696469851630297\n",
      "Progress: 30.2% ... Training loss: 0.101 ... Validation loss: 0.198iteration: 3022\n",
      "train_loss: 0.10110675324940241\n",
      "val_loss: 0.19855744194430883\n",
      "Progress: 30.2% ... Training loss: 0.101 ... Validation loss: 0.198iteration: 3023\n",
      "train_loss: 0.1015138761498548\n",
      "val_loss: 0.1988731963978108\n",
      "Progress: 30.2% ... Training loss: 0.101 ... Validation loss: 0.194iteration: 3024\n",
      "train_loss: 0.10180677270250764\n",
      "val_loss: 0.19449525864319134\n",
      "Progress: 30.2% ... Training loss: 0.108 ... Validation loss: 0.194iteration: 3025\n",
      "train_loss: 0.1088550565134755\n",
      "val_loss: 0.19426002859659391\n",
      "Progress: 30.3% ... Training loss: 0.100 ... Validation loss: 0.197iteration: 3026\n",
      "train_loss: 0.1006350563097828\n",
      "val_loss: 0.197281103860518\n",
      "Progress: 30.3% ... Training loss: 0.100 ... Validation loss: 0.194iteration: 3027\n",
      "train_loss: 0.10098321887863161\n",
      "val_loss: 0.19436167072661684\n",
      "Progress: 30.3% ... Training loss: 0.102 ... Validation loss: 0.194iteration: 3028\n",
      "train_loss: 0.10203563371120279\n",
      "val_loss: 0.19455324567275578\n",
      "Progress: 30.3% ... Training loss: 0.101 ... Validation loss: 0.202iteration: 3029\n",
      "train_loss: 0.10195019398143511\n",
      "val_loss: 0.20242422024109138\n",
      "Progress: 30.3% ... Training loss: 0.100 ... Validation loss: 0.192iteration: 3030\n",
      "train_loss: 0.10063992062310158\n",
      "val_loss: 0.19246540584511404\n",
      "Progress: 30.3% ... Training loss: 0.100 ... Validation loss: 0.197iteration: 3031\n",
      "train_loss: 0.10064116769829187\n",
      "val_loss: 0.1978710417384232\n",
      "Progress: 30.3% ... Training loss: 0.101 ... Validation loss: 0.190iteration: 3032\n",
      "train_loss: 0.10116884948507802\n",
      "val_loss: 0.19065864095579282\n",
      "Progress: 30.3% ... Training loss: 0.104 ... Validation loss: 0.193iteration: 3033\n",
      "train_loss: 0.10462132705113807\n",
      "val_loss: 0.19345085045234506\n",
      "Progress: 30.3% ... Training loss: 0.099 ... Validation loss: 0.204iteration: 3034\n",
      "train_loss: 0.0993878006660476\n",
      "val_loss: 0.2049052167230289\n",
      "Progress: 30.4% ... Training loss: 0.099 ... Validation loss: 0.201iteration: 3035\n",
      "train_loss: 0.09918816315065619\n",
      "val_loss: 0.2017548163950039\n",
      "Progress: 30.4% ... Training loss: 0.098 ... Validation loss: 0.200iteration: 3036\n",
      "train_loss: 0.09895243860582799\n",
      "val_loss: 0.20099374498469225\n",
      "Progress: 30.4% ... Training loss: 0.109 ... Validation loss: 0.224iteration: 3037\n",
      "train_loss: 0.10931294363623838\n",
      "val_loss: 0.22499263417640536\n",
      "Progress: 30.4% ... Training loss: 0.099 ... Validation loss: 0.192iteration: 3038\n",
      "train_loss: 0.09923265225822749\n",
      "val_loss: 0.19232216730950083\n",
      "Progress: 30.4% ... Training loss: 0.099 ... Validation loss: 0.191iteration: 3039\n",
      "train_loss: 0.09938597412270972\n",
      "val_loss: 0.1913318620607648\n",
      "Progress: 30.4% ... Training loss: 0.098 ... Validation loss: 0.191iteration: 3040\n",
      "train_loss: 0.09896068524386359\n",
      "val_loss: 0.1914561417978736\n",
      "Progress: 30.4% ... Training loss: 0.101 ... Validation loss: 0.197iteration: 3041\n",
      "train_loss: 0.10193563757731618\n",
      "val_loss: 0.19778172193694682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 30.4% ... Training loss: 0.099 ... Validation loss: 0.196iteration: 3042\n",
      "train_loss: 0.09980552156186473\n",
      "val_loss: 0.19692781517773272\n",
      "Progress: 30.4% ... Training loss: 0.102 ... Validation loss: 0.208iteration: 3043\n",
      "train_loss: 0.10277159136987038\n",
      "val_loss: 0.20897254780582497\n",
      "Progress: 30.4% ... Training loss: 0.104 ... Validation loss: 0.195iteration: 3044\n",
      "train_loss: 0.10445001469448942\n",
      "val_loss: 0.19583592051909549\n",
      "Progress: 30.4% ... Training loss: 0.105 ... Validation loss: 0.210iteration: 3045\n",
      "train_loss: 0.10548262333669899\n",
      "val_loss: 0.21069979335268357\n",
      "Progress: 30.5% ... Training loss: 0.105 ... Validation loss: 0.192iteration: 3046\n",
      "train_loss: 0.1050050665733697\n",
      "val_loss: 0.19204491508530994\n",
      "Progress: 30.5% ... Training loss: 0.099 ... Validation loss: 0.207iteration: 3047\n",
      "train_loss: 0.09935083977249036\n",
      "val_loss: 0.2075936028584937\n",
      "Progress: 30.5% ... Training loss: 0.112 ... Validation loss: 0.194iteration: 3048\n",
      "train_loss: 0.11237723750201736\n",
      "val_loss: 0.19460490298224617\n",
      "Progress: 30.5% ... Training loss: 0.107 ... Validation loss: 0.224iteration: 3049\n",
      "train_loss: 0.10716943332175437\n",
      "val_loss: 0.2245293347703869\n",
      "Progress: 30.5% ... Training loss: 0.100 ... Validation loss: 0.193iteration: 3050\n",
      "train_loss: 0.10057243448475839\n",
      "val_loss: 0.19375591932091302\n",
      "Progress: 30.5% ... Training loss: 0.098 ... Validation loss: 0.196iteration: 3051\n",
      "train_loss: 0.09889333822302544\n",
      "val_loss: 0.19604579532621863\n",
      "Progress: 30.5% ... Training loss: 0.098 ... Validation loss: 0.199iteration: 3052\n",
      "train_loss: 0.09873613124503805\n",
      "val_loss: 0.1990776729319557\n",
      "Progress: 30.5% ... Training loss: 0.098 ... Validation loss: 0.199iteration: 3053\n",
      "train_loss: 0.09847354457761708\n",
      "val_loss: 0.19996242927936778\n",
      "Progress: 30.5% ... Training loss: 0.115 ... Validation loss: 0.195iteration: 3054\n",
      "train_loss: 0.11553885617677322\n",
      "val_loss: 0.19514208389606869\n",
      "Progress: 30.6% ... Training loss: 0.098 ... Validation loss: 0.204iteration: 3055\n",
      "train_loss: 0.09873427914969059\n",
      "val_loss: 0.2043132370445353\n",
      "Progress: 30.6% ... Training loss: 0.099 ... Validation loss: 0.193iteration: 3056\n",
      "train_loss: 0.09984758336964694\n",
      "val_loss: 0.19337134173285636\n",
      "Progress: 30.6% ... Training loss: 0.103 ... Validation loss: 0.204iteration: 3057\n",
      "train_loss: 0.10301991284045564\n",
      "val_loss: 0.2044796104990245\n",
      "Progress: 30.6% ... Training loss: 0.099 ... Validation loss: 0.200iteration: 3058\n",
      "train_loss: 0.09902739330409924\n",
      "val_loss: 0.20021938599857134\n",
      "Progress: 30.6% ... Training loss: 0.097 ... Validation loss: 0.200iteration: 3059\n",
      "train_loss: 0.09778032850730692\n",
      "val_loss: 0.20097285828413694\n",
      "Progress: 30.6% ... Training loss: 0.098 ... Validation loss: 0.202iteration: 3060\n",
      "train_loss: 0.09825553913360419\n",
      "val_loss: 0.20264078819353631\n",
      "Progress: 30.6% ... Training loss: 0.100 ... Validation loss: 0.200iteration: 3061\n",
      "train_loss: 0.10021236799711791\n",
      "val_loss: 0.20068583446980262\n",
      "Progress: 30.6% ... Training loss: 0.110 ... Validation loss: 0.196iteration: 3062\n",
      "train_loss: 0.11008402985387639\n",
      "val_loss: 0.1961374068968391\n",
      "Progress: 30.6% ... Training loss: 0.102 ... Validation loss: 0.208iteration: 3063\n",
      "train_loss: 0.10235689840221039\n",
      "val_loss: 0.20857629073491218\n",
      "Progress: 30.6% ... Training loss: 0.099 ... Validation loss: 0.196iteration: 3064\n",
      "train_loss: 0.09912419284392325\n",
      "val_loss: 0.19608949372176596\n",
      "Progress: 30.6% ... Training loss: 0.098 ... Validation loss: 0.200iteration: 3065\n",
      "train_loss: 0.09807136421458265\n",
      "val_loss: 0.20001151105927084\n",
      "Progress: 30.7% ... Training loss: 0.101 ... Validation loss: 0.200iteration: 3066\n",
      "train_loss: 0.1012674768263224\n",
      "val_loss: 0.20042647282584702\n",
      "Progress: 30.7% ... Training loss: 0.099 ... Validation loss: 0.195iteration: 3067\n",
      "train_loss: 0.09968976454577612\n",
      "val_loss: 0.1954320697233197\n",
      "Progress: 30.7% ... Training loss: 0.098 ... Validation loss: 0.193iteration: 3068\n",
      "train_loss: 0.09816956804451057\n",
      "val_loss: 0.19313510692924726\n",
      "Progress: 30.7% ... Training loss: 0.101 ... Validation loss: 0.194iteration: 3069\n",
      "train_loss: 0.10123449374834002\n",
      "val_loss: 0.1941501724119718\n",
      "Progress: 30.7% ... Training loss: 0.107 ... Validation loss: 0.214iteration: 3070\n",
      "train_loss: 0.10788561755581105\n",
      "val_loss: 0.21451051251784442\n",
      "Progress: 30.7% ... Training loss: 0.102 ... Validation loss: 0.195iteration: 3071\n",
      "train_loss: 0.1021730472826881\n",
      "val_loss: 0.19568732205744294\n",
      "Progress: 30.7% ... Training loss: 0.098 ... Validation loss: 0.196iteration: 3072\n",
      "train_loss: 0.09876722361084651\n",
      "val_loss: 0.19689093176863398\n",
      "Progress: 30.7% ... Training loss: 0.115 ... Validation loss: 0.200iteration: 3073\n",
      "train_loss: 0.11515583161945314\n",
      "val_loss: 0.2002016442879888\n",
      "Progress: 30.7% ... Training loss: 0.123 ... Validation loss: 0.235iteration: 3074\n",
      "train_loss: 0.12368120046487517\n",
      "val_loss: 0.23585827149311184\n",
      "Progress: 30.8% ... Training loss: 0.109 ... Validation loss: 0.194iteration: 3075\n",
      "train_loss: 0.1093223735227727\n",
      "val_loss: 0.1940761654543192\n",
      "Progress: 30.8% ... Training loss: 0.118 ... Validation loss: 0.217iteration: 3076\n",
      "train_loss: 0.11836858940377588\n",
      "val_loss: 0.2179214073312189\n",
      "Progress: 30.8% ... Training loss: 0.108 ... Validation loss: 0.192iteration: 3077\n",
      "train_loss: 0.10876736236001845\n",
      "val_loss: 0.1924365605599042\n",
      "Progress: 30.8% ... Training loss: 0.100 ... Validation loss: 0.201iteration: 3078\n",
      "train_loss: 0.10094077771202414\n",
      "val_loss: 0.2010720080692965\n",
      "Progress: 30.8% ... Training loss: 0.098 ... Validation loss: 0.192iteration: 3079\n",
      "train_loss: 0.0989182027520718\n",
      "val_loss: 0.19235238148498787\n",
      "Progress: 30.8% ... Training loss: 0.097 ... Validation loss: 0.190iteration: 3080\n",
      "train_loss: 0.09768922684545679\n",
      "val_loss: 0.1907901914355238\n",
      "Progress: 30.8% ... Training loss: 0.120 ... Validation loss: 0.206iteration: 3081\n",
      "train_loss: 0.12041028579571333\n",
      "val_loss: 0.20675986412050992\n",
      "Progress: 30.8% ... Training loss: 0.100 ... Validation loss: 0.204iteration: 3082\n",
      "train_loss: 0.10083241215816939\n",
      "val_loss: 0.2042240156344018\n",
      "Progress: 30.8% ... Training loss: 0.102 ... Validation loss: 0.191iteration: 3083\n",
      "train_loss: 0.10232132639661086\n",
      "val_loss: 0.1914677456462567\n",
      "Progress: 30.8% ... Training loss: 0.097 ... Validation loss: 0.195iteration: 3084\n",
      "train_loss: 0.09778480598582487\n",
      "val_loss: 0.19521962076277077\n",
      "Progress: 30.9% ... Training loss: 0.099 ... Validation loss: 0.199iteration: 3085\n",
      "train_loss: 0.0992479884588722\n",
      "val_loss: 0.19967893362653652\n",
      "Progress: 30.9% ... Training loss: 0.097 ... Validation loss: 0.194iteration: 3086\n",
      "train_loss: 0.09790751675333566\n",
      "val_loss: 0.19497919450518658\n",
      "Progress: 30.9% ... Training loss: 0.100 ... Validation loss: 0.191iteration: 3087\n",
      "train_loss: 0.10061590701282383\n",
      "val_loss: 0.19146881463488552\n",
      "Progress: 30.9% ... Training loss: 0.097 ... Validation loss: 0.204iteration: 3088\n",
      "train_loss: 0.09787470387661609\n",
      "val_loss: 0.20499078127818218\n",
      "Progress: 30.9% ... Training loss: 0.097 ... Validation loss: 0.199iteration: 3089\n",
      "train_loss: 0.09719068389188643\n",
      "val_loss: 0.19983297679201373\n",
      "Progress: 30.9% ... Training loss: 0.097 ... Validation loss: 0.204iteration: 3090\n",
      "train_loss: 0.09760201479300284\n",
      "val_loss: 0.2045765769701262\n",
      "Progress: 30.9% ... Training loss: 0.097 ... Validation loss: 0.200iteration: 3091\n",
      "train_loss: 0.09710842774070277\n",
      "val_loss: 0.20010368577101229\n",
      "Progress: 30.9% ... Training loss: 0.097 ... Validation loss: 0.197iteration: 3092\n",
      "train_loss: 0.09714918987654933\n",
      "val_loss: 0.19793290455150006\n",
      "Progress: 30.9% ... Training loss: 0.097 ... Validation loss: 0.190iteration: 3093\n",
      "train_loss: 0.09749638960213683\n",
      "val_loss: 0.1900345222116122\n",
      "Progress: 30.9% ... Training loss: 0.100 ... Validation loss: 0.200iteration: 3094\n",
      "train_loss: 0.1002389021217913\n",
      "val_loss: 0.2008514035615362\n",
      "Progress: 30.9% ... Training loss: 0.097 ... Validation loss: 0.199iteration: 3095\n",
      "train_loss: 0.0978805605940338\n",
      "val_loss: 0.19958358561133727\n",
      "Progress: 31.0% ... Training loss: 0.107 ... Validation loss: 0.223iteration: 3096\n",
      "train_loss: 0.107321120111314\n",
      "val_loss: 0.22313740433665225\n",
      "Progress: 31.0% ... Training loss: 0.106 ... Validation loss: 0.192iteration: 3097\n",
      "train_loss: 0.10694564991155202\n",
      "val_loss: 0.19230782132229107\n",
      "Progress: 31.0% ... Training loss: 0.107 ... Validation loss: 0.204iteration: 3098\n",
      "train_loss: 0.10718668473358417\n",
      "val_loss: 0.20412420208933718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 31.0% ... Training loss: 0.103 ... Validation loss: 0.191iteration: 3099\n",
      "train_loss: 0.10339328609244576\n",
      "val_loss: 0.1917571483362369\n",
      "Progress: 31.0% ... Training loss: 0.098 ... Validation loss: 0.196iteration: 3100\n",
      "train_loss: 0.09887737192065894\n",
      "val_loss: 0.19687086983741048\n",
      "Progress: 31.0% ... Training loss: 0.096 ... Validation loss: 0.195iteration: 3101\n",
      "train_loss: 0.09693958476033761\n",
      "val_loss: 0.19520202895849512\n",
      "Progress: 31.0% ... Training loss: 0.097 ... Validation loss: 0.194iteration: 3102\n",
      "train_loss: 0.09731437073060445\n",
      "val_loss: 0.19423557425151566\n",
      "Progress: 31.0% ... Training loss: 0.100 ... Validation loss: 0.194iteration: 3103\n",
      "train_loss: 0.10078533679525453\n",
      "val_loss: 0.19411384271145601\n",
      "Progress: 31.0% ... Training loss: 0.100 ... Validation loss: 0.205iteration: 3104\n",
      "train_loss: 0.10026202912503905\n",
      "val_loss: 0.20569878191075183\n",
      "Progress: 31.1% ... Training loss: 0.100 ... Validation loss: 0.187iteration: 3105\n",
      "train_loss: 0.1008666588308173\n",
      "val_loss: 0.1874784574428519\n",
      "Progress: 31.1% ... Training loss: 0.098 ... Validation loss: 0.200iteration: 3106\n",
      "train_loss: 0.09837617881068336\n",
      "val_loss: 0.2003766424315974\n",
      "Progress: 31.1% ... Training loss: 0.098 ... Validation loss: 0.193iteration: 3107\n",
      "train_loss: 0.09824636823427879\n",
      "val_loss: 0.19336119660513001\n",
      "Progress: 31.1% ... Training loss: 0.098 ... Validation loss: 0.207iteration: 3108\n",
      "train_loss: 0.09886619658532812\n",
      "val_loss: 0.20759206130684887\n",
      "Progress: 31.1% ... Training loss: 0.100 ... Validation loss: 0.194iteration: 3109\n",
      "train_loss: 0.10066021115597605\n",
      "val_loss: 0.19413607104816183\n",
      "Progress: 31.1% ... Training loss: 0.096 ... Validation loss: 0.201iteration: 3110\n",
      "train_loss: 0.09696178861341778\n",
      "val_loss: 0.2018081673208561\n",
      "Progress: 31.1% ... Training loss: 0.097 ... Validation loss: 0.195iteration: 3111\n",
      "train_loss: 0.09704101515034128\n",
      "val_loss: 0.19592910022822185\n",
      "Progress: 31.1% ... Training loss: 0.097 ... Validation loss: 0.197iteration: 3112\n",
      "train_loss: 0.09775623082281118\n",
      "val_loss: 0.19788378476819074\n",
      "Progress: 31.1% ... Training loss: 0.112 ... Validation loss: 0.200iteration: 3113\n",
      "train_loss: 0.11293417051069303\n",
      "val_loss: 0.20019516249042468\n",
      "Progress: 31.1% ... Training loss: 0.101 ... Validation loss: 0.209iteration: 3114\n",
      "train_loss: 0.1011291108751328\n",
      "val_loss: 0.20928069290533297\n",
      "Progress: 31.1% ... Training loss: 0.102 ... Validation loss: 0.194iteration: 3115\n",
      "train_loss: 0.1023581219578005\n",
      "val_loss: 0.1941600849641666\n",
      "Progress: 31.2% ... Training loss: 0.100 ... Validation loss: 0.211iteration: 3116\n",
      "train_loss: 0.10091423752243836\n",
      "val_loss: 0.21197452560406013\n",
      "Progress: 31.2% ... Training loss: 0.105 ... Validation loss: 0.193iteration: 3117\n",
      "train_loss: 0.10574113000785441\n",
      "val_loss: 0.19313202981539412\n",
      "Progress: 31.2% ... Training loss: 0.098 ... Validation loss: 0.199iteration: 3118\n",
      "train_loss: 0.09809601634717853\n",
      "val_loss: 0.1996520134902796\n",
      "Progress: 31.2% ... Training loss: 0.101 ... Validation loss: 0.194iteration: 3119\n",
      "train_loss: 0.10157878976398417\n",
      "val_loss: 0.1945076268826375\n",
      "Progress: 31.2% ... Training loss: 0.096 ... Validation loss: 0.194iteration: 3120\n",
      "train_loss: 0.09670717167348762\n",
      "val_loss: 0.19449443700954663\n",
      "Progress: 31.2% ... Training loss: 0.095 ... Validation loss: 0.196iteration: 3121\n",
      "train_loss: 0.09591664315671103\n",
      "val_loss: 0.19652993339137914\n",
      "Progress: 31.2% ... Training loss: 0.097 ... Validation loss: 0.203iteration: 3122\n",
      "train_loss: 0.09730029889181178\n",
      "val_loss: 0.20382787584855708\n",
      "Progress: 31.2% ... Training loss: 0.103 ... Validation loss: 0.190iteration: 3123\n",
      "train_loss: 0.10343743318354987\n",
      "val_loss: 0.19095417850589114\n",
      "Progress: 31.2% ... Training loss: 0.099 ... Validation loss: 0.210iteration: 3124\n",
      "train_loss: 0.09909744760620781\n",
      "val_loss: 0.2109990366504486\n",
      "Progress: 31.2% ... Training loss: 0.097 ... Validation loss: 0.203iteration: 3125\n",
      "train_loss: 0.09772809574639582\n",
      "val_loss: 0.20330007815722498\n",
      "Progress: 31.3% ... Training loss: 0.102 ... Validation loss: 0.189iteration: 3126\n",
      "train_loss: 0.1028217131567208\n",
      "val_loss: 0.1893212594527199\n",
      "Progress: 31.3% ... Training loss: 0.114 ... Validation loss: 0.227iteration: 3127\n",
      "train_loss: 0.11426439898772879\n",
      "val_loss: 0.22784694357157048\n",
      "Progress: 31.3% ... Training loss: 0.115 ... Validation loss: 0.199iteration: 3128\n",
      "train_loss: 0.11579489686861869\n",
      "val_loss: 0.1994631094523442\n",
      "Progress: 31.3% ... Training loss: 0.114 ... Validation loss: 0.242iteration: 3129\n",
      "train_loss: 0.11471349476363006\n",
      "val_loss: 0.24209079374238562\n",
      "Progress: 31.3% ... Training loss: 0.108 ... Validation loss: 0.190iteration: 3130\n",
      "train_loss: 0.10888404668794115\n",
      "val_loss: 0.1904124111803863\n",
      "Progress: 31.3% ... Training loss: 0.127 ... Validation loss: 0.227iteration: 3131\n",
      "train_loss: 0.1273139780599984\n",
      "val_loss: 0.2276203321986796\n",
      "Progress: 31.3% ... Training loss: 0.102 ... Validation loss: 0.187iteration: 3132\n",
      "train_loss: 0.10262526948957973\n",
      "val_loss: 0.18760586876474608\n",
      "Progress: 31.3% ... Training loss: 0.112 ... Validation loss: 0.214iteration: 3133\n",
      "train_loss: 0.11299009041356303\n",
      "val_loss: 0.2142014005863964\n",
      "Progress: 31.3% ... Training loss: 0.102 ... Validation loss: 0.187iteration: 3134\n",
      "train_loss: 0.10208760326766016\n",
      "val_loss: 0.18749214944605377\n",
      "Progress: 31.4% ... Training loss: 0.097 ... Validation loss: 0.201iteration: 3135\n",
      "train_loss: 0.09731723928697666\n",
      "val_loss: 0.20143559105116418\n",
      "Progress: 31.4% ... Training loss: 0.095 ... Validation loss: 0.196iteration: 3136\n",
      "train_loss: 0.09547220654311124\n",
      "val_loss: 0.1965469496189163\n",
      "Progress: 31.4% ... Training loss: 0.097 ... Validation loss: 0.200iteration: 3137\n",
      "train_loss: 0.09761936216655928\n",
      "val_loss: 0.20002997911003254\n",
      "Progress: 31.4% ... Training loss: 0.100 ... Validation loss: 0.195iteration: 3138\n",
      "train_loss: 0.10018421365883769\n",
      "val_loss: 0.19589449140798218\n",
      "Progress: 31.4% ... Training loss: 0.095 ... Validation loss: 0.195iteration: 3139\n",
      "train_loss: 0.09556213988087327\n",
      "val_loss: 0.19557467362528358\n",
      "Progress: 31.4% ... Training loss: 0.096 ... Validation loss: 0.199iteration: 3140\n",
      "train_loss: 0.09630721767265024\n",
      "val_loss: 0.19941817785809685\n",
      "Progress: 31.4% ... Training loss: 0.101 ... Validation loss: 0.194iteration: 3141\n",
      "train_loss: 0.10169252148328738\n",
      "val_loss: 0.1940942357791973\n",
      "Progress: 31.4% ... Training loss: 0.114 ... Validation loss: 0.229iteration: 3142\n",
      "train_loss: 0.11465057967878714\n",
      "val_loss: 0.22940126292619645\n",
      "Progress: 31.4% ... Training loss: 0.105 ... Validation loss: 0.192iteration: 3143\n",
      "train_loss: 0.10563431049179713\n",
      "val_loss: 0.1923079984340023\n",
      "Progress: 31.4% ... Training loss: 0.102 ... Validation loss: 0.200iteration: 3144\n",
      "train_loss: 0.10283658390967233\n",
      "val_loss: 0.2006968496524527\n",
      "Progress: 31.4% ... Training loss: 0.100 ... Validation loss: 0.196iteration: 3145\n",
      "train_loss: 0.10023340272941327\n",
      "val_loss: 0.19631865192068762\n",
      "Progress: 31.5% ... Training loss: 0.095 ... Validation loss: 0.193iteration: 3146\n",
      "train_loss: 0.09538759241028331\n",
      "val_loss: 0.1932500954096629\n",
      "Progress: 31.5% ... Training loss: 0.097 ... Validation loss: 0.199iteration: 3147\n",
      "train_loss: 0.09785350893175579\n",
      "val_loss: 0.19915989596872338\n",
      "Progress: 31.5% ... Training loss: 0.098 ... Validation loss: 0.211iteration: 3148\n",
      "train_loss: 0.09826345754169447\n",
      "val_loss: 0.21147554122170348\n",
      "Progress: 31.5% ... Training loss: 0.096 ... Validation loss: 0.193iteration: 3149\n",
      "train_loss: 0.09669475373919566\n",
      "val_loss: 0.19314897240297266\n",
      "Progress: 31.5% ... Training loss: 0.098 ... Validation loss: 0.200iteration: 3150\n",
      "train_loss: 0.09824529614001863\n",
      "val_loss: 0.20048589625141722\n",
      "Progress: 31.5% ... Training loss: 0.102 ... Validation loss: 0.189iteration: 3151\n",
      "train_loss: 0.10252190196649988\n",
      "val_loss: 0.18978008654030554\n",
      "Progress: 31.5% ... Training loss: 0.098 ... Validation loss: 0.210iteration: 3152\n",
      "train_loss: 0.09837083194067335\n",
      "val_loss: 0.2108442013995907\n",
      "Progress: 31.5% ... Training loss: 0.097 ... Validation loss: 0.192iteration: 3153\n",
      "train_loss: 0.09759941656345172\n",
      "val_loss: 0.19241080349852255\n",
      "Progress: 31.5% ... Training loss: 0.095 ... Validation loss: 0.189iteration: 3154\n",
      "train_loss: 0.09573493967035376\n",
      "val_loss: 0.18988472583247912\n",
      "Progress: 31.6% ... Training loss: 0.095 ... Validation loss: 0.191iteration: 3155\n",
      "train_loss: 0.09510349504510292\n",
      "val_loss: 0.19181440246334805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 31.6% ... Training loss: 0.096 ... Validation loss: 0.200iteration: 3156\n",
      "train_loss: 0.09615267840722082\n",
      "val_loss: 0.2001682808297351\n",
      "Progress: 31.6% ... Training loss: 0.100 ... Validation loss: 0.192iteration: 3157\n",
      "train_loss: 0.10033815113830867\n",
      "val_loss: 0.1927165626249152\n",
      "Progress: 31.6% ... Training loss: 0.104 ... Validation loss: 0.217iteration: 3158\n",
      "train_loss: 0.10469999135357531\n",
      "val_loss: 0.21730200447481998\n",
      "Progress: 31.6% ... Training loss: 0.095 ... Validation loss: 0.186iteration: 3159\n",
      "train_loss: 0.09522193494758138\n",
      "val_loss: 0.1865388248038757\n",
      "Progress: 31.6% ... Training loss: 0.096 ... Validation loss: 0.189iteration: 3160\n",
      "train_loss: 0.09694338531871381\n",
      "val_loss: 0.18975139863028403\n",
      "Progress: 31.6% ... Training loss: 0.095 ... Validation loss: 0.197iteration: 3161\n",
      "train_loss: 0.09500654386807558\n",
      "val_loss: 0.19790468982737378\n",
      "Progress: 31.6% ... Training loss: 0.101 ... Validation loss: 0.207iteration: 3162\n",
      "train_loss: 0.10157037952715557\n",
      "val_loss: 0.2071329003688877\n",
      "Progress: 31.6% ... Training loss: 0.103 ... Validation loss: 0.185iteration: 3163\n",
      "train_loss: 0.10330368696708812\n",
      "val_loss: 0.18556184634712\n",
      "Progress: 31.6% ... Training loss: 0.098 ... Validation loss: 0.212iteration: 3164\n",
      "train_loss: 0.09857716830396093\n",
      "val_loss: 0.2129900626119511\n",
      "Progress: 31.6% ... Training loss: 0.094 ... Validation loss: 0.197iteration: 3165\n",
      "train_loss: 0.09495496044341613\n",
      "val_loss: 0.19737743549931427\n",
      "Progress: 31.7% ... Training loss: 0.098 ... Validation loss: 0.214iteration: 3166\n",
      "train_loss: 0.09843564918743959\n",
      "val_loss: 0.21465600118302663\n",
      "Progress: 31.7% ... Training loss: 0.094 ... Validation loss: 0.186iteration: 3167\n",
      "train_loss: 0.09484292197012546\n",
      "val_loss: 0.18659725945551722\n",
      "Progress: 31.7% ... Training loss: 0.098 ... Validation loss: 0.212iteration: 3168\n",
      "train_loss: 0.09836162658631992\n",
      "val_loss: 0.21219596584864145\n",
      "Progress: 31.7% ... Training loss: 0.102 ... Validation loss: 0.183iteration: 3169\n",
      "train_loss: 0.10252815312879836\n",
      "val_loss: 0.18332321696898568\n",
      "Progress: 31.7% ... Training loss: 0.099 ... Validation loss: 0.209iteration: 3170\n",
      "train_loss: 0.09957062329430554\n",
      "val_loss: 0.20969560543139254\n",
      "Progress: 31.7% ... Training loss: 0.096 ... Validation loss: 0.198iteration: 3171\n",
      "train_loss: 0.09642151948519917\n",
      "val_loss: 0.19805927833078057\n",
      "Progress: 31.7% ... Training loss: 0.096 ... Validation loss: 0.200iteration: 3172\n",
      "train_loss: 0.09671722686620758\n",
      "val_loss: 0.2007663614677918\n",
      "Progress: 31.7% ... Training loss: 0.095 ... Validation loss: 0.190iteration: 3173\n",
      "train_loss: 0.09515290077055787\n",
      "val_loss: 0.19093080127196843\n",
      "Progress: 31.7% ... Training loss: 0.097 ... Validation loss: 0.200iteration: 3174\n",
      "train_loss: 0.09765808975525246\n",
      "val_loss: 0.2007908072066141\n",
      "Progress: 31.8% ... Training loss: 0.094 ... Validation loss: 0.194iteration: 3175\n",
      "train_loss: 0.09441794354107987\n",
      "val_loss: 0.19412809993636462\n",
      "Progress: 31.8% ... Training loss: 0.095 ... Validation loss: 0.196iteration: 3176\n",
      "train_loss: 0.09535039201148784\n",
      "val_loss: 0.1967082924768324\n",
      "Progress: 31.8% ... Training loss: 0.095 ... Validation loss: 0.198iteration: 3177\n",
      "train_loss: 0.09500987823018678\n",
      "val_loss: 0.19808228834917163\n",
      "Progress: 31.8% ... Training loss: 0.097 ... Validation loss: 0.192iteration: 3178\n",
      "train_loss: 0.09732781306048528\n",
      "val_loss: 0.1923079733986507\n",
      "Progress: 31.8% ... Training loss: 0.095 ... Validation loss: 0.192iteration: 3179\n",
      "train_loss: 0.09583871748171005\n",
      "val_loss: 0.1923701322828721\n",
      "Progress: 31.8% ... Training loss: 0.097 ... Validation loss: 0.207iteration: 3180\n",
      "train_loss: 0.09732884208159313\n",
      "val_loss: 0.2078730262137601\n",
      "Progress: 31.8% ... Training loss: 0.095 ... Validation loss: 0.185iteration: 3181\n",
      "train_loss: 0.09544162793258928\n",
      "val_loss: 0.18535868403438288\n",
      "Progress: 31.8% ... Training loss: 0.095 ... Validation loss: 0.186iteration: 3182\n",
      "train_loss: 0.095361840321491\n",
      "val_loss: 0.18601290633375475\n",
      "Progress: 31.8% ... Training loss: 0.097 ... Validation loss: 0.184iteration: 3183\n",
      "train_loss: 0.09771989431310148\n",
      "val_loss: 0.18462464130023334\n",
      "Progress: 31.8% ... Training loss: 0.100 ... Validation loss: 0.184iteration: 3184\n",
      "train_loss: 0.10036304715483747\n",
      "val_loss: 0.18418387603919434\n",
      "Progress: 31.9% ... Training loss: 0.109 ... Validation loss: 0.219iteration: 3185\n",
      "train_loss: 0.10985631721539538\n",
      "val_loss: 0.2192812155744659\n",
      "Progress: 31.9% ... Training loss: 0.111 ... Validation loss: 0.193iteration: 3186\n",
      "train_loss: 0.11160545406778233\n",
      "val_loss: 0.19397318404993572\n",
      "Progress: 31.9% ... Training loss: 0.112 ... Validation loss: 0.225iteration: 3187\n",
      "train_loss: 0.11236701439070364\n",
      "val_loss: 0.22535845750227085\n",
      "Progress: 31.9% ... Training loss: 0.095 ... Validation loss: 0.186iteration: 3188\n",
      "train_loss: 0.0958464479538399\n",
      "val_loss: 0.18600130562166742\n",
      "Progress: 31.9% ... Training loss: 0.094 ... Validation loss: 0.191iteration: 3189\n",
      "train_loss: 0.09415783979176316\n",
      "val_loss: 0.1914206646497948\n",
      "Progress: 31.9% ... Training loss: 0.097 ... Validation loss: 0.199iteration: 3190\n",
      "train_loss: 0.09728731318989832\n",
      "val_loss: 0.1994003725576669\n",
      "Progress: 31.9% ... Training loss: 0.094 ... Validation loss: 0.188iteration: 3191\n",
      "train_loss: 0.09452856741091617\n",
      "val_loss: 0.18877669495871258\n",
      "Progress: 31.9% ... Training loss: 0.095 ... Validation loss: 0.188iteration: 3192\n",
      "train_loss: 0.09500503201076611\n",
      "val_loss: 0.1882454689313216\n",
      "Progress: 31.9% ... Training loss: 0.094 ... Validation loss: 0.194iteration: 3193\n",
      "train_loss: 0.09416652480157016\n",
      "val_loss: 0.19491759326543068\n",
      "Progress: 31.9% ... Training loss: 0.095 ... Validation loss: 0.200iteration: 3194\n",
      "train_loss: 0.09527764512651384\n",
      "val_loss: 0.20068955400730468\n",
      "Progress: 31.9% ... Training loss: 0.099 ... Validation loss: 0.190iteration: 3195\n",
      "train_loss: 0.09928645832687293\n",
      "val_loss: 0.19032836767296976\n",
      "Progress: 32.0% ... Training loss: 0.101 ... Validation loss: 0.209iteration: 3196\n",
      "train_loss: 0.10158804126288021\n",
      "val_loss: 0.20953316504533676\n",
      "Progress: 32.0% ... Training loss: 0.096 ... Validation loss: 0.182iteration: 3197\n",
      "train_loss: 0.09651594038016059\n",
      "val_loss: 0.18296012950054014\n",
      "Progress: 32.0% ... Training loss: 0.094 ... Validation loss: 0.193iteration: 3198\n",
      "train_loss: 0.09421558856921823\n",
      "val_loss: 0.19351023267390907\n",
      "Progress: 32.0% ... Training loss: 0.094 ... Validation loss: 0.190iteration: 3199\n",
      "train_loss: 0.09496164579814423\n",
      "val_loss: 0.19053520316471595\n",
      "Progress: 32.0% ... Training loss: 0.110 ... Validation loss: 0.236iteration: 3200\n",
      "train_loss: 0.11060054754949455\n",
      "val_loss: 0.23681671174293506\n",
      "Progress: 32.0% ... Training loss: 0.094 ... Validation loss: 0.184iteration: 3201\n",
      "train_loss: 0.0943314959522172\n",
      "val_loss: 0.1845523060366827\n",
      "Progress: 32.0% ... Training loss: 0.093 ... Validation loss: 0.196iteration: 3202\n",
      "train_loss: 0.09363833997193347\n",
      "val_loss: 0.19625972090812105\n",
      "Progress: 32.0% ... Training loss: 0.096 ... Validation loss: 0.194iteration: 3203\n",
      "train_loss: 0.09609575066126552\n",
      "val_loss: 0.19495282987798307\n",
      "Progress: 32.0% ... Training loss: 0.096 ... Validation loss: 0.212iteration: 3204\n",
      "train_loss: 0.09605646810670272\n",
      "val_loss: 0.2124930066260289\n",
      "Progress: 32.0% ... Training loss: 0.095 ... Validation loss: 0.192iteration: 3205\n",
      "train_loss: 0.09518596447408602\n",
      "val_loss: 0.19223722007107563\n",
      "Progress: 32.1% ... Training loss: 0.094 ... Validation loss: 0.200iteration: 3206\n",
      "train_loss: 0.0947543278202909\n",
      "val_loss: 0.20008005295904485\n",
      "Progress: 32.1% ... Training loss: 0.094 ... Validation loss: 0.191iteration: 3207\n",
      "train_loss: 0.09428789870327452\n",
      "val_loss: 0.19187777774260795\n",
      "Progress: 32.1% ... Training loss: 0.094 ... Validation loss: 0.194iteration: 3208\n",
      "train_loss: 0.09414848761481151\n",
      "val_loss: 0.19455648774529363\n",
      "Progress: 32.1% ... Training loss: 0.099 ... Validation loss: 0.190iteration: 3209\n",
      "train_loss: 0.09970767981409032\n",
      "val_loss: 0.19056850780736573\n",
      "Progress: 32.1% ... Training loss: 0.105 ... Validation loss: 0.211iteration: 3210\n",
      "train_loss: 0.10590913788472149\n",
      "val_loss: 0.21127148271254353\n",
      "Progress: 32.1% ... Training loss: 0.105 ... Validation loss: 0.189iteration: 3211\n",
      "train_loss: 0.10592151013039569\n",
      "val_loss: 0.18905597132712756\n",
      "Progress: 32.1% ... Training loss: 0.094 ... Validation loss: 0.195iteration: 3212\n",
      "train_loss: 0.0944070200348788\n",
      "val_loss: 0.19596421363896077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 32.1% ... Training loss: 0.093 ... Validation loss: 0.186iteration: 3213\n",
      "train_loss: 0.09395293751608833\n",
      "val_loss: 0.18625135751883992\n",
      "Progress: 32.1% ... Training loss: 0.093 ... Validation loss: 0.190iteration: 3214\n",
      "train_loss: 0.09396927934944067\n",
      "val_loss: 0.1907075536921714\n",
      "Progress: 32.1% ... Training loss: 0.096 ... Validation loss: 0.199iteration: 3215\n",
      "train_loss: 0.09637122420187506\n",
      "val_loss: 0.19972160735456823\n",
      "Progress: 32.2% ... Training loss: 0.093 ... Validation loss: 0.193iteration: 3216\n",
      "train_loss: 0.09397847019326495\n",
      "val_loss: 0.1933248207097938\n",
      "Progress: 32.2% ... Training loss: 0.093 ... Validation loss: 0.195iteration: 3217\n",
      "train_loss: 0.09337977067494399\n",
      "val_loss: 0.19511287902056462\n",
      "Progress: 32.2% ... Training loss: 0.094 ... Validation loss: 0.184iteration: 3218\n",
      "train_loss: 0.09434533034956648\n",
      "val_loss: 0.18475867966097848\n",
      "Progress: 32.2% ... Training loss: 0.095 ... Validation loss: 0.185iteration: 3219\n",
      "train_loss: 0.09540935829981899\n",
      "val_loss: 0.1854249526814747\n",
      "Progress: 32.2% ... Training loss: 0.094 ... Validation loss: 0.202iteration: 3220\n",
      "train_loss: 0.09400414036498993\n",
      "val_loss: 0.20238479232309553\n",
      "Progress: 32.2% ... Training loss: 0.096 ... Validation loss: 0.190iteration: 3221\n",
      "train_loss: 0.0960051979385338\n",
      "val_loss: 0.1900345372013549\n",
      "Progress: 32.2% ... Training loss: 0.106 ... Validation loss: 0.221iteration: 3222\n",
      "train_loss: 0.10692720000877223\n",
      "val_loss: 0.22112170193828257\n",
      "Progress: 32.2% ... Training loss: 0.116 ... Validation loss: 0.193iteration: 3223\n",
      "train_loss: 0.11605060483668002\n",
      "val_loss: 0.1933996091393752\n",
      "Progress: 32.2% ... Training loss: 0.138 ... Validation loss: 0.280iteration: 3224\n",
      "train_loss: 0.1381713085542621\n",
      "val_loss: 0.28073836868050894\n",
      "Progress: 32.2% ... Training loss: 0.098 ... Validation loss: 0.190iteration: 3225\n",
      "train_loss: 0.09877355690672822\n",
      "val_loss: 0.19055293900411388\n",
      "Progress: 32.3% ... Training loss: 0.094 ... Validation loss: 0.200iteration: 3226\n",
      "train_loss: 0.09490509927731187\n",
      "val_loss: 0.2002716046450971\n",
      "Progress: 32.3% ... Training loss: 0.093 ... Validation loss: 0.194iteration: 3227\n",
      "train_loss: 0.09373003022109423\n",
      "val_loss: 0.19442699556287393\n",
      "Progress: 32.3% ... Training loss: 0.093 ... Validation loss: 0.186iteration: 3228\n",
      "train_loss: 0.09360450256058786\n",
      "val_loss: 0.18683722963041685\n",
      "Progress: 32.3% ... Training loss: 0.095 ... Validation loss: 0.198iteration: 3229\n",
      "train_loss: 0.09552039462613902\n",
      "val_loss: 0.19877600930855405\n",
      "Progress: 32.3% ... Training loss: 0.094 ... Validation loss: 0.190iteration: 3230\n",
      "train_loss: 0.09409402645185663\n",
      "val_loss: 0.1903778964094002\n",
      "Progress: 32.3% ... Training loss: 0.095 ... Validation loss: 0.183iteration: 3231\n",
      "train_loss: 0.0957046453469599\n",
      "val_loss: 0.18347247331830246\n",
      "Progress: 32.3% ... Training loss: 0.093 ... Validation loss: 0.195iteration: 3232\n",
      "train_loss: 0.09337080213457902\n",
      "val_loss: 0.1957867423775139\n",
      "Progress: 32.3% ... Training loss: 0.093 ... Validation loss: 0.199iteration: 3233\n",
      "train_loss: 0.09391113480125146\n",
      "val_loss: 0.1993888405741437\n",
      "Progress: 32.3% ... Training loss: 0.093 ... Validation loss: 0.187iteration: 3234\n",
      "train_loss: 0.09314872296427154\n",
      "val_loss: 0.18704716875443317\n",
      "Progress: 32.4% ... Training loss: 0.107 ... Validation loss: 0.206iteration: 3235\n",
      "train_loss: 0.10717396780065588\n",
      "val_loss: 0.20611501290944292\n",
      "Progress: 32.4% ... Training loss: 0.095 ... Validation loss: 0.179iteration: 3236\n",
      "train_loss: 0.09522506482618012\n",
      "val_loss: 0.17985746097590163\n",
      "Progress: 32.4% ... Training loss: 0.095 ... Validation loss: 0.189iteration: 3237\n",
      "train_loss: 0.09573234697748478\n",
      "val_loss: 0.1890623728506303\n",
      "Progress: 32.4% ... Training loss: 0.101 ... Validation loss: 0.183iteration: 3238\n",
      "train_loss: 0.1016912405170743\n",
      "val_loss: 0.18374106660267414\n",
      "Progress: 32.4% ... Training loss: 0.100 ... Validation loss: 0.199iteration: 3239\n",
      "train_loss: 0.10005258331600425\n",
      "val_loss: 0.1993718217774485\n",
      "Progress: 32.4% ... Training loss: 0.094 ... Validation loss: 0.183iteration: 3240\n",
      "train_loss: 0.09483471305350848\n",
      "val_loss: 0.183398938836463\n",
      "Progress: 32.4% ... Training loss: 0.098 ... Validation loss: 0.190iteration: 3241\n",
      "train_loss: 0.09851309003266727\n",
      "val_loss: 0.1902114139409422\n",
      "Progress: 32.4% ... Training loss: 0.108 ... Validation loss: 0.191iteration: 3242\n",
      "train_loss: 0.10894127341707983\n",
      "val_loss: 0.19195098593967752\n",
      "Progress: 32.4% ... Training loss: 0.095 ... Validation loss: 0.196iteration: 3243\n",
      "train_loss: 0.09561508221450375\n",
      "val_loss: 0.19679590799423174\n",
      "Progress: 32.4% ... Training loss: 0.092 ... Validation loss: 0.186iteration: 3244\n",
      "train_loss: 0.09224680126062687\n",
      "val_loss: 0.18678577825287873\n",
      "Progress: 32.5% ... Training loss: 0.094 ... Validation loss: 0.183iteration: 3245\n",
      "train_loss: 0.09457783603411625\n",
      "val_loss: 0.1830000942660892\n",
      "Progress: 32.5% ... Training loss: 0.094 ... Validation loss: 0.200iteration: 3246\n",
      "train_loss: 0.09470595153302656\n",
      "val_loss: 0.2006794116631267\n",
      "Progress: 32.5% ... Training loss: 0.092 ... Validation loss: 0.186iteration: 3247\n",
      "train_loss: 0.09255873979714076\n",
      "val_loss: 0.1863221125486409\n",
      "Progress: 32.5% ... Training loss: 0.091 ... Validation loss: 0.186iteration: 3248\n",
      "train_loss: 0.09193831807643224\n",
      "val_loss: 0.18602240841232742\n",
      "Progress: 32.5% ... Training loss: 0.094 ... Validation loss: 0.205iteration: 3249\n",
      "train_loss: 0.09454736628569642\n",
      "val_loss: 0.20542056316962143\n",
      "Progress: 32.5% ... Training loss: 0.095 ... Validation loss: 0.195iteration: 3250\n",
      "train_loss: 0.09572958420460975\n",
      "val_loss: 0.19575420711475272\n",
      "Progress: 32.5% ... Training loss: 0.092 ... Validation loss: 0.185iteration: 3251\n",
      "train_loss: 0.09288739553874092\n",
      "val_loss: 0.18525533921010845\n",
      "Progress: 32.5% ... Training loss: 0.091 ... Validation loss: 0.188iteration: 3252\n",
      "train_loss: 0.09173730650904482\n",
      "val_loss: 0.18893329370085\n",
      "Progress: 32.5% ... Training loss: 0.094 ... Validation loss: 0.184iteration: 3253\n",
      "train_loss: 0.09469853750594232\n",
      "val_loss: 0.18447225424733577\n",
      "Progress: 32.5% ... Training loss: 0.095 ... Validation loss: 0.179iteration: 3254\n",
      "train_loss: 0.09505653089803888\n",
      "val_loss: 0.17932798157420235\n",
      "Progress: 32.5% ... Training loss: 0.093 ... Validation loss: 0.194iteration: 3255\n",
      "train_loss: 0.09374936115775549\n",
      "val_loss: 0.19479199316677023\n",
      "Progress: 32.6% ... Training loss: 0.092 ... Validation loss: 0.183iteration: 3256\n",
      "train_loss: 0.09202163701301919\n",
      "val_loss: 0.18366431861662053\n",
      "Progress: 32.6% ... Training loss: 0.092 ... Validation loss: 0.188iteration: 3257\n",
      "train_loss: 0.09267882486679706\n",
      "val_loss: 0.18818944991737285\n",
      "Progress: 32.6% ... Training loss: 0.094 ... Validation loss: 0.182iteration: 3258\n",
      "train_loss: 0.09424203064895582\n",
      "val_loss: 0.18299848997746251\n",
      "Progress: 32.6% ... Training loss: 0.101 ... Validation loss: 0.197iteration: 3259\n",
      "train_loss: 0.10102442764887087\n",
      "val_loss: 0.19704023727944128\n",
      "Progress: 32.6% ... Training loss: 0.103 ... Validation loss: 0.184iteration: 3260\n",
      "train_loss: 0.1037968309596797\n",
      "val_loss: 0.18474031058693727\n",
      "Progress: 32.6% ... Training loss: 0.092 ... Validation loss: 0.190iteration: 3261\n",
      "train_loss: 0.09258802428123501\n",
      "val_loss: 0.1901279078668794\n",
      "Progress: 32.6% ... Training loss: 0.091 ... Validation loss: 0.182iteration: 3262\n",
      "train_loss: 0.09189476127553468\n",
      "val_loss: 0.18277270347392946\n",
      "Progress: 32.6% ... Training loss: 0.098 ... Validation loss: 0.199iteration: 3263\n",
      "train_loss: 0.09810248016591999\n",
      "val_loss: 0.19904767247755473\n",
      "Progress: 32.6% ... Training loss: 0.096 ... Validation loss: 0.179iteration: 3264\n",
      "train_loss: 0.09623887061351735\n",
      "val_loss: 0.17905119286017768\n",
      "Progress: 32.6% ... Training loss: 0.103 ... Validation loss: 0.214iteration: 3265\n",
      "train_loss: 0.10384530476401131\n",
      "val_loss: 0.21423842492251444\n",
      "Progress: 32.7% ... Training loss: 0.092 ... Validation loss: 0.179iteration: 3266\n",
      "train_loss: 0.09229207071632778\n",
      "val_loss: 0.17961315811124265\n",
      "Progress: 32.7% ... Training loss: 0.094 ... Validation loss: 0.183iteration: 3267\n",
      "train_loss: 0.09412832681526376\n",
      "val_loss: 0.1833988652292423\n",
      "Progress: 32.7% ... Training loss: 0.094 ... Validation loss: 0.200iteration: 3268\n",
      "train_loss: 0.09418254652226145\n",
      "val_loss: 0.20073527392983803\n",
      "Progress: 32.7% ... Training loss: 0.100 ... Validation loss: 0.182iteration: 3269\n",
      "train_loss: 0.10063732762520301\n",
      "val_loss: 0.18254993895404986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 32.7% ... Training loss: 0.105 ... Validation loss: 0.228iteration: 3270\n",
      "train_loss: 0.10512192362841624\n",
      "val_loss: 0.22889629386582236\n",
      "Progress: 32.7% ... Training loss: 0.098 ... Validation loss: 0.178iteration: 3271\n",
      "train_loss: 0.09811374497089642\n",
      "val_loss: 0.17822870291079104\n",
      "Progress: 32.7% ... Training loss: 0.100 ... Validation loss: 0.191iteration: 3272\n",
      "train_loss: 0.10081271131349863\n",
      "val_loss: 0.1917059238840515\n",
      "Progress: 32.7% ... Training loss: 0.092 ... Validation loss: 0.177iteration: 3273\n",
      "train_loss: 0.0921057836069752\n",
      "val_loss: 0.17718947222698694\n",
      "Progress: 32.7% ... Training loss: 0.091 ... Validation loss: 0.180iteration: 3274\n",
      "train_loss: 0.0916333393250891\n",
      "val_loss: 0.18006986346885298\n",
      "Progress: 32.8% ... Training loss: 0.091 ... Validation loss: 0.183iteration: 3275\n",
      "train_loss: 0.09106052763097579\n",
      "val_loss: 0.18359828603423975\n",
      "Progress: 32.8% ... Training loss: 0.091 ... Validation loss: 0.187iteration: 3276\n",
      "train_loss: 0.09173099412487147\n",
      "val_loss: 0.18782507858922162\n",
      "Progress: 32.8% ... Training loss: 0.092 ... Validation loss: 0.195iteration: 3277\n",
      "train_loss: 0.09235575070627344\n",
      "val_loss: 0.19502487982819222\n",
      "Progress: 32.8% ... Training loss: 0.094 ... Validation loss: 0.180iteration: 3278\n",
      "train_loss: 0.0946829504525502\n",
      "val_loss: 0.180799070748164\n",
      "Progress: 32.8% ... Training loss: 0.092 ... Validation loss: 0.195iteration: 3279\n",
      "train_loss: 0.09258992883777684\n",
      "val_loss: 0.19556537168575033\n",
      "Progress: 32.8% ... Training loss: 0.094 ... Validation loss: 0.183iteration: 3280\n",
      "train_loss: 0.0944919469361101\n",
      "val_loss: 0.1833175572652087\n",
      "Progress: 32.8% ... Training loss: 0.093 ... Validation loss: 0.188iteration: 3281\n",
      "train_loss: 0.09324921514443285\n",
      "val_loss: 0.18827398636239867\n",
      "Progress: 32.8% ... Training loss: 0.097 ... Validation loss: 0.196iteration: 3282\n",
      "train_loss: 0.09708538426781887\n",
      "val_loss: 0.19649989139934604\n",
      "Progress: 32.8% ... Training loss: 0.094 ... Validation loss: 0.182iteration: 3283\n",
      "train_loss: 0.09457862250592355\n",
      "val_loss: 0.18216215283312612\n",
      "Progress: 32.8% ... Training loss: 0.095 ... Validation loss: 0.187iteration: 3284\n",
      "train_loss: 0.09572768821692282\n",
      "val_loss: 0.18748543840828977\n",
      "Progress: 32.9% ... Training loss: 0.092 ... Validation loss: 0.181iteration: 3285\n",
      "train_loss: 0.09284158389721461\n",
      "val_loss: 0.1812029526980475\n",
      "Progress: 32.9% ... Training loss: 0.091 ... Validation loss: 0.183iteration: 3286\n",
      "train_loss: 0.09170877205957136\n",
      "val_loss: 0.18339026384421964\n",
      "Progress: 32.9% ... Training loss: 0.095 ... Validation loss: 0.179iteration: 3287\n",
      "train_loss: 0.09504762821891068\n",
      "val_loss: 0.179974211716508\n",
      "Progress: 32.9% ... Training loss: 0.096 ... Validation loss: 0.196iteration: 3288\n",
      "train_loss: 0.096964105228764\n",
      "val_loss: 0.1960426533075684\n",
      "Progress: 32.9% ... Training loss: 0.093 ... Validation loss: 0.185iteration: 3289\n",
      "train_loss: 0.0935183269116826\n",
      "val_loss: 0.18516501716880443\n",
      "Progress: 32.9% ... Training loss: 0.092 ... Validation loss: 0.181iteration: 3290\n",
      "train_loss: 0.09275610702602606\n",
      "val_loss: 0.18151435548972025\n",
      "Progress: 32.9% ... Training loss: 0.094 ... Validation loss: 0.179iteration: 3291\n",
      "train_loss: 0.09458462785392029\n",
      "val_loss: 0.17954895680237126\n",
      "Progress: 32.9% ... Training loss: 0.100 ... Validation loss: 0.190iteration: 3292\n",
      "train_loss: 0.10022272270136555\n",
      "val_loss: 0.19072752546830804\n",
      "Progress: 32.9% ... Training loss: 0.099 ... Validation loss: 0.183iteration: 3293\n",
      "train_loss: 0.0997970972813916\n",
      "val_loss: 0.18398904456098686\n",
      "Progress: 32.9% ... Training loss: 0.106 ... Validation loss: 0.205iteration: 3294\n",
      "train_loss: 0.1066628384797327\n",
      "val_loss: 0.20525076968829267\n",
      "Progress: 33.0% ... Training loss: 0.095 ... Validation loss: 0.180iteration: 3295\n",
      "train_loss: 0.09598434580737415\n",
      "val_loss: 0.18068470844976114\n",
      "Progress: 33.0% ... Training loss: 0.115 ... Validation loss: 0.219iteration: 3296\n",
      "train_loss: 0.11501870163381203\n",
      "val_loss: 0.21933766039630823\n",
      "Progress: 33.0% ... Training loss: 0.105 ... Validation loss: 0.186iteration: 3297\n",
      "train_loss: 0.10572242995890119\n",
      "val_loss: 0.18632454466444778\n",
      "Progress: 33.0% ... Training loss: 0.092 ... Validation loss: 0.184iteration: 3298\n",
      "train_loss: 0.09278140351908228\n",
      "val_loss: 0.18495820841701313\n",
      "Progress: 33.0% ... Training loss: 0.091 ... Validation loss: 0.188iteration: 3299\n",
      "train_loss: 0.09146994157950528\n",
      "val_loss: 0.1880936361351718\n",
      "Progress: 33.0% ... Training loss: 0.091 ... Validation loss: 0.180iteration: 3300\n",
      "train_loss: 0.09110144645703595\n",
      "val_loss: 0.1804978137361843\n",
      "Progress: 33.0% ... Training loss: 0.091 ... Validation loss: 0.184iteration: 3301\n",
      "train_loss: 0.09135061863802725\n",
      "val_loss: 0.1849844485652104\n",
      "Progress: 33.0% ... Training loss: 0.094 ... Validation loss: 0.188iteration: 3302\n",
      "train_loss: 0.09408370570218963\n",
      "val_loss: 0.1881360493992435\n",
      "Progress: 33.0% ... Training loss: 0.118 ... Validation loss: 0.188iteration: 3303\n",
      "train_loss: 0.11851682225516276\n",
      "val_loss: 0.1885018088229372\n",
      "Progress: 33.0% ... Training loss: 0.122 ... Validation loss: 0.247iteration: 3304\n",
      "train_loss: 0.12207688407220288\n",
      "val_loss: 0.2478833851246614\n",
      "Progress: 33.0% ... Training loss: 0.107 ... Validation loss: 0.185iteration: 3305\n",
      "train_loss: 0.10721645257826319\n",
      "val_loss: 0.18586513110922034\n",
      "Progress: 33.1% ... Training loss: 0.104 ... Validation loss: 0.219iteration: 3306\n",
      "train_loss: 0.10424143274165414\n",
      "val_loss: 0.21929514082074372\n",
      "Progress: 33.1% ... Training loss: 0.112 ... Validation loss: 0.186iteration: 3307\n",
      "train_loss: 0.11222285025798129\n",
      "val_loss: 0.18633469701011493\n",
      "Progress: 33.1% ... Training loss: 0.098 ... Validation loss: 0.206iteration: 3308\n",
      "train_loss: 0.09801069279472925\n",
      "val_loss: 0.2066437417999875\n",
      "Progress: 33.1% ... Training loss: 0.093 ... Validation loss: 0.177iteration: 3309\n",
      "train_loss: 0.09381126921771614\n",
      "val_loss: 0.177889674972204\n",
      "Progress: 33.1% ... Training loss: 0.093 ... Validation loss: 0.195iteration: 3310\n",
      "train_loss: 0.09380369296830343\n",
      "val_loss: 0.19573365924380184\n",
      "Progress: 33.1% ... Training loss: 0.093 ... Validation loss: 0.182iteration: 3311\n",
      "train_loss: 0.09344854225808868\n",
      "val_loss: 0.18256975756801244\n",
      "Progress: 33.1% ... Training loss: 0.093 ... Validation loss: 0.189iteration: 3312\n",
      "train_loss: 0.09394226586936834\n",
      "val_loss: 0.18946085682653024\n",
      "Progress: 33.1% ... Training loss: 0.091 ... Validation loss: 0.179iteration: 3313\n",
      "train_loss: 0.09141122366680464\n",
      "val_loss: 0.17967837987212484\n",
      "Progress: 33.1% ... Training loss: 0.092 ... Validation loss: 0.177iteration: 3314\n",
      "train_loss: 0.09210868332492665\n",
      "val_loss: 0.17791382430232702\n",
      "Progress: 33.1% ... Training loss: 0.090 ... Validation loss: 0.178iteration: 3315\n",
      "train_loss: 0.09029828959648706\n",
      "val_loss: 0.17829413838101008\n",
      "Progress: 33.2% ... Training loss: 0.090 ... Validation loss: 0.183iteration: 3316\n",
      "train_loss: 0.09063835807273903\n",
      "val_loss: 0.1833373841921266\n",
      "Progress: 33.2% ... Training loss: 0.092 ... Validation loss: 0.180iteration: 3317\n",
      "train_loss: 0.09286625088688032\n",
      "val_loss: 0.18069825580905008\n",
      "Progress: 33.2% ... Training loss: 0.092 ... Validation loss: 0.191iteration: 3318\n",
      "train_loss: 0.0929132018694732\n",
      "val_loss: 0.1910138980965166\n",
      "Progress: 33.2% ... Training loss: 0.090 ... Validation loss: 0.179iteration: 3319\n",
      "train_loss: 0.0903161181559922\n",
      "val_loss: 0.1792249070763073\n",
      "Progress: 33.2% ... Training loss: 0.090 ... Validation loss: 0.179iteration: 3320\n",
      "train_loss: 0.09057753007680253\n",
      "val_loss: 0.17987710126955928\n",
      "Progress: 33.2% ... Training loss: 0.091 ... Validation loss: 0.180iteration: 3321\n",
      "train_loss: 0.09160526952401939\n",
      "val_loss: 0.1802036900588297\n",
      "Progress: 33.2% ... Training loss: 0.089 ... Validation loss: 0.183iteration: 3322\n",
      "train_loss: 0.08968563309128423\n",
      "val_loss: 0.18357875424923556\n",
      "Progress: 33.2% ... Training loss: 0.095 ... Validation loss: 0.180iteration: 3323\n",
      "train_loss: 0.09505068803967757\n",
      "val_loss: 0.18047037108774894\n",
      "Progress: 33.2% ... Training loss: 0.092 ... Validation loss: 0.188iteration: 3324\n",
      "train_loss: 0.09260810584112303\n",
      "val_loss: 0.18892700800082515\n",
      "Progress: 33.2% ... Training loss: 0.090 ... Validation loss: 0.181iteration: 3325\n",
      "train_loss: 0.09049959961572919\n",
      "val_loss: 0.1815516452022792\n",
      "Progress: 33.3% ... Training loss: 0.090 ... Validation loss: 0.177iteration: 3326\n",
      "train_loss: 0.09085162938312183\n",
      "val_loss: 0.1778502580209464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 33.3% ... Training loss: 0.089 ... Validation loss: 0.179iteration: 3327\n",
      "train_loss: 0.08998525032198938\n",
      "val_loss: 0.1798580265397012\n",
      "Progress: 33.3% ... Training loss: 0.090 ... Validation loss: 0.179iteration: 3328\n",
      "train_loss: 0.09038494427413904\n",
      "val_loss: 0.17991601441545238\n",
      "Progress: 33.3% ... Training loss: 0.095 ... Validation loss: 0.189iteration: 3329\n",
      "train_loss: 0.09573448161684915\n",
      "val_loss: 0.18993224222992713\n",
      "Progress: 33.3% ... Training loss: 0.090 ... Validation loss: 0.180iteration: 3330\n",
      "train_loss: 0.09076255236557963\n",
      "val_loss: 0.18085052623563705\n",
      "Progress: 33.3% ... Training loss: 0.094 ... Validation loss: 0.178iteration: 3331\n",
      "train_loss: 0.09405839165209959\n",
      "val_loss: 0.17846067544118474\n",
      "Progress: 33.3% ... Training loss: 0.093 ... Validation loss: 0.185iteration: 3332\n",
      "train_loss: 0.09358390949734068\n",
      "val_loss: 0.1850457855978392\n",
      "Progress: 33.3% ... Training loss: 0.093 ... Validation loss: 0.181iteration: 3333\n",
      "train_loss: 0.09350874062325966\n",
      "val_loss: 0.18130428072228205\n",
      "Progress: 33.3% ... Training loss: 0.094 ... Validation loss: 0.193iteration: 3334\n",
      "train_loss: 0.0941161719156576\n",
      "val_loss: 0.1934342420416924\n",
      "Progress: 33.4% ... Training loss: 0.091 ... Validation loss: 0.185iteration: 3335\n",
      "train_loss: 0.09178944872086625\n",
      "val_loss: 0.18505800551733637\n",
      "Progress: 33.4% ... Training loss: 0.090 ... Validation loss: 0.187iteration: 3336\n",
      "train_loss: 0.09094686893925907\n",
      "val_loss: 0.18756902673250175\n",
      "Progress: 33.4% ... Training loss: 0.091 ... Validation loss: 0.176iteration: 3337\n",
      "train_loss: 0.0914017927395583\n",
      "val_loss: 0.1766947074974647\n",
      "Progress: 33.4% ... Training loss: 0.090 ... Validation loss: 0.177iteration: 3338\n",
      "train_loss: 0.09094992515743403\n",
      "val_loss: 0.17747605204277006\n",
      "Progress: 33.4% ... Training loss: 0.093 ... Validation loss: 0.187iteration: 3339\n",
      "train_loss: 0.09379503220187205\n",
      "val_loss: 0.18727426450325516\n",
      "Progress: 33.4% ... Training loss: 0.091 ... Validation loss: 0.178iteration: 3340\n",
      "train_loss: 0.09116659202824443\n",
      "val_loss: 0.17873728986846976\n",
      "Progress: 33.4% ... Training loss: 0.090 ... Validation loss: 0.183iteration: 3341\n",
      "train_loss: 0.09041986184501422\n",
      "val_loss: 0.18342241309861687\n",
      "Progress: 33.4% ... Training loss: 0.090 ... Validation loss: 0.185iteration: 3342\n",
      "train_loss: 0.09007326260936445\n",
      "val_loss: 0.18556558084359795\n",
      "Progress: 33.4% ... Training loss: 0.090 ... Validation loss: 0.180iteration: 3343\n",
      "train_loss: 0.09082691962844983\n",
      "val_loss: 0.18075326549185242\n",
      "Progress: 33.4% ... Training loss: 0.091 ... Validation loss: 0.183iteration: 3344\n",
      "train_loss: 0.0910387035168497\n",
      "val_loss: 0.18376433404701153\n",
      "Progress: 33.5% ... Training loss: 0.108 ... Validation loss: 0.187iteration: 3345\n",
      "train_loss: 0.1082267697420281\n",
      "val_loss: 0.18759037229003153\n",
      "Progress: 33.5% ... Training loss: 0.091 ... Validation loss: 0.196iteration: 3346\n",
      "train_loss: 0.091763881334792\n",
      "val_loss: 0.19618709453532468\n",
      "Progress: 33.5% ... Training loss: 0.089 ... Validation loss: 0.185iteration: 3347\n",
      "train_loss: 0.08960680410553905\n",
      "val_loss: 0.18523470125797298\n",
      "Progress: 33.5% ... Training loss: 0.097 ... Validation loss: 0.180iteration: 3348\n",
      "train_loss: 0.09712392687944135\n",
      "val_loss: 0.18069119174059528\n",
      "Progress: 33.5% ... Training loss: 0.089 ... Validation loss: 0.183iteration: 3349\n",
      "train_loss: 0.0896345163343003\n",
      "val_loss: 0.18320136757346098\n",
      "Progress: 33.5% ... Training loss: 0.089 ... Validation loss: 0.188iteration: 3350\n",
      "train_loss: 0.0896574103364221\n",
      "val_loss: 0.18824389577538483\n",
      "Progress: 33.5% ... Training loss: 0.091 ... Validation loss: 0.185iteration: 3351\n",
      "train_loss: 0.09187502019088631\n",
      "val_loss: 0.18588445014673904\n",
      "Progress: 33.5% ... Training loss: 0.091 ... Validation loss: 0.189iteration: 3352\n",
      "train_loss: 0.09127222468606366\n",
      "val_loss: 0.1891382038720949\n",
      "Progress: 33.5% ... Training loss: 0.089 ... Validation loss: 0.190iteration: 3353\n",
      "train_loss: 0.089721462671503\n",
      "val_loss: 0.1901215726534262\n",
      "Progress: 33.5% ... Training loss: 0.089 ... Validation loss: 0.187iteration: 3354\n",
      "train_loss: 0.08972942159515096\n",
      "val_loss: 0.18707121306686866\n",
      "Progress: 33.5% ... Training loss: 0.093 ... Validation loss: 0.185iteration: 3355\n",
      "train_loss: 0.09331815634487818\n",
      "val_loss: 0.18562408625900748\n",
      "Progress: 33.6% ... Training loss: 0.100 ... Validation loss: 0.200iteration: 3356\n",
      "train_loss: 0.10004452388614136\n",
      "val_loss: 0.20012668474368947\n",
      "Progress: 33.6% ... Training loss: 0.090 ... Validation loss: 0.180iteration: 3357\n",
      "train_loss: 0.09041414769386635\n",
      "val_loss: 0.18075055357161365\n",
      "Progress: 33.6% ... Training loss: 0.089 ... Validation loss: 0.184iteration: 3358\n",
      "train_loss: 0.08966426502979151\n",
      "val_loss: 0.18427267521287088\n",
      "Progress: 33.6% ... Training loss: 0.089 ... Validation loss: 0.183iteration: 3359\n",
      "train_loss: 0.08963947979193596\n",
      "val_loss: 0.18335135783215864\n",
      "Progress: 33.6% ... Training loss: 0.092 ... Validation loss: 0.181iteration: 3360\n",
      "train_loss: 0.09287935969816646\n",
      "val_loss: 0.18107883054205004\n",
      "Progress: 33.6% ... Training loss: 0.093 ... Validation loss: 0.194iteration: 3361\n",
      "train_loss: 0.09302246415977808\n",
      "val_loss: 0.19455241097741552\n",
      "Progress: 33.6% ... Training loss: 0.090 ... Validation loss: 0.182iteration: 3362\n",
      "train_loss: 0.09011681633139072\n",
      "val_loss: 0.18236145388285982\n",
      "Progress: 33.6% ... Training loss: 0.090 ... Validation loss: 0.190iteration: 3363\n",
      "train_loss: 0.09046788558776314\n",
      "val_loss: 0.19005777294387458\n",
      "Progress: 33.6% ... Training loss: 0.090 ... Validation loss: 0.181iteration: 3364\n",
      "train_loss: 0.09088795435695111\n",
      "val_loss: 0.181093750464331\n",
      "Progress: 33.6% ... Training loss: 0.102 ... Validation loss: 0.210iteration: 3365\n",
      "train_loss: 0.10288079138976404\n",
      "val_loss: 0.2108176825712119\n",
      "Progress: 33.7% ... Training loss: 0.095 ... Validation loss: 0.179iteration: 3366\n",
      "train_loss: 0.09579099013761871\n",
      "val_loss: 0.17961942939277944\n",
      "Progress: 33.7% ... Training loss: 0.107 ... Validation loss: 0.233iteration: 3367\n",
      "train_loss: 0.1072605699251837\n",
      "val_loss: 0.23358076524422877\n",
      "Progress: 33.7% ... Training loss: 0.091 ... Validation loss: 0.173iteration: 3368\n",
      "train_loss: 0.09177105408749861\n",
      "val_loss: 0.17328590486920398\n",
      "Progress: 33.7% ... Training loss: 0.092 ... Validation loss: 0.197iteration: 3369\n",
      "train_loss: 0.09256216318690026\n",
      "val_loss: 0.19790612759613257\n",
      "Progress: 33.7% ... Training loss: 0.090 ... Validation loss: 0.179iteration: 3370\n",
      "train_loss: 0.09004511384066802\n",
      "val_loss: 0.17962971355478502\n",
      "Progress: 33.7% ... Training loss: 0.089 ... Validation loss: 0.180iteration: 3371\n",
      "train_loss: 0.08917996874114137\n",
      "val_loss: 0.1802131831909828\n",
      "Progress: 33.7% ... Training loss: 0.089 ... Validation loss: 0.182iteration: 3372\n",
      "train_loss: 0.08953271625662\n",
      "val_loss: 0.18235696657113867\n",
      "Progress: 33.7% ... Training loss: 0.088 ... Validation loss: 0.180iteration: 3373\n",
      "train_loss: 0.08878429392851013\n",
      "val_loss: 0.1806619520904667\n",
      "Progress: 33.7% ... Training loss: 0.089 ... Validation loss: 0.182iteration: 3374\n",
      "train_loss: 0.0899754295792712\n",
      "val_loss: 0.182436725597855\n",
      "Progress: 33.8% ... Training loss: 0.090 ... Validation loss: 0.175iteration: 3375\n",
      "train_loss: 0.0903698641439487\n",
      "val_loss: 0.17599885313744354\n",
      "Progress: 33.8% ... Training loss: 0.089 ... Validation loss: 0.177iteration: 3376\n",
      "train_loss: 0.08931084881725476\n",
      "val_loss: 0.17718144877847591\n",
      "Progress: 33.8% ... Training loss: 0.089 ... Validation loss: 0.187iteration: 3377\n",
      "train_loss: 0.0894835991120871\n",
      "val_loss: 0.18757800155051807\n",
      "Progress: 33.8% ... Training loss: 0.100 ... Validation loss: 0.178iteration: 3378\n",
      "train_loss: 0.10023176601485628\n",
      "val_loss: 0.17866612926883793\n",
      "Progress: 33.8% ... Training loss: 0.097 ... Validation loss: 0.202iteration: 3379\n",
      "train_loss: 0.09746250377690133\n",
      "val_loss: 0.2023716466527005\n",
      "Progress: 33.8% ... Training loss: 0.099 ... Validation loss: 0.176iteration: 3380\n",
      "train_loss: 0.09980346093044343\n",
      "val_loss: 0.17687602869983385\n",
      "Progress: 33.8% ... Training loss: 0.097 ... Validation loss: 0.209iteration: 3381\n",
      "train_loss: 0.09731662977418527\n",
      "val_loss: 0.20960978100388267\n",
      "Progress: 33.8% ... Training loss: 0.097 ... Validation loss: 0.182iteration: 3382\n",
      "train_loss: 0.09746961032071286\n",
      "val_loss: 0.18259984546594354\n",
      "Progress: 33.8% ... Training loss: 0.095 ... Validation loss: 0.211iteration: 3383\n",
      "train_loss: 0.09539805434900042\n",
      "val_loss: 0.2111486573043379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 33.8% ... Training loss: 0.105 ... Validation loss: 0.185iteration: 3384\n",
      "train_loss: 0.10543282156042524\n",
      "val_loss: 0.18551447430051857\n",
      "Progress: 33.9% ... Training loss: 0.113 ... Validation loss: 0.231iteration: 3385\n",
      "train_loss: 0.11315975204687556\n",
      "val_loss: 0.23138315989931207\n",
      "Progress: 33.9% ... Training loss: 0.125 ... Validation loss: 0.195iteration: 3386\n",
      "train_loss: 0.12517716217667324\n",
      "val_loss: 0.19580326734601874\n",
      "Progress: 33.9% ... Training loss: 0.106 ... Validation loss: 0.217iteration: 3387\n",
      "train_loss: 0.1060727135125714\n",
      "val_loss: 0.21714336145734198\n",
      "Progress: 33.9% ... Training loss: 0.090 ... Validation loss: 0.181iteration: 3388\n",
      "train_loss: 0.09049540954909636\n",
      "val_loss: 0.18101888774278274\n",
      "Progress: 33.9% ... Training loss: 0.099 ... Validation loss: 0.215iteration: 3389\n",
      "train_loss: 0.09968032342540158\n",
      "val_loss: 0.21549720501258804\n",
      "Progress: 33.9% ... Training loss: 0.090 ... Validation loss: 0.183iteration: 3390\n",
      "train_loss: 0.09052412290647928\n",
      "val_loss: 0.18386565412486422\n",
      "Progress: 33.9% ... Training loss: 0.089 ... Validation loss: 0.186iteration: 3391\n",
      "train_loss: 0.0898192021080906\n",
      "val_loss: 0.18621255905339165\n",
      "Progress: 33.9% ... Training loss: 0.088 ... Validation loss: 0.183iteration: 3392\n",
      "train_loss: 0.0886689113959189\n",
      "val_loss: 0.18308935777482852\n",
      "Progress: 33.9% ... Training loss: 0.090 ... Validation loss: 0.197iteration: 3393\n",
      "train_loss: 0.09010410207484991\n",
      "val_loss: 0.19773834535132703\n",
      "Progress: 33.9% ... Training loss: 0.088 ... Validation loss: 0.194iteration: 3394\n",
      "train_loss: 0.08863793791199642\n",
      "val_loss: 0.1940040608799036\n",
      "Progress: 34.0% ... Training loss: 0.093 ... Validation loss: 0.179iteration: 3395\n",
      "train_loss: 0.0933184533989883\n",
      "val_loss: 0.17955028260491906\n",
      "Progress: 34.0% ... Training loss: 0.089 ... Validation loss: 0.195iteration: 3396\n",
      "train_loss: 0.08983716733721932\n",
      "val_loss: 0.195333212220956\n",
      "Progress: 34.0% ... Training loss: 0.088 ... Validation loss: 0.181iteration: 3397\n",
      "train_loss: 0.08863724783885674\n",
      "val_loss: 0.1814845026954799\n",
      "Progress: 34.0% ... Training loss: 0.087 ... Validation loss: 0.182iteration: 3398\n",
      "train_loss: 0.08798126308214366\n",
      "val_loss: 0.18212808888671306\n",
      "Progress: 34.0% ... Training loss: 0.090 ... Validation loss: 0.177iteration: 3399\n",
      "train_loss: 0.09014785941221765\n",
      "val_loss: 0.17723959307243534\n",
      "Progress: 34.0% ... Training loss: 0.091 ... Validation loss: 0.201iteration: 3400\n",
      "train_loss: 0.09124210823867246\n",
      "val_loss: 0.20163335921291617\n",
      "Progress: 34.0% ... Training loss: 0.088 ... Validation loss: 0.180iteration: 3401\n",
      "train_loss: 0.08897095129564885\n",
      "val_loss: 0.18042374508858602\n",
      "Progress: 34.0% ... Training loss: 0.089 ... Validation loss: 0.191iteration: 3402\n",
      "train_loss: 0.08945045488788601\n",
      "val_loss: 0.19139465132736566\n",
      "Progress: 34.0% ... Training loss: 0.088 ... Validation loss: 0.188iteration: 3403\n",
      "train_loss: 0.08887961579490945\n",
      "val_loss: 0.18885668861318922\n",
      "Progress: 34.0% ... Training loss: 0.088 ... Validation loss: 0.188iteration: 3404\n",
      "train_loss: 0.08852401052791511\n",
      "val_loss: 0.18853901264279116\n",
      "Progress: 34.0% ... Training loss: 0.089 ... Validation loss: 0.177iteration: 3405\n",
      "train_loss: 0.08993611427110837\n",
      "val_loss: 0.17777217281715527\n",
      "Progress: 34.1% ... Training loss: 0.092 ... Validation loss: 0.200iteration: 3406\n",
      "train_loss: 0.09285524195122224\n",
      "val_loss: 0.2008439897979525\n",
      "Progress: 34.1% ... Training loss: 0.090 ... Validation loss: 0.178iteration: 3407\n",
      "train_loss: 0.09069446008170623\n",
      "val_loss: 0.17869366228263656\n",
      "Progress: 34.1% ... Training loss: 0.091 ... Validation loss: 0.193iteration: 3408\n",
      "train_loss: 0.09185827413424741\n",
      "val_loss: 0.1930356110240704\n",
      "Progress: 34.1% ... Training loss: 0.088 ... Validation loss: 0.190iteration: 3409\n",
      "train_loss: 0.088936713801212\n",
      "val_loss: 0.1906183180555619\n",
      "Progress: 34.1% ... Training loss: 0.089 ... Validation loss: 0.185iteration: 3410\n",
      "train_loss: 0.08945482164461661\n",
      "val_loss: 0.1853068261165957\n",
      "Progress: 34.1% ... Training loss: 0.088 ... Validation loss: 0.177iteration: 3411\n",
      "train_loss: 0.08821810220098054\n",
      "val_loss: 0.17787183118747296\n",
      "Progress: 34.1% ... Training loss: 0.090 ... Validation loss: 0.189iteration: 3412\n",
      "train_loss: 0.09051502547078309\n",
      "val_loss: 0.1895987491941456\n",
      "Progress: 34.1% ... Training loss: 0.096 ... Validation loss: 0.192iteration: 3413\n",
      "train_loss: 0.09612753643945801\n",
      "val_loss: 0.19209390828608713\n",
      "Progress: 34.1% ... Training loss: 0.089 ... Validation loss: 0.177iteration: 3414\n",
      "train_loss: 0.08965262342165842\n",
      "val_loss: 0.1770384634298455\n",
      "Progress: 34.1% ... Training loss: 0.097 ... Validation loss: 0.176iteration: 3415\n",
      "train_loss: 0.09755211619410284\n",
      "val_loss: 0.17629927881178614\n",
      "Progress: 34.2% ... Training loss: 0.101 ... Validation loss: 0.179iteration: 3416\n",
      "train_loss: 0.101365452086592\n",
      "val_loss: 0.17931703186200082\n",
      "Progress: 34.2% ... Training loss: 0.125 ... Validation loss: 0.247iteration: 3417\n",
      "train_loss: 0.12576844020884367\n",
      "val_loss: 0.24713982303394183\n",
      "Progress: 34.2% ... Training loss: 0.124 ... Validation loss: 0.190iteration: 3418\n",
      "train_loss: 0.12479944798074241\n",
      "val_loss: 0.19030180782215084\n",
      "Progress: 34.2% ... Training loss: 0.127 ... Validation loss: 0.240iteration: 3419\n",
      "train_loss: 0.1279861396520716\n",
      "val_loss: 0.24049358001182708\n",
      "Progress: 34.2% ... Training loss: 0.108 ... Validation loss: 0.180iteration: 3420\n",
      "train_loss: 0.10844534121187419\n",
      "val_loss: 0.1808453113212694\n",
      "Progress: 34.2% ... Training loss: 0.098 ... Validation loss: 0.207iteration: 3421\n",
      "train_loss: 0.09859174195211222\n",
      "val_loss: 0.20784196765426632\n",
      "Progress: 34.2% ... Training loss: 0.101 ... Validation loss: 0.177iteration: 3422\n",
      "train_loss: 0.1011666430145522\n",
      "val_loss: 0.17766825636109357\n",
      "Progress: 34.2% ... Training loss: 0.093 ... Validation loss: 0.196iteration: 3423\n",
      "train_loss: 0.09379376526013652\n",
      "val_loss: 0.1964139450236664\n",
      "Progress: 34.2% ... Training loss: 0.102 ... Validation loss: 0.179iteration: 3424\n",
      "train_loss: 0.10208305401661742\n",
      "val_loss: 0.17942555594705742\n",
      "Progress: 34.2% ... Training loss: 0.100 ... Validation loss: 0.191iteration: 3425\n",
      "train_loss: 0.10094510119817811\n",
      "val_loss: 0.19130842107886675\n",
      "Progress: 34.3% ... Training loss: 0.109 ... Validation loss: 0.183iteration: 3426\n",
      "train_loss: 0.10939894724653476\n",
      "val_loss: 0.18333584968344274\n",
      "Progress: 34.3% ... Training loss: 0.117 ... Validation loss: 0.223iteration: 3427\n",
      "train_loss: 0.11709379490030981\n",
      "val_loss: 0.2237621857062824\n",
      "Progress: 34.3% ... Training loss: 0.111 ... Validation loss: 0.187iteration: 3428\n",
      "train_loss: 0.11167578486426251\n",
      "val_loss: 0.1874756954519105\n",
      "Progress: 34.3% ... Training loss: 0.092 ... Validation loss: 0.186iteration: 3429\n",
      "train_loss: 0.0924592298943\n",
      "val_loss: 0.18696535831525787\n",
      "Progress: 34.3% ... Training loss: 0.097 ... Validation loss: 0.180iteration: 3430\n",
      "train_loss: 0.09773057534854811\n",
      "val_loss: 0.1803322502888524\n",
      "Progress: 34.3% ... Training loss: 0.110 ... Validation loss: 0.210iteration: 3431\n",
      "train_loss: 0.1100199710798713\n",
      "val_loss: 0.2106048705767307\n",
      "Progress: 34.3% ... Training loss: 0.106 ... Validation loss: 0.184iteration: 3432\n",
      "train_loss: 0.10643347656501674\n",
      "val_loss: 0.18434824819200668\n",
      "Progress: 34.3% ... Training loss: 0.115 ... Validation loss: 0.210iteration: 3433\n",
      "train_loss: 0.11500917818950859\n",
      "val_loss: 0.21035447215766195\n",
      "Progress: 34.3% ... Training loss: 0.111 ... Validation loss: 0.188iteration: 3434\n",
      "train_loss: 0.11166368783851947\n",
      "val_loss: 0.18882514611529985\n",
      "Progress: 34.4% ... Training loss: 0.124 ... Validation loss: 0.242iteration: 3435\n",
      "train_loss: 0.12406917095982853\n",
      "val_loss: 0.24231508953236436\n",
      "Progress: 34.4% ... Training loss: 0.111 ... Validation loss: 0.184iteration: 3436\n",
      "train_loss: 0.11116576932728106\n",
      "val_loss: 0.1843202920813797\n",
      "Progress: 34.4% ... Training loss: 0.121 ... Validation loss: 0.239iteration: 3437\n",
      "train_loss: 0.12105691566957716\n",
      "val_loss: 0.23964115964310417\n",
      "Progress: 34.4% ... Training loss: 0.111 ... Validation loss: 0.183iteration: 3438\n",
      "train_loss: 0.11185401830223413\n",
      "val_loss: 0.18369228567798856\n",
      "Progress: 34.4% ... Training loss: 0.097 ... Validation loss: 0.209iteration: 3439\n",
      "train_loss: 0.09757249429352487\n",
      "val_loss: 0.2096283872146464\n",
      "Progress: 34.4% ... Training loss: 0.089 ... Validation loss: 0.178iteration: 3440\n",
      "train_loss: 0.08992225642470147\n",
      "val_loss: 0.1788412802022847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 34.4% ... Training loss: 0.091 ... Validation loss: 0.187iteration: 3441\n",
      "train_loss: 0.09166907797148238\n",
      "val_loss: 0.18768487005041734\n",
      "Progress: 34.4% ... Training loss: 0.102 ... Validation loss: 0.177iteration: 3442\n",
      "train_loss: 0.1027486737754445\n",
      "val_loss: 0.1779763575420389\n",
      "Progress: 34.4% ... Training loss: 0.098 ... Validation loss: 0.203iteration: 3443\n",
      "train_loss: 0.09803141142974792\n",
      "val_loss: 0.20355882226695282\n",
      "Progress: 34.4% ... Training loss: 0.101 ... Validation loss: 0.180iteration: 3444\n",
      "train_loss: 0.10122298557299886\n",
      "val_loss: 0.18098541220001815\n",
      "Progress: 34.5% ... Training loss: 0.104 ... Validation loss: 0.200iteration: 3445\n",
      "train_loss: 0.10410909313859756\n",
      "val_loss: 0.20091136840899493\n",
      "Progress: 34.5% ... Training loss: 0.093 ... Validation loss: 0.177iteration: 3446\n",
      "train_loss: 0.09367229689062001\n",
      "val_loss: 0.17740396397251498\n",
      "Progress: 34.5% ... Training loss: 0.086 ... Validation loss: 0.181iteration: 3447\n",
      "train_loss: 0.0868620560157311\n",
      "val_loss: 0.18175944578601042\n",
      "Progress: 34.5% ... Training loss: 0.089 ... Validation loss: 0.187iteration: 3448\n",
      "train_loss: 0.08963840159184307\n",
      "val_loss: 0.18784011923695446\n",
      "Progress: 34.5% ... Training loss: 0.092 ... Validation loss: 0.175iteration: 3449\n",
      "train_loss: 0.09228560786749147\n",
      "val_loss: 0.17523193162191325\n",
      "Progress: 34.5% ... Training loss: 0.090 ... Validation loss: 0.193iteration: 3450\n",
      "train_loss: 0.0903542031638458\n",
      "val_loss: 0.1936847934707477\n",
      "Progress: 34.5% ... Training loss: 0.097 ... Validation loss: 0.175iteration: 3451\n",
      "train_loss: 0.09731955262134584\n",
      "val_loss: 0.1758205628632411\n",
      "Progress: 34.5% ... Training loss: 0.094 ... Validation loss: 0.203iteration: 3452\n",
      "train_loss: 0.09458104967601831\n",
      "val_loss: 0.20312128371587343\n",
      "Progress: 34.5% ... Training loss: 0.086 ... Validation loss: 0.177iteration: 3453\n",
      "train_loss: 0.08684332834043124\n",
      "val_loss: 0.17748351459673295\n",
      "Progress: 34.5% ... Training loss: 0.087 ... Validation loss: 0.177iteration: 3454\n",
      "train_loss: 0.08752304843338819\n",
      "val_loss: 0.17795473200575923\n",
      "Progress: 34.5% ... Training loss: 0.088 ... Validation loss: 0.174iteration: 3455\n",
      "train_loss: 0.0886259001460035\n",
      "val_loss: 0.1749462505960314\n",
      "Progress: 34.6% ... Training loss: 0.091 ... Validation loss: 0.199iteration: 3456\n",
      "train_loss: 0.09164971230796831\n",
      "val_loss: 0.1999167047456414\n",
      "Progress: 34.6% ... Training loss: 0.087 ... Validation loss: 0.175iteration: 3457\n",
      "train_loss: 0.08757608279911043\n",
      "val_loss: 0.17539614013948082\n",
      "Progress: 34.6% ... Training loss: 0.087 ... Validation loss: 0.183iteration: 3458\n",
      "train_loss: 0.08739506632586948\n",
      "val_loss: 0.18379474090271866\n",
      "Progress: 34.6% ... Training loss: 0.090 ... Validation loss: 0.195iteration: 3459\n",
      "train_loss: 0.09098089680276382\n",
      "val_loss: 0.1958823422765846\n",
      "Progress: 34.6% ... Training loss: 0.092 ... Validation loss: 0.174iteration: 3460\n",
      "train_loss: 0.09261967833124911\n",
      "val_loss: 0.17480138297983758\n",
      "Progress: 34.6% ... Training loss: 0.092 ... Validation loss: 0.193iteration: 3461\n",
      "train_loss: 0.09260697549164086\n",
      "val_loss: 0.19387783269409148\n",
      "Progress: 34.6% ... Training loss: 0.090 ... Validation loss: 0.173iteration: 3462\n",
      "train_loss: 0.09003411897342861\n",
      "val_loss: 0.1732487679037963\n",
      "Progress: 34.6% ... Training loss: 0.090 ... Validation loss: 0.187iteration: 3463\n",
      "train_loss: 0.09045843889199985\n",
      "val_loss: 0.187076782009918\n",
      "Progress: 34.6% ... Training loss: 0.090 ... Validation loss: 0.176iteration: 3464\n",
      "train_loss: 0.0900359788178457\n",
      "val_loss: 0.17619540452558724\n",
      "Progress: 34.6% ... Training loss: 0.087 ... Validation loss: 0.173iteration: 3465\n",
      "train_loss: 0.08705311669577276\n",
      "val_loss: 0.1739598919825275\n",
      "Progress: 34.7% ... Training loss: 0.086 ... Validation loss: 0.176iteration: 3466\n",
      "train_loss: 0.08696984349195166\n",
      "val_loss: 0.17658135235450553\n",
      "Progress: 34.7% ... Training loss: 0.089 ... Validation loss: 0.178iteration: 3467\n",
      "train_loss: 0.0895743107761554\n",
      "val_loss: 0.1786358541407025\n",
      "Progress: 34.7% ... Training loss: 0.110 ... Validation loss: 0.229iteration: 3468\n",
      "train_loss: 0.11076947042865953\n",
      "val_loss: 0.2290093951538445\n",
      "Progress: 34.7% ... Training loss: 0.102 ... Validation loss: 0.173iteration: 3469\n",
      "train_loss: 0.10221785204868265\n",
      "val_loss: 0.17349139733194388\n",
      "Progress: 34.7% ... Training loss: 0.093 ... Validation loss: 0.208iteration: 3470\n",
      "train_loss: 0.09330989148920993\n",
      "val_loss: 0.20862765091537538\n",
      "Progress: 34.7% ... Training loss: 0.089 ... Validation loss: 0.173iteration: 3471\n",
      "train_loss: 0.0896780917834923\n",
      "val_loss: 0.1737713974871944\n",
      "Progress: 34.7% ... Training loss: 0.092 ... Validation loss: 0.194iteration: 3472\n",
      "train_loss: 0.09284193261160477\n",
      "val_loss: 0.19413230435691875\n",
      "Progress: 34.7% ... Training loss: 0.087 ... Validation loss: 0.176iteration: 3473\n",
      "train_loss: 0.08752917834740333\n",
      "val_loss: 0.17602208189227755\n",
      "Progress: 34.7% ... Training loss: 0.090 ... Validation loss: 0.173iteration: 3474\n",
      "train_loss: 0.09098793598175757\n",
      "val_loss: 0.17385330700196586\n",
      "Progress: 34.8% ... Training loss: 0.097 ... Validation loss: 0.212iteration: 3475\n",
      "train_loss: 0.0977000804516748\n",
      "val_loss: 0.21279875212972463\n",
      "Progress: 34.8% ... Training loss: 0.106 ... Validation loss: 0.180iteration: 3476\n",
      "train_loss: 0.10604931804258261\n",
      "val_loss: 0.18034103323443587\n",
      "Progress: 34.8% ... Training loss: 0.097 ... Validation loss: 0.213iteration: 3477\n",
      "train_loss: 0.09794943929525447\n",
      "val_loss: 0.213701512230096\n",
      "Progress: 34.8% ... Training loss: 0.103 ... Validation loss: 0.182iteration: 3478\n",
      "train_loss: 0.10353508963019382\n",
      "val_loss: 0.18237675401401027\n",
      "Progress: 34.8% ... Training loss: 0.126 ... Validation loss: 0.253iteration: 3479\n",
      "train_loss: 0.12682068282800715\n",
      "val_loss: 0.253444159401887\n",
      "Progress: 34.8% ... Training loss: 0.120 ... Validation loss: 0.190iteration: 3480\n",
      "train_loss: 0.12060408710121069\n",
      "val_loss: 0.1903135638382954\n",
      "Progress: 34.8% ... Training loss: 0.107 ... Validation loss: 0.219iteration: 3481\n",
      "train_loss: 0.10762919174142055\n",
      "val_loss: 0.21944961856007814\n",
      "Progress: 34.8% ... Training loss: 0.118 ... Validation loss: 0.185iteration: 3482\n",
      "train_loss: 0.11810671027939201\n",
      "val_loss: 0.18587619887539197\n",
      "Progress: 34.8% ... Training loss: 0.150 ... Validation loss: 0.282iteration: 3483\n",
      "train_loss: 0.1500510429881025\n",
      "val_loss: 0.2829156970879509\n",
      "Progress: 34.8% ... Training loss: 0.132 ... Validation loss: 0.191iteration: 3484\n",
      "train_loss: 0.13288067685298438\n",
      "val_loss: 0.19183985690516792\n",
      "Progress: 34.9% ... Training loss: 0.105 ... Validation loss: 0.218iteration: 3485\n",
      "train_loss: 0.1054380984559364\n",
      "val_loss: 0.21842001119433666\n",
      "Progress: 34.9% ... Training loss: 0.095 ... Validation loss: 0.170iteration: 3486\n",
      "train_loss: 0.09589966824024049\n",
      "val_loss: 0.17085521775438622\n",
      "Progress: 34.9% ... Training loss: 0.090 ... Validation loss: 0.195iteration: 3487\n",
      "train_loss: 0.09089274305152094\n",
      "val_loss: 0.1957320162988551\n",
      "Progress: 34.9% ... Training loss: 0.087 ... Validation loss: 0.176iteration: 3488\n",
      "train_loss: 0.08717674395891672\n",
      "val_loss: 0.17668454393670086\n",
      "Progress: 34.9% ... Training loss: 0.089 ... Validation loss: 0.194iteration: 3489\n",
      "train_loss: 0.08905388127683325\n",
      "val_loss: 0.19423929025883233\n",
      "Progress: 34.9% ... Training loss: 0.087 ... Validation loss: 0.178iteration: 3490\n",
      "train_loss: 0.08729132661555361\n",
      "val_loss: 0.17829832276861432\n",
      "Progress: 34.9% ... Training loss: 0.109 ... Validation loss: 0.239iteration: 3491\n",
      "train_loss: 0.10914879772614959\n",
      "val_loss: 0.23973860489313353\n",
      "Progress: 34.9% ... Training loss: 0.126 ... Validation loss: 0.187iteration: 3492\n",
      "train_loss: 0.12689577942395627\n",
      "val_loss: 0.187383522419096\n",
      "Progress: 34.9% ... Training loss: 0.105 ... Validation loss: 0.220iteration: 3493\n",
      "train_loss: 0.10500395252569084\n",
      "val_loss: 0.22031267499173493\n",
      "Progress: 34.9% ... Training loss: 0.100 ... Validation loss: 0.175iteration: 3494\n",
      "train_loss: 0.10018816920169378\n",
      "val_loss: 0.17596360429839855\n",
      "Progress: 35.0% ... Training loss: 0.089 ... Validation loss: 0.194iteration: 3495\n",
      "train_loss: 0.08968639866507036\n",
      "val_loss: 0.19487133139164028\n",
      "Progress: 35.0% ... Training loss: 0.089 ... Validation loss: 0.180iteration: 3496\n",
      "train_loss: 0.08966218920312147\n",
      "val_loss: 0.18057333880984427\n",
      "Progress: 35.0% ... Training loss: 0.087 ... Validation loss: 0.186iteration: 3497\n",
      "train_loss: 0.08748234510397727\n",
      "val_loss: 0.18695203572350583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 35.0% ... Training loss: 0.087 ... Validation loss: 0.175iteration: 3498\n",
      "train_loss: 0.08791053344847523\n",
      "val_loss: 0.17595295637302824\n",
      "Progress: 35.0% ... Training loss: 0.087 ... Validation loss: 0.189iteration: 3499\n",
      "train_loss: 0.08747325267184705\n",
      "val_loss: 0.18901463997329487\n",
      "Progress: 35.0% ... Training loss: 0.090 ... Validation loss: 0.173iteration: 3500\n",
      "train_loss: 0.09028569953214129\n",
      "val_loss: 0.17386742300598704\n",
      "Progress: 35.0% ... Training loss: 0.086 ... Validation loss: 0.179iteration: 3501\n",
      "train_loss: 0.08686833740271134\n",
      "val_loss: 0.17926374513037832\n",
      "Progress: 35.0% ... Training loss: 0.086 ... Validation loss: 0.175iteration: 3502\n",
      "train_loss: 0.08630942491532632\n",
      "val_loss: 0.17580020232910212\n",
      "Progress: 35.0% ... Training loss: 0.094 ... Validation loss: 0.203iteration: 3503\n",
      "train_loss: 0.09476700075245435\n",
      "val_loss: 0.2032007373516615\n",
      "Progress: 35.0% ... Training loss: 0.088 ... Validation loss: 0.175iteration: 3504\n",
      "train_loss: 0.08821431118192764\n",
      "val_loss: 0.17510539833070332\n",
      "Progress: 35.0% ... Training loss: 0.087 ... Validation loss: 0.193iteration: 3505\n",
      "train_loss: 0.08763115447218547\n",
      "val_loss: 0.1931550238780491\n",
      "Progress: 35.1% ... Training loss: 0.098 ... Validation loss: 0.177iteration: 3506\n",
      "train_loss: 0.09862276180115528\n",
      "val_loss: 0.17779249924778895\n",
      "Progress: 35.1% ... Training loss: 0.089 ... Validation loss: 0.189iteration: 3507\n",
      "train_loss: 0.08933314931218247\n",
      "val_loss: 0.1899493728478802\n",
      "Progress: 35.1% ... Training loss: 0.090 ... Validation loss: 0.178iteration: 3508\n",
      "train_loss: 0.09086769192754354\n",
      "val_loss: 0.17801898508450412\n",
      "Progress: 35.1% ... Training loss: 0.087 ... Validation loss: 0.186iteration: 3509\n",
      "train_loss: 0.08731918875509473\n",
      "val_loss: 0.18616497939560264\n",
      "Progress: 35.1% ... Training loss: 0.086 ... Validation loss: 0.180iteration: 3510\n",
      "train_loss: 0.08674511630246468\n",
      "val_loss: 0.1801856794999104\n",
      "Progress: 35.1% ... Training loss: 0.090 ... Validation loss: 0.185iteration: 3511\n",
      "train_loss: 0.09001939671316042\n",
      "val_loss: 0.1851137697111409\n",
      "Progress: 35.1% ... Training loss: 0.085 ... Validation loss: 0.178iteration: 3512\n",
      "train_loss: 0.08581209964523695\n",
      "val_loss: 0.17884039077456684\n",
      "Progress: 35.1% ... Training loss: 0.086 ... Validation loss: 0.173iteration: 3513\n",
      "train_loss: 0.08648189107251597\n",
      "val_loss: 0.17365104718338487\n",
      "Progress: 35.1% ... Training loss: 0.087 ... Validation loss: 0.188iteration: 3514\n",
      "train_loss: 0.0873781401853258\n",
      "val_loss: 0.1884039106032687\n",
      "Progress: 35.1% ... Training loss: 0.085 ... Validation loss: 0.181iteration: 3515\n",
      "train_loss: 0.0857491867314046\n",
      "val_loss: 0.18190514752591638\n",
      "Progress: 35.2% ... Training loss: 0.087 ... Validation loss: 0.187iteration: 3516\n",
      "train_loss: 0.08770824743758177\n",
      "val_loss: 0.18744197856969985\n",
      "Progress: 35.2% ... Training loss: 0.086 ... Validation loss: 0.173iteration: 3517\n",
      "train_loss: 0.08686091270781109\n",
      "val_loss: 0.17367349420490424\n",
      "Progress: 35.2% ... Training loss: 0.098 ... Validation loss: 0.174iteration: 3518\n",
      "train_loss: 0.09871744159021015\n",
      "val_loss: 0.174304435901823\n",
      "Progress: 35.2% ... Training loss: 0.098 ... Validation loss: 0.202iteration: 3519\n",
      "train_loss: 0.09833823897255961\n",
      "val_loss: 0.20235897130863717\n",
      "Progress: 35.2% ... Training loss: 0.088 ... Validation loss: 0.173iteration: 3520\n",
      "train_loss: 0.08876434524201557\n",
      "val_loss: 0.17335619708287792\n",
      "Progress: 35.2% ... Training loss: 0.094 ... Validation loss: 0.175iteration: 3521\n",
      "train_loss: 0.094157450061293\n",
      "val_loss: 0.1754984557516755\n",
      "Progress: 35.2% ... Training loss: 0.089 ... Validation loss: 0.190iteration: 3522\n",
      "train_loss: 0.08974994196935522\n",
      "val_loss: 0.19036292070028263\n",
      "Progress: 35.2% ... Training loss: 0.093 ... Validation loss: 0.175iteration: 3523\n",
      "train_loss: 0.09367245914181456\n",
      "val_loss: 0.175251408439377\n",
      "Progress: 35.2% ... Training loss: 0.087 ... Validation loss: 0.180iteration: 3524\n",
      "train_loss: 0.08715729311026209\n",
      "val_loss: 0.18062568502358947\n",
      "Progress: 35.2% ... Training loss: 0.085 ... Validation loss: 0.173iteration: 3525\n",
      "train_loss: 0.08575121918331613\n",
      "val_loss: 0.1737956980745179\n",
      "Progress: 35.3% ... Training loss: 0.085 ... Validation loss: 0.178iteration: 3526\n",
      "train_loss: 0.08581144318668456\n",
      "val_loss: 0.17879338170617654\n",
      "Progress: 35.3% ... Training loss: 0.090 ... Validation loss: 0.186iteration: 3527\n",
      "train_loss: 0.0902689996725019\n",
      "val_loss: 0.18642098280606587\n",
      "Progress: 35.3% ... Training loss: 0.085 ... Validation loss: 0.173iteration: 3528\n",
      "train_loss: 0.08596935922148866\n",
      "val_loss: 0.17366354909588866\n",
      "Progress: 35.3% ... Training loss: 0.087 ... Validation loss: 0.173iteration: 3529\n",
      "train_loss: 0.08722561114524434\n",
      "val_loss: 0.17387152500573838\n",
      "Progress: 35.3% ... Training loss: 0.100 ... Validation loss: 0.205iteration: 3530\n",
      "train_loss: 0.10097118351153303\n",
      "val_loss: 0.20597271765601313\n",
      "Progress: 35.3% ... Training loss: 0.087 ... Validation loss: 0.173iteration: 3531\n",
      "train_loss: 0.087038030773928\n",
      "val_loss: 0.17333157279905345\n",
      "Progress: 35.3% ... Training loss: 0.086 ... Validation loss: 0.175iteration: 3532\n",
      "train_loss: 0.08687311065526918\n",
      "val_loss: 0.17561769820521111\n",
      "Progress: 35.3% ... Training loss: 0.086 ... Validation loss: 0.179iteration: 3533\n",
      "train_loss: 0.08609550092708904\n",
      "val_loss: 0.17931792153750475\n",
      "Progress: 35.3% ... Training loss: 0.087 ... Validation loss: 0.176iteration: 3534\n",
      "train_loss: 0.08751947183541807\n",
      "val_loss: 0.17611967655167288\n",
      "Progress: 35.4% ... Training loss: 0.094 ... Validation loss: 0.175iteration: 3535\n",
      "train_loss: 0.09468874158970943\n",
      "val_loss: 0.17588800676826885\n",
      "Progress: 35.4% ... Training loss: 0.090 ... Validation loss: 0.197iteration: 3536\n",
      "train_loss: 0.09046778570496075\n",
      "val_loss: 0.19792245837444977\n",
      "Progress: 35.4% ... Training loss: 0.087 ... Validation loss: 0.173iteration: 3537\n",
      "train_loss: 0.08790628913343648\n",
      "val_loss: 0.17360879063552806\n",
      "Progress: 35.4% ... Training loss: 0.090 ... Validation loss: 0.190iteration: 3538\n",
      "train_loss: 0.09082998968085233\n",
      "val_loss: 0.19047835045555447\n",
      "Progress: 35.4% ... Training loss: 0.087 ... Validation loss: 0.169iteration: 3539\n",
      "train_loss: 0.08746780720594144\n",
      "val_loss: 0.16932021773892536\n",
      "Progress: 35.4% ... Training loss: 0.085 ... Validation loss: 0.173iteration: 3540\n",
      "train_loss: 0.08562559709976257\n",
      "val_loss: 0.17396710013418967\n",
      "Progress: 35.4% ... Training loss: 0.085 ... Validation loss: 0.173iteration: 3541\n",
      "train_loss: 0.08562874948403919\n",
      "val_loss: 0.1734829992044453\n",
      "Progress: 35.4% ... Training loss: 0.087 ... Validation loss: 0.186iteration: 3542\n",
      "train_loss: 0.08742660674097492\n",
      "val_loss: 0.1868783335091863\n",
      "Progress: 35.4% ... Training loss: 0.087 ... Validation loss: 0.173iteration: 3543\n",
      "train_loss: 0.08715570186305689\n",
      "val_loss: 0.17345733763568386\n",
      "Progress: 35.4% ... Training loss: 0.087 ... Validation loss: 0.182iteration: 3544\n",
      "train_loss: 0.0879114876038175\n",
      "val_loss: 0.1827618126347971\n",
      "Progress: 35.5% ... Training loss: 0.088 ... Validation loss: 0.169iteration: 3545\n",
      "train_loss: 0.08879569400796683\n",
      "val_loss: 0.16941091154693996\n",
      "Progress: 35.5% ... Training loss: 0.086 ... Validation loss: 0.171iteration: 3546\n",
      "train_loss: 0.0861454047177026\n",
      "val_loss: 0.1712646396037417\n",
      "Progress: 35.5% ... Training loss: 0.085 ... Validation loss: 0.170iteration: 3547\n",
      "train_loss: 0.08563685753763275\n",
      "val_loss: 0.17004258852278284\n",
      "Progress: 35.5% ... Training loss: 0.094 ... Validation loss: 0.199iteration: 3548\n",
      "train_loss: 0.09472680358816475\n",
      "val_loss: 0.19936758314050118\n",
      "Progress: 35.5% ... Training loss: 0.087 ... Validation loss: 0.168iteration: 3549\n",
      "train_loss: 0.08764118364368018\n",
      "val_loss: 0.16838971144996143\n",
      "Progress: 35.5% ... Training loss: 0.087 ... Validation loss: 0.183iteration: 3550\n",
      "train_loss: 0.0870272704029024\n",
      "val_loss: 0.1830359820653605\n",
      "Progress: 35.5% ... Training loss: 0.094 ... Validation loss: 0.176iteration: 3551\n",
      "train_loss: 0.09495864685603744\n",
      "val_loss: 0.17602013092288982\n",
      "Progress: 35.5% ... Training loss: 0.091 ... Validation loss: 0.203iteration: 3552\n",
      "train_loss: 0.09167625361596331\n",
      "val_loss: 0.2037502907099526\n",
      "Progress: 35.5% ... Training loss: 0.094 ... Validation loss: 0.173iteration: 3553\n",
      "train_loss: 0.09451395826126527\n",
      "val_loss: 0.17300217777581584\n",
      "Progress: 35.5% ... Training loss: 0.103 ... Validation loss: 0.213iteration: 3554\n",
      "train_loss: 0.1032288972986304\n",
      "val_loss: 0.21331972836336943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 35.5% ... Training loss: 0.100 ... Validation loss: 0.176iteration: 3555\n",
      "train_loss: 0.10025183689533294\n",
      "val_loss: 0.17659313986842362\n",
      "Progress: 35.6% ... Training loss: 0.097 ... Validation loss: 0.202iteration: 3556\n",
      "train_loss: 0.0970212154034727\n",
      "val_loss: 0.20231828459171705\n",
      "Progress: 35.6% ... Training loss: 0.096 ... Validation loss: 0.182iteration: 3557\n",
      "train_loss: 0.09695997841966278\n",
      "val_loss: 0.18239617732034516\n",
      "Progress: 35.6% ... Training loss: 0.092 ... Validation loss: 0.202iteration: 3558\n",
      "train_loss: 0.09209953160570901\n",
      "val_loss: 0.20243194120764688\n",
      "Progress: 35.6% ... Training loss: 0.087 ... Validation loss: 0.172iteration: 3559\n",
      "train_loss: 0.08750196096824153\n",
      "val_loss: 0.17235593330691193\n",
      "Progress: 35.6% ... Training loss: 0.093 ... Validation loss: 0.193iteration: 3560\n",
      "train_loss: 0.09328387006841093\n",
      "val_loss: 0.1938929079122418\n",
      "Progress: 35.6% ... Training loss: 0.092 ... Validation loss: 0.176iteration: 3561\n",
      "train_loss: 0.09228165587039751\n",
      "val_loss: 0.17689849629074403\n",
      "Progress: 35.6% ... Training loss: 0.095 ... Validation loss: 0.194iteration: 3562\n",
      "train_loss: 0.09547532016083579\n",
      "val_loss: 0.19473604028417651\n",
      "Progress: 35.6% ... Training loss: 0.087 ... Validation loss: 0.179iteration: 3563\n",
      "train_loss: 0.0870021400280162\n",
      "val_loss: 0.17915326590835207\n",
      "Progress: 35.6% ... Training loss: 0.089 ... Validation loss: 0.172iteration: 3564\n",
      "train_loss: 0.08954377609061645\n",
      "val_loss: 0.17228456488588667\n",
      "Progress: 35.6% ... Training loss: 0.088 ... Validation loss: 0.199iteration: 3565\n",
      "train_loss: 0.08805334494475668\n",
      "val_loss: 0.19951744700847926\n",
      "Progress: 35.7% ... Training loss: 0.085 ... Validation loss: 0.177iteration: 3566\n",
      "train_loss: 0.08563804864557181\n",
      "val_loss: 0.177331608981834\n",
      "Progress: 35.7% ... Training loss: 0.084 ... Validation loss: 0.186iteration: 3567\n",
      "train_loss: 0.08487045528883144\n",
      "val_loss: 0.18659286772338454\n",
      "Progress: 35.7% ... Training loss: 0.085 ... Validation loss: 0.189iteration: 3568\n",
      "train_loss: 0.08538530715623417\n",
      "val_loss: 0.1893912819977037\n",
      "Progress: 35.7% ... Training loss: 0.092 ... Validation loss: 0.196iteration: 3569\n",
      "train_loss: 0.09240730971011106\n",
      "val_loss: 0.19634547687466128\n",
      "Progress: 35.7% ... Training loss: 0.089 ... Validation loss: 0.175iteration: 3570\n",
      "train_loss: 0.08994355353111709\n",
      "val_loss: 0.17584848974187856\n",
      "Progress: 35.7% ... Training loss: 0.084 ... Validation loss: 0.181iteration: 3571\n",
      "train_loss: 0.08453459992823241\n",
      "val_loss: 0.18185867952280219\n",
      "Progress: 35.7% ... Training loss: 0.084 ... Validation loss: 0.183iteration: 3572\n",
      "train_loss: 0.08488416893127496\n",
      "val_loss: 0.1833322772190758\n",
      "Progress: 35.7% ... Training loss: 0.084 ... Validation loss: 0.182iteration: 3573\n",
      "train_loss: 0.08476545904752802\n",
      "val_loss: 0.18214845777474686\n",
      "Progress: 35.7% ... Training loss: 0.088 ... Validation loss: 0.177iteration: 3574\n",
      "train_loss: 0.08861603092254286\n",
      "val_loss: 0.17780936525938265\n",
      "Progress: 35.8% ... Training loss: 0.118 ... Validation loss: 0.228iteration: 3575\n",
      "train_loss: 0.11826722892038945\n",
      "val_loss: 0.22856251601827135\n",
      "Progress: 35.8% ... Training loss: 0.130 ... Validation loss: 0.197iteration: 3576\n",
      "train_loss: 0.13012251831621838\n",
      "val_loss: 0.19754045854657892\n",
      "Progress: 35.8% ... Training loss: 0.131 ... Validation loss: 0.259iteration: 3577\n",
      "train_loss: 0.1319899319139769\n",
      "val_loss: 0.25935097086522274\n",
      "Progress: 35.8% ... Training loss: 0.125 ... Validation loss: 0.190iteration: 3578\n",
      "train_loss: 0.12551787809892495\n",
      "val_loss: 0.19005257151961902\n",
      "Progress: 35.8% ... Training loss: 0.091 ... Validation loss: 0.203iteration: 3579\n",
      "train_loss: 0.0915903958364805\n",
      "val_loss: 0.2033627631206018\n",
      "Progress: 35.8% ... Training loss: 0.086 ... Validation loss: 0.175iteration: 3580\n",
      "train_loss: 0.08613054633588896\n",
      "val_loss: 0.17596769283819697\n",
      "Progress: 35.8% ... Training loss: 0.087 ... Validation loss: 0.196iteration: 3581\n",
      "train_loss: 0.08770428730550293\n",
      "val_loss: 0.1967143641170971\n",
      "Progress: 35.8% ... Training loss: 0.100 ... Validation loss: 0.178iteration: 3582\n",
      "train_loss: 0.10046251021551907\n",
      "val_loss: 0.1783510959450063\n",
      "Progress: 35.8% ... Training loss: 0.091 ... Validation loss: 0.206iteration: 3583\n",
      "train_loss: 0.09189249274766267\n",
      "val_loss: 0.2060302967808185\n",
      "Progress: 35.8% ... Training loss: 0.109 ... Validation loss: 0.183iteration: 3584\n",
      "train_loss: 0.1092815687352209\n",
      "val_loss: 0.18368682210107865\n",
      "Progress: 35.9% ... Training loss: 0.127 ... Validation loss: 0.271iteration: 3585\n",
      "train_loss: 0.1276550095063072\n",
      "val_loss: 0.2717155451011731\n",
      "Progress: 35.9% ... Training loss: 0.158 ... Validation loss: 0.202iteration: 3586\n",
      "train_loss: 0.1582446912961815\n",
      "val_loss: 0.20224670409737638\n",
      "Progress: 35.9% ... Training loss: 0.135 ... Validation loss: 0.270iteration: 3587\n",
      "train_loss: 0.13571927918393634\n",
      "val_loss: 0.27044753598451876\n",
      "Progress: 35.9% ... Training loss: 0.114 ... Validation loss: 0.183iteration: 3588\n",
      "train_loss: 0.11407038094448058\n",
      "val_loss: 0.18317085644378375\n",
      "Progress: 35.9% ... Training loss: 0.143 ... Validation loss: 0.278iteration: 3589\n",
      "train_loss: 0.14355449611185495\n",
      "val_loss: 0.2786403762240343\n",
      "Progress: 35.9% ... Training loss: 0.157 ... Validation loss: 0.199iteration: 3590\n",
      "train_loss: 0.15716860736210414\n",
      "val_loss: 0.19994634780114848\n",
      "Progress: 35.9% ... Training loss: 0.130 ... Validation loss: 0.262iteration: 3591\n",
      "train_loss: 0.13080247355407876\n",
      "val_loss: 0.2624838653844646\n",
      "Progress: 35.9% ... Training loss: 0.130 ... Validation loss: 0.197iteration: 3592\n",
      "train_loss: 0.13036422712571002\n",
      "val_loss: 0.1974541437057681\n",
      "Progress: 35.9% ... Training loss: 0.101 ... Validation loss: 0.222iteration: 3593\n",
      "train_loss: 0.10126982214328156\n",
      "val_loss: 0.2228532188789566\n",
      "Progress: 35.9% ... Training loss: 0.100 ... Validation loss: 0.181iteration: 3594\n",
      "train_loss: 0.10056094801930358\n",
      "val_loss: 0.1813012431728038\n",
      "Progress: 36.0% ... Training loss: 0.090 ... Validation loss: 0.211iteration: 3595\n",
      "train_loss: 0.09058030697307251\n",
      "val_loss: 0.21150465460219575\n",
      "Progress: 36.0% ... Training loss: 0.086 ... Validation loss: 0.181iteration: 3596\n",
      "train_loss: 0.08652171337255017\n",
      "val_loss: 0.18135877754004587\n",
      "Progress: 36.0% ... Training loss: 0.093 ... Validation loss: 0.194iteration: 3597\n",
      "train_loss: 0.09333384304002279\n",
      "val_loss: 0.19426956920136698\n",
      "Progress: 36.0% ... Training loss: 0.084 ... Validation loss: 0.184iteration: 3598\n",
      "train_loss: 0.08428498723815027\n",
      "val_loss: 0.18469983817482571\n",
      "Progress: 36.0% ... Training loss: 0.084 ... Validation loss: 0.183iteration: 3599\n",
      "train_loss: 0.084611270162937\n",
      "val_loss: 0.18320972186838935\n",
      "Progress: 36.0% ... Training loss: 0.085 ... Validation loss: 0.191iteration: 3600\n",
      "train_loss: 0.08599612381923209\n",
      "val_loss: 0.19171719551603228\n",
      "Progress: 36.0% ... Training loss: 0.090 ... Validation loss: 0.174iteration: 3601\n",
      "train_loss: 0.09071121719584549\n",
      "val_loss: 0.1748175801636376\n",
      "Progress: 36.0% ... Training loss: 0.094 ... Validation loss: 0.198iteration: 3602\n",
      "train_loss: 0.09413269643959374\n",
      "val_loss: 0.19896170169588448\n",
      "Progress: 36.0% ... Training loss: 0.105 ... Validation loss: 0.182iteration: 3603\n",
      "train_loss: 0.10564776612234018\n",
      "val_loss: 0.18289812004939981\n",
      "Progress: 36.0% ... Training loss: 0.100 ... Validation loss: 0.211iteration: 3604\n",
      "train_loss: 0.10060166709366888\n",
      "val_loss: 0.2116963632200489\n",
      "Progress: 36.0% ... Training loss: 0.092 ... Validation loss: 0.175iteration: 3605\n",
      "train_loss: 0.09218539714443154\n",
      "val_loss: 0.17506111713441344\n",
      "Progress: 36.1% ... Training loss: 0.085 ... Validation loss: 0.176iteration: 3606\n",
      "train_loss: 0.08521611701860185\n",
      "val_loss: 0.17644645724068053\n",
      "Progress: 36.1% ... Training loss: 0.085 ... Validation loss: 0.177iteration: 3607\n",
      "train_loss: 0.0854948514209275\n",
      "val_loss: 0.17759616253989471\n",
      "Progress: 36.1% ... Training loss: 0.085 ... Validation loss: 0.186iteration: 3608\n",
      "train_loss: 0.08584579773197953\n",
      "val_loss: 0.1863090285726002\n",
      "Progress: 36.1% ... Training loss: 0.085 ... Validation loss: 0.187iteration: 3609\n",
      "train_loss: 0.08505193697040987\n",
      "val_loss: 0.18761390000040237\n",
      "Progress: 36.1% ... Training loss: 0.084 ... Validation loss: 0.181iteration: 3610\n",
      "train_loss: 0.08452034653347672\n",
      "val_loss: 0.1813437045674159\n",
      "Progress: 36.1% ... Training loss: 0.084 ... Validation loss: 0.185iteration: 3611\n",
      "train_loss: 0.08477149236038643\n",
      "val_loss: 0.1850763277133169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 36.1% ... Training loss: 0.086 ... Validation loss: 0.173iteration: 3612\n",
      "train_loss: 0.08622348816887106\n",
      "val_loss: 0.17366134067138136\n",
      "Progress: 36.1% ... Training loss: 0.090 ... Validation loss: 0.193iteration: 3613\n",
      "train_loss: 0.09012597175721122\n",
      "val_loss: 0.19324431654091573\n",
      "Progress: 36.1% ... Training loss: 0.085 ... Validation loss: 0.172iteration: 3614\n",
      "train_loss: 0.08597695985529014\n",
      "val_loss: 0.1722163524385779\n",
      "Progress: 36.1% ... Training loss: 0.085 ... Validation loss: 0.176iteration: 3615\n",
      "train_loss: 0.08537572263464312\n",
      "val_loss: 0.1765461666821368\n",
      "Progress: 36.2% ... Training loss: 0.086 ... Validation loss: 0.172iteration: 3616\n",
      "train_loss: 0.08622532185006078\n",
      "val_loss: 0.17257357818962518\n",
      "Progress: 36.2% ... Training loss: 0.085 ... Validation loss: 0.173iteration: 3617\n",
      "train_loss: 0.08526533793537129\n",
      "val_loss: 0.17352800502720023\n",
      "Progress: 36.2% ... Training loss: 0.085 ... Validation loss: 0.186iteration: 3618\n",
      "train_loss: 0.08526829961006013\n",
      "val_loss: 0.18699786482673408\n",
      "Progress: 36.2% ... Training loss: 0.085 ... Validation loss: 0.173iteration: 3619\n",
      "train_loss: 0.08524385072030483\n",
      "val_loss: 0.17385323294107255\n",
      "Progress: 36.2% ... Training loss: 0.084 ... Validation loss: 0.177iteration: 3620\n",
      "train_loss: 0.08490193459363306\n",
      "val_loss: 0.17777721590393467\n",
      "Progress: 36.2% ... Training loss: 0.085 ... Validation loss: 0.184iteration: 3621\n",
      "train_loss: 0.08561552952159814\n",
      "val_loss: 0.18478580924382845\n",
      "Progress: 36.2% ... Training loss: 0.086 ... Validation loss: 0.169iteration: 3622\n",
      "train_loss: 0.08602647014003287\n",
      "val_loss: 0.16973726500277506\n",
      "Progress: 36.2% ... Training loss: 0.087 ... Validation loss: 0.188iteration: 3623\n",
      "train_loss: 0.08784903186166698\n",
      "val_loss: 0.1889039935583703\n",
      "Progress: 36.2% ... Training loss: 0.086 ... Validation loss: 0.177iteration: 3624\n",
      "train_loss: 0.08629837332857047\n",
      "val_loss: 0.17771281838117378\n",
      "Progress: 36.2% ... Training loss: 0.085 ... Validation loss: 0.196iteration: 3625\n",
      "train_loss: 0.08509897260044252\n",
      "val_loss: 0.19679857201985587\n",
      "Progress: 36.3% ... Training loss: 0.086 ... Validation loss: 0.173iteration: 3626\n",
      "train_loss: 0.08617488509309466\n",
      "val_loss: 0.17355164803761716\n",
      "Progress: 36.3% ... Training loss: 0.085 ... Validation loss: 0.182iteration: 3627\n",
      "train_loss: 0.08508719014871673\n",
      "val_loss: 0.18250685858197163\n",
      "Progress: 36.3% ... Training loss: 0.085 ... Validation loss: 0.191iteration: 3628\n",
      "train_loss: 0.08519926892973836\n",
      "val_loss: 0.1914334572997222\n",
      "Progress: 36.3% ... Training loss: 0.088 ... Validation loss: 0.176iteration: 3629\n",
      "train_loss: 0.08847400720036591\n",
      "val_loss: 0.17647679397699736\n",
      "Progress: 36.3% ... Training loss: 0.088 ... Validation loss: 0.176iteration: 3630\n",
      "train_loss: 0.08893064464331132\n",
      "val_loss: 0.1763307196320468\n",
      "Progress: 36.3% ... Training loss: 0.083 ... Validation loss: 0.181iteration: 3631\n",
      "train_loss: 0.08378081197051968\n",
      "val_loss: 0.18120660265566368\n",
      "Progress: 36.3% ... Training loss: 0.087 ... Validation loss: 0.180iteration: 3632\n",
      "train_loss: 0.08788004345856944\n",
      "val_loss: 0.18071792392444275\n",
      "Progress: 36.3% ... Training loss: 0.089 ... Validation loss: 0.193iteration: 3633\n",
      "train_loss: 0.08953009816449234\n",
      "val_loss: 0.19397562762883291\n",
      "Progress: 36.3% ... Training loss: 0.101 ... Validation loss: 0.181iteration: 3634\n",
      "train_loss: 0.10104223311695329\n",
      "val_loss: 0.1811073619207299\n",
      "Progress: 36.4% ... Training loss: 0.112 ... Validation loss: 0.219iteration: 3635\n",
      "train_loss: 0.11289629643241522\n",
      "val_loss: 0.2198906968691614\n",
      "Progress: 36.4% ... Training loss: 0.091 ... Validation loss: 0.176iteration: 3636\n",
      "train_loss: 0.09195514854825206\n",
      "val_loss: 0.17624326286263217\n",
      "Progress: 36.4% ... Training loss: 0.091 ... Validation loss: 0.191iteration: 3637\n",
      "train_loss: 0.09177929187456328\n",
      "val_loss: 0.19175104364950019\n",
      "Progress: 36.4% ... Training loss: 0.088 ... Validation loss: 0.183iteration: 3638\n",
      "train_loss: 0.08846304765369493\n",
      "val_loss: 0.1835508953556234\n",
      "Progress: 36.4% ... Training loss: 0.085 ... Validation loss: 0.173iteration: 3639\n",
      "train_loss: 0.08550346514083443\n",
      "val_loss: 0.17395425716460605\n",
      "Progress: 36.4% ... Training loss: 0.085 ... Validation loss: 0.181iteration: 3640\n",
      "train_loss: 0.08569252645572763\n",
      "val_loss: 0.18187264336299722\n",
      "Progress: 36.4% ... Training loss: 0.084 ... Validation loss: 0.177iteration: 3641\n",
      "train_loss: 0.08444556475958889\n",
      "val_loss: 0.17773369444337797\n",
      "Progress: 36.4% ... Training loss: 0.085 ... Validation loss: 0.176iteration: 3642\n",
      "train_loss: 0.08502845417203658\n",
      "val_loss: 0.17638976826964273\n",
      "Progress: 36.4% ... Training loss: 0.084 ... Validation loss: 0.184iteration: 3643\n",
      "train_loss: 0.084906660622604\n",
      "val_loss: 0.1849942439588138\n",
      "Progress: 36.4% ... Training loss: 0.084 ... Validation loss: 0.169iteration: 3644\n",
      "train_loss: 0.08497549531457846\n",
      "val_loss: 0.16905501442396875\n",
      "Progress: 36.5% ... Training loss: 0.085 ... Validation loss: 0.178iteration: 3645\n",
      "train_loss: 0.08513755884382533\n",
      "val_loss: 0.17813125455149773\n",
      "Progress: 36.5% ... Training loss: 0.083 ... Validation loss: 0.171iteration: 3646\n",
      "train_loss: 0.08321307871267702\n",
      "val_loss: 0.17163714178079165\n",
      "Progress: 36.5% ... Training loss: 0.083 ... Validation loss: 0.175iteration: 3647\n",
      "train_loss: 0.08329066325930136\n",
      "val_loss: 0.17520552816919763\n",
      "Progress: 36.5% ... Training loss: 0.083 ... Validation loss: 0.175iteration: 3648\n",
      "train_loss: 0.08385835285307358\n",
      "val_loss: 0.17532180584100335\n",
      "Progress: 36.5% ... Training loss: 0.083 ... Validation loss: 0.178iteration: 3649\n",
      "train_loss: 0.08338799804159139\n",
      "val_loss: 0.17898350555956938\n",
      "Progress: 36.5% ... Training loss: 0.083 ... Validation loss: 0.172iteration: 3650\n",
      "train_loss: 0.08395796886080026\n",
      "val_loss: 0.1728663223496865\n",
      "Progress: 36.5% ... Training loss: 0.083 ... Validation loss: 0.175iteration: 3651\n",
      "train_loss: 0.08346835606803561\n",
      "val_loss: 0.17549529475227077\n",
      "Progress: 36.5% ... Training loss: 0.088 ... Validation loss: 0.201iteration: 3652\n",
      "train_loss: 0.08829601276439349\n",
      "val_loss: 0.20116965318185737\n",
      "Progress: 36.5% ... Training loss: 0.099 ... Validation loss: 0.174iteration: 3653\n",
      "train_loss: 0.09914498547634949\n",
      "val_loss: 0.17471712585640628\n",
      "Progress: 36.5% ... Training loss: 0.094 ... Validation loss: 0.212iteration: 3654\n",
      "train_loss: 0.09460002596335666\n",
      "val_loss: 0.21232341761611132\n",
      "Progress: 36.5% ... Training loss: 0.084 ... Validation loss: 0.172iteration: 3655\n",
      "train_loss: 0.08499163489896593\n",
      "val_loss: 0.17273759598291427\n",
      "Progress: 36.6% ... Training loss: 0.083 ... Validation loss: 0.176iteration: 3656\n",
      "train_loss: 0.08363610355326753\n",
      "val_loss: 0.176913142108916\n",
      "Progress: 36.6% ... Training loss: 0.082 ... Validation loss: 0.173iteration: 3657\n",
      "train_loss: 0.08273932695113001\n",
      "val_loss: 0.1732383289357444\n",
      "Progress: 36.6% ... Training loss: 0.094 ... Validation loss: 0.169iteration: 3658\n",
      "train_loss: 0.09415359846619882\n",
      "val_loss: 0.16957123873809213\n",
      "Progress: 36.6% ... Training loss: 0.096 ... Validation loss: 0.197iteration: 3659\n",
      "train_loss: 0.09612531854995779\n",
      "val_loss: 0.19751678296861136\n",
      "Progress: 36.6% ... Training loss: 0.093 ... Validation loss: 0.169iteration: 3660\n",
      "train_loss: 0.09339514370667719\n",
      "val_loss: 0.16963973417642597\n",
      "Progress: 36.6% ... Training loss: 0.102 ... Validation loss: 0.207iteration: 3661\n",
      "train_loss: 0.10294105719512486\n",
      "val_loss: 0.20774037403986212\n",
      "Progress: 36.6% ... Training loss: 0.092 ... Validation loss: 0.167iteration: 3662\n",
      "train_loss: 0.09251847185638686\n",
      "val_loss: 0.16749593387261485\n",
      "Progress: 36.6% ... Training loss: 0.091 ... Validation loss: 0.197iteration: 3663\n",
      "train_loss: 0.09147562969755738\n",
      "val_loss: 0.19773529885253316\n",
      "Progress: 36.6% ... Training loss: 0.084 ... Validation loss: 0.165iteration: 3664\n",
      "train_loss: 0.0849292308477835\n",
      "val_loss: 0.16546337108359274\n",
      "Progress: 36.6% ... Training loss: 0.086 ... Validation loss: 0.164iteration: 3665\n",
      "train_loss: 0.08683378783982428\n",
      "val_loss: 0.16498704016388854\n",
      "Progress: 36.7% ... Training loss: 0.087 ... Validation loss: 0.178iteration: 3666\n",
      "train_loss: 0.08726447828252415\n",
      "val_loss: 0.17830343864502432\n",
      "Progress: 36.7% ... Training loss: 0.087 ... Validation loss: 0.166iteration: 3667\n",
      "train_loss: 0.08774812113594055\n",
      "val_loss: 0.1666667436898222\n",
      "Progress: 36.7% ... Training loss: 0.086 ... Validation loss: 0.185iteration: 3668\n",
      "train_loss: 0.08679283505268624\n",
      "val_loss: 0.1850993708518867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 36.7% ... Training loss: 0.084 ... Validation loss: 0.170iteration: 3669\n",
      "train_loss: 0.08443941493870093\n",
      "val_loss: 0.17006124152685026\n",
      "Progress: 36.7% ... Training loss: 0.084 ... Validation loss: 0.174iteration: 3670\n",
      "train_loss: 0.08444171867543084\n",
      "val_loss: 0.17420525218768207\n",
      "Progress: 36.7% ... Training loss: 0.084 ... Validation loss: 0.166iteration: 3671\n",
      "train_loss: 0.08478317886132436\n",
      "val_loss: 0.1667265390820591\n",
      "Progress: 36.7% ... Training loss: 0.087 ... Validation loss: 0.183iteration: 3672\n",
      "train_loss: 0.08777134944313823\n",
      "val_loss: 0.18360529682741514\n",
      "Progress: 36.7% ... Training loss: 0.085 ... Validation loss: 0.166iteration: 3673\n",
      "train_loss: 0.08518852033449284\n",
      "val_loss: 0.16610689289350797\n",
      "Progress: 36.7% ... Training loss: 0.086 ... Validation loss: 0.167iteration: 3674\n",
      "train_loss: 0.08643079944184513\n",
      "val_loss: 0.16735398216370645\n",
      "Progress: 36.8% ... Training loss: 0.083 ... Validation loss: 0.171iteration: 3675\n",
      "train_loss: 0.0831840769897775\n",
      "val_loss: 0.17132577246284464\n",
      "Progress: 36.8% ... Training loss: 0.085 ... Validation loss: 0.183iteration: 3676\n",
      "train_loss: 0.08596238470920635\n",
      "val_loss: 0.1830753647287316\n",
      "Progress: 36.8% ... Training loss: 0.091 ... Validation loss: 0.168iteration: 3677\n",
      "train_loss: 0.09134594883298836\n",
      "val_loss: 0.16828516307878408\n",
      "Progress: 36.8% ... Training loss: 0.082 ... Validation loss: 0.175iteration: 3678\n",
      "train_loss: 0.0828594655932515\n",
      "val_loss: 0.17549301872256212\n",
      "Progress: 36.8% ... Training loss: 0.084 ... Validation loss: 0.193iteration: 3679\n",
      "train_loss: 0.08453068555533395\n",
      "val_loss: 0.1931923781352763\n",
      "Progress: 36.8% ... Training loss: 0.082 ... Validation loss: 0.179iteration: 3680\n",
      "train_loss: 0.0827585654944229\n",
      "val_loss: 0.17910534689995086\n",
      "Progress: 36.8% ... Training loss: 0.085 ... Validation loss: 0.169iteration: 3681\n",
      "train_loss: 0.08512721988168798\n",
      "val_loss: 0.16953586711869426\n",
      "Progress: 36.8% ... Training loss: 0.085 ... Validation loss: 0.184iteration: 3682\n",
      "train_loss: 0.08500168711374617\n",
      "val_loss: 0.1844437610973714\n",
      "Progress: 36.8% ... Training loss: 0.086 ... Validation loss: 0.168iteration: 3683\n",
      "train_loss: 0.08629746944625594\n",
      "val_loss: 0.1685093607479548\n",
      "Progress: 36.8% ... Training loss: 0.088 ... Validation loss: 0.181iteration: 3684\n",
      "train_loss: 0.08890870001842231\n",
      "val_loss: 0.18188965446280053\n",
      "Progress: 36.9% ... Training loss: 0.087 ... Validation loss: 0.167iteration: 3685\n",
      "train_loss: 0.08700501278397303\n",
      "val_loss: 0.1671309822509552\n",
      "Progress: 36.9% ... Training loss: 0.085 ... Validation loss: 0.184iteration: 3686\n",
      "train_loss: 0.08566497776360602\n",
      "val_loss: 0.18496627446831093\n",
      "Progress: 36.9% ... Training loss: 0.082 ... Validation loss: 0.169iteration: 3687\n",
      "train_loss: 0.08278024131408213\n",
      "val_loss: 0.16977575266830186\n",
      "Progress: 36.9% ... Training loss: 0.084 ... Validation loss: 0.185iteration: 3688\n",
      "train_loss: 0.08404492134426976\n",
      "val_loss: 0.18500292577349498\n",
      "Progress: 36.9% ... Training loss: 0.083 ... Validation loss: 0.166iteration: 3689\n",
      "train_loss: 0.08382072223626065\n",
      "val_loss: 0.16657433157541265\n",
      "Progress: 36.9% ... Training loss: 0.085 ... Validation loss: 0.178iteration: 3690\n",
      "train_loss: 0.08541130525403781\n",
      "val_loss: 0.17864303034010193\n",
      "Progress: 36.9% ... Training loss: 0.096 ... Validation loss: 0.170iteration: 3691\n",
      "train_loss: 0.09641062355923334\n",
      "val_loss: 0.17042312654500774\n",
      "Progress: 36.9% ... Training loss: 0.088 ... Validation loss: 0.192iteration: 3692\n",
      "train_loss: 0.08819954954596204\n",
      "val_loss: 0.1926420726422545\n",
      "Progress: 36.9% ... Training loss: 0.103 ... Validation loss: 0.173iteration: 3693\n",
      "train_loss: 0.10319281306064228\n",
      "val_loss: 0.1731172604400362\n",
      "Progress: 36.9% ... Training loss: 0.108 ... Validation loss: 0.228iteration: 3694\n",
      "train_loss: 0.10817041699500572\n",
      "val_loss: 0.2281846729247984\n",
      "Progress: 37.0% ... Training loss: 0.134 ... Validation loss: 0.192iteration: 3695\n",
      "train_loss: 0.13461439434552144\n",
      "val_loss: 0.19205979270925924\n",
      "Progress: 37.0% ... Training loss: 0.116 ... Validation loss: 0.245iteration: 3696\n",
      "train_loss: 0.11626432728985389\n",
      "val_loss: 0.24521668209793535\n",
      "Progress: 37.0% ... Training loss: 0.100 ... Validation loss: 0.169iteration: 3697\n",
      "train_loss: 0.10003590819038759\n",
      "val_loss: 0.16906383768645494\n",
      "Progress: 37.0% ... Training loss: 0.096 ... Validation loss: 0.202iteration: 3698\n",
      "train_loss: 0.0965738398425206\n",
      "val_loss: 0.20275892687636596\n",
      "Progress: 37.0% ... Training loss: 0.115 ... Validation loss: 0.180iteration: 3699\n",
      "train_loss: 0.11504261160391484\n",
      "val_loss: 0.18046116377535637\n",
      "Progress: 37.0% ... Training loss: 0.109 ... Validation loss: 0.212iteration: 3700\n",
      "train_loss: 0.10922749587671464\n",
      "val_loss: 0.21243620683294395\n",
      "Progress: 37.0% ... Training loss: 0.092 ... Validation loss: 0.168iteration: 3701\n",
      "train_loss: 0.09213314505619319\n",
      "val_loss: 0.1687400091835352\n",
      "Progress: 37.0% ... Training loss: 0.085 ... Validation loss: 0.183iteration: 3702\n",
      "train_loss: 0.08506036356139575\n",
      "val_loss: 0.18325303374045987\n",
      "Progress: 37.0% ... Training loss: 0.097 ... Validation loss: 0.172iteration: 3703\n",
      "train_loss: 0.09776375161519303\n",
      "val_loss: 0.17274209887208838\n",
      "Progress: 37.0% ... Training loss: 0.100 ... Validation loss: 0.209iteration: 3704\n",
      "train_loss: 0.1004292024413613\n",
      "val_loss: 0.2092617460587072\n",
      "Progress: 37.0% ... Training loss: 0.087 ... Validation loss: 0.168iteration: 3705\n",
      "train_loss: 0.08769747190204168\n",
      "val_loss: 0.16815731230436112\n",
      "Progress: 37.1% ... Training loss: 0.084 ... Validation loss: 0.184iteration: 3706\n",
      "train_loss: 0.08458105639366205\n",
      "val_loss: 0.18451070716173712\n",
      "Progress: 37.1% ... Training loss: 0.082 ... Validation loss: 0.174iteration: 3707\n",
      "train_loss: 0.08233152498391176\n",
      "val_loss: 0.17491770959254024\n",
      "Progress: 37.1% ... Training loss: 0.082 ... Validation loss: 0.177iteration: 3708\n",
      "train_loss: 0.0820231018878864\n",
      "val_loss: 0.1770526474682313\n",
      "Progress: 37.1% ... Training loss: 0.082 ... Validation loss: 0.171iteration: 3709\n",
      "train_loss: 0.08260819529924457\n",
      "val_loss: 0.17131177454747193\n",
      "Progress: 37.1% ... Training loss: 0.082 ... Validation loss: 0.169iteration: 3710\n",
      "train_loss: 0.08269252389105788\n",
      "val_loss: 0.16939530291304472\n",
      "Progress: 37.1% ... Training loss: 0.083 ... Validation loss: 0.164iteration: 3711\n",
      "train_loss: 0.08396952717524692\n",
      "val_loss: 0.16453190069237092\n",
      "Progress: 37.1% ... Training loss: 0.084 ... Validation loss: 0.174iteration: 3712\n",
      "train_loss: 0.0842276573121489\n",
      "val_loss: 0.17424050177635492\n",
      "Progress: 37.1% ... Training loss: 0.081 ... Validation loss: 0.177iteration: 3713\n",
      "train_loss: 0.08188651666455113\n",
      "val_loss: 0.1773737705635502\n",
      "Progress: 37.1% ... Training loss: 0.088 ... Validation loss: 0.185iteration: 3714\n",
      "train_loss: 0.08887329536508307\n",
      "val_loss: 0.18592593557792378\n",
      "Progress: 37.1% ... Training loss: 0.088 ... Validation loss: 0.166iteration: 3715\n",
      "train_loss: 0.08812385796917\n",
      "val_loss: 0.1669539935643179\n",
      "Progress: 37.2% ... Training loss: 0.082 ... Validation loss: 0.173iteration: 3716\n",
      "train_loss: 0.08253781110282815\n",
      "val_loss: 0.17370254871802127\n",
      "Progress: 37.2% ... Training loss: 0.082 ... Validation loss: 0.176iteration: 3717\n",
      "train_loss: 0.08299690224818386\n",
      "val_loss: 0.17655497915471124\n",
      "Progress: 37.2% ... Training loss: 0.082 ... Validation loss: 0.179iteration: 3718\n",
      "train_loss: 0.08208774733170159\n",
      "val_loss: 0.1795159803995981\n",
      "Progress: 37.2% ... Training loss: 0.082 ... Validation loss: 0.167iteration: 3719\n",
      "train_loss: 0.08236728561355111\n",
      "val_loss: 0.16700698737513925\n",
      "Progress: 37.2% ... Training loss: 0.084 ... Validation loss: 0.172iteration: 3720\n",
      "train_loss: 0.08438555380695882\n",
      "val_loss: 0.17224649304324938\n",
      "Progress: 37.2% ... Training loss: 0.082 ... Validation loss: 0.166iteration: 3721\n",
      "train_loss: 0.08232548941471475\n",
      "val_loss: 0.16633799908839794\n",
      "Progress: 37.2% ... Training loss: 0.083 ... Validation loss: 0.184iteration: 3722\n",
      "train_loss: 0.0836342013615861\n",
      "val_loss: 0.18451764384627714\n",
      "Progress: 37.2% ... Training loss: 0.093 ... Validation loss: 0.166iteration: 3723\n",
      "train_loss: 0.09389658364863523\n",
      "val_loss: 0.16677995819623823\n",
      "Progress: 37.2% ... Training loss: 0.096 ... Validation loss: 0.193iteration: 3724\n",
      "train_loss: 0.09629252272207339\n",
      "val_loss: 0.19362284485997278\n",
      "Progress: 37.2% ... Training loss: 0.105 ... Validation loss: 0.172iteration: 3725\n",
      "train_loss: 0.10587784023988572\n",
      "val_loss: 0.1722511202599439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 37.3% ... Training loss: 0.098 ... Validation loss: 0.197iteration: 3726\n",
      "train_loss: 0.09817834991400463\n",
      "val_loss: 0.1976770401960169\n",
      "Progress: 37.3% ... Training loss: 0.091 ... Validation loss: 0.163iteration: 3727\n",
      "train_loss: 0.09166212366643066\n",
      "val_loss: 0.16344522099532746\n",
      "Progress: 37.3% ... Training loss: 0.098 ... Validation loss: 0.214iteration: 3728\n",
      "train_loss: 0.09858277645002497\n",
      "val_loss: 0.214191786020447\n",
      "Progress: 37.3% ... Training loss: 0.114 ... Validation loss: 0.174iteration: 3729\n",
      "train_loss: 0.11441147551874865\n",
      "val_loss: 0.17402440580412887\n",
      "Progress: 37.3% ... Training loss: 0.111 ... Validation loss: 0.255iteration: 3730\n",
      "train_loss: 0.11190107250516274\n",
      "val_loss: 0.2555872077056855\n",
      "Progress: 37.3% ... Training loss: 0.083 ... Validation loss: 0.174iteration: 3731\n",
      "train_loss: 0.08387327439357313\n",
      "val_loss: 0.17415889837078583\n",
      "Progress: 37.3% ... Training loss: 0.087 ... Validation loss: 0.186iteration: 3732\n",
      "train_loss: 0.08737251663543419\n",
      "val_loss: 0.18697805481088905\n",
      "Progress: 37.3% ... Training loss: 0.083 ... Validation loss: 0.176iteration: 3733\n",
      "train_loss: 0.08334772902935897\n",
      "val_loss: 0.1764884751886153\n",
      "Progress: 37.3% ... Training loss: 0.085 ... Validation loss: 0.188iteration: 3734\n",
      "train_loss: 0.08533772536834121\n",
      "val_loss: 0.18800581695862853\n",
      "Progress: 37.4% ... Training loss: 0.084 ... Validation loss: 0.182iteration: 3735\n",
      "train_loss: 0.08407849234372294\n",
      "val_loss: 0.1823934660488376\n",
      "Progress: 37.4% ... Training loss: 0.083 ... Validation loss: 0.164iteration: 3736\n",
      "train_loss: 0.08303291944118846\n",
      "val_loss: 0.1641968569319721\n",
      "Progress: 37.4% ... Training loss: 0.088 ... Validation loss: 0.181iteration: 3737\n",
      "train_loss: 0.08878447407289985\n",
      "val_loss: 0.18196952181743495\n",
      "Progress: 37.4% ... Training loss: 0.089 ... Validation loss: 0.165iteration: 3738\n",
      "train_loss: 0.08967069394929368\n",
      "val_loss: 0.16521092215623231\n",
      "Progress: 37.4% ... Training loss: 0.099 ... Validation loss: 0.204iteration: 3739\n",
      "train_loss: 0.09940083405028106\n",
      "val_loss: 0.20482385980273904\n",
      "Progress: 37.4% ... Training loss: 0.087 ... Validation loss: 0.164iteration: 3740\n",
      "train_loss: 0.08703672545409459\n",
      "val_loss: 0.16448708618710997\n",
      "Progress: 37.4% ... Training loss: 0.088 ... Validation loss: 0.179iteration: 3741\n",
      "train_loss: 0.08872244819551958\n",
      "val_loss: 0.1797639063649725\n",
      "Progress: 37.4% ... Training loss: 0.082 ... Validation loss: 0.169iteration: 3742\n",
      "train_loss: 0.0827054797526745\n",
      "val_loss: 0.16917963498705293\n",
      "Progress: 37.4% ... Training loss: 0.082 ... Validation loss: 0.173iteration: 3743\n",
      "train_loss: 0.08245374415073728\n",
      "val_loss: 0.17345646211810567\n",
      "Progress: 37.4% ... Training loss: 0.085 ... Validation loss: 0.167iteration: 3744\n",
      "train_loss: 0.0855626635243192\n",
      "val_loss: 0.16700432236441135\n",
      "Progress: 37.5% ... Training loss: 0.087 ... Validation loss: 0.185iteration: 3745\n",
      "train_loss: 0.08744142245228773\n",
      "val_loss: 0.18506652727711706\n",
      "Progress: 37.5% ... Training loss: 0.087 ... Validation loss: 0.163iteration: 3746\n",
      "train_loss: 0.08758826194076362\n",
      "val_loss: 0.16320785373086205\n",
      "Progress: 37.5% ... Training loss: 0.091 ... Validation loss: 0.197iteration: 3747\n",
      "train_loss: 0.09102977330090319\n",
      "val_loss: 0.1971300283807953\n",
      "Progress: 37.5% ... Training loss: 0.103 ... Validation loss: 0.167iteration: 3748\n",
      "train_loss: 0.10348169589864925\n",
      "val_loss: 0.16748868665523464\n",
      "Progress: 37.5% ... Training loss: 0.087 ... Validation loss: 0.181iteration: 3749\n",
      "train_loss: 0.08728995544877129\n",
      "val_loss: 0.18143141942908522\n",
      "Progress: 37.5% ... Training loss: 0.084 ... Validation loss: 0.165iteration: 3750\n",
      "train_loss: 0.08417245936677722\n",
      "val_loss: 0.16555898947322054\n",
      "Progress: 37.5% ... Training loss: 0.082 ... Validation loss: 0.167iteration: 3751\n",
      "train_loss: 0.08219241550765094\n",
      "val_loss: 0.1671685254095147\n",
      "Progress: 37.5% ... Training loss: 0.084 ... Validation loss: 0.179iteration: 3752\n",
      "train_loss: 0.08470998971228445\n",
      "val_loss: 0.17983687482277816\n",
      "Progress: 37.5% ... Training loss: 0.096 ... Validation loss: 0.190iteration: 3753\n",
      "train_loss: 0.09642126095970367\n",
      "val_loss: 0.19031828200665993\n",
      "Progress: 37.5% ... Training loss: 0.102 ... Validation loss: 0.165iteration: 3754\n",
      "train_loss: 0.1025518919291403\n",
      "val_loss: 0.16586315146336292\n",
      "Progress: 37.5% ... Training loss: 0.104 ... Validation loss: 0.200iteration: 3755\n",
      "train_loss: 0.10483389867181622\n",
      "val_loss: 0.2006947330099916\n",
      "Progress: 37.6% ... Training loss: 0.100 ... Validation loss: 0.170iteration: 3756\n",
      "train_loss: 0.10073148473459412\n",
      "val_loss: 0.1703271021269205\n",
      "Progress: 37.6% ... Training loss: 0.098 ... Validation loss: 0.192iteration: 3757\n",
      "train_loss: 0.09867355184935234\n",
      "val_loss: 0.19275362975506324\n",
      "Progress: 37.6% ... Training loss: 0.129 ... Validation loss: 0.182iteration: 3758\n",
      "train_loss: 0.12915606418280576\n",
      "val_loss: 0.18214946709353602\n",
      "Progress: 37.6% ... Training loss: 0.144 ... Validation loss: 0.253iteration: 3759\n",
      "train_loss: 0.14487939281553608\n",
      "val_loss: 0.25362133045427687\n",
      "Progress: 37.6% ... Training loss: 0.120 ... Validation loss: 0.180iteration: 3760\n",
      "train_loss: 0.12045197508219717\n",
      "val_loss: 0.18007264981137822\n",
      "Progress: 37.6% ... Training loss: 0.137 ... Validation loss: 0.248iteration: 3761\n",
      "train_loss: 0.1371010367319247\n",
      "val_loss: 0.2489269531080683\n",
      "Progress: 37.6% ... Training loss: 0.117 ... Validation loss: 0.182iteration: 3762\n",
      "train_loss: 0.11705525912190547\n",
      "val_loss: 0.18267127434213384\n",
      "Progress: 37.6% ... Training loss: 0.092 ... Validation loss: 0.184iteration: 3763\n",
      "train_loss: 0.09273124147874794\n",
      "val_loss: 0.18406374181844304\n",
      "Progress: 37.6% ... Training loss: 0.082 ... Validation loss: 0.167iteration: 3764\n",
      "train_loss: 0.08276219242783767\n",
      "val_loss: 0.16747729973818307\n",
      "Progress: 37.6% ... Training loss: 0.083 ... Validation loss: 0.178iteration: 3765\n",
      "train_loss: 0.08397336097448918\n",
      "val_loss: 0.1788568438425189\n",
      "Progress: 37.7% ... Training loss: 0.087 ... Validation loss: 0.184iteration: 3766\n",
      "train_loss: 0.08774365863937016\n",
      "val_loss: 0.18472468990285387\n",
      "Progress: 37.7% ... Training loss: 0.084 ... Validation loss: 0.163iteration: 3767\n",
      "train_loss: 0.08451769114151861\n",
      "val_loss: 0.16352198086348504\n",
      "Progress: 37.7% ... Training loss: 0.087 ... Validation loss: 0.177iteration: 3768\n",
      "train_loss: 0.08735867557528156\n",
      "val_loss: 0.17799725662383795\n",
      "Progress: 37.7% ... Training loss: 0.124 ... Validation loss: 0.188iteration: 3769\n",
      "train_loss: 0.12416002371786414\n",
      "val_loss: 0.18898899336836772\n",
      "Progress: 37.7% ... Training loss: 0.114 ... Validation loss: 0.229iteration: 3770\n",
      "train_loss: 0.11432689865088115\n",
      "val_loss: 0.22991248887344878\n",
      "Progress: 37.7% ... Training loss: 0.093 ... Validation loss: 0.166iteration: 3771\n",
      "train_loss: 0.09340114909770171\n",
      "val_loss: 0.16691517249731372\n",
      "Progress: 37.7% ... Training loss: 0.082 ... Validation loss: 0.183iteration: 3772\n",
      "train_loss: 0.08290619991216307\n",
      "val_loss: 0.1831226955849779\n",
      "Progress: 37.7% ... Training loss: 0.081 ... Validation loss: 0.169iteration: 3773\n",
      "train_loss: 0.08145480386242716\n",
      "val_loss: 0.16966901922303101\n",
      "Progress: 37.7% ... Training loss: 0.081 ... Validation loss: 0.166iteration: 3774\n",
      "train_loss: 0.08177642847594029\n",
      "val_loss: 0.1666097931048629\n",
      "Progress: 37.8% ... Training loss: 0.083 ... Validation loss: 0.176iteration: 3775\n",
      "train_loss: 0.08308959494558968\n",
      "val_loss: 0.17610510331689688\n",
      "Progress: 37.8% ... Training loss: 0.082 ... Validation loss: 0.168iteration: 3776\n",
      "train_loss: 0.08219273918935127\n",
      "val_loss: 0.1684144341871388\n",
      "Progress: 37.8% ... Training loss: 0.086 ... Validation loss: 0.179iteration: 3777\n",
      "train_loss: 0.08624048517456472\n",
      "val_loss: 0.17939597130595722\n",
      "Progress: 37.8% ... Training loss: 0.091 ... Validation loss: 0.161iteration: 3778\n",
      "train_loss: 0.09157678918869572\n",
      "val_loss: 0.16194916958728311\n",
      "Progress: 37.8% ... Training loss: 0.089 ... Validation loss: 0.180iteration: 3779\n",
      "train_loss: 0.08919099532234967\n",
      "val_loss: 0.18068349339490947\n",
      "Progress: 37.8% ... Training loss: 0.095 ... Validation loss: 0.167iteration: 3780\n",
      "train_loss: 0.09500785996450833\n",
      "val_loss: 0.1672987267033214\n",
      "Progress: 37.8% ... Training loss: 0.113 ... Validation loss: 0.219iteration: 3781\n",
      "train_loss: 0.11378150113761235\n",
      "val_loss: 0.21985464312385797\n",
      "Progress: 37.8% ... Training loss: 0.098 ... Validation loss: 0.168iteration: 3782\n",
      "train_loss: 0.09860697976898834\n",
      "val_loss: 0.16852283935180246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 37.8% ... Training loss: 0.133 ... Validation loss: 0.238iteration: 3783\n",
      "train_loss: 0.1334856385354799\n",
      "val_loss: 0.23879719529247656\n",
      "Progress: 37.8% ... Training loss: 0.099 ... Validation loss: 0.170iteration: 3784\n",
      "train_loss: 0.09903698828560173\n",
      "val_loss: 0.17045984244827186\n",
      "Progress: 37.9% ... Training loss: 0.093 ... Validation loss: 0.197iteration: 3785\n",
      "train_loss: 0.09377219866751543\n",
      "val_loss: 0.19768005365747804\n",
      "Progress: 37.9% ... Training loss: 0.106 ... Validation loss: 0.173iteration: 3786\n",
      "train_loss: 0.10643375743114288\n",
      "val_loss: 0.17399493197726446\n",
      "Progress: 37.9% ... Training loss: 0.086 ... Validation loss: 0.189iteration: 3787\n",
      "train_loss: 0.08614662767749248\n",
      "val_loss: 0.18928795829776376\n",
      "Progress: 37.9% ... Training loss: 0.093 ... Validation loss: 0.169iteration: 3788\n",
      "train_loss: 0.09357550267539946\n",
      "val_loss: 0.16934533158681073\n",
      "Progress: 37.9% ... Training loss: 0.109 ... Validation loss: 0.229iteration: 3789\n",
      "train_loss: 0.10997263518516809\n",
      "val_loss: 0.22989003491901172\n",
      "Progress: 37.9% ... Training loss: 0.100 ... Validation loss: 0.167iteration: 3790\n",
      "train_loss: 0.10095222852492236\n",
      "val_loss: 0.16737022149243805\n",
      "Progress: 37.9% ... Training loss: 0.118 ... Validation loss: 0.246iteration: 3791\n",
      "train_loss: 0.1189970451203924\n",
      "val_loss: 0.2460280472062242\n",
      "Progress: 37.9% ... Training loss: 0.117 ... Validation loss: 0.176iteration: 3792\n",
      "train_loss: 0.11771189037806443\n",
      "val_loss: 0.1761823193498588\n",
      "Progress: 37.9% ... Training loss: 0.102 ... Validation loss: 0.208iteration: 3793\n",
      "train_loss: 0.10259973928243456\n",
      "val_loss: 0.208682801351632\n",
      "Progress: 37.9% ... Training loss: 0.131 ... Validation loss: 0.189iteration: 3794\n",
      "train_loss: 0.1314183442772356\n",
      "val_loss: 0.1891933879800651\n",
      "Progress: 38.0% ... Training loss: 0.095 ... Validation loss: 0.200iteration: 3795\n",
      "train_loss: 0.09563290207978799\n",
      "val_loss: 0.20066631018044498\n",
      "Progress: 38.0% ... Training loss: 0.084 ... Validation loss: 0.165iteration: 3796\n",
      "train_loss: 0.08451919151831992\n",
      "val_loss: 0.16549910613712918\n",
      "Progress: 38.0% ... Training loss: 0.082 ... Validation loss: 0.172iteration: 3797\n",
      "train_loss: 0.08278254841287702\n",
      "val_loss: 0.17284737349480916\n",
      "Progress: 38.0% ... Training loss: 0.085 ... Validation loss: 0.175iteration: 3798\n",
      "train_loss: 0.08525072295234248\n",
      "val_loss: 0.17575299314949894\n",
      "Progress: 38.0% ... Training loss: 0.092 ... Validation loss: 0.164iteration: 3799\n",
      "train_loss: 0.09239835840661687\n",
      "val_loss: 0.1642560557914516\n",
      "Progress: 38.0% ... Training loss: 0.118 ... Validation loss: 0.227iteration: 3800\n",
      "train_loss: 0.11841314775793782\n",
      "val_loss: 0.2274935167344272\n",
      "Progress: 38.0% ... Training loss: 0.103 ... Validation loss: 0.168iteration: 3801\n",
      "train_loss: 0.10363991288622461\n",
      "val_loss: 0.1683306573054898\n",
      "Progress: 38.0% ... Training loss: 0.100 ... Validation loss: 0.226iteration: 3802\n",
      "train_loss: 0.10001515879956985\n",
      "val_loss: 0.22659995274483966\n",
      "Progress: 38.0% ... Training loss: 0.088 ... Validation loss: 0.168iteration: 3803\n",
      "train_loss: 0.08884892819990407\n",
      "val_loss: 0.168053334284437\n",
      "Progress: 38.0% ... Training loss: 0.083 ... Validation loss: 0.192iteration: 3804\n",
      "train_loss: 0.08321542115244127\n",
      "val_loss: 0.19276886672187016\n",
      "Progress: 38.0% ... Training loss: 0.082 ... Validation loss: 0.186iteration: 3805\n",
      "train_loss: 0.08217824184112617\n",
      "val_loss: 0.186640710247911\n",
      "Progress: 38.1% ... Training loss: 0.081 ... Validation loss: 0.172iteration: 3806\n",
      "train_loss: 0.08123165690826858\n",
      "val_loss: 0.17289650047538418\n",
      "Progress: 38.1% ... Training loss: 0.086 ... Validation loss: 0.190iteration: 3807\n",
      "train_loss: 0.08629669121716553\n",
      "val_loss: 0.19000378537636695\n",
      "Progress: 38.1% ... Training loss: 0.084 ... Validation loss: 0.163iteration: 3808\n",
      "train_loss: 0.08449415811505129\n",
      "val_loss: 0.16360017426575943\n",
      "Progress: 38.1% ... Training loss: 0.081 ... Validation loss: 0.171iteration: 3809\n",
      "train_loss: 0.08109443781834674\n",
      "val_loss: 0.17172345339224737\n",
      "Progress: 38.1% ... Training loss: 0.096 ... Validation loss: 0.184iteration: 3810\n",
      "train_loss: 0.09640460757319896\n",
      "val_loss: 0.1848935565641738\n",
      "Progress: 38.1% ... Training loss: 0.105 ... Validation loss: 0.174iteration: 3811\n",
      "train_loss: 0.10564713454523843\n",
      "val_loss: 0.17401107596450088\n",
      "Progress: 38.1% ... Training loss: 0.126 ... Validation loss: 0.249iteration: 3812\n",
      "train_loss: 0.1261472188770633\n",
      "val_loss: 0.24989971943521583\n",
      "Progress: 38.1% ... Training loss: 0.104 ... Validation loss: 0.169iteration: 3813\n",
      "train_loss: 0.10455160260734765\n",
      "val_loss: 0.16929485315762127\n",
      "Progress: 38.1% ... Training loss: 0.087 ... Validation loss: 0.193iteration: 3814\n",
      "train_loss: 0.08717933992394444\n",
      "val_loss: 0.1931066688161429\n",
      "Progress: 38.1% ... Training loss: 0.090 ... Validation loss: 0.167iteration: 3815\n",
      "train_loss: 0.090404039174435\n",
      "val_loss: 0.16741759331332215\n",
      "Progress: 38.2% ... Training loss: 0.085 ... Validation loss: 0.192iteration: 3816\n",
      "train_loss: 0.08571526394164032\n",
      "val_loss: 0.19241751327226123\n",
      "Progress: 38.2% ... Training loss: 0.111 ... Validation loss: 0.177iteration: 3817\n",
      "train_loss: 0.11160370128618168\n",
      "val_loss: 0.17757548053013686\n",
      "Progress: 38.2% ... Training loss: 0.116 ... Validation loss: 0.249iteration: 3818\n",
      "train_loss: 0.11615685772635244\n",
      "val_loss: 0.2499976879555565\n",
      "Progress: 38.2% ... Training loss: 0.113 ... Validation loss: 0.180iteration: 3819\n",
      "train_loss: 0.11315805811282793\n",
      "val_loss: 0.1803965006900428\n",
      "Progress: 38.2% ... Training loss: 0.095 ... Validation loss: 0.213iteration: 3820\n",
      "train_loss: 0.0957226902738436\n",
      "val_loss: 0.21373237232803152\n",
      "Progress: 38.2% ... Training loss: 0.087 ... Validation loss: 0.171iteration: 3821\n",
      "train_loss: 0.08774782445478567\n",
      "val_loss: 0.17120382851263433\n",
      "Progress: 38.2% ... Training loss: 0.085 ... Validation loss: 0.199iteration: 3822\n",
      "train_loss: 0.08572086915820577\n",
      "val_loss: 0.19919535191295357\n",
      "Progress: 38.2% ... Training loss: 0.099 ... Validation loss: 0.175iteration: 3823\n",
      "train_loss: 0.09955408500982574\n",
      "val_loss: 0.17540923516186482\n",
      "Progress: 38.2% ... Training loss: 0.094 ... Validation loss: 0.218iteration: 3824\n",
      "train_loss: 0.094052083668884\n",
      "val_loss: 0.2187843746481476\n",
      "Progress: 38.2% ... Training loss: 0.081 ... Validation loss: 0.169iteration: 3825\n",
      "train_loss: 0.08141194641769187\n",
      "val_loss: 0.169219747624624\n",
      "Progress: 38.3% ... Training loss: 0.083 ... Validation loss: 0.172iteration: 3826\n",
      "train_loss: 0.08306572223573669\n",
      "val_loss: 0.17213734082913423\n",
      "Progress: 38.3% ... Training loss: 0.084 ... Validation loss: 0.181iteration: 3827\n",
      "train_loss: 0.0844753144054359\n",
      "val_loss: 0.1819446397064058\n",
      "Progress: 38.3% ... Training loss: 0.080 ... Validation loss: 0.173iteration: 3828\n",
      "train_loss: 0.0804069897572694\n",
      "val_loss: 0.17309947288403427\n",
      "Progress: 38.3% ... Training loss: 0.086 ... Validation loss: 0.170iteration: 3829\n",
      "train_loss: 0.08659802897181292\n",
      "val_loss: 0.17037410978272563\n",
      "Progress: 38.3% ... Training loss: 0.099 ... Validation loss: 0.205iteration: 3830\n",
      "train_loss: 0.09942962770718486\n",
      "val_loss: 0.20565926252086383\n",
      "Progress: 38.3% ... Training loss: 0.091 ... Validation loss: 0.169iteration: 3831\n",
      "train_loss: 0.09130503944220061\n",
      "val_loss: 0.16922323461570812\n",
      "Progress: 38.3% ... Training loss: 0.088 ... Validation loss: 0.177iteration: 3832\n",
      "train_loss: 0.08823740751639564\n",
      "val_loss: 0.17725403332530587\n",
      "Progress: 38.3% ... Training loss: 0.089 ... Validation loss: 0.167iteration: 3833\n",
      "train_loss: 0.08938617695914172\n",
      "val_loss: 0.16741036864120337\n",
      "Progress: 38.3% ... Training loss: 0.087 ... Validation loss: 0.188iteration: 3834\n",
      "train_loss: 0.08708994782477425\n",
      "val_loss: 0.18887884639273736\n",
      "Progress: 38.4% ... Training loss: 0.080 ... Validation loss: 0.167iteration: 3835\n",
      "train_loss: 0.0806513560704664\n",
      "val_loss: 0.1677426327450819\n",
      "Progress: 38.4% ... Training loss: 0.081 ... Validation loss: 0.168iteration: 3836\n",
      "train_loss: 0.08122141104891002\n",
      "val_loss: 0.1684200360721469\n",
      "Progress: 38.4% ... Training loss: 0.082 ... Validation loss: 0.166iteration: 3837\n",
      "train_loss: 0.08295524598583848\n",
      "val_loss: 0.1664385762860478\n",
      "Progress: 38.4% ... Training loss: 0.080 ... Validation loss: 0.168iteration: 3838\n",
      "train_loss: 0.08095119742658036\n",
      "val_loss: 0.1681943897137197\n",
      "Progress: 38.4% ... Training loss: 0.080 ... Validation loss: 0.173iteration: 3839\n",
      "train_loss: 0.08030923871892695\n",
      "val_loss: 0.17317907256455453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 38.4% ... Training loss: 0.086 ... Validation loss: 0.182iteration: 3840\n",
      "train_loss: 0.08663875074343913\n",
      "val_loss: 0.18202496893067127\n",
      "Progress: 38.4% ... Training loss: 0.086 ... Validation loss: 0.166iteration: 3841\n",
      "train_loss: 0.08699670525871184\n",
      "val_loss: 0.16627106244334114\n",
      "Progress: 38.4% ... Training loss: 0.080 ... Validation loss: 0.174iteration: 3842\n",
      "train_loss: 0.08053714666959595\n",
      "val_loss: 0.1744827303616393\n",
      "Progress: 38.4% ... Training loss: 0.105 ... Validation loss: 0.171iteration: 3843\n",
      "train_loss: 0.10581234559239563\n",
      "val_loss: 0.17109538854232706\n",
      "Progress: 38.4% ... Training loss: 0.117 ... Validation loss: 0.236iteration: 3844\n",
      "train_loss: 0.11706063611495181\n",
      "val_loss: 0.23670194456540095\n",
      "Progress: 38.5% ... Training loss: 0.105 ... Validation loss: 0.172iteration: 3845\n",
      "train_loss: 0.10563060948114798\n",
      "val_loss: 0.17200749956332914\n",
      "Progress: 38.5% ... Training loss: 0.091 ... Validation loss: 0.212iteration: 3846\n",
      "train_loss: 0.09193640417848732\n",
      "val_loss: 0.21245035054452993\n",
      "Progress: 38.5% ... Training loss: 0.092 ... Validation loss: 0.168iteration: 3847\n",
      "train_loss: 0.09285225406783473\n",
      "val_loss: 0.16896267337441256\n",
      "Progress: 38.5% ... Training loss: 0.086 ... Validation loss: 0.195iteration: 3848\n",
      "train_loss: 0.08619026652396186\n",
      "val_loss: 0.19574483203804885\n",
      "Progress: 38.5% ... Training loss: 0.083 ... Validation loss: 0.168iteration: 3849\n",
      "train_loss: 0.08310533650082096\n",
      "val_loss: 0.16866136420750288\n",
      "Progress: 38.5% ... Training loss: 0.080 ... Validation loss: 0.178iteration: 3850\n",
      "train_loss: 0.08048186924982548\n",
      "val_loss: 0.17819856132217662\n",
      "Progress: 38.5% ... Training loss: 0.081 ... Validation loss: 0.187iteration: 3851\n",
      "train_loss: 0.08158990625386657\n",
      "val_loss: 0.18786146305214524\n",
      "Progress: 38.5% ... Training loss: 0.081 ... Validation loss: 0.173iteration: 3852\n",
      "train_loss: 0.08104710634376722\n",
      "val_loss: 0.1738360076095009\n",
      "Progress: 38.5% ... Training loss: 0.082 ... Validation loss: 0.165iteration: 3853\n",
      "train_loss: 0.0822514329013892\n",
      "val_loss: 0.16573682817364127\n",
      "Progress: 38.5% ... Training loss: 0.082 ... Validation loss: 0.182iteration: 3854\n",
      "train_loss: 0.0823375549977503\n",
      "val_loss: 0.18203633630902144\n",
      "Progress: 38.5% ... Training loss: 0.081 ... Validation loss: 0.172iteration: 3855\n",
      "train_loss: 0.08184981304700907\n",
      "val_loss: 0.17283601339478583\n",
      "Progress: 38.6% ... Training loss: 0.082 ... Validation loss: 0.173iteration: 3856\n",
      "train_loss: 0.0824853388418763\n",
      "val_loss: 0.17315205658158928\n",
      "Progress: 38.6% ... Training loss: 0.081 ... Validation loss: 0.176iteration: 3857\n",
      "train_loss: 0.08152333658407658\n",
      "val_loss: 0.17620637995572383\n",
      "Progress: 38.6% ... Training loss: 0.085 ... Validation loss: 0.162iteration: 3858\n",
      "train_loss: 0.08562594107231518\n",
      "val_loss: 0.16270437438733473\n",
      "Progress: 38.6% ... Training loss: 0.081 ... Validation loss: 0.177iteration: 3859\n",
      "train_loss: 0.08156296635971269\n",
      "val_loss: 0.17737295948326112\n",
      "Progress: 38.6% ... Training loss: 0.082 ... Validation loss: 0.164iteration: 3860\n",
      "train_loss: 0.08219142792367089\n",
      "val_loss: 0.1641848840124495\n",
      "Progress: 38.6% ... Training loss: 0.080 ... Validation loss: 0.174iteration: 3861\n",
      "train_loss: 0.08037502418384453\n",
      "val_loss: 0.17405701218366465\n",
      "Progress: 38.6% ... Training loss: 0.084 ... Validation loss: 0.167iteration: 3862\n",
      "train_loss: 0.08417187549366344\n",
      "val_loss: 0.1671133872950441\n",
      "Progress: 38.6% ... Training loss: 0.081 ... Validation loss: 0.179iteration: 3863\n",
      "train_loss: 0.08139790871250561\n",
      "val_loss: 0.1799493055445982\n",
      "Progress: 38.6% ... Training loss: 0.082 ... Validation loss: 0.179iteration: 3864\n",
      "train_loss: 0.08279762539514882\n",
      "val_loss: 0.17934006118635779\n",
      "Progress: 38.6% ... Training loss: 0.079 ... Validation loss: 0.172iteration: 3865\n",
      "train_loss: 0.07955578508766563\n",
      "val_loss: 0.17261129971084038\n",
      "Progress: 38.7% ... Training loss: 0.082 ... Validation loss: 0.187iteration: 3866\n",
      "train_loss: 0.08252019573587042\n",
      "val_loss: 0.1870632490403316\n",
      "Progress: 38.7% ... Training loss: 0.081 ... Validation loss: 0.167iteration: 3867\n",
      "train_loss: 0.0819415031195274\n",
      "val_loss: 0.16740857443553978\n",
      "Progress: 38.7% ... Training loss: 0.082 ... Validation loss: 0.183iteration: 3868\n",
      "train_loss: 0.08298164960055762\n",
      "val_loss: 0.1831232392457287\n",
      "Progress: 38.7% ... Training loss: 0.079 ... Validation loss: 0.167iteration: 3869\n",
      "train_loss: 0.07978700266913291\n",
      "val_loss: 0.16770382547870788\n",
      "Progress: 38.7% ... Training loss: 0.079 ... Validation loss: 0.169iteration: 3870\n",
      "train_loss: 0.07978897537985451\n",
      "val_loss: 0.16933051976517638\n",
      "Progress: 38.7% ... Training loss: 0.079 ... Validation loss: 0.172iteration: 3871\n",
      "train_loss: 0.0793947247883663\n",
      "val_loss: 0.17270563785471793\n",
      "Progress: 38.7% ... Training loss: 0.080 ... Validation loss: 0.169iteration: 3872\n",
      "train_loss: 0.08073448389065076\n",
      "val_loss: 0.16936154097543796\n",
      "Progress: 38.7% ... Training loss: 0.087 ... Validation loss: 0.194iteration: 3873\n",
      "train_loss: 0.0878867264714319\n",
      "val_loss: 0.194570795225933\n",
      "Progress: 38.7% ... Training loss: 0.082 ... Validation loss: 0.165iteration: 3874\n",
      "train_loss: 0.08257943628682107\n",
      "val_loss: 0.1653763119145491\n",
      "Progress: 38.8% ... Training loss: 0.093 ... Validation loss: 0.214iteration: 3875\n",
      "train_loss: 0.09355573854894052\n",
      "val_loss: 0.21462574777047988\n",
      "Progress: 38.8% ... Training loss: 0.092 ... Validation loss: 0.173iteration: 3876\n",
      "train_loss: 0.09256691962489141\n",
      "val_loss: 0.17364667074551177\n",
      "Progress: 38.8% ... Training loss: 0.086 ... Validation loss: 0.189iteration: 3877\n",
      "train_loss: 0.0862733551810854\n",
      "val_loss: 0.18922787030649424\n",
      "Progress: 38.8% ... Training loss: 0.093 ... Validation loss: 0.169iteration: 3878\n",
      "train_loss: 0.09312267749816554\n",
      "val_loss: 0.16975337686860928\n",
      "Progress: 38.8% ... Training loss: 0.083 ... Validation loss: 0.181iteration: 3879\n",
      "train_loss: 0.08319857195699333\n",
      "val_loss: 0.1819750147965996\n",
      "Progress: 38.8% ... Training loss: 0.081 ... Validation loss: 0.171iteration: 3880\n",
      "train_loss: 0.08171268145558509\n",
      "val_loss: 0.1714868001712246\n",
      "Progress: 38.8% ... Training loss: 0.085 ... Validation loss: 0.164iteration: 3881\n",
      "train_loss: 0.08537279672438917\n",
      "val_loss: 0.16400342938496823\n",
      "Progress: 38.8% ... Training loss: 0.080 ... Validation loss: 0.163iteration: 3882\n",
      "train_loss: 0.08023331058191832\n",
      "val_loss: 0.16328175402571263\n",
      "Progress: 38.8% ... Training loss: 0.086 ... Validation loss: 0.174iteration: 3883\n",
      "train_loss: 0.08636439395843828\n",
      "val_loss: 0.17445716278192366\n",
      "Progress: 38.8% ... Training loss: 0.082 ... Validation loss: 0.160iteration: 3884\n",
      "train_loss: 0.08261493206811384\n",
      "val_loss: 0.1601364999908153\n",
      "Progress: 38.9% ... Training loss: 0.081 ... Validation loss: 0.166iteration: 3885\n",
      "train_loss: 0.08124901909139062\n",
      "val_loss: 0.16669587401550753\n",
      "Progress: 38.9% ... Training loss: 0.082 ... Validation loss: 0.156iteration: 3886\n",
      "train_loss: 0.08274149661214265\n",
      "val_loss: 0.15679981148602654\n",
      "Progress: 38.9% ... Training loss: 0.083 ... Validation loss: 0.176iteration: 3887\n",
      "train_loss: 0.0837085033601746\n",
      "val_loss: 0.1763314764717117\n",
      "Progress: 38.9% ... Training loss: 0.085 ... Validation loss: 0.163iteration: 3888\n",
      "train_loss: 0.0850207281907078\n",
      "val_loss: 0.1636209551318972\n",
      "Progress: 38.9% ... Training loss: 0.081 ... Validation loss: 0.172iteration: 3889\n",
      "train_loss: 0.08109687182471856\n",
      "val_loss: 0.17278837420333454\n",
      "Progress: 38.9% ... Training loss: 0.090 ... Validation loss: 0.165iteration: 3890\n",
      "train_loss: 0.09060192804673714\n",
      "val_loss: 0.16596162472434614\n",
      "Progress: 38.9% ... Training loss: 0.095 ... Validation loss: 0.203iteration: 3891\n",
      "train_loss: 0.09503568149708373\n",
      "val_loss: 0.20397935897114955\n",
      "Progress: 38.9% ... Training loss: 0.098 ... Validation loss: 0.167iteration: 3892\n",
      "train_loss: 0.09806140970049876\n",
      "val_loss: 0.1678253569056412\n",
      "Progress: 38.9% ... Training loss: 0.117 ... Validation loss: 0.236iteration: 3893\n",
      "train_loss: 0.11708515068940739\n",
      "val_loss: 0.23693589918288233\n",
      "Progress: 38.9% ... Training loss: 0.114 ... Validation loss: 0.178iteration: 3894\n",
      "train_loss: 0.11465266974246875\n",
      "val_loss: 0.17870181421602366\n",
      "Progress: 39.0% ... Training loss: 0.135 ... Validation loss: 0.233iteration: 3895\n",
      "train_loss: 0.135864609602135\n",
      "val_loss: 0.23304297751651876\n",
      "Progress: 39.0% ... Training loss: 0.135 ... Validation loss: 0.195iteration: 3896\n",
      "train_loss: 0.13568301565420068\n",
      "val_loss: 0.19542147790583866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 39.0% ... Training loss: 0.099 ... Validation loss: 0.214iteration: 3897\n",
      "train_loss: 0.09974115669092218\n",
      "val_loss: 0.21433805794669283\n",
      "Progress: 39.0% ... Training loss: 0.134 ... Validation loss: 0.187iteration: 3898\n",
      "train_loss: 0.13408433623231397\n",
      "val_loss: 0.18794368339963008\n",
      "Progress: 39.0% ... Training loss: 0.140 ... Validation loss: 0.262iteration: 3899\n",
      "train_loss: 0.14018507432305474\n",
      "val_loss: 0.2626034400482391\n",
      "Progress: 39.0% ... Training loss: 0.090 ... Validation loss: 0.165iteration: 3900\n",
      "train_loss: 0.09047096694326202\n",
      "val_loss: 0.16521238088736104\n",
      "Progress: 39.0% ... Training loss: 0.094 ... Validation loss: 0.189iteration: 3901\n",
      "train_loss: 0.0941526320372854\n",
      "val_loss: 0.18927083048354756\n",
      "Progress: 39.0% ... Training loss: 0.107 ... Validation loss: 0.181iteration: 3902\n",
      "train_loss: 0.10738318404599784\n",
      "val_loss: 0.1811125909730021\n",
      "Progress: 39.0% ... Training loss: 0.096 ... Validation loss: 0.193iteration: 3903\n",
      "train_loss: 0.09697314686252727\n",
      "val_loss: 0.1937978303058124\n",
      "Progress: 39.0% ... Training loss: 0.097 ... Validation loss: 0.174iteration: 3904\n",
      "train_loss: 0.09719421066877987\n",
      "val_loss: 0.17416821620948203\n",
      "Progress: 39.0% ... Training loss: 0.085 ... Validation loss: 0.183iteration: 3905\n",
      "train_loss: 0.08581557349819162\n",
      "val_loss: 0.18366011485639497\n",
      "Progress: 39.1% ... Training loss: 0.089 ... Validation loss: 0.169iteration: 3906\n",
      "train_loss: 0.08974095026378276\n",
      "val_loss: 0.16930656956038093\n",
      "Progress: 39.1% ... Training loss: 0.092 ... Validation loss: 0.194iteration: 3907\n",
      "train_loss: 0.09212087233350355\n",
      "val_loss: 0.19434490134946303\n",
      "Progress: 39.1% ... Training loss: 0.086 ... Validation loss: 0.165iteration: 3908\n",
      "train_loss: 0.0867439136683358\n",
      "val_loss: 0.16538397464016213\n",
      "Progress: 39.1% ... Training loss: 0.088 ... Validation loss: 0.192iteration: 3909\n",
      "train_loss: 0.0883501882512631\n",
      "val_loss: 0.1927920469931791\n",
      "Progress: 39.1% ... Training loss: 0.079 ... Validation loss: 0.178iteration: 3910\n",
      "train_loss: 0.07960538725722145\n",
      "val_loss: 0.1781431170382407\n",
      "Progress: 39.1% ... Training loss: 0.079 ... Validation loss: 0.167iteration: 3911\n",
      "train_loss: 0.07939977587337457\n",
      "val_loss: 0.16704140696860278\n",
      "Progress: 39.1% ... Training loss: 0.079 ... Validation loss: 0.172iteration: 3912\n",
      "train_loss: 0.07929114520463361\n",
      "val_loss: 0.17206494351026375\n",
      "Progress: 39.1% ... Training loss: 0.081 ... Validation loss: 0.168iteration: 3913\n",
      "train_loss: 0.08192957257594244\n",
      "val_loss: 0.16815208195943526\n",
      "Progress: 39.1% ... Training loss: 0.082 ... Validation loss: 0.180iteration: 3914\n",
      "train_loss: 0.08219375125869201\n",
      "val_loss: 0.18044835768623102\n",
      "Progress: 39.1% ... Training loss: 0.080 ... Validation loss: 0.170iteration: 3915\n",
      "train_loss: 0.08086651479835824\n",
      "val_loss: 0.17007972319663864\n",
      "Progress: 39.2% ... Training loss: 0.080 ... Validation loss: 0.180iteration: 3916\n",
      "train_loss: 0.08025958277734462\n",
      "val_loss: 0.18006117798996643\n",
      "Progress: 39.2% ... Training loss: 0.083 ... Validation loss: 0.164iteration: 3917\n",
      "train_loss: 0.08310575642928539\n",
      "val_loss: 0.16456153977462007\n",
      "Progress: 39.2% ... Training loss: 0.083 ... Validation loss: 0.209iteration: 3918\n",
      "train_loss: 0.0837177587886319\n",
      "val_loss: 0.20907202628058486\n",
      "Progress: 39.2% ... Training loss: 0.082 ... Validation loss: 0.169iteration: 3919\n",
      "train_loss: 0.08277326894010135\n",
      "val_loss: 0.1696722487000914\n",
      "Progress: 39.2% ... Training loss: 0.080 ... Validation loss: 0.176iteration: 3920\n",
      "train_loss: 0.08012134538476116\n",
      "val_loss: 0.1765765166726648\n",
      "Progress: 39.2% ... Training loss: 0.079 ... Validation loss: 0.177iteration: 3921\n",
      "train_loss: 0.07952647901033646\n",
      "val_loss: 0.17760204905549146\n",
      "Progress: 39.2% ... Training loss: 0.080 ... Validation loss: 0.180iteration: 3922\n",
      "train_loss: 0.08036225949931158\n",
      "val_loss: 0.18080741088711444\n",
      "Progress: 39.2% ... Training loss: 0.087 ... Validation loss: 0.169iteration: 3923\n",
      "train_loss: 0.08726886992250434\n",
      "val_loss: 0.16923617033441088\n",
      "Progress: 39.2% ... Training loss: 0.104 ... Validation loss: 0.235iteration: 3924\n",
      "train_loss: 0.1046273440501637\n",
      "val_loss: 0.23592005637416294\n",
      "Progress: 39.2% ... Training loss: 0.097 ... Validation loss: 0.164iteration: 3925\n",
      "train_loss: 0.09797847671690615\n",
      "val_loss: 0.1644072129501386\n",
      "Progress: 39.3% ... Training loss: 0.085 ... Validation loss: 0.191iteration: 3926\n",
      "train_loss: 0.08555101625094079\n",
      "val_loss: 0.19110278387951862\n",
      "Progress: 39.3% ... Training loss: 0.083 ... Validation loss: 0.161iteration: 3927\n",
      "train_loss: 0.0832420744988349\n",
      "val_loss: 0.16105164379165385\n",
      "Progress: 39.3% ... Training loss: 0.079 ... Validation loss: 0.177iteration: 3928\n",
      "train_loss: 0.07912736430413657\n",
      "val_loss: 0.17794864090358806\n",
      "Progress: 39.3% ... Training loss: 0.079 ... Validation loss: 0.172iteration: 3929\n",
      "train_loss: 0.07967385234179998\n",
      "val_loss: 0.1722296929356557\n",
      "Progress: 39.3% ... Training loss: 0.079 ... Validation loss: 0.167iteration: 3930\n",
      "train_loss: 0.07983356348106345\n",
      "val_loss: 0.1671712244744259\n",
      "Progress: 39.3% ... Training loss: 0.080 ... Validation loss: 0.166iteration: 3931\n",
      "train_loss: 0.08002938336957734\n",
      "val_loss: 0.16667280241435695\n",
      "Progress: 39.3% ... Training loss: 0.079 ... Validation loss: 0.166iteration: 3932\n",
      "train_loss: 0.07933708857671853\n",
      "val_loss: 0.1666817087515431\n",
      "Progress: 39.3% ... Training loss: 0.082 ... Validation loss: 0.189iteration: 3933\n",
      "train_loss: 0.08213695447316016\n",
      "val_loss: 0.18918385841722238\n",
      "Progress: 39.3% ... Training loss: 0.082 ... Validation loss: 0.163iteration: 3934\n",
      "train_loss: 0.08267258945602722\n",
      "val_loss: 0.1633214146426827\n",
      "Progress: 39.4% ... Training loss: 0.083 ... Validation loss: 0.188iteration: 3935\n",
      "train_loss: 0.08336872671666251\n",
      "val_loss: 0.18828942313373093\n",
      "Progress: 39.4% ... Training loss: 0.084 ... Validation loss: 0.161iteration: 3936\n",
      "train_loss: 0.08410311426382844\n",
      "val_loss: 0.1618971672457246\n",
      "Progress: 39.4% ... Training loss: 0.096 ... Validation loss: 0.198iteration: 3937\n",
      "train_loss: 0.0962154193951723\n",
      "val_loss: 0.1986896344477731\n",
      "Progress: 39.4% ... Training loss: 0.085 ... Validation loss: 0.163iteration: 3938\n",
      "train_loss: 0.08583234515775696\n",
      "val_loss: 0.1636694623111834\n",
      "Progress: 39.4% ... Training loss: 0.086 ... Validation loss: 0.189iteration: 3939\n",
      "train_loss: 0.08691690075033055\n",
      "val_loss: 0.18946257509605724\n",
      "Progress: 39.4% ... Training loss: 0.080 ... Validation loss: 0.161iteration: 3940\n",
      "train_loss: 0.08051179667589377\n",
      "val_loss: 0.16194265155207627\n",
      "Progress: 39.4% ... Training loss: 0.083 ... Validation loss: 0.162iteration: 3941\n",
      "train_loss: 0.08357688594919585\n",
      "val_loss: 0.16226944216462863\n",
      "Progress: 39.4% ... Training loss: 0.079 ... Validation loss: 0.164iteration: 3942\n",
      "train_loss: 0.07949927668095952\n",
      "val_loss: 0.16484048486372024\n",
      "Progress: 39.4% ... Training loss: 0.086 ... Validation loss: 0.184iteration: 3943\n",
      "train_loss: 0.08677036382592049\n",
      "val_loss: 0.18492848158539354\n",
      "Progress: 39.4% ... Training loss: 0.078 ... Validation loss: 0.167iteration: 3944\n",
      "train_loss: 0.07891902952293621\n",
      "val_loss: 0.1678359623808326\n",
      "Progress: 39.5% ... Training loss: 0.078 ... Validation loss: 0.169iteration: 3945\n",
      "train_loss: 0.07897322501368395\n",
      "val_loss: 0.1690504336350749\n",
      "Progress: 39.5% ... Training loss: 0.078 ... Validation loss: 0.168iteration: 3946\n",
      "train_loss: 0.07869999418575578\n",
      "val_loss: 0.168135662676476\n",
      "Progress: 39.5% ... Training loss: 0.079 ... Validation loss: 0.166iteration: 3947\n",
      "train_loss: 0.07954138669340592\n",
      "val_loss: 0.16693831178835605\n",
      "Progress: 39.5% ... Training loss: 0.078 ... Validation loss: 0.170iteration: 3948\n",
      "train_loss: 0.07878515653823186\n",
      "val_loss: 0.17064051863915336\n",
      "Progress: 39.5% ... Training loss: 0.082 ... Validation loss: 0.177iteration: 3949\n",
      "train_loss: 0.08234342514130909\n",
      "val_loss: 0.17751380688191548\n",
      "Progress: 39.5% ... Training loss: 0.086 ... Validation loss: 0.192iteration: 3950\n",
      "train_loss: 0.08613386475792681\n",
      "val_loss: 0.19233253747406023\n",
      "Progress: 39.5% ... Training loss: 0.088 ... Validation loss: 0.166iteration: 3951\n",
      "train_loss: 0.08800088624306973\n",
      "val_loss: 0.1668819412762705\n",
      "Progress: 39.5% ... Training loss: 0.087 ... Validation loss: 0.190iteration: 3952\n",
      "train_loss: 0.0875744873986545\n",
      "val_loss: 0.19056326454520003\n",
      "Progress: 39.5% ... Training loss: 0.096 ... Validation loss: 0.171iteration: 3953\n",
      "train_loss: 0.09608586012439453\n",
      "val_loss: 0.17106431111051726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 39.5% ... Training loss: 0.086 ... Validation loss: 0.199iteration: 3954\n",
      "train_loss: 0.0867628349228232\n",
      "val_loss: 0.19934605721499218\n",
      "Progress: 39.5% ... Training loss: 0.084 ... Validation loss: 0.168iteration: 3955\n",
      "train_loss: 0.08459682867680088\n",
      "val_loss: 0.16846732704805784\n",
      "Progress: 39.6% ... Training loss: 0.081 ... Validation loss: 0.183iteration: 3956\n",
      "train_loss: 0.08198310899061657\n",
      "val_loss: 0.18391641724230504\n",
      "Progress: 39.6% ... Training loss: 0.079 ... Validation loss: 0.168iteration: 3957\n",
      "train_loss: 0.07936276893920165\n",
      "val_loss: 0.16865036924267346\n",
      "Progress: 39.6% ... Training loss: 0.078 ... Validation loss: 0.174iteration: 3958\n",
      "train_loss: 0.07898727948846844\n",
      "val_loss: 0.17435365825861315\n",
      "Progress: 39.6% ... Training loss: 0.082 ... Validation loss: 0.166iteration: 3959\n",
      "train_loss: 0.08286953988391645\n",
      "val_loss: 0.16681049926595992\n",
      "Progress: 39.6% ... Training loss: 0.079 ... Validation loss: 0.178iteration: 3960\n",
      "train_loss: 0.07957927699246418\n",
      "val_loss: 0.1788622612485993\n",
      "Progress: 39.6% ... Training loss: 0.080 ... Validation loss: 0.170iteration: 3961\n",
      "train_loss: 0.08011487055930175\n",
      "val_loss: 0.17034422944322355\n",
      "Progress: 39.6% ... Training loss: 0.084 ... Validation loss: 0.178iteration: 3962\n",
      "train_loss: 0.08442942168040433\n",
      "val_loss: 0.17878886801123275\n",
      "Progress: 39.6% ... Training loss: 0.079 ... Validation loss: 0.166iteration: 3963\n",
      "train_loss: 0.07988618413045694\n",
      "val_loss: 0.16609304844627115\n",
      "Progress: 39.6% ... Training loss: 0.078 ... Validation loss: 0.168iteration: 3964\n",
      "train_loss: 0.07825522888616399\n",
      "val_loss: 0.16809901003236744\n",
      "Progress: 39.6% ... Training loss: 0.080 ... Validation loss: 0.170iteration: 3965\n",
      "train_loss: 0.08073287005375931\n",
      "val_loss: 0.1704701116275782\n",
      "Progress: 39.7% ... Training loss: 0.080 ... Validation loss: 0.157iteration: 3966\n",
      "train_loss: 0.08035432471212352\n",
      "val_loss: 0.15762986613461116\n",
      "Progress: 39.7% ... Training loss: 0.080 ... Validation loss: 0.165iteration: 3967\n",
      "train_loss: 0.0809247923749302\n",
      "val_loss: 0.16588034874515126\n",
      "Progress: 39.7% ... Training loss: 0.079 ... Validation loss: 0.164iteration: 3968\n",
      "train_loss: 0.07975349452275916\n",
      "val_loss: 0.16471275827549534\n",
      "Progress: 39.7% ... Training loss: 0.087 ... Validation loss: 0.182iteration: 3969\n",
      "train_loss: 0.08704862763351236\n",
      "val_loss: 0.18259863404562332\n",
      "Progress: 39.7% ... Training loss: 0.080 ... Validation loss: 0.163iteration: 3970\n",
      "train_loss: 0.0804460736822787\n",
      "val_loss: 0.16316310788245858\n",
      "Progress: 39.7% ... Training loss: 0.079 ... Validation loss: 0.165iteration: 3971\n",
      "train_loss: 0.07951818445190477\n",
      "val_loss: 0.16576357331975167\n",
      "Progress: 39.7% ... Training loss: 0.080 ... Validation loss: 0.160iteration: 3972\n",
      "train_loss: 0.08053013751879987\n",
      "val_loss: 0.16021584071664247\n",
      "Progress: 39.7% ... Training loss: 0.083 ... Validation loss: 0.163iteration: 3973\n",
      "train_loss: 0.08334629992437421\n",
      "val_loss: 0.16315872706456339\n",
      "Progress: 39.7% ... Training loss: 0.080 ... Validation loss: 0.178iteration: 3974\n",
      "train_loss: 0.08051356391346562\n",
      "val_loss: 0.17806567751739363\n",
      "Progress: 39.8% ... Training loss: 0.081 ... Validation loss: 0.165iteration: 3975\n",
      "train_loss: 0.08180752495434601\n",
      "val_loss: 0.16525299477146677\n",
      "Progress: 39.8% ... Training loss: 0.079 ... Validation loss: 0.171iteration: 3976\n",
      "train_loss: 0.07950261727765451\n",
      "val_loss: 0.17139271464136344\n",
      "Progress: 39.8% ... Training loss: 0.093 ... Validation loss: 0.168iteration: 3977\n",
      "train_loss: 0.09359867715222142\n",
      "val_loss: 0.16811187864294397\n",
      "Progress: 39.8% ... Training loss: 0.127 ... Validation loss: 0.239iteration: 3978\n",
      "train_loss: 0.12771641462987093\n",
      "val_loss: 0.23917956870741047\n",
      "Progress: 39.8% ... Training loss: 0.102 ... Validation loss: 0.172iteration: 3979\n",
      "train_loss: 0.10279180565200913\n",
      "val_loss: 0.17243599759925138\n",
      "Progress: 39.8% ... Training loss: 0.092 ... Validation loss: 0.210iteration: 3980\n",
      "train_loss: 0.09286736379766633\n",
      "val_loss: 0.2101678310198874\n",
      "Progress: 39.8% ... Training loss: 0.098 ... Validation loss: 0.170iteration: 3981\n",
      "train_loss: 0.09890231731739181\n",
      "val_loss: 0.1709965181968839\n",
      "Progress: 39.8% ... Training loss: 0.085 ... Validation loss: 0.189iteration: 3982\n",
      "train_loss: 0.0854432655123494\n",
      "val_loss: 0.18945449409040951\n",
      "Progress: 39.8% ... Training loss: 0.079 ... Validation loss: 0.163iteration: 3983\n",
      "train_loss: 0.07997398834577164\n",
      "val_loss: 0.16327384523284863\n",
      "Progress: 39.8% ... Training loss: 0.079 ... Validation loss: 0.175iteration: 3984\n",
      "train_loss: 0.07986138006014964\n",
      "val_loss: 0.17508284498129495\n",
      "Progress: 39.9% ... Training loss: 0.085 ... Validation loss: 0.164iteration: 3985\n",
      "train_loss: 0.08510677343527921\n",
      "val_loss: 0.16443329793329337\n",
      "Progress: 39.9% ... Training loss: 0.100 ... Validation loss: 0.225iteration: 3986\n",
      "train_loss: 0.1008707895517901\n",
      "val_loss: 0.22582872527089218\n",
      "Progress: 39.9% ... Training loss: 0.088 ... Validation loss: 0.161iteration: 3987\n",
      "train_loss: 0.0883763900302551\n",
      "val_loss: 0.1611547799882449\n",
      "Progress: 39.9% ... Training loss: 0.088 ... Validation loss: 0.197iteration: 3988\n",
      "train_loss: 0.08858881549635658\n",
      "val_loss: 0.1977720095872698\n",
      "Progress: 39.9% ... Training loss: 0.088 ... Validation loss: 0.160iteration: 3989\n",
      "train_loss: 0.08894252202443567\n",
      "val_loss: 0.1607554688278941\n",
      "Progress: 39.9% ... Training loss: 0.123 ... Validation loss: 0.228iteration: 3990\n",
      "train_loss: 0.12300031951358799\n",
      "val_loss: 0.2289334750454684\n",
      "Progress: 39.9% ... Training loss: 0.108 ... Validation loss: 0.173iteration: 3991\n",
      "train_loss: 0.10812739859479263\n",
      "val_loss: 0.17388595545612212\n",
      "Progress: 39.9% ... Training loss: 0.097 ... Validation loss: 0.217iteration: 3992\n",
      "train_loss: 0.09765162200351346\n",
      "val_loss: 0.21714038726980978\n",
      "Progress: 39.9% ... Training loss: 0.097 ... Validation loss: 0.170iteration: 3993\n",
      "train_loss: 0.09744611454783086\n",
      "val_loss: 0.17064447558489598\n",
      "Progress: 39.9% ... Training loss: 0.123 ... Validation loss: 0.269iteration: 3994\n",
      "train_loss: 0.12307348205464118\n",
      "val_loss: 0.26980304048308457\n",
      "Progress: 40.0% ... Training loss: 0.125 ... Validation loss: 0.174iteration: 3995\n",
      "train_loss: 0.125505091942439\n",
      "val_loss: 0.1748692333547517\n",
      "Progress: 40.0% ... Training loss: 0.121 ... Validation loss: 0.252iteration: 3996\n",
      "train_loss: 0.12137097087093009\n",
      "val_loss: 0.25268638962392126\n",
      "Progress: 40.0% ... Training loss: 0.091 ... Validation loss: 0.164iteration: 3997\n",
      "train_loss: 0.09191257425758072\n",
      "val_loss: 0.16491964686583227\n",
      "Progress: 40.0% ... Training loss: 0.086 ... Validation loss: 0.192iteration: 3998\n",
      "train_loss: 0.08695519936189326\n",
      "val_loss: 0.19257283708365527\n",
      "Progress: 40.0% ... Training loss: 0.098 ... Validation loss: 0.163iteration: 3999\n",
      "train_loss: 0.09806156063730748\n",
      "val_loss: 0.16327844370378714\n",
      "Progress: 40.0% ... Training loss: 0.078 ... Validation loss: 0.166iteration: 4000\n",
      "train_loss: 0.07834965006716019\n",
      "val_loss: 0.16602289553480964\n",
      "Progress: 40.0% ... Training loss: 0.079 ... Validation loss: 0.162iteration: 4001\n",
      "train_loss: 0.07934513723512357\n",
      "val_loss: 0.16264584562888793\n",
      "Progress: 40.0% ... Training loss: 0.081 ... Validation loss: 0.172iteration: 4002\n",
      "train_loss: 0.081659196144781\n",
      "val_loss: 0.17299904766200502\n",
      "Progress: 40.0% ... Training loss: 0.078 ... Validation loss: 0.163iteration: 4003\n",
      "train_loss: 0.07885503868861769\n",
      "val_loss: 0.16396860211027683\n",
      "Progress: 40.0% ... Training loss: 0.079 ... Validation loss: 0.171iteration: 4004\n",
      "train_loss: 0.0794541387641168\n",
      "val_loss: 0.17153941625080726\n",
      "Progress: 40.0% ... Training loss: 0.083 ... Validation loss: 0.162iteration: 4005\n",
      "train_loss: 0.08354444788264231\n",
      "val_loss: 0.16247647196367698\n",
      "Progress: 40.1% ... Training loss: 0.079 ... Validation loss: 0.180iteration: 4006\n",
      "train_loss: 0.07995972261659276\n",
      "val_loss: 0.18065148387697003\n",
      "Progress: 40.1% ... Training loss: 0.085 ... Validation loss: 0.162iteration: 4007\n",
      "train_loss: 0.08544126358808553\n",
      "val_loss: 0.16213565765743024\n",
      "Progress: 40.1% ... Training loss: 0.102 ... Validation loss: 0.234iteration: 4008\n",
      "train_loss: 0.10207665416035853\n",
      "val_loss: 0.2347304180931431\n",
      "Progress: 40.1% ... Training loss: 0.086 ... Validation loss: 0.161iteration: 4009\n",
      "train_loss: 0.08639944009638341\n",
      "val_loss: 0.16193328961894465\n",
      "Progress: 40.1% ... Training loss: 0.104 ... Validation loss: 0.216iteration: 4010\n",
      "train_loss: 0.10449134475704361\n",
      "val_loss: 0.21637878244166547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 40.1% ... Training loss: 0.093 ... Validation loss: 0.159iteration: 4011\n",
      "train_loss: 0.09338198286320007\n",
      "val_loss: 0.15939832270244866\n",
      "Progress: 40.1% ... Training loss: 0.088 ... Validation loss: 0.207iteration: 4012\n",
      "train_loss: 0.08821087813275183\n",
      "val_loss: 0.2077086510417982\n",
      "Progress: 40.1% ... Training loss: 0.091 ... Validation loss: 0.161iteration: 4013\n",
      "train_loss: 0.09132921404758362\n",
      "val_loss: 0.16171135772330084\n",
      "Progress: 40.1% ... Training loss: 0.112 ... Validation loss: 0.241iteration: 4014\n",
      "train_loss: 0.1121832156428133\n",
      "val_loss: 0.24167349002678556\n",
      "Progress: 40.1% ... Training loss: 0.083 ... Validation loss: 0.163iteration: 4015\n",
      "train_loss: 0.08368859535379472\n",
      "val_loss: 0.1636194933505028\n",
      "Progress: 40.2% ... Training loss: 0.083 ... Validation loss: 0.196iteration: 4016\n",
      "train_loss: 0.08380316385667413\n",
      "val_loss: 0.19616996595399722\n",
      "Progress: 40.2% ... Training loss: 0.080 ... Validation loss: 0.167iteration: 4017\n",
      "train_loss: 0.08054221353970761\n",
      "val_loss: 0.1673751637747414\n",
      "Progress: 40.2% ... Training loss: 0.080 ... Validation loss: 0.169iteration: 4018\n",
      "train_loss: 0.08095137430186507\n",
      "val_loss: 0.16914223546667045\n",
      "Progress: 40.2% ... Training loss: 0.082 ... Validation loss: 0.183iteration: 4019\n",
      "train_loss: 0.08255833286917202\n",
      "val_loss: 0.18326962449947726\n",
      "Progress: 40.2% ... Training loss: 0.078 ... Validation loss: 0.162iteration: 4020\n",
      "train_loss: 0.0786580496881817\n",
      "val_loss: 0.16203884359654946\n",
      "Progress: 40.2% ... Training loss: 0.078 ... Validation loss: 0.175iteration: 4021\n",
      "train_loss: 0.0788938648511965\n",
      "val_loss: 0.17591311199656723\n",
      "Progress: 40.2% ... Training loss: 0.078 ... Validation loss: 0.170iteration: 4022\n",
      "train_loss: 0.07856397001108281\n",
      "val_loss: 0.17062643054194804\n",
      "Progress: 40.2% ... Training loss: 0.078 ... Validation loss: 0.167iteration: 4023\n",
      "train_loss: 0.07841208463130621\n",
      "val_loss: 0.1673677510839989\n",
      "Progress: 40.2% ... Training loss: 0.095 ... Validation loss: 0.164iteration: 4024\n",
      "train_loss: 0.09513706757980123\n",
      "val_loss: 0.16457150487493463\n",
      "Progress: 40.2% ... Training loss: 0.079 ... Validation loss: 0.189iteration: 4025\n",
      "train_loss: 0.07943051320946223\n",
      "val_loss: 0.18930496442487005\n",
      "Progress: 40.3% ... Training loss: 0.081 ... Validation loss: 0.166iteration: 4026\n",
      "train_loss: 0.08182725811041315\n",
      "val_loss: 0.1664710020684452\n",
      "Progress: 40.3% ... Training loss: 0.086 ... Validation loss: 0.205iteration: 4027\n",
      "train_loss: 0.086815399981326\n",
      "val_loss: 0.20574285015001129\n",
      "Progress: 40.3% ... Training loss: 0.082 ... Validation loss: 0.163iteration: 4028\n",
      "train_loss: 0.08271043909052618\n",
      "val_loss: 0.1636795571842658\n",
      "Progress: 40.3% ... Training loss: 0.086 ... Validation loss: 0.186iteration: 4029\n",
      "train_loss: 0.08671040945963959\n",
      "val_loss: 0.18665891330697346\n",
      "Progress: 40.3% ... Training loss: 0.084 ... Validation loss: 0.158iteration: 4030\n",
      "train_loss: 0.08405917014926483\n",
      "val_loss: 0.15857186315049326\n",
      "Progress: 40.3% ... Training loss: 0.085 ... Validation loss: 0.192iteration: 4031\n",
      "train_loss: 0.08573952455583772\n",
      "val_loss: 0.19263607532615523\n",
      "Progress: 40.3% ... Training loss: 0.081 ... Validation loss: 0.162iteration: 4032\n",
      "train_loss: 0.08151686886343146\n",
      "val_loss: 0.16268120309244666\n",
      "Progress: 40.3% ... Training loss: 0.082 ... Validation loss: 0.184iteration: 4033\n",
      "train_loss: 0.08242082382992125\n",
      "val_loss: 0.1845461846907494\n",
      "Progress: 40.3% ... Training loss: 0.083 ... Validation loss: 0.161iteration: 4034\n",
      "train_loss: 0.08372077793662348\n",
      "val_loss: 0.16127753675905998\n",
      "Progress: 40.4% ... Training loss: 0.082 ... Validation loss: 0.164iteration: 4035\n",
      "train_loss: 0.08256141956099951\n",
      "val_loss: 0.16461925203082553\n",
      "Progress: 40.4% ... Training loss: 0.078 ... Validation loss: 0.162iteration: 4036\n",
      "train_loss: 0.07857729702664959\n",
      "val_loss: 0.16287111941547355\n",
      "Progress: 40.4% ... Training loss: 0.083 ... Validation loss: 0.164iteration: 4037\n",
      "train_loss: 0.08307397954137288\n",
      "val_loss: 0.16407509057599412\n",
      "Progress: 40.4% ... Training loss: 0.089 ... Validation loss: 0.200iteration: 4038\n",
      "train_loss: 0.08940291938635911\n",
      "val_loss: 0.20094430290721993\n",
      "Progress: 40.4% ... Training loss: 0.102 ... Validation loss: 0.169iteration: 4039\n",
      "train_loss: 0.10222224685899799\n",
      "val_loss: 0.16958024795322058\n",
      "Progress: 40.4% ... Training loss: 0.097 ... Validation loss: 0.194iteration: 4040\n",
      "train_loss: 0.09786193145903385\n",
      "val_loss: 0.1946742645863875\n",
      "Progress: 40.4% ... Training loss: 0.090 ... Validation loss: 0.164iteration: 4041\n",
      "train_loss: 0.09046020725927985\n",
      "val_loss: 0.1648531178404067\n",
      "Progress: 40.4% ... Training loss: 0.083 ... Validation loss: 0.178iteration: 4042\n",
      "train_loss: 0.0832152093723157\n",
      "val_loss: 0.17880530000345374\n",
      "Progress: 40.4% ... Training loss: 0.080 ... Validation loss: 0.160iteration: 4043\n",
      "train_loss: 0.08025648503713113\n",
      "val_loss: 0.16065489388980222\n",
      "Progress: 40.4% ... Training loss: 0.077 ... Validation loss: 0.171iteration: 4044\n",
      "train_loss: 0.07739271445149341\n",
      "val_loss: 0.17162782774318333\n",
      "Progress: 40.5% ... Training loss: 0.079 ... Validation loss: 0.177iteration: 4045\n",
      "train_loss: 0.07986057410685579\n",
      "val_loss: 0.1771655948685054\n",
      "Progress: 40.5% ... Training loss: 0.081 ... Validation loss: 0.181iteration: 4046\n",
      "train_loss: 0.08193082461114815\n",
      "val_loss: 0.18123571761684512\n",
      "Progress: 40.5% ... Training loss: 0.078 ... Validation loss: 0.162iteration: 4047\n",
      "train_loss: 0.07860560273130951\n",
      "val_loss: 0.16201957583030807\n",
      "Progress: 40.5% ... Training loss: 0.078 ... Validation loss: 0.173iteration: 4048\n",
      "train_loss: 0.07815741996189826\n",
      "val_loss: 0.17332233327441268\n",
      "Progress: 40.5% ... Training loss: 0.078 ... Validation loss: 0.167iteration: 4049\n",
      "train_loss: 0.07817648999105066\n",
      "val_loss: 0.1676970248382298\n",
      "Progress: 40.5% ... Training loss: 0.080 ... Validation loss: 0.188iteration: 4050\n",
      "train_loss: 0.08013372537044838\n",
      "val_loss: 0.1889525239327504\n",
      "Progress: 40.5% ... Training loss: 0.084 ... Validation loss: 0.161iteration: 4051\n",
      "train_loss: 0.08433124514655249\n",
      "val_loss: 0.16117870594112851\n",
      "Progress: 40.5% ... Training loss: 0.081 ... Validation loss: 0.194iteration: 4052\n",
      "train_loss: 0.0815824969884706\n",
      "val_loss: 0.194758720823361\n",
      "Progress: 40.5% ... Training loss: 0.082 ... Validation loss: 0.160iteration: 4053\n",
      "train_loss: 0.0823962190194324\n",
      "val_loss: 0.16085380883040015\n",
      "Progress: 40.5% ... Training loss: 0.085 ... Validation loss: 0.185iteration: 4054\n",
      "train_loss: 0.08560826025662326\n",
      "val_loss: 0.1857736343153173\n",
      "Progress: 40.5% ... Training loss: 0.097 ... Validation loss: 0.166iteration: 4055\n",
      "train_loss: 0.09742958332540058\n",
      "val_loss: 0.16655501415742288\n",
      "Progress: 40.6% ... Training loss: 0.080 ... Validation loss: 0.187iteration: 4056\n",
      "train_loss: 0.08046759089982276\n",
      "val_loss: 0.18773639228216782\n",
      "Progress: 40.6% ... Training loss: 0.077 ... Validation loss: 0.165iteration: 4057\n",
      "train_loss: 0.07781109636289146\n",
      "val_loss: 0.16590662449231205\n",
      "Progress: 40.6% ... Training loss: 0.081 ... Validation loss: 0.183iteration: 4058\n",
      "train_loss: 0.08192714789188833\n",
      "val_loss: 0.18314230951582586\n",
      "Progress: 40.6% ... Training loss: 0.078 ... Validation loss: 0.167iteration: 4059\n",
      "train_loss: 0.07813857089387542\n",
      "val_loss: 0.16764251986595927\n",
      "Progress: 40.6% ... Training loss: 0.078 ... Validation loss: 0.171iteration: 4060\n",
      "train_loss: 0.07828835930606078\n",
      "val_loss: 0.17168025537649065\n",
      "Progress: 40.6% ... Training loss: 0.077 ... Validation loss: 0.167iteration: 4061\n",
      "train_loss: 0.07756583163655248\n",
      "val_loss: 0.1674813873834995\n",
      "Progress: 40.6% ... Training loss: 0.077 ... Validation loss: 0.164iteration: 4062\n",
      "train_loss: 0.07744280071265289\n",
      "val_loss: 0.1649786536097037\n",
      "Progress: 40.6% ... Training loss: 0.078 ... Validation loss: 0.164iteration: 4063\n",
      "train_loss: 0.07813640622403943\n",
      "val_loss: 0.16444075207721082\n",
      "Progress: 40.6% ... Training loss: 0.079 ... Validation loss: 0.162iteration: 4064\n",
      "train_loss: 0.07970170237161141\n",
      "val_loss: 0.1622992097300105\n",
      "Progress: 40.6% ... Training loss: 0.078 ... Validation loss: 0.169iteration: 4065\n",
      "train_loss: 0.07812555705987295\n",
      "val_loss: 0.16953991086295087\n",
      "Progress: 40.7% ... Training loss: 0.080 ... Validation loss: 0.161iteration: 4066\n",
      "train_loss: 0.08044932479635514\n",
      "val_loss: 0.1611233638389472\n",
      "Progress: 40.7% ... Training loss: 0.079 ... Validation loss: 0.184iteration: 4067\n",
      "train_loss: 0.07955514951176579\n",
      "val_loss: 0.18445974955141847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 40.7% ... Training loss: 0.078 ... Validation loss: 0.165iteration: 4068\n",
      "train_loss: 0.07844771988359675\n",
      "val_loss: 0.1652284839169143\n",
      "Progress: 40.7% ... Training loss: 0.077 ... Validation loss: 0.168iteration: 4069\n",
      "train_loss: 0.0778739488313612\n",
      "val_loss: 0.16860926903217865\n",
      "Progress: 40.7% ... Training loss: 0.077 ... Validation loss: 0.168iteration: 4070\n",
      "train_loss: 0.07797366959892683\n",
      "val_loss: 0.16873584755193335\n",
      "Progress: 40.7% ... Training loss: 0.086 ... Validation loss: 0.157iteration: 4071\n",
      "train_loss: 0.08671190844751026\n",
      "val_loss: 0.15783393125767745\n",
      "Progress: 40.7% ... Training loss: 0.092 ... Validation loss: 0.190iteration: 4072\n",
      "train_loss: 0.09251334212718751\n",
      "val_loss: 0.19081283166107788\n",
      "Progress: 40.7% ... Training loss: 0.084 ... Validation loss: 0.155iteration: 4073\n",
      "train_loss: 0.08425454872696506\n",
      "val_loss: 0.15582700183981263\n",
      "Progress: 40.7% ... Training loss: 0.082 ... Validation loss: 0.171iteration: 4074\n",
      "train_loss: 0.08270089443332522\n",
      "val_loss: 0.17106508298977466\n",
      "Progress: 40.8% ... Training loss: 0.082 ... Validation loss: 0.162iteration: 4075\n",
      "train_loss: 0.0828643787249647\n",
      "val_loss: 0.16281346252954557\n",
      "Progress: 40.8% ... Training loss: 0.086 ... Validation loss: 0.195iteration: 4076\n",
      "train_loss: 0.08632882783620024\n",
      "val_loss: 0.1950736216905136\n",
      "Progress: 40.8% ... Training loss: 0.083 ... Validation loss: 0.163iteration: 4077\n",
      "train_loss: 0.08350907533204928\n",
      "val_loss: 0.1639121726001576\n",
      "Progress: 40.8% ... Training loss: 0.076 ... Validation loss: 0.169iteration: 4078\n",
      "train_loss: 0.07652556895813159\n",
      "val_loss: 0.16937991754413642\n",
      "Progress: 40.8% ... Training loss: 0.077 ... Validation loss: 0.167iteration: 4079\n",
      "train_loss: 0.0773251556450827\n",
      "val_loss: 0.1679552590348348\n",
      "Progress: 40.8% ... Training loss: 0.078 ... Validation loss: 0.163iteration: 4080\n",
      "train_loss: 0.0781515910115525\n",
      "val_loss: 0.16367954054221573\n",
      "Progress: 40.8% ... Training loss: 0.079 ... Validation loss: 0.179iteration: 4081\n",
      "train_loss: 0.07947640045246614\n",
      "val_loss: 0.17941992233962695\n",
      "Progress: 40.8% ... Training loss: 0.080 ... Validation loss: 0.156iteration: 4082\n",
      "train_loss: 0.08085825183986581\n",
      "val_loss: 0.1561172687800506\n",
      "Progress: 40.8% ... Training loss: 0.076 ... Validation loss: 0.169iteration: 4083\n",
      "train_loss: 0.07690504821144083\n",
      "val_loss: 0.1690641458232655\n",
      "Progress: 40.8% ... Training loss: 0.077 ... Validation loss: 0.180iteration: 4084\n",
      "train_loss: 0.07794231644782264\n",
      "val_loss: 0.18028362922900495\n",
      "Progress: 40.9% ... Training loss: 0.077 ... Validation loss: 0.179iteration: 4085\n",
      "train_loss: 0.0774888696594026\n",
      "val_loss: 0.1794417337180401\n",
      "Progress: 40.9% ... Training loss: 0.087 ... Validation loss: 0.162iteration: 4086\n",
      "train_loss: 0.08737604226759689\n",
      "val_loss: 0.1621757060042682\n",
      "Progress: 40.9% ... Training loss: 0.080 ... Validation loss: 0.196iteration: 4087\n",
      "train_loss: 0.08007983209295638\n",
      "val_loss: 0.19682709386677533\n",
      "Progress: 40.9% ... Training loss: 0.078 ... Validation loss: 0.188iteration: 4088\n",
      "train_loss: 0.07881253106156519\n",
      "val_loss: 0.18898198313108555\n",
      "Progress: 40.9% ... Training loss: 0.077 ... Validation loss: 0.167iteration: 4089\n",
      "train_loss: 0.07732591312608986\n",
      "val_loss: 0.1678281620627474\n",
      "Progress: 40.9% ... Training loss: 0.083 ... Validation loss: 0.189iteration: 4090\n",
      "train_loss: 0.08374943327151997\n",
      "val_loss: 0.18905031644446824\n",
      "Progress: 40.9% ... Training loss: 0.077 ... Validation loss: 0.167iteration: 4091\n",
      "train_loss: 0.07732536039143165\n",
      "val_loss: 0.16746516208687845\n",
      "Progress: 40.9% ... Training loss: 0.078 ... Validation loss: 0.157iteration: 4092\n",
      "train_loss: 0.07803073607660752\n",
      "val_loss: 0.15742158504189493\n",
      "Progress: 40.9% ... Training loss: 0.088 ... Validation loss: 0.185iteration: 4093\n",
      "train_loss: 0.0888235732631342\n",
      "val_loss: 0.18564479397055172\n",
      "Progress: 40.9% ... Training loss: 0.080 ... Validation loss: 0.161iteration: 4094\n",
      "train_loss: 0.08027756324173985\n",
      "val_loss: 0.16117642709252533\n",
      "Progress: 41.0% ... Training loss: 0.077 ... Validation loss: 0.172iteration: 4095\n",
      "train_loss: 0.07752495786886433\n",
      "val_loss: 0.17206013822862068\n",
      "Progress: 41.0% ... Training loss: 0.077 ... Validation loss: 0.169iteration: 4096\n",
      "train_loss: 0.07705040882546844\n",
      "val_loss: 0.1695835536891661\n",
      "Progress: 41.0% ... Training loss: 0.077 ... Validation loss: 0.159iteration: 4097\n",
      "train_loss: 0.07705554299109006\n",
      "val_loss: 0.15919451146499203\n",
      "Progress: 41.0% ... Training loss: 0.077 ... Validation loss: 0.165iteration: 4098\n",
      "train_loss: 0.07713990828561995\n",
      "val_loss: 0.16530811928173356\n",
      "Progress: 41.0% ... Training loss: 0.078 ... Validation loss: 0.166iteration: 4099\n",
      "train_loss: 0.07808911508394333\n",
      "val_loss: 0.16697009423192546\n",
      "Progress: 41.0% ... Training loss: 0.077 ... Validation loss: 0.155iteration: 4100\n",
      "train_loss: 0.07783548112383674\n",
      "val_loss: 0.15579659759300482\n",
      "Progress: 41.0% ... Training loss: 0.082 ... Validation loss: 0.157iteration: 4101\n",
      "train_loss: 0.08257943685109084\n",
      "val_loss: 0.15779913883703825\n",
      "Progress: 41.0% ... Training loss: 0.087 ... Validation loss: 0.203iteration: 4102\n",
      "train_loss: 0.08771829856698712\n",
      "val_loss: 0.20389556536554013\n",
      "Progress: 41.0% ... Training loss: 0.080 ... Validation loss: 0.169iteration: 4103\n",
      "train_loss: 0.0807910136258415\n",
      "val_loss: 0.1691323701389712\n",
      "Progress: 41.0% ... Training loss: 0.078 ... Validation loss: 0.175iteration: 4104\n",
      "train_loss: 0.07843130119459256\n",
      "val_loss: 0.17590909884130168\n",
      "Progress: 41.0% ... Training loss: 0.078 ... Validation loss: 0.160iteration: 4105\n",
      "train_loss: 0.07807243238899965\n",
      "val_loss: 0.16060103097873663\n",
      "Progress: 41.1% ... Training loss: 0.077 ... Validation loss: 0.167iteration: 4106\n",
      "train_loss: 0.07701033056471943\n",
      "val_loss: 0.16770395511362698\n",
      "Progress: 41.1% ... Training loss: 0.081 ... Validation loss: 0.162iteration: 4107\n",
      "train_loss: 0.08107401886010136\n",
      "val_loss: 0.1623770379922064\n",
      "Progress: 41.1% ... Training loss: 0.082 ... Validation loss: 0.183iteration: 4108\n",
      "train_loss: 0.08205049654389204\n",
      "val_loss: 0.18353373857920788\n",
      "Progress: 41.1% ... Training loss: 0.088 ... Validation loss: 0.157iteration: 4109\n",
      "train_loss: 0.08852133889341844\n",
      "val_loss: 0.15789586291617314\n",
      "Progress: 41.1% ... Training loss: 0.077 ... Validation loss: 0.166iteration: 4110\n",
      "train_loss: 0.07735325547779788\n",
      "val_loss: 0.16690471576836544\n",
      "Progress: 41.1% ... Training loss: 0.083 ... Validation loss: 0.183iteration: 4111\n",
      "train_loss: 0.0834476759586756\n",
      "val_loss: 0.18313665518540564\n",
      "Progress: 41.1% ... Training loss: 0.077 ... Validation loss: 0.165iteration: 4112\n",
      "train_loss: 0.07773193204106131\n",
      "val_loss: 0.165643731340119\n",
      "Progress: 41.1% ... Training loss: 0.077 ... Validation loss: 0.173iteration: 4113\n",
      "train_loss: 0.07715290458242322\n",
      "val_loss: 0.17338589525780052\n",
      "Progress: 41.1% ... Training loss: 0.076 ... Validation loss: 0.166iteration: 4114\n",
      "train_loss: 0.07655877454700213\n",
      "val_loss: 0.16613193364886064\n",
      "Progress: 41.1% ... Training loss: 0.077 ... Validation loss: 0.170iteration: 4115\n",
      "train_loss: 0.07768794047988009\n",
      "val_loss: 0.170297108148043\n",
      "Progress: 41.2% ... Training loss: 0.076 ... Validation loss: 0.166iteration: 4116\n",
      "train_loss: 0.07665143383143401\n",
      "val_loss: 0.16698641546613377\n",
      "Progress: 41.2% ... Training loss: 0.076 ... Validation loss: 0.167iteration: 4117\n",
      "train_loss: 0.0766433205186262\n",
      "val_loss: 0.1670141819798275\n",
      "Progress: 41.2% ... Training loss: 0.095 ... Validation loss: 0.164iteration: 4118\n",
      "train_loss: 0.09560375142249021\n",
      "val_loss: 0.16488872458626352\n",
      "Progress: 41.2% ... Training loss: 0.095 ... Validation loss: 0.206iteration: 4119\n",
      "train_loss: 0.09539353915250817\n",
      "val_loss: 0.20608269598204088\n",
      "Progress: 41.2% ... Training loss: 0.090 ... Validation loss: 0.159iteration: 4120\n",
      "train_loss: 0.09063841224985732\n",
      "val_loss: 0.15987823759209377\n",
      "Progress: 41.2% ... Training loss: 0.083 ... Validation loss: 0.196iteration: 4121\n",
      "train_loss: 0.08342564783146297\n",
      "val_loss: 0.19668386437867816\n",
      "Progress: 41.2% ... Training loss: 0.084 ... Validation loss: 0.163iteration: 4122\n",
      "train_loss: 0.08406771130121976\n",
      "val_loss: 0.16300862564306226\n",
      "Progress: 41.2% ... Training loss: 0.078 ... Validation loss: 0.188iteration: 4123\n",
      "train_loss: 0.0788267652606167\n",
      "val_loss: 0.18824240513086238\n",
      "Progress: 41.2% ... Training loss: 0.076 ... Validation loss: 0.174iteration: 4124\n",
      "train_loss: 0.07646550198678478\n",
      "val_loss: 0.17458556588121046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 41.2% ... Training loss: 0.083 ... Validation loss: 0.201iteration: 4125\n",
      "train_loss: 0.08395821493247503\n",
      "val_loss: 0.20129844764662144\n",
      "Progress: 41.3% ... Training loss: 0.086 ... Validation loss: 0.163iteration: 4126\n",
      "train_loss: 0.08699733729611085\n",
      "val_loss: 0.1630161520879742\n",
      "Progress: 41.3% ... Training loss: 0.101 ... Validation loss: 0.237iteration: 4127\n",
      "train_loss: 0.10160814025180992\n",
      "val_loss: 0.23729105273439394\n",
      "Progress: 41.3% ... Training loss: 0.101 ... Validation loss: 0.165iteration: 4128\n",
      "train_loss: 0.10100251424968416\n",
      "val_loss: 0.16566196233371472\n",
      "Progress: 41.3% ... Training loss: 0.106 ... Validation loss: 0.230iteration: 4129\n",
      "train_loss: 0.10695291093829191\n",
      "val_loss: 0.23079617249117712\n",
      "Progress: 41.3% ... Training loss: 0.104 ... Validation loss: 0.163iteration: 4130\n",
      "train_loss: 0.10448667075755488\n",
      "val_loss: 0.16383725886582287\n",
      "Progress: 41.3% ... Training loss: 0.111 ... Validation loss: 0.242iteration: 4131\n",
      "train_loss: 0.11169247071368985\n",
      "val_loss: 0.24298779597348039\n",
      "Progress: 41.3% ... Training loss: 0.089 ... Validation loss: 0.158iteration: 4132\n",
      "train_loss: 0.08996173118543953\n",
      "val_loss: 0.15853943375223772\n",
      "Progress: 41.3% ... Training loss: 0.080 ... Validation loss: 0.186iteration: 4133\n",
      "train_loss: 0.08087549729503239\n",
      "val_loss: 0.18622061256724526\n",
      "Progress: 41.3% ... Training loss: 0.078 ... Validation loss: 0.167iteration: 4134\n",
      "train_loss: 0.07810359628678394\n",
      "val_loss: 0.16731338819666822\n",
      "Progress: 41.4% ... Training loss: 0.077 ... Validation loss: 0.191iteration: 4135\n",
      "train_loss: 0.07775486711653848\n",
      "val_loss: 0.1918630508225173\n",
      "Progress: 41.4% ... Training loss: 0.077 ... Validation loss: 0.170iteration: 4136\n",
      "train_loss: 0.0775595780813595\n",
      "val_loss: 0.17002789856804243\n",
      "Progress: 41.4% ... Training loss: 0.077 ... Validation loss: 0.195iteration: 4137\n",
      "train_loss: 0.07781654208275576\n",
      "val_loss: 0.1951941999499503\n",
      "Progress: 41.4% ... Training loss: 0.083 ... Validation loss: 0.167iteration: 4138\n",
      "train_loss: 0.08325968391339217\n",
      "val_loss: 0.16700924334711534\n",
      "Progress: 41.4% ... Training loss: 0.081 ... Validation loss: 0.206iteration: 4139\n",
      "train_loss: 0.08111397062806952\n",
      "val_loss: 0.20694072715189143\n",
      "Progress: 41.4% ... Training loss: 0.077 ... Validation loss: 0.168iteration: 4140\n",
      "train_loss: 0.07716666495931378\n",
      "val_loss: 0.1686530670332412\n",
      "Progress: 41.4% ... Training loss: 0.078 ... Validation loss: 0.163iteration: 4141\n",
      "train_loss: 0.07804611601212162\n",
      "val_loss: 0.1634178217465866\n",
      "Progress: 41.4% ... Training loss: 0.081 ... Validation loss: 0.191iteration: 4142\n",
      "train_loss: 0.08134069666043768\n",
      "val_loss: 0.19166292096474716\n",
      "Progress: 41.4% ... Training loss: 0.097 ... Validation loss: 0.161iteration: 4143\n",
      "train_loss: 0.09706510899521545\n",
      "val_loss: 0.1619748061567383\n",
      "Progress: 41.4% ... Training loss: 0.098 ... Validation loss: 0.226iteration: 4144\n",
      "train_loss: 0.09895754314925609\n",
      "val_loss: 0.22645531233791738\n",
      "Progress: 41.5% ... Training loss: 0.098 ... Validation loss: 0.158iteration: 4145\n",
      "train_loss: 0.09806901791782958\n",
      "val_loss: 0.15886080130662994\n",
      "Progress: 41.5% ... Training loss: 0.100 ... Validation loss: 0.228iteration: 4146\n",
      "train_loss: 0.1005583275106827\n",
      "val_loss: 0.2283065256614796\n",
      "Progress: 41.5% ... Training loss: 0.099 ... Validation loss: 0.165iteration: 4147\n",
      "train_loss: 0.09905268125181349\n",
      "val_loss: 0.1655372047662007\n",
      "Progress: 41.5% ... Training loss: 0.108 ... Validation loss: 0.224iteration: 4148\n",
      "train_loss: 0.10804088488213841\n",
      "val_loss: 0.2244407244186406\n",
      "Progress: 41.5% ... Training loss: 0.109 ... Validation loss: 0.168iteration: 4149\n",
      "train_loss: 0.10991496221526817\n",
      "val_loss: 0.16870412783356833\n",
      "Progress: 41.5% ... Training loss: 0.128 ... Validation loss: 0.280iteration: 4150\n",
      "train_loss: 0.12885043866207171\n",
      "val_loss: 0.2808243166128932\n",
      "Progress: 41.5% ... Training loss: 0.128 ... Validation loss: 0.173iteration: 4151\n",
      "train_loss: 0.1289884178309928\n",
      "val_loss: 0.17373484682112814\n",
      "Progress: 41.5% ... Training loss: 0.146 ... Validation loss: 0.284iteration: 4152\n",
      "train_loss: 0.14624207401454345\n",
      "val_loss: 0.2848554915885524\n",
      "Progress: 41.5% ... Training loss: 0.148 ... Validation loss: 0.189iteration: 4153\n",
      "train_loss: 0.14869799979092052\n",
      "val_loss: 0.18929798851393925\n",
      "Progress: 41.5% ... Training loss: 0.158 ... Validation loss: 0.304iteration: 4154\n",
      "train_loss: 0.15851018356576332\n",
      "val_loss: 0.30473471753469733\n",
      "Progress: 41.5% ... Training loss: 0.181 ... Validation loss: 0.205iteration: 4155\n",
      "train_loss: 0.18179160757188811\n",
      "val_loss: 0.20545040210530582\n",
      "Progress: 41.6% ... Training loss: 0.126 ... Validation loss: 0.299iteration: 4156\n",
      "train_loss: 0.12660621278601394\n",
      "val_loss: 0.29907948322724287\n",
      "Progress: 41.6% ... Training loss: 0.102 ... Validation loss: 0.162iteration: 4157\n",
      "train_loss: 0.1022884316259005\n",
      "val_loss: 0.1627288087714263\n",
      "Progress: 41.6% ... Training loss: 0.088 ... Validation loss: 0.211iteration: 4158\n",
      "train_loss: 0.08882165987985614\n",
      "val_loss: 0.21193923359345623\n",
      "Progress: 41.6% ... Training loss: 0.094 ... Validation loss: 0.163iteration: 4159\n",
      "train_loss: 0.09479799132809696\n",
      "val_loss: 0.16305279691818056\n",
      "Progress: 41.6% ... Training loss: 0.083 ... Validation loss: 0.211iteration: 4160\n",
      "train_loss: 0.08303983606650378\n",
      "val_loss: 0.21110835663406186\n",
      "Progress: 41.6% ... Training loss: 0.114 ... Validation loss: 0.168iteration: 4161\n",
      "train_loss: 0.1141192944254454\n",
      "val_loss: 0.16830669911169724\n",
      "Progress: 41.6% ... Training loss: 0.114 ... Validation loss: 0.239iteration: 4162\n",
      "train_loss: 0.11432688163614582\n",
      "val_loss: 0.23983018920281698\n",
      "Progress: 41.6% ... Training loss: 0.118 ... Validation loss: 0.177iteration: 4163\n",
      "train_loss: 0.11841396603953926\n",
      "val_loss: 0.1774362460492154\n",
      "Progress: 41.6% ... Training loss: 0.171 ... Validation loss: 0.298iteration: 4164\n",
      "train_loss: 0.17157999792968526\n",
      "val_loss: 0.29811152200774943\n",
      "Progress: 41.6% ... Training loss: 0.162 ... Validation loss: 0.204iteration: 4165\n",
      "train_loss: 0.16223531353566736\n",
      "val_loss: 0.20465536458641723\n",
      "Progress: 41.7% ... Training loss: 0.148 ... Validation loss: 0.317iteration: 4166\n",
      "train_loss: 0.1484840351678968\n",
      "val_loss: 0.31766326512633153\n",
      "Progress: 41.7% ... Training loss: 0.139 ... Validation loss: 0.182iteration: 4167\n",
      "train_loss: 0.13992857787784577\n",
      "val_loss: 0.18229354993357932\n",
      "Progress: 41.7% ... Training loss: 0.099 ... Validation loss: 0.257iteration: 4168\n",
      "train_loss: 0.099090684302656\n",
      "val_loss: 0.2573990990371208\n",
      "Progress: 41.7% ... Training loss: 0.084 ... Validation loss: 0.162iteration: 4169\n",
      "train_loss: 0.08440370848874335\n",
      "val_loss: 0.16297352989444408\n",
      "Progress: 41.7% ... Training loss: 0.078 ... Validation loss: 0.167iteration: 4170\n",
      "train_loss: 0.07826547329248866\n",
      "val_loss: 0.16790785280698814\n",
      "Progress: 41.7% ... Training loss: 0.076 ... Validation loss: 0.170iteration: 4171\n",
      "train_loss: 0.07696271431184983\n",
      "val_loss: 0.17026201498274748\n",
      "Progress: 41.7% ... Training loss: 0.077 ... Validation loss: 0.166iteration: 4172\n",
      "train_loss: 0.07769740070229017\n",
      "val_loss: 0.16679121851792936\n",
      "Progress: 41.7% ... Training loss: 0.078 ... Validation loss: 0.178iteration: 4173\n",
      "train_loss: 0.07838919548086805\n",
      "val_loss: 0.1786123690700152\n",
      "Progress: 41.7% ... Training loss: 0.077 ... Validation loss: 0.162iteration: 4174\n",
      "train_loss: 0.07775504471564916\n",
      "val_loss: 0.16298491631751835\n",
      "Progress: 41.8% ... Training loss: 0.079 ... Validation loss: 0.162iteration: 4175\n",
      "train_loss: 0.07960814955385011\n",
      "val_loss: 0.16279300623748197\n",
      "Progress: 41.8% ... Training loss: 0.087 ... Validation loss: 0.187iteration: 4176\n",
      "train_loss: 0.08771716531623983\n",
      "val_loss: 0.1875599524097516\n",
      "Progress: 41.8% ... Training loss: 0.092 ... Validation loss: 0.167iteration: 4177\n",
      "train_loss: 0.09210794527933833\n",
      "val_loss: 0.16718789679858545\n",
      "Progress: 41.8% ... Training loss: 0.099 ... Validation loss: 0.219iteration: 4178\n",
      "train_loss: 0.09971446392252163\n",
      "val_loss: 0.21974521960191598\n",
      "Progress: 41.8% ... Training loss: 0.086 ... Validation loss: 0.160iteration: 4179\n",
      "train_loss: 0.0860394737093476\n",
      "val_loss: 0.1608818324602942\n",
      "Progress: 41.8% ... Training loss: 0.085 ... Validation loss: 0.195iteration: 4180\n",
      "train_loss: 0.08521920297216401\n",
      "val_loss: 0.1955302932531796\n",
      "Progress: 41.8% ... Training loss: 0.083 ... Validation loss: 0.163iteration: 4181\n",
      "train_loss: 0.08359312577537312\n",
      "val_loss: 0.16341947886143854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 41.8% ... Training loss: 0.078 ... Validation loss: 0.182iteration: 4182\n",
      "train_loss: 0.0780027814142061\n",
      "val_loss: 0.18243442835793006\n",
      "Progress: 41.8% ... Training loss: 0.077 ... Validation loss: 0.166iteration: 4183\n",
      "train_loss: 0.07729218286700171\n",
      "val_loss: 0.1669797176864542\n",
      "Progress: 41.8% ... Training loss: 0.078 ... Validation loss: 0.164iteration: 4184\n",
      "train_loss: 0.07803965375350806\n",
      "val_loss: 0.1647812510288023\n",
      "Progress: 41.9% ... Training loss: 0.076 ... Validation loss: 0.174iteration: 4185\n",
      "train_loss: 0.07631957368800266\n",
      "val_loss: 0.1742392505632216\n",
      "Progress: 41.9% ... Training loss: 0.077 ... Validation loss: 0.164iteration: 4186\n",
      "train_loss: 0.07709476024469168\n",
      "val_loss: 0.16480309579162666\n",
      "Progress: 41.9% ... Training loss: 0.085 ... Validation loss: 0.201iteration: 4187\n",
      "train_loss: 0.08576594377610347\n",
      "val_loss: 0.20150583719362597\n",
      "Progress: 41.9% ... Training loss: 0.088 ... Validation loss: 0.159iteration: 4188\n",
      "train_loss: 0.0880843283911791\n",
      "val_loss: 0.1593829315617402\n",
      "Progress: 41.9% ... Training loss: 0.091 ... Validation loss: 0.211iteration: 4189\n",
      "train_loss: 0.09123504969389207\n",
      "val_loss: 0.21162666490461043\n",
      "Progress: 41.9% ... Training loss: 0.077 ... Validation loss: 0.167iteration: 4190\n",
      "train_loss: 0.07797683060770817\n",
      "val_loss: 0.16707828644885664\n",
      "Progress: 41.9% ... Training loss: 0.083 ... Validation loss: 0.200iteration: 4191\n",
      "train_loss: 0.08386656211588561\n",
      "val_loss: 0.20058838978088722\n",
      "Progress: 41.9% ... Training loss: 0.090 ... Validation loss: 0.161iteration: 4192\n",
      "train_loss: 0.09027510425540268\n",
      "val_loss: 0.16135443766841495\n",
      "Progress: 41.9% ... Training loss: 0.079 ... Validation loss: 0.178iteration: 4193\n",
      "train_loss: 0.07937183153433698\n",
      "val_loss: 0.17892433599366164\n",
      "Progress: 41.9% ... Training loss: 0.077 ... Validation loss: 0.162iteration: 4194\n",
      "train_loss: 0.07734508348855958\n",
      "val_loss: 0.16256157282823633\n",
      "Progress: 42.0% ... Training loss: 0.075 ... Validation loss: 0.172iteration: 4195\n",
      "train_loss: 0.07598725495120853\n",
      "val_loss: 0.17248800899682828\n",
      "Progress: 42.0% ... Training loss: 0.077 ... Validation loss: 0.186iteration: 4196\n",
      "train_loss: 0.07757928299611447\n",
      "val_loss: 0.18662153331149464\n",
      "Progress: 42.0% ... Training loss: 0.086 ... Validation loss: 0.160iteration: 4197\n",
      "train_loss: 0.08647878909196699\n",
      "val_loss: 0.1600823882078878\n",
      "Progress: 42.0% ... Training loss: 0.079 ... Validation loss: 0.187iteration: 4198\n",
      "train_loss: 0.07983485477027578\n",
      "val_loss: 0.18785708859444958\n",
      "Progress: 42.0% ... Training loss: 0.085 ... Validation loss: 0.160iteration: 4199\n",
      "train_loss: 0.08512958733489495\n",
      "val_loss: 0.16054602715243582\n",
      "Progress: 42.0% ... Training loss: 0.084 ... Validation loss: 0.204iteration: 4200\n",
      "train_loss: 0.08484248872101922\n",
      "val_loss: 0.20476968866631742\n",
      "Progress: 42.0% ... Training loss: 0.077 ... Validation loss: 0.163iteration: 4201\n",
      "train_loss: 0.07720659334325156\n",
      "val_loss: 0.16373333973618803\n",
      "Progress: 42.0% ... Training loss: 0.076 ... Validation loss: 0.177iteration: 4202\n",
      "train_loss: 0.07653354927734168\n",
      "val_loss: 0.17744031897209714\n",
      "Progress: 42.0% ... Training loss: 0.079 ... Validation loss: 0.164iteration: 4203\n",
      "train_loss: 0.07967660728282257\n",
      "val_loss: 0.16411392420727072\n",
      "Progress: 42.0% ... Training loss: 0.077 ... Validation loss: 0.183iteration: 4204\n",
      "train_loss: 0.07774735452377456\n",
      "val_loss: 0.1833861765439532\n",
      "Progress: 42.0% ... Training loss: 0.077 ... Validation loss: 0.178iteration: 4205\n",
      "train_loss: 0.07774236961998472\n",
      "val_loss: 0.17805828580707633\n",
      "Progress: 42.1% ... Training loss: 0.082 ... Validation loss: 0.156iteration: 4206\n",
      "train_loss: 0.08287606267323833\n",
      "val_loss: 0.15634702221209457\n",
      "Progress: 42.1% ... Training loss: 0.080 ... Validation loss: 0.185iteration: 4207\n",
      "train_loss: 0.08062343440332519\n",
      "val_loss: 0.18580105516517925\n",
      "Progress: 42.1% ... Training loss: 0.076 ... Validation loss: 0.163iteration: 4208\n",
      "train_loss: 0.07618108981980898\n",
      "val_loss: 0.16305259983381298\n",
      "Progress: 42.1% ... Training loss: 0.076 ... Validation loss: 0.179iteration: 4209\n",
      "train_loss: 0.07622153922630555\n",
      "val_loss: 0.1790551693393383\n",
      "Progress: 42.1% ... Training loss: 0.079 ... Validation loss: 0.182iteration: 4210\n",
      "train_loss: 0.07921813048156857\n",
      "val_loss: 0.18297893700363307\n",
      "Progress: 42.1% ... Training loss: 0.077 ... Validation loss: 0.163iteration: 4211\n",
      "train_loss: 0.07795301329279354\n",
      "val_loss: 0.16368818588568096\n",
      "Progress: 42.1% ... Training loss: 0.077 ... Validation loss: 0.168iteration: 4212\n",
      "train_loss: 0.07707276636338783\n",
      "val_loss: 0.16838402580205075\n",
      "Progress: 42.1% ... Training loss: 0.075 ... Validation loss: 0.166iteration: 4213\n",
      "train_loss: 0.07599503821131806\n",
      "val_loss: 0.16666488050548808\n",
      "Progress: 42.1% ... Training loss: 0.076 ... Validation loss: 0.182iteration: 4214\n",
      "train_loss: 0.07634400786126061\n",
      "val_loss: 0.18277875640519822\n",
      "Progress: 42.1% ... Training loss: 0.076 ... Validation loss: 0.167iteration: 4215\n",
      "train_loss: 0.07649023665476999\n",
      "val_loss: 0.16724876063448443\n",
      "Progress: 42.2% ... Training loss: 0.079 ... Validation loss: 0.193iteration: 4216\n",
      "train_loss: 0.07980297785742191\n",
      "val_loss: 0.19386514343837496\n",
      "Progress: 42.2% ... Training loss: 0.083 ... Validation loss: 0.168iteration: 4217\n",
      "train_loss: 0.0836710833386149\n",
      "val_loss: 0.16804941014313476\n",
      "Progress: 42.2% ... Training loss: 0.084 ... Validation loss: 0.217iteration: 4218\n",
      "train_loss: 0.08477243448727681\n",
      "val_loss: 0.2176299630304728\n",
      "Progress: 42.2% ... Training loss: 0.079 ... Validation loss: 0.168iteration: 4219\n",
      "train_loss: 0.07994258059188637\n",
      "val_loss: 0.16863167413862198\n",
      "Progress: 42.2% ... Training loss: 0.077 ... Validation loss: 0.188iteration: 4220\n",
      "train_loss: 0.07781512457637803\n",
      "val_loss: 0.1884428373120195\n",
      "Progress: 42.2% ... Training loss: 0.076 ... Validation loss: 0.172iteration: 4221\n",
      "train_loss: 0.07666654870757145\n",
      "val_loss: 0.1722120477047091\n",
      "Progress: 42.2% ... Training loss: 0.075 ... Validation loss: 0.176iteration: 4222\n",
      "train_loss: 0.07550916081026292\n",
      "val_loss: 0.17606895858515487\n",
      "Progress: 42.2% ... Training loss: 0.082 ... Validation loss: 0.192iteration: 4223\n",
      "train_loss: 0.08281281503736686\n",
      "val_loss: 0.19216800597851666\n",
      "Progress: 42.2% ... Training loss: 0.089 ... Validation loss: 0.164iteration: 4224\n",
      "train_loss: 0.08977116332333283\n",
      "val_loss: 0.16498924952273963\n",
      "Progress: 42.2% ... Training loss: 0.084 ... Validation loss: 0.199iteration: 4225\n",
      "train_loss: 0.08408119148560451\n",
      "val_loss: 0.19969120980916621\n",
      "Progress: 42.3% ... Training loss: 0.094 ... Validation loss: 0.162iteration: 4226\n",
      "train_loss: 0.09424152128728801\n",
      "val_loss: 0.1622934566411163\n",
      "Progress: 42.3% ... Training loss: 0.082 ... Validation loss: 0.202iteration: 4227\n",
      "train_loss: 0.08298357974911952\n",
      "val_loss: 0.20234274261499374\n",
      "Progress: 42.3% ... Training loss: 0.085 ... Validation loss: 0.164iteration: 4228\n",
      "train_loss: 0.08585570375445023\n",
      "val_loss: 0.1645365175962727\n",
      "Progress: 42.3% ... Training loss: 0.076 ... Validation loss: 0.178iteration: 4229\n",
      "train_loss: 0.07695768007683985\n",
      "val_loss: 0.17849710715756864\n",
      "Progress: 42.3% ... Training loss: 0.075 ... Validation loss: 0.162iteration: 4230\n",
      "train_loss: 0.07554658271623958\n",
      "val_loss: 0.16296386298563212\n",
      "Progress: 42.3% ... Training loss: 0.078 ... Validation loss: 0.158iteration: 4231\n",
      "train_loss: 0.07848643334391993\n",
      "val_loss: 0.15884126261092232\n",
      "Progress: 42.3% ... Training loss: 0.075 ... Validation loss: 0.167iteration: 4232\n",
      "train_loss: 0.07573383340223644\n",
      "val_loss: 0.1671416643150144\n",
      "Progress: 42.3% ... Training loss: 0.075 ... Validation loss: 0.166iteration: 4233\n",
      "train_loss: 0.07548022955331112\n",
      "val_loss: 0.1665505881080066\n",
      "Progress: 42.3% ... Training loss: 0.076 ... Validation loss: 0.162iteration: 4234\n",
      "train_loss: 0.07674631223389596\n",
      "val_loss: 0.16298099844681777\n",
      "Progress: 42.4% ... Training loss: 0.104 ... Validation loss: 0.217iteration: 4235\n",
      "train_loss: 0.10499083198138678\n",
      "val_loss: 0.21747408984436478\n",
      "Progress: 42.4% ... Training loss: 0.091 ... Validation loss: 0.156iteration: 4236\n",
      "train_loss: 0.09195986082346568\n",
      "val_loss: 0.15633871381027245\n",
      "Progress: 42.4% ... Training loss: 0.086 ... Validation loss: 0.216iteration: 4237\n",
      "train_loss: 0.08619664316658535\n",
      "val_loss: 0.2160748578269809\n",
      "Progress: 42.4% ... Training loss: 0.082 ... Validation loss: 0.160iteration: 4238\n",
      "train_loss: 0.08200042060709208\n",
      "val_loss: 0.16078630910082448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 42.4% ... Training loss: 0.080 ... Validation loss: 0.196iteration: 4239\n",
      "train_loss: 0.08070056082622264\n",
      "val_loss: 0.19661462505843808\n",
      "Progress: 42.4% ... Training loss: 0.077 ... Validation loss: 0.168iteration: 4240\n",
      "train_loss: 0.07735402130433518\n",
      "val_loss: 0.1680635515507903\n",
      "Progress: 42.4% ... Training loss: 0.078 ... Validation loss: 0.197iteration: 4241\n",
      "train_loss: 0.07864272923079502\n",
      "val_loss: 0.19721249274895167\n",
      "Progress: 42.4% ... Training loss: 0.079 ... Validation loss: 0.157iteration: 4242\n",
      "train_loss: 0.07970645725699878\n",
      "val_loss: 0.15784445983725665\n",
      "Progress: 42.4% ... Training loss: 0.075 ... Validation loss: 0.178iteration: 4243\n",
      "train_loss: 0.07574104350347481\n",
      "val_loss: 0.1787357088620842\n",
      "Progress: 42.4% ... Training loss: 0.083 ... Validation loss: 0.160iteration: 4244\n",
      "train_loss: 0.08317306517817862\n",
      "val_loss: 0.16059674791355158\n",
      "Progress: 42.5% ... Training loss: 0.086 ... Validation loss: 0.203iteration: 4245\n",
      "train_loss: 0.0860934700972966\n",
      "val_loss: 0.20393759679331144\n",
      "Progress: 42.5% ... Training loss: 0.102 ... Validation loss: 0.162iteration: 4246\n",
      "train_loss: 0.10259108052567184\n",
      "val_loss: 0.16269010886318577\n",
      "Progress: 42.5% ... Training loss: 0.095 ... Validation loss: 0.228iteration: 4247\n",
      "train_loss: 0.09596987932442515\n",
      "val_loss: 0.2289601999682939\n",
      "Progress: 42.5% ... Training loss: 0.079 ... Validation loss: 0.159iteration: 4248\n",
      "train_loss: 0.07948643995991678\n",
      "val_loss: 0.15970257942183735\n",
      "Progress: 42.5% ... Training loss: 0.077 ... Validation loss: 0.179iteration: 4249\n",
      "train_loss: 0.07714378534629568\n",
      "val_loss: 0.17900343633440133\n",
      "Progress: 42.5% ... Training loss: 0.078 ... Validation loss: 0.162iteration: 4250\n",
      "train_loss: 0.07865043321808698\n",
      "val_loss: 0.1628714903736769\n",
      "Progress: 42.5% ... Training loss: 0.075 ... Validation loss: 0.169iteration: 4251\n",
      "train_loss: 0.07566572055046397\n",
      "val_loss: 0.16984375596459939\n",
      "Progress: 42.5% ... Training loss: 0.076 ... Validation loss: 0.164iteration: 4252\n",
      "train_loss: 0.07698640280834723\n",
      "val_loss: 0.16470136804823662\n",
      "Progress: 42.5% ... Training loss: 0.077 ... Validation loss: 0.160iteration: 4253\n",
      "train_loss: 0.07707717666031397\n",
      "val_loss: 0.1608764127407757\n",
      "Progress: 42.5% ... Training loss: 0.075 ... Validation loss: 0.168iteration: 4254\n",
      "train_loss: 0.07508925537750294\n",
      "val_loss: 0.16803220958873574\n",
      "Progress: 42.5% ... Training loss: 0.075 ... Validation loss: 0.164iteration: 4255\n",
      "train_loss: 0.07538412714712707\n",
      "val_loss: 0.16457827936599548\n",
      "Progress: 42.6% ... Training loss: 0.076 ... Validation loss: 0.184iteration: 4256\n",
      "train_loss: 0.07654276863310873\n",
      "val_loss: 0.1840644578025823\n",
      "Progress: 42.6% ... Training loss: 0.076 ... Validation loss: 0.166iteration: 4257\n",
      "train_loss: 0.07648111144487202\n",
      "val_loss: 0.16605236502335752\n",
      "Progress: 42.6% ... Training loss: 0.081 ... Validation loss: 0.190iteration: 4258\n",
      "train_loss: 0.08123068018372745\n",
      "val_loss: 0.19090938547504893\n",
      "Progress: 42.6% ... Training loss: 0.075 ... Validation loss: 0.167iteration: 4259\n",
      "train_loss: 0.07553174248243141\n",
      "val_loss: 0.1671758344423697\n",
      "Progress: 42.6% ... Training loss: 0.074 ... Validation loss: 0.171iteration: 4260\n",
      "train_loss: 0.0749756790727312\n",
      "val_loss: 0.1719174812051777\n",
      "Progress: 42.6% ... Training loss: 0.075 ... Validation loss: 0.168iteration: 4261\n",
      "train_loss: 0.07515256440316512\n",
      "val_loss: 0.16869179077403043\n",
      "Progress: 42.6% ... Training loss: 0.078 ... Validation loss: 0.182iteration: 4262\n",
      "train_loss: 0.07837338303882832\n",
      "val_loss: 0.1820623584221804\n",
      "Progress: 42.6% ... Training loss: 0.077 ... Validation loss: 0.160iteration: 4263\n",
      "train_loss: 0.07724022101794742\n",
      "val_loss: 0.16049046683135598\n",
      "Progress: 42.6% ... Training loss: 0.080 ... Validation loss: 0.188iteration: 4264\n",
      "train_loss: 0.08075777235459745\n",
      "val_loss: 0.1884381483944155\n",
      "Progress: 42.6% ... Training loss: 0.084 ... Validation loss: 0.163iteration: 4265\n",
      "train_loss: 0.084092034996186\n",
      "val_loss: 0.16370597372216947\n",
      "Progress: 42.7% ... Training loss: 0.077 ... Validation loss: 0.199iteration: 4266\n",
      "train_loss: 0.07778330185754584\n",
      "val_loss: 0.1993489222698394\n",
      "Progress: 42.7% ... Training loss: 0.078 ... Validation loss: 0.161iteration: 4267\n",
      "train_loss: 0.07854336110152735\n",
      "val_loss: 0.16120476461815877\n",
      "Progress: 42.7% ... Training loss: 0.079 ... Validation loss: 0.197iteration: 4268\n",
      "train_loss: 0.07943220843435357\n",
      "val_loss: 0.19789819439320672\n",
      "Progress: 42.7% ... Training loss: 0.083 ... Validation loss: 0.162iteration: 4269\n",
      "train_loss: 0.08329079599549895\n",
      "val_loss: 0.16225075078810935\n",
      "Progress: 42.7% ... Training loss: 0.075 ... Validation loss: 0.182iteration: 4270\n",
      "train_loss: 0.07551977651355563\n",
      "val_loss: 0.1823026318296109\n",
      "Progress: 42.7% ... Training loss: 0.077 ... Validation loss: 0.191iteration: 4271\n",
      "train_loss: 0.07777257295320036\n",
      "val_loss: 0.1912530101397065\n",
      "Progress: 42.7% ... Training loss: 0.079 ... Validation loss: 0.159iteration: 4272\n",
      "train_loss: 0.07964079331356122\n",
      "val_loss: 0.1599597682505077\n",
      "Progress: 42.7% ... Training loss: 0.076 ... Validation loss: 0.170iteration: 4273\n",
      "train_loss: 0.07604713001777255\n",
      "val_loss: 0.17015762395592515\n",
      "Progress: 42.7% ... Training loss: 0.076 ... Validation loss: 0.178iteration: 4274\n",
      "train_loss: 0.07629324092926965\n",
      "val_loss: 0.1780270651359706\n",
      "Progress: 42.8% ... Training loss: 0.081 ... Validation loss: 0.158iteration: 4275\n",
      "train_loss: 0.08106180349470513\n",
      "val_loss: 0.15803640253004983\n",
      "Progress: 42.8% ... Training loss: 0.084 ... Validation loss: 0.211iteration: 4276\n",
      "train_loss: 0.08471523900342322\n",
      "val_loss: 0.21172842668376723\n",
      "Progress: 42.8% ... Training loss: 0.084 ... Validation loss: 0.162iteration: 4277\n",
      "train_loss: 0.08497745819367751\n",
      "val_loss: 0.16236561255859647\n",
      "Progress: 42.8% ... Training loss: 0.079 ... Validation loss: 0.191iteration: 4278\n",
      "train_loss: 0.07926525545971515\n",
      "val_loss: 0.1915411099207402\n",
      "Progress: 42.8% ... Training loss: 0.074 ... Validation loss: 0.171iteration: 4279\n",
      "train_loss: 0.07489238896530104\n",
      "val_loss: 0.171959796596054\n",
      "Progress: 42.8% ... Training loss: 0.075 ... Validation loss: 0.181iteration: 4280\n",
      "train_loss: 0.07534665944038438\n",
      "val_loss: 0.18156249673939343\n",
      "Progress: 42.8% ... Training loss: 0.087 ... Validation loss: 0.159iteration: 4281\n",
      "train_loss: 0.08706355366905062\n",
      "val_loss: 0.15954072558061938\n",
      "Progress: 42.8% ... Training loss: 0.076 ... Validation loss: 0.180iteration: 4282\n",
      "train_loss: 0.07621046143748296\n",
      "val_loss: 0.18096936092091515\n",
      "Progress: 42.8% ... Training loss: 0.075 ... Validation loss: 0.178iteration: 4283\n",
      "train_loss: 0.07530852034058359\n",
      "val_loss: 0.17801016117740778\n",
      "Progress: 42.8% ... Training loss: 0.075 ... Validation loss: 0.168iteration: 4284\n",
      "train_loss: 0.07589335747046097\n",
      "val_loss: 0.1680882099536196\n",
      "Progress: 42.9% ... Training loss: 0.075 ... Validation loss: 0.176iteration: 4285\n",
      "train_loss: 0.07560926090903704\n",
      "val_loss: 0.17627727759791428\n",
      "Progress: 42.9% ... Training loss: 0.088 ... Validation loss: 0.156iteration: 4286\n",
      "train_loss: 0.08834458709531566\n",
      "val_loss: 0.15633567931430822\n",
      "Progress: 42.9% ... Training loss: 0.084 ... Validation loss: 0.190iteration: 4287\n",
      "train_loss: 0.08495336256270311\n",
      "val_loss: 0.19093176100184903\n",
      "Progress: 42.9% ... Training loss: 0.106 ... Validation loss: 0.166iteration: 4288\n",
      "train_loss: 0.10635980842664004\n",
      "val_loss: 0.16613278842806775\n",
      "Progress: 42.9% ... Training loss: 0.085 ... Validation loss: 0.194iteration: 4289\n",
      "train_loss: 0.0852355057405394\n",
      "val_loss: 0.19438716534831763\n",
      "Progress: 42.9% ... Training loss: 0.092 ... Validation loss: 0.165iteration: 4290\n",
      "train_loss: 0.09208300316898321\n",
      "val_loss: 0.16526644838942156\n",
      "Progress: 42.9% ... Training loss: 0.155 ... Validation loss: 0.277iteration: 4291\n",
      "train_loss: 0.15582080052329816\n",
      "val_loss: 0.2777929970101121\n",
      "Progress: 42.9% ... Training loss: 0.148 ... Validation loss: 0.197iteration: 4292\n",
      "train_loss: 0.14831714658406614\n",
      "val_loss: 0.1973695226333122\n",
      "Progress: 42.9% ... Training loss: 0.104 ... Validation loss: 0.199iteration: 4293\n",
      "train_loss: 0.10493945745789826\n",
      "val_loss: 0.1994627874843382\n",
      "Progress: 42.9% ... Training loss: 0.097 ... Validation loss: 0.168iteration: 4294\n",
      "train_loss: 0.09771619852141943\n",
      "val_loss: 0.16897355761564584\n",
      "Progress: 43.0% ... Training loss: 0.080 ... Validation loss: 0.168iteration: 4295\n",
      "train_loss: 0.08037700592961096\n",
      "val_loss: 0.1685714116496992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 43.0% ... Training loss: 0.075 ... Validation loss: 0.159iteration: 4296\n",
      "train_loss: 0.07521970712781575\n",
      "val_loss: 0.1594044561383616\n",
      "Progress: 43.0% ... Training loss: 0.076 ... Validation loss: 0.161iteration: 4297\n",
      "train_loss: 0.0764943464900417\n",
      "val_loss: 0.16132156941983528\n",
      "Progress: 43.0% ... Training loss: 0.077 ... Validation loss: 0.158iteration: 4298\n",
      "train_loss: 0.07712185650962872\n",
      "val_loss: 0.15837720942535383\n",
      "Progress: 43.0% ... Training loss: 0.088 ... Validation loss: 0.197iteration: 4299\n",
      "train_loss: 0.08832194724704662\n",
      "val_loss: 0.19748150816060459\n",
      "Progress: 43.0% ... Training loss: 0.084 ... Validation loss: 0.156iteration: 4300\n",
      "train_loss: 0.08430678726752737\n",
      "val_loss: 0.15607363635150012\n",
      "Progress: 43.0% ... Training loss: 0.085 ... Validation loss: 0.187iteration: 4301\n",
      "train_loss: 0.08557447407119352\n",
      "val_loss: 0.18728192277389186\n",
      "Progress: 43.0% ... Training loss: 0.086 ... Validation loss: 0.157iteration: 4302\n",
      "train_loss: 0.08641073373944438\n",
      "val_loss: 0.15766171524725267\n",
      "Progress: 43.0% ... Training loss: 0.087 ... Validation loss: 0.199iteration: 4303\n",
      "train_loss: 0.08765136810088772\n",
      "val_loss: 0.19902643052777735\n",
      "Progress: 43.0% ... Training loss: 0.091 ... Validation loss: 0.162iteration: 4304\n",
      "train_loss: 0.09117812333825188\n",
      "val_loss: 0.16295533640870283\n",
      "Progress: 43.0% ... Training loss: 0.101 ... Validation loss: 0.213iteration: 4305\n",
      "train_loss: 0.10196196040575983\n",
      "val_loss: 0.2139924955092073\n",
      "Progress: 43.1% ... Training loss: 0.095 ... Validation loss: 0.159iteration: 4306\n",
      "train_loss: 0.09503039417267717\n",
      "val_loss: 0.15907480928162118\n",
      "Progress: 43.1% ... Training loss: 0.089 ... Validation loss: 0.219iteration: 4307\n",
      "train_loss: 0.0894290836462932\n",
      "val_loss: 0.21989837268979437\n",
      "Progress: 43.1% ... Training loss: 0.081 ... Validation loss: 0.160iteration: 4308\n",
      "train_loss: 0.08188118657612387\n",
      "val_loss: 0.16016846250120131\n",
      "Progress: 43.1% ... Training loss: 0.075 ... Validation loss: 0.174iteration: 4309\n",
      "train_loss: 0.07552223401569588\n",
      "val_loss: 0.1749582494808017\n",
      "Progress: 43.1% ... Training loss: 0.081 ... Validation loss: 0.187iteration: 4310\n",
      "train_loss: 0.08174136077521803\n",
      "val_loss: 0.18731453653191613\n",
      "Progress: 43.1% ... Training loss: 0.079 ... Validation loss: 0.157iteration: 4311\n",
      "train_loss: 0.07930034641704063\n",
      "val_loss: 0.15715244822492538\n",
      "Progress: 43.1% ... Training loss: 0.079 ... Validation loss: 0.162iteration: 4312\n",
      "train_loss: 0.07969869273995095\n",
      "val_loss: 0.16203163509338048\n",
      "Progress: 43.1% ... Training loss: 0.084 ... Validation loss: 0.197iteration: 4313\n",
      "train_loss: 0.08417230804272922\n",
      "val_loss: 0.19728938198231577\n",
      "Progress: 43.1% ... Training loss: 0.082 ... Validation loss: 0.156iteration: 4314\n",
      "train_loss: 0.08236641370570552\n",
      "val_loss: 0.1562086664624312\n",
      "Progress: 43.1% ... Training loss: 0.081 ... Validation loss: 0.185iteration: 4315\n",
      "train_loss: 0.08123895202449688\n",
      "val_loss: 0.185951455293856\n",
      "Progress: 43.2% ... Training loss: 0.083 ... Validation loss: 0.157iteration: 4316\n",
      "train_loss: 0.08315161474402664\n",
      "val_loss: 0.15746617532148297\n",
      "Progress: 43.2% ... Training loss: 0.083 ... Validation loss: 0.194iteration: 4317\n",
      "train_loss: 0.08391637196439247\n",
      "val_loss: 0.19472449843797845\n",
      "Progress: 43.2% ... Training loss: 0.096 ... Validation loss: 0.158iteration: 4318\n",
      "train_loss: 0.09680221556259698\n",
      "val_loss: 0.15873683325490023\n",
      "Progress: 43.2% ... Training loss: 0.095 ... Validation loss: 0.215iteration: 4319\n",
      "train_loss: 0.09580837840082927\n",
      "val_loss: 0.21510709353552349\n",
      "Progress: 43.2% ... Training loss: 0.096 ... Validation loss: 0.157iteration: 4320\n",
      "train_loss: 0.09684865997843249\n",
      "val_loss: 0.1579003224377047\n",
      "Progress: 43.2% ... Training loss: 0.114 ... Validation loss: 0.223iteration: 4321\n",
      "train_loss: 0.11482239881668886\n",
      "val_loss: 0.22382479977282305\n",
      "Progress: 43.2% ... Training loss: 0.111 ... Validation loss: 0.164iteration: 4322\n",
      "train_loss: 0.11164044314039491\n",
      "val_loss: 0.16482043939441562\n",
      "Progress: 43.2% ... Training loss: 0.106 ... Validation loss: 0.213iteration: 4323\n",
      "train_loss: 0.10615543406301263\n",
      "val_loss: 0.21360236859620546\n",
      "Progress: 43.2% ... Training loss: 0.082 ... Validation loss: 0.154iteration: 4324\n",
      "train_loss: 0.08280297961810172\n",
      "val_loss: 0.1543220196241838\n",
      "Progress: 43.2% ... Training loss: 0.087 ... Validation loss: 0.194iteration: 4325\n",
      "train_loss: 0.08750839278267132\n",
      "val_loss: 0.19457470977234684\n",
      "Progress: 43.3% ... Training loss: 0.090 ... Validation loss: 0.158iteration: 4326\n",
      "train_loss: 0.09052993583929131\n",
      "val_loss: 0.1585808202290555\n",
      "Progress: 43.3% ... Training loss: 0.089 ... Validation loss: 0.198iteration: 4327\n",
      "train_loss: 0.08956997688327438\n",
      "val_loss: 0.1989913301418481\n",
      "Progress: 43.3% ... Training loss: 0.087 ... Validation loss: 0.160iteration: 4328\n",
      "train_loss: 0.0877440387389662\n",
      "val_loss: 0.16029381637753728\n",
      "Progress: 43.3% ... Training loss: 0.075 ... Validation loss: 0.173iteration: 4329\n",
      "train_loss: 0.07584571174203177\n",
      "val_loss: 0.173293755033676\n",
      "Progress: 43.3% ... Training loss: 0.077 ... Validation loss: 0.161iteration: 4330\n",
      "train_loss: 0.07735412271504846\n",
      "val_loss: 0.1616969631345266\n",
      "Progress: 43.3% ... Training loss: 0.077 ... Validation loss: 0.174iteration: 4331\n",
      "train_loss: 0.07765532880550195\n",
      "val_loss: 0.17417866201888665\n",
      "Progress: 43.3% ... Training loss: 0.079 ... Validation loss: 0.154iteration: 4332\n",
      "train_loss: 0.07949676625237855\n",
      "val_loss: 0.15489520573972942\n",
      "Progress: 43.3% ... Training loss: 0.095 ... Validation loss: 0.197iteration: 4333\n",
      "train_loss: 0.09583481959986495\n",
      "val_loss: 0.1973099578539281\n",
      "Progress: 43.3% ... Training loss: 0.085 ... Validation loss: 0.158iteration: 4334\n",
      "train_loss: 0.08555865136617495\n",
      "val_loss: 0.15872680564874514\n",
      "Progress: 43.4% ... Training loss: 0.076 ... Validation loss: 0.169iteration: 4335\n",
      "train_loss: 0.07621208596714393\n",
      "val_loss: 0.16917120528955465\n",
      "Progress: 43.4% ... Training loss: 0.074 ... Validation loss: 0.165iteration: 4336\n",
      "train_loss: 0.07435421965479375\n",
      "val_loss: 0.16597402032809508\n",
      "Progress: 43.4% ... Training loss: 0.074 ... Validation loss: 0.173iteration: 4337\n",
      "train_loss: 0.0748177874088353\n",
      "val_loss: 0.1731335089525967\n",
      "Progress: 43.4% ... Training loss: 0.077 ... Validation loss: 0.163iteration: 4338\n",
      "train_loss: 0.07761296002647848\n",
      "val_loss: 0.16391642610693796\n",
      "Progress: 43.4% ... Training loss: 0.076 ... Validation loss: 0.179iteration: 4339\n",
      "train_loss: 0.07665129202251868\n",
      "val_loss: 0.17906953654661045\n",
      "Progress: 43.4% ... Training loss: 0.080 ... Validation loss: 0.155iteration: 4340\n",
      "train_loss: 0.08063666728645312\n",
      "val_loss: 0.15578161572595917\n",
      "Progress: 43.4% ... Training loss: 0.075 ... Validation loss: 0.176iteration: 4341\n",
      "train_loss: 0.07544647839958907\n",
      "val_loss: 0.17678347333901057\n",
      "Progress: 43.4% ... Training loss: 0.074 ... Validation loss: 0.167iteration: 4342\n",
      "train_loss: 0.07444732262666945\n",
      "val_loss: 0.16794010481538524\n",
      "Progress: 43.4% ... Training loss: 0.074 ... Validation loss: 0.165iteration: 4343\n",
      "train_loss: 0.07456522223744694\n",
      "val_loss: 0.16555152439707427\n",
      "Progress: 43.4% ... Training loss: 0.074 ... Validation loss: 0.172iteration: 4344\n",
      "train_loss: 0.07455921376555431\n",
      "val_loss: 0.17239778587408758\n",
      "Progress: 43.5% ... Training loss: 0.075 ... Validation loss: 0.172iteration: 4345\n",
      "train_loss: 0.07518171824242549\n",
      "val_loss: 0.17266707823163543\n",
      "Progress: 43.5% ... Training loss: 0.078 ... Validation loss: 0.185iteration: 4346\n",
      "train_loss: 0.07867214201833654\n",
      "val_loss: 0.18568665360425265\n",
      "Progress: 43.5% ... Training loss: 0.095 ... Validation loss: 0.156iteration: 4347\n",
      "train_loss: 0.09558830584742052\n",
      "val_loss: 0.15664878203446342\n",
      "Progress: 43.5% ... Training loss: 0.092 ... Validation loss: 0.225iteration: 4348\n",
      "train_loss: 0.09275734579901646\n",
      "val_loss: 0.22547235556310025\n",
      "Progress: 43.5% ... Training loss: 0.076 ... Validation loss: 0.163iteration: 4349\n",
      "train_loss: 0.07647194034130998\n",
      "val_loss: 0.16330536251919517\n",
      "Progress: 43.5% ... Training loss: 0.074 ... Validation loss: 0.176iteration: 4350\n",
      "train_loss: 0.07478994323757149\n",
      "val_loss: 0.1765600059188729\n",
      "Progress: 43.5% ... Training loss: 0.076 ... Validation loss: 0.164iteration: 4351\n",
      "train_loss: 0.07604241128478222\n",
      "val_loss: 0.1641297202399206\n",
      "Progress: 43.5% ... Training loss: 0.076 ... Validation loss: 0.189iteration: 4352\n",
      "train_loss: 0.07685853522367792\n",
      "val_loss: 0.1896706850242864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 43.5% ... Training loss: 0.074 ... Validation loss: 0.171iteration: 4353\n",
      "train_loss: 0.07478722526707919\n",
      "val_loss: 0.17129059007101125\n",
      "Progress: 43.5% ... Training loss: 0.081 ... Validation loss: 0.158iteration: 4354\n",
      "train_loss: 0.08160821714061181\n",
      "val_loss: 0.15874330930858835\n",
      "Progress: 43.5% ... Training loss: 0.082 ... Validation loss: 0.200iteration: 4355\n",
      "train_loss: 0.08238916230241061\n",
      "val_loss: 0.200640358124595\n",
      "Progress: 43.6% ... Training loss: 0.075 ... Validation loss: 0.167iteration: 4356\n",
      "train_loss: 0.07535123208131426\n",
      "val_loss: 0.16771893076405037\n",
      "Progress: 43.6% ... Training loss: 0.076 ... Validation loss: 0.164iteration: 4357\n",
      "train_loss: 0.07619942621875768\n",
      "val_loss: 0.164144259591358\n",
      "Progress: 43.6% ... Training loss: 0.078 ... Validation loss: 0.201iteration: 4358\n",
      "train_loss: 0.07870767102059614\n",
      "val_loss: 0.2018433232210972\n",
      "Progress: 43.6% ... Training loss: 0.075 ... Validation loss: 0.175iteration: 4359\n",
      "train_loss: 0.07503896011295838\n",
      "val_loss: 0.17533778221134938\n",
      "Progress: 43.6% ... Training loss: 0.075 ... Validation loss: 0.165iteration: 4360\n",
      "train_loss: 0.07548649034776432\n",
      "val_loss: 0.16540469422472887\n",
      "Progress: 43.6% ... Training loss: 0.074 ... Validation loss: 0.185iteration: 4361\n",
      "train_loss: 0.0746474844307088\n",
      "val_loss: 0.18544193570856077\n",
      "Progress: 43.6% ... Training loss: 0.074 ... Validation loss: 0.172iteration: 4362\n",
      "train_loss: 0.0747312087289184\n",
      "val_loss: 0.17235427781893245\n",
      "Progress: 43.6% ... Training loss: 0.074 ... Validation loss: 0.182iteration: 4363\n",
      "train_loss: 0.074853204544532\n",
      "val_loss: 0.1821144033706317\n",
      "Progress: 43.6% ... Training loss: 0.073 ... Validation loss: 0.169iteration: 4364\n",
      "train_loss: 0.07399781542266855\n",
      "val_loss: 0.16924922241587698\n",
      "Progress: 43.6% ... Training loss: 0.085 ... Validation loss: 0.202iteration: 4365\n",
      "train_loss: 0.08586752840008649\n",
      "val_loss: 0.20224339618785606\n",
      "Progress: 43.7% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 4366\n",
      "train_loss: 0.08182105145267046\n",
      "val_loss: 0.15461239392236995\n",
      "Progress: 43.7% ... Training loss: 0.074 ... Validation loss: 0.179iteration: 4367\n",
      "train_loss: 0.07492812283178373\n",
      "val_loss: 0.17985976472655654\n",
      "Progress: 43.7% ... Training loss: 0.076 ... Validation loss: 0.179iteration: 4368\n",
      "train_loss: 0.07678239253320783\n",
      "val_loss: 0.17948590745603044\n",
      "Progress: 43.7% ... Training loss: 0.075 ... Validation loss: 0.189iteration: 4369\n",
      "train_loss: 0.07541604726233186\n",
      "val_loss: 0.1891494896284134\n",
      "Progress: 43.7% ... Training loss: 0.081 ... Validation loss: 0.162iteration: 4370\n",
      "train_loss: 0.08186985056306115\n",
      "val_loss: 0.16228580438069748\n",
      "Progress: 43.7% ... Training loss: 0.074 ... Validation loss: 0.177iteration: 4371\n",
      "train_loss: 0.07438239269279753\n",
      "val_loss: 0.17710526995533712\n",
      "Progress: 43.7% ... Training loss: 0.075 ... Validation loss: 0.163iteration: 4372\n",
      "train_loss: 0.07576928804578138\n",
      "val_loss: 0.1637173163582814\n",
      "Progress: 43.7% ... Training loss: 0.075 ... Validation loss: 0.177iteration: 4373\n",
      "train_loss: 0.07543517869521554\n",
      "val_loss: 0.17729552881011\n",
      "Progress: 43.7% ... Training loss: 0.074 ... Validation loss: 0.173iteration: 4374\n",
      "train_loss: 0.07499270766544922\n",
      "val_loss: 0.1730669229889704\n",
      "Progress: 43.8% ... Training loss: 0.074 ... Validation loss: 0.178iteration: 4375\n",
      "train_loss: 0.07448276161496643\n",
      "val_loss: 0.1781641773689443\n",
      "Progress: 43.8% ... Training loss: 0.075 ... Validation loss: 0.163iteration: 4376\n",
      "train_loss: 0.07551115586196072\n",
      "val_loss: 0.16367415456818474\n",
      "Progress: 43.8% ... Training loss: 0.078 ... Validation loss: 0.158iteration: 4377\n",
      "train_loss: 0.07802082362132153\n",
      "val_loss: 0.1587980729055338\n",
      "Progress: 43.8% ... Training loss: 0.077 ... Validation loss: 0.160iteration: 4378\n",
      "train_loss: 0.07715925954167269\n",
      "val_loss: 0.16020979398309326\n",
      "Progress: 43.8% ... Training loss: 0.074 ... Validation loss: 0.173iteration: 4379\n",
      "train_loss: 0.07433152761739836\n",
      "val_loss: 0.17333432930508363\n",
      "Progress: 43.8% ... Training loss: 0.077 ... Validation loss: 0.160iteration: 4380\n",
      "train_loss: 0.07731082394596998\n",
      "val_loss: 0.16004159805579798\n",
      "Progress: 43.8% ... Training loss: 0.081 ... Validation loss: 0.213iteration: 4381\n",
      "train_loss: 0.08120033222243436\n",
      "val_loss: 0.21313689721593504\n",
      "Progress: 43.8% ... Training loss: 0.090 ... Validation loss: 0.157iteration: 4382\n",
      "train_loss: 0.09029397562524014\n",
      "val_loss: 0.1571733778894631\n",
      "Progress: 43.8% ... Training loss: 0.079 ... Validation loss: 0.200iteration: 4383\n",
      "train_loss: 0.07945144569317164\n",
      "val_loss: 0.20085784212230381\n",
      "Progress: 43.8% ... Training loss: 0.083 ... Validation loss: 0.154iteration: 4384\n",
      "train_loss: 0.08370028075012352\n",
      "val_loss: 0.1543882278067489\n",
      "Progress: 43.9% ... Training loss: 0.077 ... Validation loss: 0.184iteration: 4385\n",
      "train_loss: 0.07788145251946577\n",
      "val_loss: 0.1844225190547139\n",
      "Progress: 43.9% ... Training loss: 0.078 ... Validation loss: 0.159iteration: 4386\n",
      "train_loss: 0.07802381552074193\n",
      "val_loss: 0.15925016957514054\n",
      "Progress: 43.9% ... Training loss: 0.076 ... Validation loss: 0.188iteration: 4387\n",
      "train_loss: 0.07670221420160762\n",
      "val_loss: 0.18888832302046898\n",
      "Progress: 43.9% ... Training loss: 0.074 ... Validation loss: 0.159iteration: 4388\n",
      "train_loss: 0.07430391746621391\n",
      "val_loss: 0.1599871379958396\n",
      "Progress: 43.9% ... Training loss: 0.078 ... Validation loss: 0.184iteration: 4389\n",
      "train_loss: 0.07895500868028525\n",
      "val_loss: 0.18411965647558665\n",
      "Progress: 43.9% ... Training loss: 0.108 ... Validation loss: 0.163iteration: 4390\n",
      "train_loss: 0.10836592948679093\n",
      "val_loss: 0.1636394555014735\n",
      "Progress: 43.9% ... Training loss: 0.096 ... Validation loss: 0.200iteration: 4391\n",
      "train_loss: 0.09665783006724216\n",
      "val_loss: 0.20094696643696933\n",
      "Progress: 43.9% ... Training loss: 0.079 ... Validation loss: 0.151iteration: 4392\n",
      "train_loss: 0.07970007272625879\n",
      "val_loss: 0.15186314510898433\n",
      "Progress: 43.9% ... Training loss: 0.081 ... Validation loss: 0.164iteration: 4393\n",
      "train_loss: 0.08131556479161764\n",
      "val_loss: 0.16477103874646562\n",
      "Progress: 43.9% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 4394\n",
      "train_loss: 0.08124086192841504\n",
      "val_loss: 0.15485662459618302\n",
      "Progress: 44.0% ... Training loss: 0.078 ... Validation loss: 0.181iteration: 4395\n",
      "train_loss: 0.0787904408075068\n",
      "val_loss: 0.18179865597704783\n",
      "Progress: 44.0% ... Training loss: 0.082 ... Validation loss: 0.163iteration: 4396\n",
      "train_loss: 0.08238101341649526\n",
      "val_loss: 0.1634627725822505\n",
      "Progress: 44.0% ... Training loss: 0.079 ... Validation loss: 0.175iteration: 4397\n",
      "train_loss: 0.07902383848019183\n",
      "val_loss: 0.1751040118616294\n",
      "Progress: 44.0% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 4398\n",
      "train_loss: 0.08152625994337445\n",
      "val_loss: 0.1540421379276324\n",
      "Progress: 44.0% ... Training loss: 0.086 ... Validation loss: 0.181iteration: 4399\n",
      "train_loss: 0.0865411925015562\n",
      "val_loss: 0.181157758767409\n",
      "Progress: 44.0% ... Training loss: 0.087 ... Validation loss: 0.157iteration: 4400\n",
      "train_loss: 0.08703406646781407\n",
      "val_loss: 0.15701020985824402\n",
      "Progress: 44.0% ... Training loss: 0.084 ... Validation loss: 0.198iteration: 4401\n",
      "train_loss: 0.08457750271088343\n",
      "val_loss: 0.19816156043770397\n",
      "Progress: 44.0% ... Training loss: 0.088 ... Validation loss: 0.159iteration: 4402\n",
      "train_loss: 0.08807422732729986\n",
      "val_loss: 0.1596327334405728\n",
      "Progress: 44.0% ... Training loss: 0.083 ... Validation loss: 0.199iteration: 4403\n",
      "train_loss: 0.08399020014118212\n",
      "val_loss: 0.1996149539938277\n",
      "Progress: 44.0% ... Training loss: 0.082 ... Validation loss: 0.155iteration: 4404\n",
      "train_loss: 0.08234547216108976\n",
      "val_loss: 0.15581866468231634\n",
      "Progress: 44.0% ... Training loss: 0.079 ... Validation loss: 0.179iteration: 4405\n",
      "train_loss: 0.07903071175296181\n",
      "val_loss: 0.17916068232559396\n",
      "Progress: 44.1% ... Training loss: 0.079 ... Validation loss: 0.160iteration: 4406\n",
      "train_loss: 0.07951215581341953\n",
      "val_loss: 0.1605595843477818\n",
      "Progress: 44.1% ... Training loss: 0.086 ... Validation loss: 0.201iteration: 4407\n",
      "train_loss: 0.08608066637081838\n",
      "val_loss: 0.20150058813156738\n",
      "Progress: 44.1% ... Training loss: 0.075 ... Validation loss: 0.166iteration: 4408\n",
      "train_loss: 0.07553747256976914\n",
      "val_loss: 0.1664573380880003\n",
      "Progress: 44.1% ... Training loss: 0.074 ... Validation loss: 0.176iteration: 4409\n",
      "train_loss: 0.07402876544960289\n",
      "val_loss: 0.17613597780604223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 44.1% ... Training loss: 0.078 ... Validation loss: 0.184iteration: 4410\n",
      "train_loss: 0.07850451968209538\n",
      "val_loss: 0.18465111914036644\n",
      "Progress: 44.1% ... Training loss: 0.077 ... Validation loss: 0.157iteration: 4411\n",
      "train_loss: 0.07711911180586317\n",
      "val_loss: 0.15742399209000185\n",
      "Progress: 44.1% ... Training loss: 0.078 ... Validation loss: 0.184iteration: 4412\n",
      "train_loss: 0.07843162590512927\n",
      "val_loss: 0.18457028027700528\n",
      "Progress: 44.1% ... Training loss: 0.083 ... Validation loss: 0.156iteration: 4413\n",
      "train_loss: 0.0831438183455223\n",
      "val_loss: 0.15613198998842\n",
      "Progress: 44.1% ... Training loss: 0.085 ... Validation loss: 0.191iteration: 4414\n",
      "train_loss: 0.08551683905307823\n",
      "val_loss: 0.19134182350941506\n",
      "Progress: 44.1% ... Training loss: 0.099 ... Validation loss: 0.163iteration: 4415\n",
      "train_loss: 0.09991040150159203\n",
      "val_loss: 0.16376659358656495\n",
      "Progress: 44.2% ... Training loss: 0.104 ... Validation loss: 0.213iteration: 4416\n",
      "train_loss: 0.10412449572521974\n",
      "val_loss: 0.21399149575932222\n",
      "Progress: 44.2% ... Training loss: 0.103 ... Validation loss: 0.171iteration: 4417\n",
      "train_loss: 0.10383471495750635\n",
      "val_loss: 0.17132321039351958\n",
      "Progress: 44.2% ... Training loss: 0.082 ... Validation loss: 0.188iteration: 4418\n",
      "train_loss: 0.0827715266438324\n",
      "val_loss: 0.18843149724115044\n",
      "Progress: 44.2% ... Training loss: 0.076 ... Validation loss: 0.158iteration: 4419\n",
      "train_loss: 0.0760313166735788\n",
      "val_loss: 0.15864036512951657\n",
      "Progress: 44.2% ... Training loss: 0.074 ... Validation loss: 0.170iteration: 4420\n",
      "train_loss: 0.07487207581816757\n",
      "val_loss: 0.17059211065050534\n",
      "Progress: 44.2% ... Training loss: 0.079 ... Validation loss: 0.162iteration: 4421\n",
      "train_loss: 0.07916814962849278\n",
      "val_loss: 0.162091267695939\n",
      "Progress: 44.2% ... Training loss: 0.076 ... Validation loss: 0.182iteration: 4422\n",
      "train_loss: 0.07641144536245696\n",
      "val_loss: 0.18244239164221815\n",
      "Progress: 44.2% ... Training loss: 0.073 ... Validation loss: 0.179iteration: 4423\n",
      "train_loss: 0.07389966112880263\n",
      "val_loss: 0.1790042744353517\n",
      "Progress: 44.2% ... Training loss: 0.076 ... Validation loss: 0.159iteration: 4424\n",
      "train_loss: 0.07657824503005674\n",
      "val_loss: 0.15914267220838818\n",
      "Progress: 44.2% ... Training loss: 0.073 ... Validation loss: 0.168iteration: 4425\n",
      "train_loss: 0.07397612880915047\n",
      "val_loss: 0.16810650485611411\n",
      "Progress: 44.3% ... Training loss: 0.075 ... Validation loss: 0.169iteration: 4426\n",
      "train_loss: 0.07589334892376844\n",
      "val_loss: 0.16963520731524562\n",
      "Progress: 44.3% ... Training loss: 0.073 ... Validation loss: 0.180iteration: 4427\n",
      "train_loss: 0.07369906882467651\n",
      "val_loss: 0.18040923312073992\n",
      "Progress: 44.3% ... Training loss: 0.073 ... Validation loss: 0.167iteration: 4428\n",
      "train_loss: 0.07373938485492583\n",
      "val_loss: 0.167541873775464\n",
      "Progress: 44.3% ... Training loss: 0.073 ... Validation loss: 0.175iteration: 4429\n",
      "train_loss: 0.07357155357319178\n",
      "val_loss: 0.17598665846863806\n",
      "Progress: 44.3% ... Training loss: 0.075 ... Validation loss: 0.164iteration: 4430\n",
      "train_loss: 0.07574466724011035\n",
      "val_loss: 0.16495353956092382\n",
      "Progress: 44.3% ... Training loss: 0.075 ... Validation loss: 0.176iteration: 4431\n",
      "train_loss: 0.0757631629177987\n",
      "val_loss: 0.17635602744862777\n",
      "Progress: 44.3% ... Training loss: 0.074 ... Validation loss: 0.162iteration: 4432\n",
      "train_loss: 0.07405655158244981\n",
      "val_loss: 0.1624352172395383\n",
      "Progress: 44.3% ... Training loss: 0.073 ... Validation loss: 0.176iteration: 4433\n",
      "train_loss: 0.07337681689938431\n",
      "val_loss: 0.1769786670333035\n",
      "Progress: 44.3% ... Training loss: 0.074 ... Validation loss: 0.165iteration: 4434\n",
      "train_loss: 0.07445278502861322\n",
      "val_loss: 0.16587824303072318\n",
      "Progress: 44.4% ... Training loss: 0.074 ... Validation loss: 0.162iteration: 4435\n",
      "train_loss: 0.07425873394821561\n",
      "val_loss: 0.16293155128265202\n",
      "Progress: 44.4% ... Training loss: 0.075 ... Validation loss: 0.182iteration: 4436\n",
      "train_loss: 0.07507129578973301\n",
      "val_loss: 0.18244739761132184\n",
      "Progress: 44.4% ... Training loss: 0.076 ... Validation loss: 0.163iteration: 4437\n",
      "train_loss: 0.0764650963276267\n",
      "val_loss: 0.1630118022475164\n",
      "Progress: 44.4% ... Training loss: 0.073 ... Validation loss: 0.164iteration: 4438\n",
      "train_loss: 0.07382297487626271\n",
      "val_loss: 0.16488326010843965\n",
      "Progress: 44.4% ... Training loss: 0.075 ... Validation loss: 0.167iteration: 4439\n",
      "train_loss: 0.07556877963120119\n",
      "val_loss: 0.16725197125145427\n",
      "Progress: 44.4% ... Training loss: 0.075 ... Validation loss: 0.162iteration: 4440\n",
      "train_loss: 0.07529272204960852\n",
      "val_loss: 0.1623136060594638\n",
      "Progress: 44.4% ... Training loss: 0.074 ... Validation loss: 0.175iteration: 4441\n",
      "train_loss: 0.07476121844551642\n",
      "val_loss: 0.17595391800961724\n",
      "Progress: 44.4% ... Training loss: 0.075 ... Validation loss: 0.168iteration: 4442\n",
      "train_loss: 0.07549850784807428\n",
      "val_loss: 0.1688016813368684\n",
      "Progress: 44.4% ... Training loss: 0.074 ... Validation loss: 0.186iteration: 4443\n",
      "train_loss: 0.07469690957010765\n",
      "val_loss: 0.18628654777377665\n",
      "Progress: 44.4% ... Training loss: 0.075 ... Validation loss: 0.158iteration: 4444\n",
      "train_loss: 0.07524942989383492\n",
      "val_loss: 0.1583936718944255\n",
      "Progress: 44.5% ... Training loss: 0.075 ... Validation loss: 0.181iteration: 4445\n",
      "train_loss: 0.07521267519490836\n",
      "val_loss: 0.181606600902445\n",
      "Progress: 44.5% ... Training loss: 0.074 ... Validation loss: 0.162iteration: 4446\n",
      "train_loss: 0.07452829111346176\n",
      "val_loss: 0.1627141590782938\n",
      "Progress: 44.5% ... Training loss: 0.073 ... Validation loss: 0.165iteration: 4447\n",
      "train_loss: 0.07355085662642413\n",
      "val_loss: 0.16526787802578502\n",
      "Progress: 44.5% ... Training loss: 0.083 ... Validation loss: 0.182iteration: 4448\n",
      "train_loss: 0.0837424213992529\n",
      "val_loss: 0.18284612391613564\n",
      "Progress: 44.5% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 4449\n",
      "train_loss: 0.08182633550122542\n",
      "val_loss: 0.15448192642022224\n",
      "Progress: 44.5% ... Training loss: 0.096 ... Validation loss: 0.225iteration: 4450\n",
      "train_loss: 0.09665999592657502\n",
      "val_loss: 0.22590336308302533\n",
      "Progress: 44.5% ... Training loss: 0.086 ... Validation loss: 0.158iteration: 4451\n",
      "train_loss: 0.08688177438307874\n",
      "val_loss: 0.1583387753388241\n",
      "Progress: 44.5% ... Training loss: 0.077 ... Validation loss: 0.187iteration: 4452\n",
      "train_loss: 0.07708611489275147\n",
      "val_loss: 0.18733234619656228\n",
      "Progress: 44.5% ... Training loss: 0.073 ... Validation loss: 0.160iteration: 4453\n",
      "train_loss: 0.07349453129921991\n",
      "val_loss: 0.1605146918642789\n",
      "Progress: 44.5% ... Training loss: 0.080 ... Validation loss: 0.183iteration: 4454\n",
      "train_loss: 0.08089859459846546\n",
      "val_loss: 0.18376437203727605\n",
      "Progress: 44.5% ... Training loss: 0.073 ... Validation loss: 0.168iteration: 4455\n",
      "train_loss: 0.07374408991855991\n",
      "val_loss: 0.1685608051782918\n",
      "Progress: 44.6% ... Training loss: 0.074 ... Validation loss: 0.163iteration: 4456\n",
      "train_loss: 0.07491850623042985\n",
      "val_loss: 0.1630788061795874\n",
      "Progress: 44.6% ... Training loss: 0.073 ... Validation loss: 0.170iteration: 4457\n",
      "train_loss: 0.07345864689017229\n",
      "val_loss: 0.17014829603467527\n",
      "Progress: 44.6% ... Training loss: 0.077 ... Validation loss: 0.162iteration: 4458\n",
      "train_loss: 0.0771447431944539\n",
      "val_loss: 0.16202526706683504\n",
      "Progress: 44.6% ... Training loss: 0.090 ... Validation loss: 0.228iteration: 4459\n",
      "train_loss: 0.0909398277157236\n",
      "val_loss: 0.22885692740617136\n",
      "Progress: 44.6% ... Training loss: 0.096 ... Validation loss: 0.157iteration: 4460\n",
      "train_loss: 0.096971104328905\n",
      "val_loss: 0.15769466670251778\n",
      "Progress: 44.6% ... Training loss: 0.092 ... Validation loss: 0.214iteration: 4461\n",
      "train_loss: 0.09200015203341191\n",
      "val_loss: 0.21431284002223386\n",
      "Progress: 44.6% ... Training loss: 0.099 ... Validation loss: 0.159iteration: 4462\n",
      "train_loss: 0.09915327383918021\n",
      "val_loss: 0.1597955623285391\n",
      "Progress: 44.6% ... Training loss: 0.138 ... Validation loss: 0.244iteration: 4463\n",
      "train_loss: 0.13858130881690223\n",
      "val_loss: 0.24462700003361523\n",
      "Progress: 44.6% ... Training loss: 0.164 ... Validation loss: 0.198iteration: 4464\n",
      "train_loss: 0.1648113132810239\n",
      "val_loss: 0.19808683620721285\n",
      "Progress: 44.6% ... Training loss: 0.184 ... Validation loss: 0.333iteration: 4465\n",
      "train_loss: 0.18407118995183339\n",
      "val_loss: 0.3330976005689689\n",
      "Progress: 44.7% ... Training loss: 0.137 ... Validation loss: 0.177iteration: 4466\n",
      "train_loss: 0.13786364143480234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.1776794005114066\n",
      "Progress: 44.7% ... Training loss: 0.168 ... Validation loss: 0.329iteration: 4467\n",
      "train_loss: 0.16830724627740368\n",
      "val_loss: 0.3294854347391292\n",
      "Progress: 44.7% ... Training loss: 0.145 ... Validation loss: 0.179iteration: 4468\n",
      "train_loss: 0.1452255499701878\n",
      "val_loss: 0.17990317854181173\n",
      "Progress: 44.7% ... Training loss: 0.123 ... Validation loss: 0.273iteration: 4469\n",
      "train_loss: 0.12323138489878463\n",
      "val_loss: 0.27301585957014324\n",
      "Progress: 44.7% ... Training loss: 0.091 ... Validation loss: 0.160iteration: 4470\n",
      "train_loss: 0.09141549577482055\n",
      "val_loss: 0.16043531629448082\n",
      "Progress: 44.7% ... Training loss: 0.096 ... Validation loss: 0.223iteration: 4471\n",
      "train_loss: 0.09684182756147165\n",
      "val_loss: 0.22355322693008656\n",
      "Progress: 44.7% ... Training loss: 0.095 ... Validation loss: 0.161iteration: 4472\n",
      "train_loss: 0.09539756959277435\n",
      "val_loss: 0.1611860459346227\n",
      "Progress: 44.7% ... Training loss: 0.077 ... Validation loss: 0.180iteration: 4473\n",
      "train_loss: 0.07786134946234458\n",
      "val_loss: 0.18073782403194552\n",
      "Progress: 44.7% ... Training loss: 0.073 ... Validation loss: 0.168iteration: 4474\n",
      "train_loss: 0.07368188726258815\n",
      "val_loss: 0.16815894052032354\n",
      "Progress: 44.8% ... Training loss: 0.074 ... Validation loss: 0.169iteration: 4475\n",
      "train_loss: 0.07494476669711116\n",
      "val_loss: 0.16980137306014711\n",
      "Progress: 44.8% ... Training loss: 0.076 ... Validation loss: 0.161iteration: 4476\n",
      "train_loss: 0.07657679269336713\n",
      "val_loss: 0.16168972751119\n",
      "Progress: 44.8% ... Training loss: 0.078 ... Validation loss: 0.180iteration: 4477\n",
      "train_loss: 0.07841691231226658\n",
      "val_loss: 0.1806196434302925\n",
      "Progress: 44.8% ... Training loss: 0.073 ... Validation loss: 0.165iteration: 4478\n",
      "train_loss: 0.07382095679464698\n",
      "val_loss: 0.1657806161320682\n",
      "Progress: 44.8% ... Training loss: 0.073 ... Validation loss: 0.170iteration: 4479\n",
      "train_loss: 0.07367835269665485\n",
      "val_loss: 0.17097604005610553\n",
      "Progress: 44.8% ... Training loss: 0.082 ... Validation loss: 0.162iteration: 4480\n",
      "train_loss: 0.0829659433833476\n",
      "val_loss: 0.1623550361805677\n",
      "Progress: 44.8% ... Training loss: 0.090 ... Validation loss: 0.187iteration: 4481\n",
      "train_loss: 0.09029409428914581\n",
      "val_loss: 0.1877425628550034\n",
      "Progress: 44.8% ... Training loss: 0.082 ... Validation loss: 0.154iteration: 4482\n",
      "train_loss: 0.08226513687718293\n",
      "val_loss: 0.15425662999605969\n",
      "Progress: 44.8% ... Training loss: 0.074 ... Validation loss: 0.175iteration: 4483\n",
      "train_loss: 0.07477712181079951\n",
      "val_loss: 0.17557376380777745\n",
      "Progress: 44.8% ... Training loss: 0.076 ... Validation loss: 0.164iteration: 4484\n",
      "train_loss: 0.07634035225551139\n",
      "val_loss: 0.16469627886727878\n",
      "Progress: 44.9% ... Training loss: 0.074 ... Validation loss: 0.168iteration: 4485\n",
      "train_loss: 0.07434903186868551\n",
      "val_loss: 0.16877744900807098\n",
      "Progress: 44.9% ... Training loss: 0.073 ... Validation loss: 0.164iteration: 4486\n",
      "train_loss: 0.07304296065904221\n",
      "val_loss: 0.16497892215461912\n",
      "Progress: 44.9% ... Training loss: 0.073 ... Validation loss: 0.166iteration: 4487\n",
      "train_loss: 0.07384600496147269\n",
      "val_loss: 0.16697200677876545\n",
      "Progress: 44.9% ... Training loss: 0.074 ... Validation loss: 0.162iteration: 4488\n",
      "train_loss: 0.07459992420582882\n",
      "val_loss: 0.1622072319790622\n",
      "Progress: 44.9% ... Training loss: 0.073 ... Validation loss: 0.174iteration: 4489\n",
      "train_loss: 0.07395840218584858\n",
      "val_loss: 0.1745628803683914\n",
      "Progress: 44.9% ... Training loss: 0.080 ... Validation loss: 0.198iteration: 4490\n",
      "train_loss: 0.08025974079607236\n",
      "val_loss: 0.19881173186529955\n",
      "Progress: 44.9% ... Training loss: 0.079 ... Validation loss: 0.161iteration: 4491\n",
      "train_loss: 0.07901943605897144\n",
      "val_loss: 0.16129646905422487\n",
      "Progress: 44.9% ... Training loss: 0.074 ... Validation loss: 0.173iteration: 4492\n",
      "train_loss: 0.07443976560341127\n",
      "val_loss: 0.17321661995701987\n",
      "Progress: 44.9% ... Training loss: 0.075 ... Validation loss: 0.165iteration: 4493\n",
      "train_loss: 0.07524486289687542\n",
      "val_loss: 0.16570399083199522\n",
      "Progress: 44.9% ... Training loss: 0.073 ... Validation loss: 0.178iteration: 4494\n",
      "train_loss: 0.0736944445818218\n",
      "val_loss: 0.17860288673735003\n",
      "Progress: 45.0% ... Training loss: 0.080 ... Validation loss: 0.168iteration: 4495\n",
      "train_loss: 0.08060173025780136\n",
      "val_loss: 0.16892206485595726\n",
      "Progress: 45.0% ... Training loss: 0.079 ... Validation loss: 0.201iteration: 4496\n",
      "train_loss: 0.07993303116258176\n",
      "val_loss: 0.20138742150494274\n",
      "Progress: 45.0% ... Training loss: 0.073 ... Validation loss: 0.172iteration: 4497\n",
      "train_loss: 0.07368014267023563\n",
      "val_loss: 0.17226169248971318\n",
      "Progress: 45.0% ... Training loss: 0.073 ... Validation loss: 0.167iteration: 4498\n",
      "train_loss: 0.07300268052284678\n",
      "val_loss: 0.16727641537556734\n",
      "Progress: 45.0% ... Training loss: 0.072 ... Validation loss: 0.170iteration: 4499\n",
      "train_loss: 0.07261514949035801\n",
      "val_loss: 0.17026128503479626\n",
      "Progress: 45.0% ... Training loss: 0.073 ... Validation loss: 0.181iteration: 4500\n",
      "train_loss: 0.07324106419000033\n",
      "val_loss: 0.18140312074130965\n",
      "Progress: 45.0% ... Training loss: 0.072 ... Validation loss: 0.175iteration: 4501\n",
      "train_loss: 0.07270266983207138\n",
      "val_loss: 0.1754540609124372\n",
      "Progress: 45.0% ... Training loss: 0.073 ... Validation loss: 0.171iteration: 4502\n",
      "train_loss: 0.07360638132088956\n",
      "val_loss: 0.17117669768003477\n",
      "Progress: 45.0% ... Training loss: 0.073 ... Validation loss: 0.160iteration: 4503\n",
      "train_loss: 0.07392037698666974\n",
      "val_loss: 0.16022629611835185\n",
      "Progress: 45.0% ... Training loss: 0.073 ... Validation loss: 0.184iteration: 4504\n",
      "train_loss: 0.07321757363361313\n",
      "val_loss: 0.18406175015970405\n",
      "Progress: 45.0% ... Training loss: 0.076 ... Validation loss: 0.161iteration: 4505\n",
      "train_loss: 0.07641752824752093\n",
      "val_loss: 0.16172724444091072\n",
      "Progress: 45.1% ... Training loss: 0.074 ... Validation loss: 0.164iteration: 4506\n",
      "train_loss: 0.07495942970126424\n",
      "val_loss: 0.16417898292725816\n",
      "Progress: 45.1% ... Training loss: 0.075 ... Validation loss: 0.186iteration: 4507\n",
      "train_loss: 0.07542231708737915\n",
      "val_loss: 0.1869618767698036\n",
      "Progress: 45.1% ... Training loss: 0.074 ... Validation loss: 0.163iteration: 4508\n",
      "train_loss: 0.07433551370159569\n",
      "val_loss: 0.16349588391829883\n",
      "Progress: 45.1% ... Training loss: 0.075 ... Validation loss: 0.180iteration: 4509\n",
      "train_loss: 0.07525462109301516\n",
      "val_loss: 0.1809651185539427\n",
      "Progress: 45.1% ... Training loss: 0.074 ... Validation loss: 0.162iteration: 4510\n",
      "train_loss: 0.0744403024830515\n",
      "val_loss: 0.1620296331714742\n",
      "Progress: 45.1% ... Training loss: 0.086 ... Validation loss: 0.191iteration: 4511\n",
      "train_loss: 0.08625228754166209\n",
      "val_loss: 0.19111296759278076\n",
      "Progress: 45.1% ... Training loss: 0.086 ... Validation loss: 0.160iteration: 4512\n",
      "train_loss: 0.08673481566535723\n",
      "val_loss: 0.16084849496407902\n",
      "Progress: 45.1% ... Training loss: 0.078 ... Validation loss: 0.193iteration: 4513\n",
      "train_loss: 0.07897739940661909\n",
      "val_loss: 0.19378475110639212\n",
      "Progress: 45.1% ... Training loss: 0.074 ... Validation loss: 0.181iteration: 4514\n",
      "train_loss: 0.0740339907503107\n",
      "val_loss: 0.18115965129747025\n",
      "Progress: 45.1% ... Training loss: 0.077 ... Validation loss: 0.167iteration: 4515\n",
      "train_loss: 0.07794472003699164\n",
      "val_loss: 0.16703048784427904\n",
      "Progress: 45.2% ... Training loss: 0.078 ... Validation loss: 0.203iteration: 4516\n",
      "train_loss: 0.07851761271511279\n",
      "val_loss: 0.20330371581443157\n",
      "Progress: 45.2% ... Training loss: 0.092 ... Validation loss: 0.160iteration: 4517\n",
      "train_loss: 0.09273582409412368\n",
      "val_loss: 0.16017389944289184\n",
      "Progress: 45.2% ... Training loss: 0.168 ... Validation loss: 0.314iteration: 4518\n",
      "train_loss: 0.16862241647075507\n",
      "val_loss: 0.314421405597889\n",
      "Progress: 45.2% ... Training loss: 0.153 ... Validation loss: 0.182iteration: 4519\n",
      "train_loss: 0.15386234380442323\n",
      "val_loss: 0.18216558715669787\n",
      "Progress: 45.2% ... Training loss: 0.125 ... Validation loss: 0.286iteration: 4520\n",
      "train_loss: 0.12563803586993058\n",
      "val_loss: 0.286332389283493\n",
      "Progress: 45.2% ... Training loss: 0.146 ... Validation loss: 0.180iteration: 4521\n",
      "train_loss: 0.14676055178660444\n",
      "val_loss: 0.18042668595019912\n",
      "Progress: 45.2% ... Training loss: 0.170 ... Validation loss: 0.295iteration: 4522\n",
      "train_loss: 0.1703533586427901\n",
      "val_loss: 0.29563712974345774\n",
      "Progress: 45.2% ... Training loss: 0.116 ... Validation loss: 0.163iteration: 4523\n",
      "train_loss: 0.1161529764577314\n",
      "val_loss: 0.16384829591537312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 45.2% ... Training loss: 0.093 ... Validation loss: 0.220iteration: 4524\n",
      "train_loss: 0.09334727512135527\n",
      "val_loss: 0.2209718674952974\n",
      "Progress: 45.2% ... Training loss: 0.091 ... Validation loss: 0.157iteration: 4525\n",
      "train_loss: 0.09195040106676809\n",
      "val_loss: 0.15795508672037634\n",
      "Progress: 45.3% ... Training loss: 0.093 ... Validation loss: 0.241iteration: 4526\n",
      "train_loss: 0.09388474746673901\n",
      "val_loss: 0.24145423566204047\n",
      "Progress: 45.3% ... Training loss: 0.103 ... Validation loss: 0.162iteration: 4527\n",
      "train_loss: 0.10348363303694715\n",
      "val_loss: 0.1625229970290991\n",
      "Progress: 45.3% ... Training loss: 0.089 ... Validation loss: 0.215iteration: 4528\n",
      "train_loss: 0.08943163149008128\n",
      "val_loss: 0.21520323500458874\n",
      "Progress: 45.3% ... Training loss: 0.089 ... Validation loss: 0.163iteration: 4529\n",
      "train_loss: 0.08903525501968626\n",
      "val_loss: 0.16320331384993939\n",
      "Progress: 45.3% ... Training loss: 0.110 ... Validation loss: 0.242iteration: 4530\n",
      "train_loss: 0.11064478961380245\n",
      "val_loss: 0.242834568800061\n",
      "Progress: 45.3% ... Training loss: 0.113 ... Validation loss: 0.167iteration: 4531\n",
      "train_loss: 0.11325895705521125\n",
      "val_loss: 0.16740118416902663\n",
      "Progress: 45.3% ... Training loss: 0.093 ... Validation loss: 0.217iteration: 4532\n",
      "train_loss: 0.09371508767658947\n",
      "val_loss: 0.2177933698866076\n",
      "Progress: 45.3% ... Training loss: 0.077 ... Validation loss: 0.158iteration: 4533\n",
      "train_loss: 0.07787583048542736\n",
      "val_loss: 0.1586273079578386\n",
      "Progress: 45.3% ... Training loss: 0.073 ... Validation loss: 0.174iteration: 4534\n",
      "train_loss: 0.07373075143219514\n",
      "val_loss: 0.17490073214552815\n",
      "Progress: 45.4% ... Training loss: 0.074 ... Validation loss: 0.191iteration: 4535\n",
      "train_loss: 0.07477095687768479\n",
      "val_loss: 0.19134132328759046\n",
      "Progress: 45.4% ... Training loss: 0.076 ... Validation loss: 0.174iteration: 4536\n",
      "train_loss: 0.07607979470587503\n",
      "val_loss: 0.17495594185707117\n",
      "Progress: 45.4% ... Training loss: 0.073 ... Validation loss: 0.167iteration: 4537\n",
      "train_loss: 0.07312877316571514\n",
      "val_loss: 0.1677804383190345\n",
      "Progress: 45.4% ... Training loss: 0.073 ... Validation loss: 0.160iteration: 4538\n",
      "train_loss: 0.07345818293592773\n",
      "val_loss: 0.16052725856506922\n",
      "Progress: 45.4% ... Training loss: 0.077 ... Validation loss: 0.181iteration: 4539\n",
      "train_loss: 0.07707351330700454\n",
      "val_loss: 0.1815931605100197\n",
      "Progress: 45.4% ... Training loss: 0.073 ... Validation loss: 0.167iteration: 4540\n",
      "train_loss: 0.07320505106917279\n",
      "val_loss: 0.16799428020610144\n",
      "Progress: 45.4% ... Training loss: 0.077 ... Validation loss: 0.184iteration: 4541\n",
      "train_loss: 0.07715456403778048\n",
      "val_loss: 0.18400542143306092\n",
      "Progress: 45.4% ... Training loss: 0.083 ... Validation loss: 0.160iteration: 4542\n",
      "train_loss: 0.08309648708310687\n",
      "val_loss: 0.16059197053938123\n",
      "Progress: 45.4% ... Training loss: 0.079 ... Validation loss: 0.194iteration: 4543\n",
      "train_loss: 0.07962660569870425\n",
      "val_loss: 0.19447531391154513\n",
      "Progress: 45.4% ... Training loss: 0.075 ... Validation loss: 0.172iteration: 4544\n",
      "train_loss: 0.07584573288007966\n",
      "val_loss: 0.17257664531795633\n",
      "Progress: 45.5% ... Training loss: 0.076 ... Validation loss: 0.178iteration: 4545\n",
      "train_loss: 0.07604585316790907\n",
      "val_loss: 0.17849185962425299\n",
      "Progress: 45.5% ... Training loss: 0.073 ... Validation loss: 0.162iteration: 4546\n",
      "train_loss: 0.0733576168332871\n",
      "val_loss: 0.16263869856348231\n",
      "Progress: 45.5% ... Training loss: 0.074 ... Validation loss: 0.165iteration: 4547\n",
      "train_loss: 0.07484265861890733\n",
      "val_loss: 0.16572890413040592\n",
      "Progress: 45.5% ... Training loss: 0.072 ... Validation loss: 0.171iteration: 4548\n",
      "train_loss: 0.07268869316775037\n",
      "val_loss: 0.17175927110507447\n",
      "Progress: 45.5% ... Training loss: 0.078 ... Validation loss: 0.193iteration: 4549\n",
      "train_loss: 0.07832157229635485\n",
      "val_loss: 0.19300897040129034\n",
      "Progress: 45.5% ... Training loss: 0.082 ... Validation loss: 0.163iteration: 4550\n",
      "train_loss: 0.08290235790295894\n",
      "val_loss: 0.16393634711515526\n",
      "Progress: 45.5% ... Training loss: 0.076 ... Validation loss: 0.192iteration: 4551\n",
      "train_loss: 0.07664805333164329\n",
      "val_loss: 0.19229156402394879\n",
      "Progress: 45.5% ... Training loss: 0.073 ... Validation loss: 0.179iteration: 4552\n",
      "train_loss: 0.07340071993231526\n",
      "val_loss: 0.17979825989146078\n",
      "Progress: 45.5% ... Training loss: 0.072 ... Validation loss: 0.179iteration: 4553\n",
      "train_loss: 0.07295554132383414\n",
      "val_loss: 0.1791348996038549\n",
      "Progress: 45.5% ... Training loss: 0.077 ... Validation loss: 0.195iteration: 4554\n",
      "train_loss: 0.07794463928722883\n",
      "val_loss: 0.19586993589549087\n",
      "Progress: 45.5% ... Training loss: 0.077 ... Validation loss: 0.160iteration: 4555\n",
      "train_loss: 0.07765517855948832\n",
      "val_loss: 0.16096093067238368\n",
      "Progress: 45.6% ... Training loss: 0.072 ... Validation loss: 0.172iteration: 4556\n",
      "train_loss: 0.07233253898259051\n",
      "val_loss: 0.17213581141520026\n",
      "Progress: 45.6% ... Training loss: 0.072 ... Validation loss: 0.176iteration: 4557\n",
      "train_loss: 0.072682840483614\n",
      "val_loss: 0.17672764233637445\n",
      "Progress: 45.6% ... Training loss: 0.071 ... Validation loss: 0.170iteration: 4558\n",
      "train_loss: 0.07194018924926539\n",
      "val_loss: 0.17097767174167697\n",
      "Progress: 45.6% ... Training loss: 0.072 ... Validation loss: 0.166iteration: 4559\n",
      "train_loss: 0.07220961604343767\n",
      "val_loss: 0.1665604812030266\n",
      "Progress: 45.6% ... Training loss: 0.073 ... Validation loss: 0.169iteration: 4560\n",
      "train_loss: 0.07338307007783858\n",
      "val_loss: 0.1697686796752109\n",
      "Progress: 45.6% ... Training loss: 0.072 ... Validation loss: 0.169iteration: 4561\n",
      "train_loss: 0.07251318192907084\n",
      "val_loss: 0.16944904504961905\n",
      "Progress: 45.6% ... Training loss: 0.076 ... Validation loss: 0.161iteration: 4562\n",
      "train_loss: 0.07640670628657847\n",
      "val_loss: 0.1614181592035419\n",
      "Progress: 45.6% ... Training loss: 0.074 ... Validation loss: 0.167iteration: 4563\n",
      "train_loss: 0.07438941966894803\n",
      "val_loss: 0.167900276889606\n",
      "Progress: 45.6% ... Training loss: 0.075 ... Validation loss: 0.155iteration: 4564\n",
      "train_loss: 0.07546025241103099\n",
      "val_loss: 0.15571969154444518\n",
      "Progress: 45.6% ... Training loss: 0.080 ... Validation loss: 0.180iteration: 4565\n",
      "train_loss: 0.08010801210538558\n",
      "val_loss: 0.18041855107572724\n",
      "Progress: 45.7% ... Training loss: 0.079 ... Validation loss: 0.158iteration: 4566\n",
      "train_loss: 0.07966151896103467\n",
      "val_loss: 0.15812757436580893\n",
      "Progress: 45.7% ... Training loss: 0.074 ... Validation loss: 0.164iteration: 4567\n",
      "train_loss: 0.07424651788845284\n",
      "val_loss: 0.16410854105572323\n",
      "Progress: 45.7% ... Training loss: 0.083 ... Validation loss: 0.165iteration: 4568\n",
      "train_loss: 0.08300976993974687\n",
      "val_loss: 0.1658958779086606\n",
      "Progress: 45.7% ... Training loss: 0.073 ... Validation loss: 0.184iteration: 4569\n",
      "train_loss: 0.07362831687499465\n",
      "val_loss: 0.18407980618080302\n",
      "Progress: 45.7% ... Training loss: 0.077 ... Validation loss: 0.158iteration: 4570\n",
      "train_loss: 0.07761875977702892\n",
      "val_loss: 0.15856649666508138\n",
      "Progress: 45.7% ... Training loss: 0.076 ... Validation loss: 0.179iteration: 4571\n",
      "train_loss: 0.07692044246475918\n",
      "val_loss: 0.17936259499077706\n",
      "Progress: 45.7% ... Training loss: 0.078 ... Validation loss: 0.159iteration: 4572\n",
      "train_loss: 0.07828764104762422\n",
      "val_loss: 0.15919769627319022\n",
      "Progress: 45.7% ... Training loss: 0.079 ... Validation loss: 0.204iteration: 4573\n",
      "train_loss: 0.07918657425737362\n",
      "val_loss: 0.20453271602555792\n",
      "Progress: 45.7% ... Training loss: 0.085 ... Validation loss: 0.165iteration: 4574\n",
      "train_loss: 0.08509202635531883\n",
      "val_loss: 0.16555696343504728\n",
      "Progress: 45.8% ... Training loss: 0.082 ... Validation loss: 0.215iteration: 4575\n",
      "train_loss: 0.08251543533120835\n",
      "val_loss: 0.21511719572886187\n",
      "Progress: 45.8% ... Training loss: 0.076 ... Validation loss: 0.161iteration: 4576\n",
      "train_loss: 0.07672456220880033\n",
      "val_loss: 0.16194422095089914\n",
      "Progress: 45.8% ... Training loss: 0.072 ... Validation loss: 0.162iteration: 4577\n",
      "train_loss: 0.07226454674064621\n",
      "val_loss: 0.162148819130072\n",
      "Progress: 45.8% ... Training loss: 0.072 ... Validation loss: 0.164iteration: 4578\n",
      "train_loss: 0.07273125121724498\n",
      "val_loss: 0.1643871798832691\n",
      "Progress: 45.8% ... Training loss: 0.071 ... Validation loss: 0.171iteration: 4579\n",
      "train_loss: 0.07199655962204098\n",
      "val_loss: 0.1712279838735526\n",
      "Progress: 45.8% ... Training loss: 0.072 ... Validation loss: 0.164iteration: 4580\n",
      "train_loss: 0.07267518475189909\n",
      "val_loss: 0.16429592648458946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 45.8% ... Training loss: 0.072 ... Validation loss: 0.178iteration: 4581\n",
      "train_loss: 0.07281625021994392\n",
      "val_loss: 0.1782253308624936\n",
      "Progress: 45.8% ... Training loss: 0.072 ... Validation loss: 0.176iteration: 4582\n",
      "train_loss: 0.07238430408979772\n",
      "val_loss: 0.17689360882073488\n",
      "Progress: 45.8% ... Training loss: 0.072 ... Validation loss: 0.170iteration: 4583\n",
      "train_loss: 0.07235529187177316\n",
      "val_loss: 0.17063606969742104\n",
      "Progress: 45.8% ... Training loss: 0.073 ... Validation loss: 0.159iteration: 4584\n",
      "train_loss: 0.07384637881775329\n",
      "val_loss: 0.15997304950779836\n",
      "Progress: 45.9% ... Training loss: 0.081 ... Validation loss: 0.190iteration: 4585\n",
      "train_loss: 0.08140632735247065\n",
      "val_loss: 0.19074584038330192\n",
      "Progress: 45.9% ... Training loss: 0.094 ... Validation loss: 0.156iteration: 4586\n",
      "train_loss: 0.0946019798997069\n",
      "val_loss: 0.15695826811579952\n",
      "Progress: 45.9% ... Training loss: 0.089 ... Validation loss: 0.195iteration: 4587\n",
      "train_loss: 0.0897317876119707\n",
      "val_loss: 0.19501715686406335\n",
      "Progress: 45.9% ... Training loss: 0.090 ... Validation loss: 0.157iteration: 4588\n",
      "train_loss: 0.09007484330389741\n",
      "val_loss: 0.1571055904711635\n",
      "Progress: 45.9% ... Training loss: 0.076 ... Validation loss: 0.189iteration: 4589\n",
      "train_loss: 0.07672241252369713\n",
      "val_loss: 0.18902670357510193\n",
      "Progress: 45.9% ... Training loss: 0.098 ... Validation loss: 0.160iteration: 4590\n",
      "train_loss: 0.09856096886431234\n",
      "val_loss: 0.1600967442275477\n",
      "Progress: 45.9% ... Training loss: 0.085 ... Validation loss: 0.204iteration: 4591\n",
      "train_loss: 0.08591641777332099\n",
      "val_loss: 0.20408661197951083\n",
      "Progress: 45.9% ... Training loss: 0.085 ... Validation loss: 0.155iteration: 4592\n",
      "train_loss: 0.08506968222055743\n",
      "val_loss: 0.1557967429641479\n",
      "Progress: 45.9% ... Training loss: 0.078 ... Validation loss: 0.183iteration: 4593\n",
      "train_loss: 0.07884882300773148\n",
      "val_loss: 0.18315086167772018\n",
      "Progress: 45.9% ... Training loss: 0.075 ... Validation loss: 0.157iteration: 4594\n",
      "train_loss: 0.07529739286896929\n",
      "val_loss: 0.15798112445143767\n",
      "Progress: 46.0% ... Training loss: 0.081 ... Validation loss: 0.198iteration: 4595\n",
      "train_loss: 0.0812566860278113\n",
      "val_loss: 0.19835608480298833\n",
      "Progress: 46.0% ... Training loss: 0.073 ... Validation loss: 0.172iteration: 4596\n",
      "train_loss: 0.07321911481564043\n",
      "val_loss: 0.17248339104419894\n",
      "Progress: 46.0% ... Training loss: 0.075 ... Validation loss: 0.156iteration: 4597\n",
      "train_loss: 0.07517066458821739\n",
      "val_loss: 0.15654151799760724\n",
      "Progress: 46.0% ... Training loss: 0.074 ... Validation loss: 0.173iteration: 4598\n",
      "train_loss: 0.0741009653059438\n",
      "val_loss: 0.17323691392631343\n",
      "Progress: 46.0% ... Training loss: 0.072 ... Validation loss: 0.175iteration: 4599\n",
      "train_loss: 0.07258263473426534\n",
      "val_loss: 0.17549870871146037\n",
      "Progress: 46.0% ... Training loss: 0.074 ... Validation loss: 0.156iteration: 4600\n",
      "train_loss: 0.0744392265283658\n",
      "val_loss: 0.15678111671950756\n",
      "Progress: 46.0% ... Training loss: 0.074 ... Validation loss: 0.173iteration: 4601\n",
      "train_loss: 0.07427600863738863\n",
      "val_loss: 0.17371817301421943\n",
      "Progress: 46.0% ... Training loss: 0.074 ... Validation loss: 0.168iteration: 4602\n",
      "train_loss: 0.07449239508759112\n",
      "val_loss: 0.1687000327300012\n",
      "Progress: 46.0% ... Training loss: 0.074 ... Validation loss: 0.161iteration: 4603\n",
      "train_loss: 0.07444226627770353\n",
      "val_loss: 0.16135511008661338\n",
      "Progress: 46.0% ... Training loss: 0.075 ... Validation loss: 0.163iteration: 4604\n",
      "train_loss: 0.0755229852677225\n",
      "val_loss: 0.1633461520295825\n",
      "Progress: 46.0% ... Training loss: 0.079 ... Validation loss: 0.197iteration: 4605\n",
      "train_loss: 0.07906661996297705\n",
      "val_loss: 0.19762416620123655\n",
      "Progress: 46.1% ... Training loss: 0.072 ... Validation loss: 0.161iteration: 4606\n",
      "train_loss: 0.07264826034395815\n",
      "val_loss: 0.16129791009150138\n",
      "Progress: 46.1% ... Training loss: 0.079 ... Validation loss: 0.181iteration: 4607\n",
      "train_loss: 0.07944654291684462\n",
      "val_loss: 0.18170664403855763\n",
      "Progress: 46.1% ... Training loss: 0.074 ... Validation loss: 0.158iteration: 4608\n",
      "train_loss: 0.07492121838547022\n",
      "val_loss: 0.15841502826224457\n",
      "Progress: 46.1% ... Training loss: 0.075 ... Validation loss: 0.170iteration: 4609\n",
      "train_loss: 0.07509552122529535\n",
      "val_loss: 0.17053644790442188\n",
      "Progress: 46.1% ... Training loss: 0.075 ... Validation loss: 0.160iteration: 4610\n",
      "train_loss: 0.07588038286117253\n",
      "val_loss: 0.16091859520794677\n",
      "Progress: 46.1% ... Training loss: 0.078 ... Validation loss: 0.191iteration: 4611\n",
      "train_loss: 0.07883706018152305\n",
      "val_loss: 0.19167655840145517\n",
      "Progress: 46.1% ... Training loss: 0.084 ... Validation loss: 0.156iteration: 4612\n",
      "train_loss: 0.08468924714582965\n",
      "val_loss: 0.1564830143950649\n",
      "Progress: 46.1% ... Training loss: 0.076 ... Validation loss: 0.192iteration: 4613\n",
      "train_loss: 0.07691078550397473\n",
      "val_loss: 0.19259413188095115\n",
      "Progress: 46.1% ... Training loss: 0.071 ... Validation loss: 0.170iteration: 4614\n",
      "train_loss: 0.07158924689351287\n",
      "val_loss: 0.17099014242797272\n",
      "Progress: 46.1% ... Training loss: 0.072 ... Validation loss: 0.164iteration: 4615\n",
      "train_loss: 0.07218816606384909\n",
      "val_loss: 0.16429157897089702\n",
      "Progress: 46.2% ... Training loss: 0.073 ... Validation loss: 0.166iteration: 4616\n",
      "train_loss: 0.07323275453557944\n",
      "val_loss: 0.16628735178645074\n",
      "Progress: 46.2% ... Training loss: 0.072 ... Validation loss: 0.160iteration: 4617\n",
      "train_loss: 0.07258918151027044\n",
      "val_loss: 0.16053635474345723\n",
      "Progress: 46.2% ... Training loss: 0.072 ... Validation loss: 0.164iteration: 4618\n",
      "train_loss: 0.07267919415968738\n",
      "val_loss: 0.16490526245863327\n",
      "Progress: 46.2% ... Training loss: 0.073 ... Validation loss: 0.165iteration: 4619\n",
      "train_loss: 0.07314132982865512\n",
      "val_loss: 0.16519272732600113\n",
      "Progress: 46.2% ... Training loss: 0.072 ... Validation loss: 0.168iteration: 4620\n",
      "train_loss: 0.07281140067999713\n",
      "val_loss: 0.1688746107486237\n",
      "Progress: 46.2% ... Training loss: 0.075 ... Validation loss: 0.156iteration: 4621\n",
      "train_loss: 0.07501243082525635\n",
      "val_loss: 0.15677011381664793\n",
      "Progress: 46.2% ... Training loss: 0.071 ... Validation loss: 0.168iteration: 4622\n",
      "train_loss: 0.07182631321450683\n",
      "val_loss: 0.16844617327405412\n",
      "Progress: 46.2% ... Training loss: 0.074 ... Validation loss: 0.175iteration: 4623\n",
      "train_loss: 0.07466913282825594\n",
      "val_loss: 0.17592292640066245\n",
      "Progress: 46.2% ... Training loss: 0.072 ... Validation loss: 0.159iteration: 4624\n",
      "train_loss: 0.07295030762993339\n",
      "val_loss: 0.15985304005962395\n",
      "Progress: 46.2% ... Training loss: 0.093 ... Validation loss: 0.215iteration: 4625\n",
      "train_loss: 0.09352499415967871\n",
      "val_loss: 0.21549338202502588\n",
      "Progress: 46.3% ... Training loss: 0.087 ... Validation loss: 0.161iteration: 4626\n",
      "train_loss: 0.0872815865120639\n",
      "val_loss: 0.1618281244282458\n",
      "Progress: 46.3% ... Training loss: 0.084 ... Validation loss: 0.222iteration: 4627\n",
      "train_loss: 0.08418043970525793\n",
      "val_loss: 0.22274302427346698\n",
      "Progress: 46.3% ... Training loss: 0.086 ... Validation loss: 0.156iteration: 4628\n",
      "train_loss: 0.08654846804419689\n",
      "val_loss: 0.15603680366778822\n",
      "Progress: 46.3% ... Training loss: 0.105 ... Validation loss: 0.265iteration: 4629\n",
      "train_loss: 0.10578666044247666\n",
      "val_loss: 0.26507386192573595\n",
      "Progress: 46.3% ... Training loss: 0.093 ... Validation loss: 0.160iteration: 4630\n",
      "train_loss: 0.09371669834949732\n",
      "val_loss: 0.1600257860203102\n",
      "Progress: 46.3% ... Training loss: 0.085 ... Validation loss: 0.211iteration: 4631\n",
      "train_loss: 0.08597471267014546\n",
      "val_loss: 0.2119314736618285\n",
      "Progress: 46.3% ... Training loss: 0.102 ... Validation loss: 0.157iteration: 4632\n",
      "train_loss: 0.10247694301823766\n",
      "val_loss: 0.1574124740131876\n",
      "Progress: 46.3% ... Training loss: 0.105 ... Validation loss: 0.250iteration: 4633\n",
      "train_loss: 0.10522265830834354\n",
      "val_loss: 0.25028119971594337\n",
      "Progress: 46.3% ... Training loss: 0.100 ... Validation loss: 0.152iteration: 4634\n",
      "train_loss: 0.1009561712870352\n",
      "val_loss: 0.15201930858378793\n",
      "Progress: 46.4% ... Training loss: 0.117 ... Validation loss: 0.245iteration: 4635\n",
      "train_loss: 0.11732297814732132\n",
      "val_loss: 0.24563535478206977\n",
      "Progress: 46.4% ... Training loss: 0.107 ... Validation loss: 0.158iteration: 4636\n",
      "train_loss: 0.10783306316972385\n",
      "val_loss: 0.1582950315352212\n",
      "Progress: 46.4% ... Training loss: 0.096 ... Validation loss: 0.248iteration: 4637\n",
      "train_loss: 0.09632861082035786\n",
      "val_loss: 0.24898437801550494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 46.4% ... Training loss: 0.090 ... Validation loss: 0.155iteration: 4638\n",
      "train_loss: 0.09029355428263518\n",
      "val_loss: 0.1554963059299378\n",
      "Progress: 46.4% ... Training loss: 0.085 ... Validation loss: 0.212iteration: 4639\n",
      "train_loss: 0.08515868981279257\n",
      "val_loss: 0.2127630961609242\n",
      "Progress: 46.4% ... Training loss: 0.083 ... Validation loss: 0.155iteration: 4640\n",
      "train_loss: 0.08390692454952768\n",
      "val_loss: 0.1556188798080863\n",
      "Progress: 46.4% ... Training loss: 0.086 ... Validation loss: 0.221iteration: 4641\n",
      "train_loss: 0.08639299858723709\n",
      "val_loss: 0.22178344210549458\n",
      "Progress: 46.4% ... Training loss: 0.077 ... Validation loss: 0.173iteration: 4642\n",
      "train_loss: 0.0779088711807842\n",
      "val_loss: 0.17306271402584958\n",
      "Progress: 46.4% ... Training loss: 0.081 ... Validation loss: 0.204iteration: 4643\n",
      "train_loss: 0.0819491634266835\n",
      "val_loss: 0.20411425950440995\n",
      "Progress: 46.4% ... Training loss: 0.072 ... Validation loss: 0.165iteration: 4644\n",
      "train_loss: 0.07250590604268098\n",
      "val_loss: 0.1657843136556942\n",
      "Progress: 46.5% ... Training loss: 0.072 ... Validation loss: 0.171iteration: 4645\n",
      "train_loss: 0.07244972937097141\n",
      "val_loss: 0.1715037083577651\n",
      "Progress: 46.5% ... Training loss: 0.072 ... Validation loss: 0.185iteration: 4646\n",
      "train_loss: 0.07246786346741979\n",
      "val_loss: 0.18586187142328225\n",
      "Progress: 46.5% ... Training loss: 0.073 ... Validation loss: 0.190iteration: 4647\n",
      "train_loss: 0.07301131946555794\n",
      "val_loss: 0.1908184115982382\n",
      "Progress: 46.5% ... Training loss: 0.072 ... Validation loss: 0.170iteration: 4648\n",
      "train_loss: 0.07234529240678557\n",
      "val_loss: 0.17076501201560979\n",
      "Progress: 46.5% ... Training loss: 0.077 ... Validation loss: 0.191iteration: 4649\n",
      "train_loss: 0.0773427926102276\n",
      "val_loss: 0.1910586105993561\n",
      "Progress: 46.5% ... Training loss: 0.072 ... Validation loss: 0.166iteration: 4650\n",
      "train_loss: 0.0726022893376636\n",
      "val_loss: 0.16612585466982652\n",
      "Progress: 46.5% ... Training loss: 0.072 ... Validation loss: 0.166iteration: 4651\n",
      "train_loss: 0.0724016543379606\n",
      "val_loss: 0.16687759167127364\n",
      "Progress: 46.5% ... Training loss: 0.073 ... Validation loss: 0.165iteration: 4652\n",
      "train_loss: 0.07386323272244893\n",
      "val_loss: 0.1659043744390181\n",
      "Progress: 46.5% ... Training loss: 0.072 ... Validation loss: 0.176iteration: 4653\n",
      "train_loss: 0.07268742885600163\n",
      "val_loss: 0.17616867376387857\n",
      "Progress: 46.5% ... Training loss: 0.073 ... Validation loss: 0.179iteration: 4654\n",
      "train_loss: 0.07390570221700876\n",
      "val_loss: 0.17957933505830562\n",
      "Progress: 46.5% ... Training loss: 0.073 ... Validation loss: 0.163iteration: 4655\n",
      "train_loss: 0.07359552924659037\n",
      "val_loss: 0.1636112707217461\n",
      "Progress: 46.6% ... Training loss: 0.079 ... Validation loss: 0.199iteration: 4656\n",
      "train_loss: 0.07937483913874664\n",
      "val_loss: 0.19941887170639935\n",
      "Progress: 46.6% ... Training loss: 0.072 ... Validation loss: 0.166iteration: 4657\n",
      "train_loss: 0.07284965285499002\n",
      "val_loss: 0.16685634150869397\n",
      "Progress: 46.6% ... Training loss: 0.072 ... Validation loss: 0.174iteration: 4658\n",
      "train_loss: 0.072152758416357\n",
      "val_loss: 0.17499828504008927\n",
      "Progress: 46.6% ... Training loss: 0.072 ... Validation loss: 0.179iteration: 4659\n",
      "train_loss: 0.0725849704563786\n",
      "val_loss: 0.17923674737586792\n",
      "Progress: 46.6% ... Training loss: 0.071 ... Validation loss: 0.171iteration: 4660\n",
      "train_loss: 0.07187170750677939\n",
      "val_loss: 0.17165361100043172\n",
      "Progress: 46.6% ... Training loss: 0.072 ... Validation loss: 0.180iteration: 4661\n",
      "train_loss: 0.0729504602946216\n",
      "val_loss: 0.18006083115363344\n",
      "Progress: 46.6% ... Training loss: 0.074 ... Validation loss: 0.184iteration: 4662\n",
      "train_loss: 0.07452803111434816\n",
      "val_loss: 0.18441556299105796\n",
      "Progress: 46.6% ... Training loss: 0.073 ... Validation loss: 0.168iteration: 4663\n",
      "train_loss: 0.07337235179335454\n",
      "val_loss: 0.16819735356207677\n",
      "Progress: 46.6% ... Training loss: 0.085 ... Validation loss: 0.205iteration: 4664\n",
      "train_loss: 0.08557994997239667\n",
      "val_loss: 0.20588136292938078\n",
      "Progress: 46.6% ... Training loss: 0.074 ... Validation loss: 0.158iteration: 4665\n",
      "train_loss: 0.07438851155223458\n",
      "val_loss: 0.15874506998828758\n",
      "Progress: 46.7% ... Training loss: 0.074 ... Validation loss: 0.176iteration: 4666\n",
      "train_loss: 0.07469603292485555\n",
      "val_loss: 0.1769326404818699\n",
      "Progress: 46.7% ... Training loss: 0.080 ... Validation loss: 0.152iteration: 4667\n",
      "train_loss: 0.08017811632657465\n",
      "val_loss: 0.15225411466043084\n",
      "Progress: 46.7% ... Training loss: 0.072 ... Validation loss: 0.175iteration: 4668\n",
      "train_loss: 0.07213642932864037\n",
      "val_loss: 0.17566765887472283\n",
      "Progress: 46.7% ... Training loss: 0.072 ... Validation loss: 0.176iteration: 4669\n",
      "train_loss: 0.07252817463730112\n",
      "val_loss: 0.17626028864504756\n",
      "Progress: 46.7% ... Training loss: 0.074 ... Validation loss: 0.160iteration: 4670\n",
      "train_loss: 0.07467461946849914\n",
      "val_loss: 0.16021751177113083\n",
      "Progress: 46.7% ... Training loss: 0.082 ... Validation loss: 0.191iteration: 4671\n",
      "train_loss: 0.08222669278411414\n",
      "val_loss: 0.19151235416684947\n",
      "Progress: 46.7% ... Training loss: 0.072 ... Validation loss: 0.163iteration: 4672\n",
      "train_loss: 0.07215885609363178\n",
      "val_loss: 0.16377191958289214\n",
      "Progress: 46.7% ... Training loss: 0.080 ... Validation loss: 0.201iteration: 4673\n",
      "train_loss: 0.0803800223150252\n",
      "val_loss: 0.2013851330494881\n",
      "Progress: 46.7% ... Training loss: 0.090 ... Validation loss: 0.151iteration: 4674\n",
      "train_loss: 0.09087372076709162\n",
      "val_loss: 0.15163538389965273\n",
      "Progress: 46.8% ... Training loss: 0.074 ... Validation loss: 0.192iteration: 4675\n",
      "train_loss: 0.07409812840015317\n",
      "val_loss: 0.192661062079087\n",
      "Progress: 46.8% ... Training loss: 0.077 ... Validation loss: 0.160iteration: 4676\n",
      "train_loss: 0.07791528433093403\n",
      "val_loss: 0.16032043450797523\n",
      "Progress: 46.8% ... Training loss: 0.078 ... Validation loss: 0.194iteration: 4677\n",
      "train_loss: 0.07871934542153626\n",
      "val_loss: 0.19497348715858023\n",
      "Progress: 46.8% ... Training loss: 0.074 ... Validation loss: 0.160iteration: 4678\n",
      "train_loss: 0.07494175894204698\n",
      "val_loss: 0.16065385370506677\n",
      "Progress: 46.8% ... Training loss: 0.072 ... Validation loss: 0.173iteration: 4679\n",
      "train_loss: 0.07215984464992295\n",
      "val_loss: 0.17346831153307923\n",
      "Progress: 46.8% ... Training loss: 0.073 ... Validation loss: 0.174iteration: 4680\n",
      "train_loss: 0.07322880787822006\n",
      "val_loss: 0.17409714514016725\n",
      "Progress: 46.8% ... Training loss: 0.071 ... Validation loss: 0.167iteration: 4681\n",
      "train_loss: 0.07197939985995847\n",
      "val_loss: 0.1679131971787956\n",
      "Progress: 46.8% ... Training loss: 0.071 ... Validation loss: 0.174iteration: 4682\n",
      "train_loss: 0.07158460781879947\n",
      "val_loss: 0.17442861552615213\n",
      "Progress: 46.8% ... Training loss: 0.072 ... Validation loss: 0.174iteration: 4683\n",
      "train_loss: 0.07210792769394965\n",
      "val_loss: 0.17428821685876408\n",
      "Progress: 46.8% ... Training loss: 0.075 ... Validation loss: 0.185iteration: 4684\n",
      "train_loss: 0.07555362503986808\n",
      "val_loss: 0.18546405910902364\n",
      "Progress: 46.9% ... Training loss: 0.078 ... Validation loss: 0.161iteration: 4685\n",
      "train_loss: 0.07809822705949603\n",
      "val_loss: 0.1619429290981007\n",
      "Progress: 46.9% ... Training loss: 0.090 ... Validation loss: 0.245iteration: 4686\n",
      "train_loss: 0.0905790742237463\n",
      "val_loss: 0.2451412477967961\n",
      "Progress: 46.9% ... Training loss: 0.082 ... Validation loss: 0.161iteration: 4687\n",
      "train_loss: 0.08252625810930408\n",
      "val_loss: 0.16140036557259865\n",
      "Progress: 46.9% ... Training loss: 0.084 ... Validation loss: 0.215iteration: 4688\n",
      "train_loss: 0.08485546079759515\n",
      "val_loss: 0.21552367603894582\n",
      "Progress: 46.9% ... Training loss: 0.087 ... Validation loss: 0.158iteration: 4689\n",
      "train_loss: 0.08788450193079342\n",
      "val_loss: 0.15833232117832594\n",
      "Progress: 46.9% ... Training loss: 0.095 ... Validation loss: 0.260iteration: 4690\n",
      "train_loss: 0.09550878244296776\n",
      "val_loss: 0.2601878981152049\n",
      "Progress: 46.9% ... Training loss: 0.092 ... Validation loss: 0.159iteration: 4691\n",
      "train_loss: 0.09249521192028538\n",
      "val_loss: 0.15975916993910394\n",
      "Progress: 46.9% ... Training loss: 0.102 ... Validation loss: 0.261iteration: 4692\n",
      "train_loss: 0.10233080982186676\n",
      "val_loss: 0.2619570856659411\n",
      "Progress: 46.9% ... Training loss: 0.090 ... Validation loss: 0.152iteration: 4693\n",
      "train_loss: 0.09015901120009776\n",
      "val_loss: 0.15222670947497502\n",
      "Progress: 46.9% ... Training loss: 0.080 ... Validation loss: 0.207iteration: 4694\n",
      "train_loss: 0.08045414513632217\n",
      "val_loss: 0.2070656515938101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 47.0% ... Training loss: 0.074 ... Validation loss: 0.187iteration: 4695\n",
      "train_loss: 0.07449714576516189\n",
      "val_loss: 0.1874987419994977\n",
      "Progress: 47.0% ... Training loss: 0.072 ... Validation loss: 0.166iteration: 4696\n",
      "train_loss: 0.07263511127424091\n",
      "val_loss: 0.16667362127326465\n",
      "Progress: 47.0% ... Training loss: 0.075 ... Validation loss: 0.166iteration: 4697\n",
      "train_loss: 0.07564032202453928\n",
      "val_loss: 0.1668223024946272\n",
      "Progress: 47.0% ... Training loss: 0.079 ... Validation loss: 0.199iteration: 4698\n",
      "train_loss: 0.07946361800455777\n",
      "val_loss: 0.19957488879687982\n",
      "Progress: 47.0% ... Training loss: 0.071 ... Validation loss: 0.168iteration: 4699\n",
      "train_loss: 0.0713846218158568\n",
      "val_loss: 0.16818807033098482\n",
      "Progress: 47.0% ... Training loss: 0.071 ... Validation loss: 0.181iteration: 4700\n",
      "train_loss: 0.07186106979498337\n",
      "val_loss: 0.18162927233223541\n",
      "Progress: 47.0% ... Training loss: 0.073 ... Validation loss: 0.164iteration: 4701\n",
      "train_loss: 0.07314042889232686\n",
      "val_loss: 0.16406123454692637\n",
      "Progress: 47.0% ... Training loss: 0.085 ... Validation loss: 0.201iteration: 4702\n",
      "train_loss: 0.08560125966386811\n",
      "val_loss: 0.20113723160098196\n",
      "Progress: 47.0% ... Training loss: 0.076 ... Validation loss: 0.158iteration: 4703\n",
      "train_loss: 0.07691284431774144\n",
      "val_loss: 0.15877958380123983\n",
      "Progress: 47.0% ... Training loss: 0.085 ... Validation loss: 0.209iteration: 4704\n",
      "train_loss: 0.0850392379751633\n",
      "val_loss: 0.2098018303911648\n",
      "Progress: 47.0% ... Training loss: 0.076 ... Validation loss: 0.161iteration: 4705\n",
      "train_loss: 0.07647325546659418\n",
      "val_loss: 0.16161003926086398\n",
      "Progress: 47.1% ... Training loss: 0.073 ... Validation loss: 0.190iteration: 4706\n",
      "train_loss: 0.07375839506404319\n",
      "val_loss: 0.19034368992102715\n",
      "Progress: 47.1% ... Training loss: 0.076 ... Validation loss: 0.159iteration: 4707\n",
      "train_loss: 0.0760180581956126\n",
      "val_loss: 0.15971829974973048\n",
      "Progress: 47.1% ... Training loss: 0.089 ... Validation loss: 0.221iteration: 4708\n",
      "train_loss: 0.08901585730049434\n",
      "val_loss: 0.22126439191103317\n",
      "Progress: 47.1% ... Training loss: 0.107 ... Validation loss: 0.162iteration: 4709\n",
      "train_loss: 0.1074711490465514\n",
      "val_loss: 0.16225504601423463\n",
      "Progress: 47.1% ... Training loss: 0.116 ... Validation loss: 0.280iteration: 4710\n",
      "train_loss: 0.11675127319270068\n",
      "val_loss: 0.2807033896738873\n",
      "Progress: 47.1% ... Training loss: 0.115 ... Validation loss: 0.162iteration: 4711\n",
      "train_loss: 0.11510648558336666\n",
      "val_loss: 0.1626491352411957\n",
      "Progress: 47.1% ... Training loss: 0.092 ... Validation loss: 0.210iteration: 4712\n",
      "train_loss: 0.09220174442483393\n",
      "val_loss: 0.21009987551095047\n",
      "Progress: 47.1% ... Training loss: 0.082 ... Validation loss: 0.154iteration: 4713\n",
      "train_loss: 0.08287149467339165\n",
      "val_loss: 0.15478577259560636\n",
      "Progress: 47.1% ... Training loss: 0.082 ... Validation loss: 0.208iteration: 4714\n",
      "train_loss: 0.08212081925886919\n",
      "val_loss: 0.20821130592194115\n",
      "Progress: 47.1% ... Training loss: 0.100 ... Validation loss: 0.162iteration: 4715\n",
      "train_loss: 0.10032995625662712\n",
      "val_loss: 0.16228767501863414\n",
      "Progress: 47.2% ... Training loss: 0.086 ... Validation loss: 0.225iteration: 4716\n",
      "train_loss: 0.0869816951905571\n",
      "val_loss: 0.2253946702813859\n",
      "Progress: 47.2% ... Training loss: 0.079 ... Validation loss: 0.162iteration: 4717\n",
      "train_loss: 0.07980928381178792\n",
      "val_loss: 0.162192084129752\n",
      "Progress: 47.2% ... Training loss: 0.078 ... Validation loss: 0.213iteration: 4718\n",
      "train_loss: 0.07873876484030357\n",
      "val_loss: 0.21329190807250709\n",
      "Progress: 47.2% ... Training loss: 0.072 ... Validation loss: 0.172iteration: 4719\n",
      "train_loss: 0.0725221434762089\n",
      "val_loss: 0.17239055054825617\n",
      "Progress: 47.2% ... Training loss: 0.073 ... Validation loss: 0.200iteration: 4720\n",
      "train_loss: 0.07353074581954504\n",
      "val_loss: 0.20006398567937655\n",
      "Progress: 47.2% ... Training loss: 0.076 ... Validation loss: 0.168iteration: 4721\n",
      "train_loss: 0.07666792955375128\n",
      "val_loss: 0.16874110924444116\n",
      "Progress: 47.2% ... Training loss: 0.072 ... Validation loss: 0.194iteration: 4722\n",
      "train_loss: 0.07249581497087511\n",
      "val_loss: 0.19470693968002356\n",
      "Progress: 47.2% ... Training loss: 0.071 ... Validation loss: 0.180iteration: 4723\n",
      "train_loss: 0.07152321113964169\n",
      "val_loss: 0.18050571202870466\n",
      "Progress: 47.2% ... Training loss: 0.071 ... Validation loss: 0.183iteration: 4724\n",
      "train_loss: 0.07197953929357971\n",
      "val_loss: 0.18396047700454557\n",
      "Progress: 47.2% ... Training loss: 0.070 ... Validation loss: 0.179iteration: 4725\n",
      "train_loss: 0.07092757064262664\n",
      "val_loss: 0.179442963309343\n",
      "Progress: 47.3% ... Training loss: 0.072 ... Validation loss: 0.164iteration: 4726\n",
      "train_loss: 0.07227304681060245\n",
      "val_loss: 0.16455254265286204\n",
      "Progress: 47.3% ... Training loss: 0.071 ... Validation loss: 0.178iteration: 4727\n",
      "train_loss: 0.07171843529859355\n",
      "val_loss: 0.1788416669556179\n",
      "Progress: 47.3% ... Training loss: 0.071 ... Validation loss: 0.166iteration: 4728\n",
      "train_loss: 0.07155194079934914\n",
      "val_loss: 0.16600170499633582\n",
      "Progress: 47.3% ... Training loss: 0.077 ... Validation loss: 0.196iteration: 4729\n",
      "train_loss: 0.0775930350876381\n",
      "val_loss: 0.19690747456307206\n",
      "Progress: 47.3% ... Training loss: 0.075 ... Validation loss: 0.158iteration: 4730\n",
      "train_loss: 0.07553635787263642\n",
      "val_loss: 0.15856965496887707\n",
      "Progress: 47.3% ... Training loss: 0.079 ... Validation loss: 0.210iteration: 4731\n",
      "train_loss: 0.07994016444357499\n",
      "val_loss: 0.21029768114296377\n",
      "Progress: 47.3% ... Training loss: 0.073 ... Validation loss: 0.158iteration: 4732\n",
      "train_loss: 0.0735388376757142\n",
      "val_loss: 0.15878800924990746\n",
      "Progress: 47.3% ... Training loss: 0.071 ... Validation loss: 0.175iteration: 4733\n",
      "train_loss: 0.07108255630704627\n",
      "val_loss: 0.17571173807159512\n",
      "Progress: 47.3% ... Training loss: 0.072 ... Validation loss: 0.173iteration: 4734\n",
      "train_loss: 0.07225658523771165\n",
      "val_loss: 0.1737458581032552\n",
      "Progress: 47.4% ... Training loss: 0.075 ... Validation loss: 0.167iteration: 4735\n",
      "train_loss: 0.07585598785976777\n",
      "val_loss: 0.16701885665092173\n",
      "Progress: 47.4% ... Training loss: 0.077 ... Validation loss: 0.192iteration: 4736\n",
      "train_loss: 0.07741643721754869\n",
      "val_loss: 0.19217001710900677\n",
      "Progress: 47.4% ... Training loss: 0.071 ... Validation loss: 0.158iteration: 4737\n",
      "train_loss: 0.0718315746244269\n",
      "val_loss: 0.1583315943504453\n",
      "Progress: 47.4% ... Training loss: 0.075 ... Validation loss: 0.186iteration: 4738\n",
      "train_loss: 0.07567163977608483\n",
      "val_loss: 0.18610978105471535\n",
      "Progress: 47.4% ... Training loss: 0.081 ... Validation loss: 0.164iteration: 4739\n",
      "train_loss: 0.0818595871798527\n",
      "val_loss: 0.1646535760715689\n",
      "Progress: 47.4% ... Training loss: 0.096 ... Validation loss: 0.240iteration: 4740\n",
      "train_loss: 0.09647433128321568\n",
      "val_loss: 0.240738256865153\n",
      "Progress: 47.4% ... Training loss: 0.080 ... Validation loss: 0.163iteration: 4741\n",
      "train_loss: 0.08096049034313703\n",
      "val_loss: 0.16324961798167495\n",
      "Progress: 47.4% ... Training loss: 0.072 ... Validation loss: 0.183iteration: 4742\n",
      "train_loss: 0.07269545924933184\n",
      "val_loss: 0.18357173615346692\n",
      "Progress: 47.4% ... Training loss: 0.073 ... Validation loss: 0.163iteration: 4743\n",
      "train_loss: 0.0738784428351165\n",
      "val_loss: 0.1634187188449016\n",
      "Progress: 47.4% ... Training loss: 0.071 ... Validation loss: 0.176iteration: 4744\n",
      "train_loss: 0.07153996300831698\n",
      "val_loss: 0.1769886818904795\n",
      "Progress: 47.5% ... Training loss: 0.070 ... Validation loss: 0.171iteration: 4745\n",
      "train_loss: 0.0708824407482598\n",
      "val_loss: 0.1718946245837401\n",
      "Progress: 47.5% ... Training loss: 0.073 ... Validation loss: 0.166iteration: 4746\n",
      "train_loss: 0.07367057259433851\n",
      "val_loss: 0.1662703471625634\n",
      "Progress: 47.5% ... Training loss: 0.075 ... Validation loss: 0.164iteration: 4747\n",
      "train_loss: 0.07573958566277623\n",
      "val_loss: 0.1640491840001\n",
      "Progress: 47.5% ... Training loss: 0.072 ... Validation loss: 0.185iteration: 4748\n",
      "train_loss: 0.07230732345402176\n",
      "val_loss: 0.18521619231387385\n",
      "Progress: 47.5% ... Training loss: 0.072 ... Validation loss: 0.162iteration: 4749\n",
      "train_loss: 0.07223950632474507\n",
      "val_loss: 0.1621198000319766\n",
      "Progress: 47.5% ... Training loss: 0.073 ... Validation loss: 0.195iteration: 4750\n",
      "train_loss: 0.07309413940372786\n",
      "val_loss: 0.1958649807309719\n",
      "Progress: 47.5% ... Training loss: 0.071 ... Validation loss: 0.169iteration: 4751\n",
      "train_loss: 0.0717385577926031\n",
      "val_loss: 0.1694902156814674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 47.5% ... Training loss: 0.071 ... Validation loss: 0.164iteration: 4752\n",
      "train_loss: 0.07195021479875627\n",
      "val_loss: 0.16414121738565496\n",
      "Progress: 47.5% ... Training loss: 0.071 ... Validation loss: 0.174iteration: 4753\n",
      "train_loss: 0.07139643952396187\n",
      "val_loss: 0.17405319490110616\n",
      "Progress: 47.5% ... Training loss: 0.079 ... Validation loss: 0.152iteration: 4754\n",
      "train_loss: 0.07990852635757877\n",
      "val_loss: 0.1526120330289065\n",
      "Progress: 47.5% ... Training loss: 0.073 ... Validation loss: 0.186iteration: 4755\n",
      "train_loss: 0.07370174854017597\n",
      "val_loss: 0.18621784813282666\n",
      "Progress: 47.6% ... Training loss: 0.086 ... Validation loss: 0.154iteration: 4756\n",
      "train_loss: 0.08637363983335891\n",
      "val_loss: 0.15462425502794935\n",
      "Progress: 47.6% ... Training loss: 0.088 ... Validation loss: 0.212iteration: 4757\n",
      "train_loss: 0.088248552505636\n",
      "val_loss: 0.21295807499578664\n",
      "Progress: 47.6% ... Training loss: 0.077 ... Validation loss: 0.160iteration: 4758\n",
      "train_loss: 0.07720572576482758\n",
      "val_loss: 0.16030169886046425\n",
      "Progress: 47.6% ... Training loss: 0.072 ... Validation loss: 0.193iteration: 4759\n",
      "train_loss: 0.07282072847823816\n",
      "val_loss: 0.1938062407349474\n",
      "Progress: 47.6% ... Training loss: 0.077 ... Validation loss: 0.160iteration: 4760\n",
      "train_loss: 0.0773007381959583\n",
      "val_loss: 0.16070663408785177\n",
      "Progress: 47.6% ... Training loss: 0.079 ... Validation loss: 0.201iteration: 4761\n",
      "train_loss: 0.07912637977923301\n",
      "val_loss: 0.20121803886448428\n",
      "Progress: 47.6% ... Training loss: 0.083 ... Validation loss: 0.157iteration: 4762\n",
      "train_loss: 0.08339666436063302\n",
      "val_loss: 0.15763101539844288\n",
      "Progress: 47.6% ... Training loss: 0.101 ... Validation loss: 0.251iteration: 4763\n",
      "train_loss: 0.1016285628734033\n",
      "val_loss: 0.25168704678757287\n",
      "Progress: 47.6% ... Training loss: 0.108 ... Validation loss: 0.161iteration: 4764\n",
      "train_loss: 0.1086263991691979\n",
      "val_loss: 0.16132768163132824\n",
      "Progress: 47.6% ... Training loss: 0.101 ... Validation loss: 0.239iteration: 4765\n",
      "train_loss: 0.10174085648450129\n",
      "val_loss: 0.2390608356785853\n",
      "Progress: 47.7% ... Training loss: 0.087 ... Validation loss: 0.151iteration: 4766\n",
      "train_loss: 0.08710562868616506\n",
      "val_loss: 0.15166812616111838\n",
      "Progress: 47.7% ... Training loss: 0.075 ... Validation loss: 0.199iteration: 4767\n",
      "train_loss: 0.07588463700519021\n",
      "val_loss: 0.19915353820444864\n",
      "Progress: 47.7% ... Training loss: 0.073 ... Validation loss: 0.164iteration: 4768\n",
      "train_loss: 0.07365564534624437\n",
      "val_loss: 0.16488185455210594\n",
      "Progress: 47.7% ... Training loss: 0.078 ... Validation loss: 0.208iteration: 4769\n",
      "train_loss: 0.07843960763350424\n",
      "val_loss: 0.20801725734061932\n",
      "Progress: 47.7% ... Training loss: 0.078 ... Validation loss: 0.161iteration: 4770\n",
      "train_loss: 0.07812960486479537\n",
      "val_loss: 0.1610930781077127\n",
      "Progress: 47.7% ... Training loss: 0.072 ... Validation loss: 0.195iteration: 4771\n",
      "train_loss: 0.07277216936925368\n",
      "val_loss: 0.19566412453895907\n",
      "Progress: 47.7% ... Training loss: 0.070 ... Validation loss: 0.176iteration: 4772\n",
      "train_loss: 0.07069354850086979\n",
      "val_loss: 0.17606071095180828\n",
      "Progress: 47.7% ... Training loss: 0.079 ... Validation loss: 0.159iteration: 4773\n",
      "train_loss: 0.07909712406693252\n",
      "val_loss: 0.1596089921652276\n",
      "Progress: 47.7% ... Training loss: 0.073 ... Validation loss: 0.182iteration: 4774\n",
      "train_loss: 0.07332603600410283\n",
      "val_loss: 0.18210241030331334\n",
      "Progress: 47.8% ... Training loss: 0.076 ... Validation loss: 0.165iteration: 4775\n",
      "train_loss: 0.07660251508384835\n",
      "val_loss: 0.16545542178236486\n",
      "Progress: 47.8% ... Training loss: 0.072 ... Validation loss: 0.190iteration: 4776\n",
      "train_loss: 0.07207806354969476\n",
      "val_loss: 0.19011671978940461\n",
      "Progress: 47.8% ... Training loss: 0.074 ... Validation loss: 0.165iteration: 4777\n",
      "train_loss: 0.07441605030442329\n",
      "val_loss: 0.16500233519410173\n",
      "Progress: 47.8% ... Training loss: 0.075 ... Validation loss: 0.208iteration: 4778\n",
      "train_loss: 0.07532738537352587\n",
      "val_loss: 0.20856819844217178\n",
      "Progress: 47.8% ... Training loss: 0.073 ... Validation loss: 0.169iteration: 4779\n",
      "train_loss: 0.07344545485739226\n",
      "val_loss: 0.16952695509832588\n",
      "Progress: 47.8% ... Training loss: 0.073 ... Validation loss: 0.192iteration: 4780\n",
      "train_loss: 0.0736407042423905\n",
      "val_loss: 0.19252722064592248\n",
      "Progress: 47.8% ... Training loss: 0.071 ... Validation loss: 0.174iteration: 4781\n",
      "train_loss: 0.07125179895625629\n",
      "val_loss: 0.17477730705921077\n",
      "Progress: 47.8% ... Training loss: 0.072 ... Validation loss: 0.165iteration: 4782\n",
      "train_loss: 0.07262680235048485\n",
      "val_loss: 0.1654675663132098\n",
      "Progress: 47.8% ... Training loss: 0.071 ... Validation loss: 0.175iteration: 4783\n",
      "train_loss: 0.07152322643974393\n",
      "val_loss: 0.175104116523575\n",
      "Progress: 47.8% ... Training loss: 0.074 ... Validation loss: 0.176iteration: 4784\n",
      "train_loss: 0.07492825139292975\n",
      "val_loss: 0.17620925473058932\n",
      "Progress: 47.9% ... Training loss: 0.071 ... Validation loss: 0.194iteration: 4785\n",
      "train_loss: 0.07171384277363015\n",
      "val_loss: 0.19444895264874976\n",
      "Progress: 47.9% ... Training loss: 0.071 ... Validation loss: 0.201iteration: 4786\n",
      "train_loss: 0.07169091088971968\n",
      "val_loss: 0.20127339656461143\n",
      "Progress: 47.9% ... Training loss: 0.071 ... Validation loss: 0.177iteration: 4787\n",
      "train_loss: 0.07102302702067723\n",
      "val_loss: 0.17722041017940615\n",
      "Progress: 47.9% ... Training loss: 0.070 ... Validation loss: 0.173iteration: 4788\n",
      "train_loss: 0.07043379345158435\n",
      "val_loss: 0.17355940242057813\n",
      "Progress: 47.9% ... Training loss: 0.071 ... Validation loss: 0.168iteration: 4789\n",
      "train_loss: 0.07171793890550111\n",
      "val_loss: 0.16806559822309922\n",
      "Progress: 47.9% ... Training loss: 0.072 ... Validation loss: 0.192iteration: 4790\n",
      "train_loss: 0.07285906855744144\n",
      "val_loss: 0.19261878957643816\n",
      "Progress: 47.9% ... Training loss: 0.073 ... Validation loss: 0.170iteration: 4791\n",
      "train_loss: 0.07350422971521929\n",
      "val_loss: 0.17082627005651882\n",
      "Progress: 47.9% ... Training loss: 0.072 ... Validation loss: 0.194iteration: 4792\n",
      "train_loss: 0.07241127586253172\n",
      "val_loss: 0.19426156974079573\n",
      "Progress: 47.9% ... Training loss: 0.072 ... Validation loss: 0.167iteration: 4793\n",
      "train_loss: 0.07258531922568874\n",
      "val_loss: 0.16772560707165954\n",
      "Progress: 47.9% ... Training loss: 0.071 ... Validation loss: 0.179iteration: 4794\n",
      "train_loss: 0.07116423071449492\n",
      "val_loss: 0.17990491758965227\n",
      "Progress: 48.0% ... Training loss: 0.074 ... Validation loss: 0.197iteration: 4795\n",
      "train_loss: 0.07471203982738248\n",
      "val_loss: 0.19765223016209876\n",
      "Progress: 48.0% ... Training loss: 0.074 ... Validation loss: 0.165iteration: 4796\n",
      "train_loss: 0.07439358492206288\n",
      "val_loss: 0.16576306502935526\n",
      "Progress: 48.0% ... Training loss: 0.071 ... Validation loss: 0.168iteration: 4797\n",
      "train_loss: 0.07112134701608239\n",
      "val_loss: 0.16887187618334856\n",
      "Progress: 48.0% ... Training loss: 0.076 ... Validation loss: 0.164iteration: 4798\n",
      "train_loss: 0.07606654571352482\n",
      "val_loss: 0.16404677172396367\n",
      "Progress: 48.0% ... Training loss: 0.078 ... Validation loss: 0.208iteration: 4799\n",
      "train_loss: 0.07820096635750504\n",
      "val_loss: 0.208277428700512\n",
      "Progress: 48.0% ... Training loss: 0.071 ... Validation loss: 0.170iteration: 4800\n",
      "train_loss: 0.07166081815706069\n",
      "val_loss: 0.17024093804545534\n",
      "Progress: 48.0% ... Training loss: 0.070 ... Validation loss: 0.180iteration: 4801\n",
      "train_loss: 0.07075635976444569\n",
      "val_loss: 0.18012958786395453\n",
      "Progress: 48.0% ... Training loss: 0.082 ... Validation loss: 0.162iteration: 4802\n",
      "train_loss: 0.08270482496524968\n",
      "val_loss: 0.16229988035790724\n",
      "Progress: 48.0% ... Training loss: 0.070 ... Validation loss: 0.191iteration: 4803\n",
      "train_loss: 0.0709602077547698\n",
      "val_loss: 0.19133112313598014\n",
      "Progress: 48.0% ... Training loss: 0.072 ... Validation loss: 0.172iteration: 4804\n",
      "train_loss: 0.0720951887067643\n",
      "val_loss: 0.17208861167688327\n",
      "Progress: 48.0% ... Training loss: 0.070 ... Validation loss: 0.181iteration: 4805\n",
      "train_loss: 0.07062835595243606\n",
      "val_loss: 0.18135680191173514\n",
      "Progress: 48.1% ... Training loss: 0.070 ... Validation loss: 0.184iteration: 4806\n",
      "train_loss: 0.07046543647367266\n",
      "val_loss: 0.18437490759125189\n",
      "Progress: 48.1% ... Training loss: 0.071 ... Validation loss: 0.195iteration: 4807\n",
      "train_loss: 0.07162261041958479\n",
      "val_loss: 0.19538414691302528\n",
      "Progress: 48.1% ... Training loss: 0.071 ... Validation loss: 0.172iteration: 4808\n",
      "train_loss: 0.07140095387722604\n",
      "val_loss: 0.17283286274566084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 48.1% ... Training loss: 0.074 ... Validation loss: 0.184iteration: 4809\n",
      "train_loss: 0.07414307415853875\n",
      "val_loss: 0.18495143176370304\n",
      "Progress: 48.1% ... Training loss: 0.071 ... Validation loss: 0.169iteration: 4810\n",
      "train_loss: 0.07156698097404947\n",
      "val_loss: 0.16980840608679776\n",
      "Progress: 48.1% ... Training loss: 0.071 ... Validation loss: 0.183iteration: 4811\n",
      "train_loss: 0.0715818794544752\n",
      "val_loss: 0.18329525577653338\n",
      "Progress: 48.1% ... Training loss: 0.079 ... Validation loss: 0.162iteration: 4812\n",
      "train_loss: 0.07998832886956019\n",
      "val_loss: 0.16262443057466527\n",
      "Progress: 48.1% ... Training loss: 0.078 ... Validation loss: 0.210iteration: 4813\n",
      "train_loss: 0.07813735459244754\n",
      "val_loss: 0.21001111671468436\n",
      "Progress: 48.1% ... Training loss: 0.073 ... Validation loss: 0.193iteration: 4814\n",
      "train_loss: 0.07304109732637981\n",
      "val_loss: 0.19351646073565074\n",
      "Progress: 48.1% ... Training loss: 0.071 ... Validation loss: 0.185iteration: 4815\n",
      "train_loss: 0.0717927977717765\n",
      "val_loss: 0.18590621820400682\n",
      "Progress: 48.2% ... Training loss: 0.071 ... Validation loss: 0.189iteration: 4816\n",
      "train_loss: 0.07143690406343368\n",
      "val_loss: 0.18971237455212123\n",
      "Progress: 48.2% ... Training loss: 0.076 ... Validation loss: 0.164iteration: 4817\n",
      "train_loss: 0.07687147697399697\n",
      "val_loss: 0.16450302312304158\n",
      "Progress: 48.2% ... Training loss: 0.073 ... Validation loss: 0.186iteration: 4818\n",
      "train_loss: 0.07367726793886653\n",
      "val_loss: 0.18687676354556954\n",
      "Progress: 48.2% ... Training loss: 0.077 ... Validation loss: 0.161iteration: 4819\n",
      "train_loss: 0.07771283605623762\n",
      "val_loss: 0.16162009628237578\n",
      "Progress: 48.2% ... Training loss: 0.094 ... Validation loss: 0.254iteration: 4820\n",
      "train_loss: 0.09442082308957842\n",
      "val_loss: 0.25468411243690203\n",
      "Progress: 48.2% ... Training loss: 0.112 ... Validation loss: 0.162iteration: 4821\n",
      "train_loss: 0.1127931974741955\n",
      "val_loss: 0.16253431896681406\n",
      "Progress: 48.2% ... Training loss: 0.176 ... Validation loss: 0.329iteration: 4822\n",
      "train_loss: 0.17665743687833235\n",
      "val_loss: 0.32931974194104946\n",
      "Progress: 48.2% ... Training loss: 0.128 ... Validation loss: 0.167iteration: 4823\n",
      "train_loss: 0.12846209392953614\n",
      "val_loss: 0.1678036818137816\n",
      "Progress: 48.2% ... Training loss: 0.130 ... Validation loss: 0.271iteration: 4824\n",
      "train_loss: 0.13050449224109908\n",
      "val_loss: 0.27172352428167673\n",
      "Progress: 48.2% ... Training loss: 0.087 ... Validation loss: 0.162iteration: 4825\n",
      "train_loss: 0.08708678832210578\n",
      "val_loss: 0.16228389969997967\n",
      "Progress: 48.3% ... Training loss: 0.075 ... Validation loss: 0.195iteration: 4826\n",
      "train_loss: 0.0751487414429523\n",
      "val_loss: 0.19589441224995272\n",
      "Progress: 48.3% ... Training loss: 0.076 ... Validation loss: 0.162iteration: 4827\n",
      "train_loss: 0.07602619959122174\n",
      "val_loss: 0.16269986453798135\n",
      "Progress: 48.3% ... Training loss: 0.070 ... Validation loss: 0.167iteration: 4828\n",
      "train_loss: 0.0706393938492635\n",
      "val_loss: 0.16731152240433375\n",
      "Progress: 48.3% ... Training loss: 0.071 ... Validation loss: 0.173iteration: 4829\n",
      "train_loss: 0.0713560335519416\n",
      "val_loss: 0.1739067506785419\n",
      "Progress: 48.3% ... Training loss: 0.072 ... Validation loss: 0.181iteration: 4830\n",
      "train_loss: 0.07258165505475847\n",
      "val_loss: 0.18102104974026506\n",
      "Progress: 48.3% ... Training loss: 0.077 ... Validation loss: 0.153iteration: 4831\n",
      "train_loss: 0.07777480368968817\n",
      "val_loss: 0.1531856946439762\n",
      "Progress: 48.3% ... Training loss: 0.073 ... Validation loss: 0.183iteration: 4832\n",
      "train_loss: 0.07383401461612482\n",
      "val_loss: 0.1831598542173213\n",
      "Progress: 48.3% ... Training loss: 0.071 ... Validation loss: 0.160iteration: 4833\n",
      "train_loss: 0.0717841718610821\n",
      "val_loss: 0.16017292806023292\n",
      "Progress: 48.3% ... Training loss: 0.074 ... Validation loss: 0.184iteration: 4834\n",
      "train_loss: 0.07492151105606874\n",
      "val_loss: 0.1845536866146747\n",
      "Progress: 48.4% ... Training loss: 0.072 ... Validation loss: 0.161iteration: 4835\n",
      "train_loss: 0.07277988538125062\n",
      "val_loss: 0.1616360826620423\n",
      "Progress: 48.4% ... Training loss: 0.072 ... Validation loss: 0.183iteration: 4836\n",
      "train_loss: 0.07265331108116185\n",
      "val_loss: 0.18300987300042199\n",
      "Progress: 48.4% ... Training loss: 0.070 ... Validation loss: 0.166iteration: 4837\n",
      "train_loss: 0.07089486110149511\n",
      "val_loss: 0.16664123789293203\n",
      "Progress: 48.4% ... Training loss: 0.070 ... Validation loss: 0.165iteration: 4838\n",
      "train_loss: 0.07041593466307111\n",
      "val_loss: 0.16520158585700587\n",
      "Progress: 48.4% ... Training loss: 0.074 ... Validation loss: 0.188iteration: 4839\n",
      "train_loss: 0.07481577173258529\n",
      "val_loss: 0.18876775477093674\n",
      "Progress: 48.4% ... Training loss: 0.081 ... Validation loss: 0.156iteration: 4840\n",
      "train_loss: 0.08186866089804014\n",
      "val_loss: 0.1565996262016219\n",
      "Progress: 48.4% ... Training loss: 0.097 ... Validation loss: 0.225iteration: 4841\n",
      "train_loss: 0.09789717269514085\n",
      "val_loss: 0.22577397109788314\n",
      "Progress: 48.4% ... Training loss: 0.092 ... Validation loss: 0.158iteration: 4842\n",
      "train_loss: 0.0923389847408115\n",
      "val_loss: 0.15865179557093798\n",
      "Progress: 48.4% ... Training loss: 0.096 ... Validation loss: 0.231iteration: 4843\n",
      "train_loss: 0.09698788199373423\n",
      "val_loss: 0.23167919972086615\n",
      "Progress: 48.4% ... Training loss: 0.083 ... Validation loss: 0.153iteration: 4844\n",
      "train_loss: 0.08357984376732915\n",
      "val_loss: 0.15320025217636155\n",
      "Progress: 48.5% ... Training loss: 0.070 ... Validation loss: 0.177iteration: 4845\n",
      "train_loss: 0.07061384830966429\n",
      "val_loss: 0.17791583833359081\n",
      "Progress: 48.5% ... Training loss: 0.071 ... Validation loss: 0.180iteration: 4846\n",
      "train_loss: 0.07165269719534942\n",
      "val_loss: 0.1802712001199444\n",
      "Progress: 48.5% ... Training loss: 0.077 ... Validation loss: 0.161iteration: 4847\n",
      "train_loss: 0.07757316124917363\n",
      "val_loss: 0.16116162715934132\n",
      "Progress: 48.5% ... Training loss: 0.077 ... Validation loss: 0.204iteration: 4848\n",
      "train_loss: 0.07790858205028736\n",
      "val_loss: 0.20430218804450873\n",
      "Progress: 48.5% ... Training loss: 0.077 ... Validation loss: 0.163iteration: 4849\n",
      "train_loss: 0.0775681291085119\n",
      "val_loss: 0.16329307089245937\n",
      "Progress: 48.5% ... Training loss: 0.087 ... Validation loss: 0.237iteration: 4850\n",
      "train_loss: 0.08721813544593715\n",
      "val_loss: 0.23773438305524977\n",
      "Progress: 48.5% ... Training loss: 0.081 ... Validation loss: 0.160iteration: 4851\n",
      "train_loss: 0.08113674883713874\n",
      "val_loss: 0.16047649760617738\n",
      "Progress: 48.5% ... Training loss: 0.071 ... Validation loss: 0.186iteration: 4852\n",
      "train_loss: 0.07162192035467357\n",
      "val_loss: 0.18671116802623758\n",
      "Progress: 48.5% ... Training loss: 0.071 ... Validation loss: 0.165iteration: 4853\n",
      "train_loss: 0.071480229018061\n",
      "val_loss: 0.16588000141632075\n",
      "Progress: 48.5% ... Training loss: 0.071 ... Validation loss: 0.168iteration: 4854\n",
      "train_loss: 0.07111772545190197\n",
      "val_loss: 0.1682617983921067\n",
      "Progress: 48.5% ... Training loss: 0.085 ... Validation loss: 0.167iteration: 4855\n",
      "train_loss: 0.08590296516224638\n",
      "val_loss: 0.1676027340223532\n",
      "Progress: 48.6% ... Training loss: 0.084 ... Validation loss: 0.237iteration: 4856\n",
      "train_loss: 0.08417927415496718\n",
      "val_loss: 0.23713471264908104\n",
      "Progress: 48.6% ... Training loss: 0.071 ... Validation loss: 0.170iteration: 4857\n",
      "train_loss: 0.07161522834830329\n",
      "val_loss: 0.17021597741907568\n",
      "Progress: 48.6% ... Training loss: 0.087 ... Validation loss: 0.219iteration: 4858\n",
      "train_loss: 0.08720251007705312\n",
      "val_loss: 0.219239219226087\n",
      "Progress: 48.6% ... Training loss: 0.076 ... Validation loss: 0.159iteration: 4859\n",
      "train_loss: 0.07682478880504637\n",
      "val_loss: 0.15942392016661244\n",
      "Progress: 48.6% ... Training loss: 0.077 ... Validation loss: 0.207iteration: 4860\n",
      "train_loss: 0.07778184144834573\n",
      "val_loss: 0.20770840858523265\n",
      "Progress: 48.6% ... Training loss: 0.075 ... Validation loss: 0.164iteration: 4861\n",
      "train_loss: 0.07552664011423761\n",
      "val_loss: 0.16414697218010849\n",
      "Progress: 48.6% ... Training loss: 0.080 ... Validation loss: 0.209iteration: 4862\n",
      "train_loss: 0.08092424790677656\n",
      "val_loss: 0.20957180672587286\n",
      "Progress: 48.6% ... Training loss: 0.078 ... Validation loss: 0.153iteration: 4863\n",
      "train_loss: 0.07826517188945992\n",
      "val_loss: 0.15326758683540403\n",
      "Progress: 48.6% ... Training loss: 0.073 ... Validation loss: 0.196iteration: 4864\n",
      "train_loss: 0.07385645134481308\n",
      "val_loss: 0.19636422411913085\n",
      "Progress: 48.6% ... Training loss: 0.070 ... Validation loss: 0.164iteration: 4865\n",
      "train_loss: 0.0703187102909311\n",
      "val_loss: 0.1644312718951219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 48.7% ... Training loss: 0.072 ... Validation loss: 0.174iteration: 4866\n",
      "train_loss: 0.07215222676396389\n",
      "val_loss: 0.1740793462927289\n",
      "Progress: 48.7% ... Training loss: 0.075 ... Validation loss: 0.158iteration: 4867\n",
      "train_loss: 0.07567144909656083\n",
      "val_loss: 0.1582138694396063\n",
      "Progress: 48.7% ... Training loss: 0.074 ... Validation loss: 0.190iteration: 4868\n",
      "train_loss: 0.07433848717894931\n",
      "val_loss: 0.19079826391023696\n",
      "Progress: 48.7% ... Training loss: 0.085 ... Validation loss: 0.153iteration: 4869\n",
      "train_loss: 0.08572860662582323\n",
      "val_loss: 0.1533300728512776\n",
      "Progress: 48.7% ... Training loss: 0.080 ... Validation loss: 0.216iteration: 4870\n",
      "train_loss: 0.08030071798374608\n",
      "val_loss: 0.21666820726790664\n",
      "Progress: 48.7% ... Training loss: 0.078 ... Validation loss: 0.167iteration: 4871\n",
      "train_loss: 0.07851205698737095\n",
      "val_loss: 0.16734961695013553\n",
      "Progress: 48.7% ... Training loss: 0.074 ... Validation loss: 0.196iteration: 4872\n",
      "train_loss: 0.07416332026791418\n",
      "val_loss: 0.1968107029986527\n",
      "Progress: 48.7% ... Training loss: 0.071 ... Validation loss: 0.162iteration: 4873\n",
      "train_loss: 0.07191140549845054\n",
      "val_loss: 0.1628198893422321\n",
      "Progress: 48.7% ... Training loss: 0.070 ... Validation loss: 0.174iteration: 4874\n",
      "train_loss: 0.0707529183172947\n",
      "val_loss: 0.17492708931886927\n",
      "Progress: 48.8% ... Training loss: 0.070 ... Validation loss: 0.159iteration: 4875\n",
      "train_loss: 0.07025954129157656\n",
      "val_loss: 0.15965040223360824\n",
      "Progress: 48.8% ... Training loss: 0.072 ... Validation loss: 0.165iteration: 4876\n",
      "train_loss: 0.07262423723787839\n",
      "val_loss: 0.16504037216011844\n",
      "Progress: 48.8% ... Training loss: 0.070 ... Validation loss: 0.171iteration: 4877\n",
      "train_loss: 0.07014943489721544\n",
      "val_loss: 0.17128210683478576\n",
      "Progress: 48.8% ... Training loss: 0.071 ... Validation loss: 0.168iteration: 4878\n",
      "train_loss: 0.0719789232654194\n",
      "val_loss: 0.16898617443501487\n",
      "Progress: 48.8% ... Training loss: 0.071 ... Validation loss: 0.156iteration: 4879\n",
      "train_loss: 0.07168697102608719\n",
      "val_loss: 0.1567261105484243\n",
      "Progress: 48.8% ... Training loss: 0.070 ... Validation loss: 0.175iteration: 4880\n",
      "train_loss: 0.0700024644541333\n",
      "val_loss: 0.1756652613910877\n",
      "Progress: 48.8% ... Training loss: 0.069 ... Validation loss: 0.177iteration: 4881\n",
      "train_loss: 0.06981991195931483\n",
      "val_loss: 0.17794717517507214\n",
      "Progress: 48.8% ... Training loss: 0.076 ... Validation loss: 0.188iteration: 4882\n",
      "train_loss: 0.07645196005138968\n",
      "val_loss: 0.18883578825317232\n",
      "Progress: 48.8% ... Training loss: 0.079 ... Validation loss: 0.155iteration: 4883\n",
      "train_loss: 0.0790840457444513\n",
      "val_loss: 0.1555871618618389\n",
      "Progress: 48.8% ... Training loss: 0.069 ... Validation loss: 0.170iteration: 4884\n",
      "train_loss: 0.06953082328346594\n",
      "val_loss: 0.17009382104649584\n",
      "Progress: 48.9% ... Training loss: 0.069 ... Validation loss: 0.170iteration: 4885\n",
      "train_loss: 0.06986048356531215\n",
      "val_loss: 0.17065315665552733\n",
      "Progress: 48.9% ... Training loss: 0.071 ... Validation loss: 0.166iteration: 4886\n",
      "train_loss: 0.07145276229794668\n",
      "val_loss: 0.16641381042737705\n",
      "Progress: 48.9% ... Training loss: 0.074 ... Validation loss: 0.195iteration: 4887\n",
      "train_loss: 0.0742998016261131\n",
      "val_loss: 0.19507589955577898\n",
      "Progress: 48.9% ... Training loss: 0.077 ... Validation loss: 0.154iteration: 4888\n",
      "train_loss: 0.07798980706457166\n",
      "val_loss: 0.15426919938672407\n",
      "Progress: 48.9% ... Training loss: 0.086 ... Validation loss: 0.205iteration: 4889\n",
      "train_loss: 0.08687049998999027\n",
      "val_loss: 0.20501853778547227\n",
      "Progress: 48.9% ... Training loss: 0.090 ... Validation loss: 0.156iteration: 4890\n",
      "train_loss: 0.09090666061195937\n",
      "val_loss: 0.1567257724071776\n",
      "Progress: 48.9% ... Training loss: 0.115 ... Validation loss: 0.238iteration: 4891\n",
      "train_loss: 0.11526654544577693\n",
      "val_loss: 0.23826842903903175\n",
      "Progress: 48.9% ... Training loss: 0.080 ... Validation loss: 0.158iteration: 4892\n",
      "train_loss: 0.0804820596762898\n",
      "val_loss: 0.1587957409525105\n",
      "Progress: 48.9% ... Training loss: 0.070 ... Validation loss: 0.166iteration: 4893\n",
      "train_loss: 0.07071725118876547\n",
      "val_loss: 0.1669364636057996\n",
      "Progress: 48.9% ... Training loss: 0.071 ... Validation loss: 0.182iteration: 4894\n",
      "train_loss: 0.07148394199361063\n",
      "val_loss: 0.1829848672407968\n",
      "Progress: 49.0% ... Training loss: 0.078 ... Validation loss: 0.205iteration: 4895\n",
      "train_loss: 0.0783061948102467\n",
      "val_loss: 0.20558439340538118\n",
      "Progress: 49.0% ... Training loss: 0.073 ... Validation loss: 0.163iteration: 4896\n",
      "train_loss: 0.07387878033217239\n",
      "val_loss: 0.16340417508661392\n",
      "Progress: 49.0% ... Training loss: 0.072 ... Validation loss: 0.189iteration: 4897\n",
      "train_loss: 0.0724532196577683\n",
      "val_loss: 0.18981082409421307\n",
      "Progress: 49.0% ... Training loss: 0.072 ... Validation loss: 0.167iteration: 4898\n",
      "train_loss: 0.07251024317529833\n",
      "val_loss: 0.16712004031347788\n",
      "Progress: 49.0% ... Training loss: 0.074 ... Validation loss: 0.182iteration: 4899\n",
      "train_loss: 0.07461506962734121\n",
      "val_loss: 0.18214539681472475\n",
      "Progress: 49.0% ... Training loss: 0.071 ... Validation loss: 0.154iteration: 4900\n",
      "train_loss: 0.0717856014734961\n",
      "val_loss: 0.1549072230206728\n",
      "Progress: 49.0% ... Training loss: 0.077 ... Validation loss: 0.186iteration: 4901\n",
      "train_loss: 0.07716893905665038\n",
      "val_loss: 0.18660844809758556\n",
      "Progress: 49.0% ... Training loss: 0.073 ... Validation loss: 0.163iteration: 4902\n",
      "train_loss: 0.07344317222290495\n",
      "val_loss: 0.16335534334186794\n",
      "Progress: 49.0% ... Training loss: 0.081 ... Validation loss: 0.206iteration: 4903\n",
      "train_loss: 0.08132009983023915\n",
      "val_loss: 0.2060328216267896\n",
      "Progress: 49.0% ... Training loss: 0.079 ... Validation loss: 0.158iteration: 4904\n",
      "train_loss: 0.07908399910165143\n",
      "val_loss: 0.15861029050465145\n",
      "Progress: 49.0% ... Training loss: 0.081 ... Validation loss: 0.217iteration: 4905\n",
      "train_loss: 0.08172697494797687\n",
      "val_loss: 0.2171755308906589\n",
      "Progress: 49.1% ... Training loss: 0.081 ... Validation loss: 0.152iteration: 4906\n",
      "train_loss: 0.08195317350260313\n",
      "val_loss: 0.1529364106907111\n",
      "Progress: 49.1% ... Training loss: 0.079 ... Validation loss: 0.202iteration: 4907\n",
      "train_loss: 0.07967759071176098\n",
      "val_loss: 0.20279150671295224\n",
      "Progress: 49.1% ... Training loss: 0.086 ... Validation loss: 0.157iteration: 4908\n",
      "train_loss: 0.08688608450034424\n",
      "val_loss: 0.15700667351235892\n",
      "Progress: 49.1% ... Training loss: 0.075 ... Validation loss: 0.204iteration: 4909\n",
      "train_loss: 0.07540861910097638\n",
      "val_loss: 0.20470212665210086\n",
      "Progress: 49.1% ... Training loss: 0.070 ... Validation loss: 0.167iteration: 4910\n",
      "train_loss: 0.07075987813283373\n",
      "val_loss: 0.16761995978477423\n",
      "Progress: 49.1% ... Training loss: 0.069 ... Validation loss: 0.172iteration: 4911\n",
      "train_loss: 0.0694161205382014\n",
      "val_loss: 0.17290505932890882\n",
      "Progress: 49.1% ... Training loss: 0.070 ... Validation loss: 0.170iteration: 4912\n",
      "train_loss: 0.07007982787299978\n",
      "val_loss: 0.17066091595505556\n",
      "Progress: 49.1% ... Training loss: 0.069 ... Validation loss: 0.164iteration: 4913\n",
      "train_loss: 0.06998186278823797\n",
      "val_loss: 0.16496599222106473\n",
      "Progress: 49.1% ... Training loss: 0.072 ... Validation loss: 0.185iteration: 4914\n",
      "train_loss: 0.07203036443823808\n",
      "val_loss: 0.18515915231103286\n",
      "Progress: 49.1% ... Training loss: 0.074 ... Validation loss: 0.168iteration: 4915\n",
      "train_loss: 0.07488324475064181\n",
      "val_loss: 0.16820213548341081\n",
      "Progress: 49.2% ... Training loss: 0.073 ... Validation loss: 0.185iteration: 4916\n",
      "train_loss: 0.07340376226829864\n",
      "val_loss: 0.18581054534854424\n",
      "Progress: 49.2% ... Training loss: 0.071 ... Validation loss: 0.158iteration: 4917\n",
      "train_loss: 0.07138328915424993\n",
      "val_loss: 0.15894809439209054\n",
      "Progress: 49.2% ... Training loss: 0.074 ... Validation loss: 0.176iteration: 4918\n",
      "train_loss: 0.07492849067433487\n",
      "val_loss: 0.1761454955306768\n",
      "Progress: 49.2% ... Training loss: 0.083 ... Validation loss: 0.156iteration: 4919\n",
      "train_loss: 0.08301689052198753\n",
      "val_loss: 0.15632347770070523\n",
      "Progress: 49.2% ... Training loss: 0.092 ... Validation loss: 0.217iteration: 4920\n",
      "train_loss: 0.09250436972953302\n",
      "val_loss: 0.21782720256731286\n",
      "Progress: 49.2% ... Training loss: 0.089 ... Validation loss: 0.156iteration: 4921\n",
      "train_loss: 0.08986756830756065\n",
      "val_loss: 0.1565032967881195\n",
      "Progress: 49.2% ... Training loss: 0.121 ... Validation loss: 0.261iteration: 4922\n",
      "train_loss: 0.12153598361418227\n",
      "val_loss: 0.26171040009082924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 49.2% ... Training loss: 0.100 ... Validation loss: 0.155iteration: 4923\n",
      "train_loss: 0.10011777478852132\n",
      "val_loss: 0.15561722970492775\n",
      "Progress: 49.2% ... Training loss: 0.081 ... Validation loss: 0.215iteration: 4924\n",
      "train_loss: 0.08110215719045917\n",
      "val_loss: 0.21595466824268275\n",
      "Progress: 49.2% ... Training loss: 0.085 ... Validation loss: 0.158iteration: 4925\n",
      "train_loss: 0.08523317359527184\n",
      "val_loss: 0.1584169527061987\n",
      "Progress: 49.3% ... Training loss: 0.082 ... Validation loss: 0.209iteration: 4926\n",
      "train_loss: 0.08256348605300953\n",
      "val_loss: 0.20956497746412994\n",
      "Progress: 49.3% ... Training loss: 0.075 ... Validation loss: 0.156iteration: 4927\n",
      "train_loss: 0.07550382153125935\n",
      "val_loss: 0.15600849242485457\n",
      "Progress: 49.3% ... Training loss: 0.070 ... Validation loss: 0.173iteration: 4928\n",
      "train_loss: 0.07069970199324524\n",
      "val_loss: 0.1734392462287208\n",
      "Progress: 49.3% ... Training loss: 0.072 ... Validation loss: 0.159iteration: 4929\n",
      "train_loss: 0.07237529727585998\n",
      "val_loss: 0.15900307359611845\n",
      "Progress: 49.3% ... Training loss: 0.071 ... Validation loss: 0.179iteration: 4930\n",
      "train_loss: 0.07124924524681367\n",
      "val_loss: 0.1790335644772072\n",
      "Progress: 49.3% ... Training loss: 0.069 ... Validation loss: 0.166iteration: 4931\n",
      "train_loss: 0.06957127718200845\n",
      "val_loss: 0.1666263371029016\n",
      "Progress: 49.3% ... Training loss: 0.069 ... Validation loss: 0.165iteration: 4932\n",
      "train_loss: 0.06949283815346168\n",
      "val_loss: 0.16586048855663693\n",
      "Progress: 49.3% ... Training loss: 0.069 ... Validation loss: 0.166iteration: 4933\n",
      "train_loss: 0.06982672001735675\n",
      "val_loss: 0.16667054116516275\n",
      "Progress: 49.3% ... Training loss: 0.070 ... Validation loss: 0.180iteration: 4934\n",
      "train_loss: 0.07028639302152472\n",
      "val_loss: 0.18017064928096863\n",
      "Progress: 49.4% ... Training loss: 0.074 ... Validation loss: 0.160iteration: 4935\n",
      "train_loss: 0.07430855563547684\n",
      "val_loss: 0.160488113615938\n",
      "Progress: 49.4% ... Training loss: 0.087 ... Validation loss: 0.206iteration: 4936\n",
      "train_loss: 0.08750051602874055\n",
      "val_loss: 0.20673917315469525\n",
      "Progress: 49.4% ... Training loss: 0.089 ... Validation loss: 0.155iteration: 4937\n",
      "train_loss: 0.089192921753505\n",
      "val_loss: 0.15502720530307346\n",
      "Progress: 49.4% ... Training loss: 0.092 ... Validation loss: 0.246iteration: 4938\n",
      "train_loss: 0.09279239340704856\n",
      "val_loss: 0.2463438550874203\n",
      "Progress: 49.4% ... Training loss: 0.092 ... Validation loss: 0.154iteration: 4939\n",
      "train_loss: 0.09290630010827618\n",
      "val_loss: 0.1542307160251401\n",
      "Progress: 49.4% ... Training loss: 0.130 ... Validation loss: 0.286iteration: 4940\n",
      "train_loss: 0.13016951066096563\n",
      "val_loss: 0.2861629615388009\n",
      "Progress: 49.4% ... Training loss: 0.090 ... Validation loss: 0.156iteration: 4941\n",
      "train_loss: 0.09083411872921003\n",
      "val_loss: 0.15629644852469096\n",
      "Progress: 49.4% ... Training loss: 0.080 ... Validation loss: 0.205iteration: 4942\n",
      "train_loss: 0.08031498261012597\n",
      "val_loss: 0.205900390608557\n",
      "Progress: 49.4% ... Training loss: 0.078 ... Validation loss: 0.155iteration: 4943\n",
      "train_loss: 0.07802347113480451\n",
      "val_loss: 0.155888311857495\n",
      "Progress: 49.4% ... Training loss: 0.079 ... Validation loss: 0.203iteration: 4944\n",
      "train_loss: 0.07942468381527981\n",
      "val_loss: 0.20342450177968796\n",
      "Progress: 49.5% ... Training loss: 0.071 ... Validation loss: 0.156iteration: 4945\n",
      "train_loss: 0.07179410502933616\n",
      "val_loss: 0.1569802285773968\n",
      "Progress: 49.5% ... Training loss: 0.070 ... Validation loss: 0.162iteration: 4946\n",
      "train_loss: 0.07085291934146741\n",
      "val_loss: 0.16272474861360184\n",
      "Progress: 49.5% ... Training loss: 0.072 ... Validation loss: 0.162iteration: 4947\n",
      "train_loss: 0.07248276750186597\n",
      "val_loss: 0.16247378038268442\n",
      "Progress: 49.5% ... Training loss: 0.071 ... Validation loss: 0.174iteration: 4948\n",
      "train_loss: 0.07157975793289305\n",
      "val_loss: 0.17463006504217657\n",
      "Progress: 49.5% ... Training loss: 0.071 ... Validation loss: 0.171iteration: 4949\n",
      "train_loss: 0.07130633434523238\n",
      "val_loss: 0.17112306584079862\n",
      "Progress: 49.5% ... Training loss: 0.069 ... Validation loss: 0.171iteration: 4950\n",
      "train_loss: 0.0695786362953452\n",
      "val_loss: 0.17124216258146727\n",
      "Progress: 49.5% ... Training loss: 0.071 ... Validation loss: 0.160iteration: 4951\n",
      "train_loss: 0.07178713874853421\n",
      "val_loss: 0.16027025685234336\n",
      "Progress: 49.5% ... Training loss: 0.070 ... Validation loss: 0.180iteration: 4952\n",
      "train_loss: 0.07087613938484158\n",
      "val_loss: 0.1806131667862961\n",
      "Progress: 49.5% ... Training loss: 0.070 ... Validation loss: 0.162iteration: 4953\n",
      "train_loss: 0.07074373064969139\n",
      "val_loss: 0.1628737498313538\n",
      "Progress: 49.5% ... Training loss: 0.073 ... Validation loss: 0.192iteration: 4954\n",
      "train_loss: 0.07364303896545811\n",
      "val_loss: 0.1922694669094394\n",
      "Progress: 49.5% ... Training loss: 0.074 ... Validation loss: 0.155iteration: 4955\n",
      "train_loss: 0.07488426666892027\n",
      "val_loss: 0.15531303371333308\n",
      "Progress: 49.6% ... Training loss: 0.078 ... Validation loss: 0.205iteration: 4956\n",
      "train_loss: 0.0784913255629136\n",
      "val_loss: 0.20530068059050105\n",
      "Progress: 49.6% ... Training loss: 0.084 ... Validation loss: 0.156iteration: 4957\n",
      "train_loss: 0.08443620297187253\n",
      "val_loss: 0.15698488745636557\n",
      "Progress: 49.6% ... Training loss: 0.092 ... Validation loss: 0.216iteration: 4958\n",
      "train_loss: 0.09258781725636536\n",
      "val_loss: 0.2168667618669862\n",
      "Progress: 49.6% ... Training loss: 0.080 ... Validation loss: 0.152iteration: 4959\n",
      "train_loss: 0.08016405271296244\n",
      "val_loss: 0.152696974783097\n",
      "Progress: 49.6% ... Training loss: 0.073 ... Validation loss: 0.201iteration: 4960\n",
      "train_loss: 0.07396949548679012\n",
      "val_loss: 0.2012218554060219\n",
      "Progress: 49.6% ... Training loss: 0.083 ... Validation loss: 0.160iteration: 4961\n",
      "train_loss: 0.08398780463551316\n",
      "val_loss: 0.16027490031759517\n",
      "Progress: 49.6% ... Training loss: 0.097 ... Validation loss: 0.249iteration: 4962\n",
      "train_loss: 0.09718533780832996\n",
      "val_loss: 0.24987750610135756\n",
      "Progress: 49.6% ... Training loss: 0.107 ... Validation loss: 0.156iteration: 4963\n",
      "train_loss: 0.10751938299337058\n",
      "val_loss: 0.15608770964994825\n",
      "Progress: 49.6% ... Training loss: 0.087 ... Validation loss: 0.242iteration: 4964\n",
      "train_loss: 0.0878376429542104\n",
      "val_loss: 0.24233794048076643\n",
      "Progress: 49.6% ... Training loss: 0.081 ... Validation loss: 0.156iteration: 4965\n",
      "train_loss: 0.08188371513049583\n",
      "val_loss: 0.15676904445937329\n",
      "Progress: 49.7% ... Training loss: 0.096 ... Validation loss: 0.236iteration: 4966\n",
      "train_loss: 0.09692728045237883\n",
      "val_loss: 0.23685533314301568\n",
      "Progress: 49.7% ... Training loss: 0.085 ... Validation loss: 0.152iteration: 4967\n",
      "train_loss: 0.08570491543753703\n",
      "val_loss: 0.15219851109338603\n",
      "Progress: 49.7% ... Training loss: 0.075 ... Validation loss: 0.208iteration: 4968\n",
      "train_loss: 0.07576339574295535\n",
      "val_loss: 0.2085618015425278\n",
      "Progress: 49.7% ... Training loss: 0.076 ... Validation loss: 0.157iteration: 4969\n",
      "train_loss: 0.07638348331521617\n",
      "val_loss: 0.15709950960486568\n",
      "Progress: 49.7% ... Training loss: 0.078 ... Validation loss: 0.230iteration: 4970\n",
      "train_loss: 0.07857143955263682\n",
      "val_loss: 0.2302543289755201\n",
      "Progress: 49.7% ... Training loss: 0.080 ... Validation loss: 0.161iteration: 4971\n",
      "train_loss: 0.08088112938501671\n",
      "val_loss: 0.16116471713235367\n",
      "Progress: 49.7% ... Training loss: 0.087 ... Validation loss: 0.239iteration: 4972\n",
      "train_loss: 0.08725925225858822\n",
      "val_loss: 0.2396599275631892\n",
      "Progress: 49.7% ... Training loss: 0.102 ... Validation loss: 0.158iteration: 4973\n",
      "train_loss: 0.1029974806358193\n",
      "val_loss: 0.15806056470227856\n",
      "Progress: 49.7% ... Training loss: 0.075 ... Validation loss: 0.213iteration: 4974\n",
      "train_loss: 0.0752411297315375\n",
      "val_loss: 0.21365140491907636\n",
      "Progress: 49.8% ... Training loss: 0.074 ... Validation loss: 0.164iteration: 4975\n",
      "train_loss: 0.07427011699000753\n",
      "val_loss: 0.1645490656794865\n",
      "Progress: 49.8% ... Training loss: 0.083 ... Validation loss: 0.221iteration: 4976\n",
      "train_loss: 0.083229656379542\n",
      "val_loss: 0.22123420713262074\n",
      "Progress: 49.8% ... Training loss: 0.089 ... Validation loss: 0.153iteration: 4977\n",
      "train_loss: 0.08991849523909765\n",
      "val_loss: 0.15362832446256383\n",
      "Progress: 49.8% ... Training loss: 0.077 ... Validation loss: 0.211iteration: 4978\n",
      "train_loss: 0.07712810476900193\n",
      "val_loss: 0.2114912660631261\n",
      "Progress: 49.8% ... Training loss: 0.089 ... Validation loss: 0.156iteration: 4979\n",
      "train_loss: 0.08930048988145181\n",
      "val_loss: 0.1569543156638748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 49.8% ... Training loss: 0.070 ... Validation loss: 0.169iteration: 4980\n",
      "train_loss: 0.07081158226758341\n",
      "val_loss: 0.169435960493883\n",
      "Progress: 49.8% ... Training loss: 0.070 ... Validation loss: 0.181iteration: 4981\n",
      "train_loss: 0.07077355284588582\n",
      "val_loss: 0.18160922697823073\n",
      "Progress: 49.8% ... Training loss: 0.071 ... Validation loss: 0.174iteration: 4982\n",
      "train_loss: 0.07145921330729212\n",
      "val_loss: 0.1748548209977091\n",
      "Progress: 49.8% ... Training loss: 0.072 ... Validation loss: 0.185iteration: 4983\n",
      "train_loss: 0.0722702363547918\n",
      "val_loss: 0.18520019039289698\n",
      "Progress: 49.8% ... Training loss: 0.074 ... Validation loss: 0.160iteration: 4984\n",
      "train_loss: 0.0746734003789066\n",
      "val_loss: 0.16038211315154513\n",
      "Progress: 49.9% ... Training loss: 0.070 ... Validation loss: 0.181iteration: 4985\n",
      "train_loss: 0.07060707113670363\n",
      "val_loss: 0.1818281869342777\n",
      "Progress: 49.9% ... Training loss: 0.069 ... Validation loss: 0.175iteration: 4986\n",
      "train_loss: 0.06987882257463306\n",
      "val_loss: 0.17535136993038664\n",
      "Progress: 49.9% ... Training loss: 0.072 ... Validation loss: 0.159iteration: 4987\n",
      "train_loss: 0.07295481648733287\n",
      "val_loss: 0.15906340447423262\n",
      "Progress: 49.9% ... Training loss: 0.073 ... Validation loss: 0.195iteration: 4988\n",
      "train_loss: 0.07332899060411636\n",
      "val_loss: 0.19531851187809643\n",
      "Progress: 49.9% ... Training loss: 0.071 ... Validation loss: 0.161iteration: 4989\n",
      "train_loss: 0.07153906687466645\n",
      "val_loss: 0.16179680567908278\n",
      "Progress: 49.9% ... Training loss: 0.071 ... Validation loss: 0.184iteration: 4990\n",
      "train_loss: 0.071588907352034\n",
      "val_loss: 0.18484080872511738\n",
      "Progress: 49.9% ... Training loss: 0.075 ... Validation loss: 0.164iteration: 4991\n",
      "train_loss: 0.0755437367426736\n",
      "val_loss: 0.1644001992435397\n",
      "Progress: 49.9% ... Training loss: 0.073 ... Validation loss: 0.192iteration: 4992\n",
      "train_loss: 0.0739166796231568\n",
      "val_loss: 0.19242928069733212\n",
      "Progress: 49.9% ... Training loss: 0.072 ... Validation loss: 0.161iteration: 4993\n",
      "train_loss: 0.0727325219797628\n",
      "val_loss: 0.1615112171628042\n",
      "Progress: 49.9% ... Training loss: 0.077 ... Validation loss: 0.209iteration: 4994\n",
      "train_loss: 0.07761128302996881\n",
      "val_loss: 0.20942350133279897\n",
      "Progress: 50.0% ... Training loss: 0.108 ... Validation loss: 0.163iteration: 4995\n",
      "train_loss: 0.10888425442507645\n",
      "val_loss: 0.16363704916811092\n",
      "Progress: 50.0% ... Training loss: 0.103 ... Validation loss: 0.261iteration: 4996\n",
      "train_loss: 0.10366407478479\n",
      "val_loss: 0.26126585759590193\n",
      "Progress: 50.0% ... Training loss: 0.090 ... Validation loss: 0.157iteration: 4997\n",
      "train_loss: 0.09010986427540563\n",
      "val_loss: 0.15755174098017954\n",
      "Progress: 50.0% ... Training loss: 0.092 ... Validation loss: 0.237iteration: 4998\n",
      "train_loss: 0.09256081344465579\n",
      "val_loss: 0.2376442495127297\n",
      "Progress: 50.0% ... Training loss: 0.090 ... Validation loss: 0.154iteration: 4999\n",
      "train_loss: 0.09098830483605413\n",
      "val_loss: 0.15433132843832728\n",
      "Progress: 50.0% ... Training loss: 0.087 ... Validation loss: 0.208iteration: 5000\n",
      "train_loss: 0.08798236328121\n",
      "val_loss: 0.208103858562124\n",
      "Progress: 50.0% ... Training loss: 0.077 ... Validation loss: 0.156iteration: 5001\n",
      "train_loss: 0.07762215802005741\n",
      "val_loss: 0.15685512198492368\n",
      "Progress: 50.0% ... Training loss: 0.073 ... Validation loss: 0.189iteration: 5002\n",
      "train_loss: 0.0736568973930692\n",
      "val_loss: 0.18961269011238263\n",
      "Progress: 50.0% ... Training loss: 0.079 ... Validation loss: 0.164iteration: 5003\n",
      "train_loss: 0.07999451236584164\n",
      "val_loss: 0.16430839044600928\n",
      "Progress: 50.0% ... Training loss: 0.070 ... Validation loss: 0.186iteration: 5004\n",
      "train_loss: 0.0703409886765498\n",
      "val_loss: 0.18639013833625512\n",
      "Progress: 50.0% ... Training loss: 0.069 ... Validation loss: 0.162iteration: 5005\n",
      "train_loss: 0.06949524554485438\n",
      "val_loss: 0.16242247284513744\n",
      "Progress: 50.1% ... Training loss: 0.073 ... Validation loss: 0.159iteration: 5006\n",
      "train_loss: 0.07395714974487756\n",
      "val_loss: 0.1595332907503613\n",
      "Progress: 50.1% ... Training loss: 0.073 ... Validation loss: 0.186iteration: 5007\n",
      "train_loss: 0.07341017081063547\n",
      "val_loss: 0.18621693933719846\n",
      "Progress: 50.1% ... Training loss: 0.074 ... Validation loss: 0.155iteration: 5008\n",
      "train_loss: 0.0745041489571562\n",
      "val_loss: 0.15565525871226865\n",
      "Progress: 50.1% ... Training loss: 0.080 ... Validation loss: 0.218iteration: 5009\n",
      "train_loss: 0.08042831361262448\n",
      "val_loss: 0.21878234183257128\n",
      "Progress: 50.1% ... Training loss: 0.086 ... Validation loss: 0.159iteration: 5010\n",
      "train_loss: 0.08679636289244683\n",
      "val_loss: 0.15915258449356154\n",
      "Progress: 50.1% ... Training loss: 0.104 ... Validation loss: 0.249iteration: 5011\n",
      "train_loss: 0.10414747576433475\n",
      "val_loss: 0.24958939229225713\n",
      "Progress: 50.1% ... Training loss: 0.093 ... Validation loss: 0.155iteration: 5012\n",
      "train_loss: 0.09351517504226244\n",
      "val_loss: 0.15511863903447534\n",
      "Progress: 50.1% ... Training loss: 0.077 ... Validation loss: 0.212iteration: 5013\n",
      "train_loss: 0.07744410655192806\n",
      "val_loss: 0.21214472324068992\n",
      "Progress: 50.1% ... Training loss: 0.072 ... Validation loss: 0.164iteration: 5014\n",
      "train_loss: 0.07292719343238203\n",
      "val_loss: 0.16458841962358198\n",
      "Progress: 50.1% ... Training loss: 0.070 ... Validation loss: 0.187iteration: 5015\n",
      "train_loss: 0.07070868245754613\n",
      "val_loss: 0.18795492404611025\n",
      "Progress: 50.2% ... Training loss: 0.074 ... Validation loss: 0.158iteration: 5016\n",
      "train_loss: 0.07403389700534584\n",
      "val_loss: 0.158752258575413\n",
      "Progress: 50.2% ... Training loss: 0.082 ... Validation loss: 0.213iteration: 5017\n",
      "train_loss: 0.08233874914763303\n",
      "val_loss: 0.21379183696356197\n",
      "Progress: 50.2% ... Training loss: 0.074 ... Validation loss: 0.158iteration: 5018\n",
      "train_loss: 0.07459758244375028\n",
      "val_loss: 0.158465223347896\n",
      "Progress: 50.2% ... Training loss: 0.070 ... Validation loss: 0.182iteration: 5019\n",
      "train_loss: 0.07027712736519347\n",
      "val_loss: 0.18219564767027824\n",
      "Progress: 50.2% ... Training loss: 0.070 ... Validation loss: 0.174iteration: 5020\n",
      "train_loss: 0.07087882475167971\n",
      "val_loss: 0.1745036753161555\n",
      "Progress: 50.2% ... Training loss: 0.079 ... Validation loss: 0.191iteration: 5021\n",
      "train_loss: 0.07944557314703356\n",
      "val_loss: 0.19175498644323494\n",
      "Progress: 50.2% ... Training loss: 0.085 ... Validation loss: 0.152iteration: 5022\n",
      "train_loss: 0.08564531259302594\n",
      "val_loss: 0.15256918523037893\n",
      "Progress: 50.2% ... Training loss: 0.072 ... Validation loss: 0.180iteration: 5023\n",
      "train_loss: 0.07228296341584328\n",
      "val_loss: 0.18029376657844923\n",
      "Progress: 50.2% ... Training loss: 0.069 ... Validation loss: 0.163iteration: 5024\n",
      "train_loss: 0.06929025775938853\n",
      "val_loss: 0.16369788581094105\n",
      "Progress: 50.2% ... Training loss: 0.069 ... Validation loss: 0.162iteration: 5025\n",
      "train_loss: 0.06922494004411882\n",
      "val_loss: 0.1625573071505639\n",
      "Progress: 50.3% ... Training loss: 0.069 ... Validation loss: 0.170iteration: 5026\n",
      "train_loss: 0.06996789272643075\n",
      "val_loss: 0.17018641161741224\n",
      "Progress: 50.3% ... Training loss: 0.070 ... Validation loss: 0.171iteration: 5027\n",
      "train_loss: 0.07001751759829157\n",
      "val_loss: 0.17163479400765663\n",
      "Progress: 50.3% ... Training loss: 0.072 ... Validation loss: 0.159iteration: 5028\n",
      "train_loss: 0.07201384822713981\n",
      "val_loss: 0.15901642947744865\n",
      "Progress: 50.3% ... Training loss: 0.077 ... Validation loss: 0.194iteration: 5029\n",
      "train_loss: 0.07702716854415388\n",
      "val_loss: 0.19454400599970983\n",
      "Progress: 50.3% ... Training loss: 0.071 ... Validation loss: 0.161iteration: 5030\n",
      "train_loss: 0.0716671400778119\n",
      "val_loss: 0.16157523135391372\n",
      "Progress: 50.3% ... Training loss: 0.069 ... Validation loss: 0.169iteration: 5031\n",
      "train_loss: 0.06944145040602004\n",
      "val_loss: 0.1691645133367899\n",
      "Progress: 50.3% ... Training loss: 0.069 ... Validation loss: 0.175iteration: 5032\n",
      "train_loss: 0.06934289074150883\n",
      "val_loss: 0.17502544804407036\n",
      "Progress: 50.3% ... Training loss: 0.075 ... Validation loss: 0.193iteration: 5033\n",
      "train_loss: 0.07578970806859565\n",
      "val_loss: 0.19314227721270616\n",
      "Progress: 50.3% ... Training loss: 0.076 ... Validation loss: 0.155iteration: 5034\n",
      "train_loss: 0.07658299511514897\n",
      "val_loss: 0.1551612592695459\n",
      "Progress: 50.4% ... Training loss: 0.077 ... Validation loss: 0.192iteration: 5035\n",
      "train_loss: 0.07790335356449245\n",
      "val_loss: 0.19274655362984333\n",
      "Progress: 50.4% ... Training loss: 0.088 ... Validation loss: 0.157iteration: 5036\n",
      "train_loss: 0.08864489374266023\n",
      "val_loss: 0.15727962572115084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50.4% ... Training loss: 0.072 ... Validation loss: 0.187iteration: 5037\n",
      "train_loss: 0.0728040539530941\n",
      "val_loss: 0.1877419819765661\n",
      "Progress: 50.4% ... Training loss: 0.079 ... Validation loss: 0.156iteration: 5038\n",
      "train_loss: 0.07981665621481984\n",
      "val_loss: 0.15618439644415774\n",
      "Progress: 50.4% ... Training loss: 0.069 ... Validation loss: 0.176iteration: 5039\n",
      "train_loss: 0.06988968153430791\n",
      "val_loss: 0.17692895401827127\n",
      "Progress: 50.4% ... Training loss: 0.070 ... Validation loss: 0.170iteration: 5040\n",
      "train_loss: 0.07002959363832509\n",
      "val_loss: 0.170969500019884\n",
      "Progress: 50.4% ... Training loss: 0.069 ... Validation loss: 0.189iteration: 5041\n",
      "train_loss: 0.06970880451198556\n",
      "val_loss: 0.1894799700971924\n",
      "Progress: 50.4% ... Training loss: 0.069 ... Validation loss: 0.171iteration: 5042\n",
      "train_loss: 0.06975410621092827\n",
      "val_loss: 0.1712624188118088\n",
      "Progress: 50.4% ... Training loss: 0.076 ... Validation loss: 0.194iteration: 5043\n",
      "train_loss: 0.07647989830489624\n",
      "val_loss: 0.19439796662486433\n",
      "Progress: 50.4% ... Training loss: 0.072 ... Validation loss: 0.162iteration: 5044\n",
      "train_loss: 0.07223370522257593\n",
      "val_loss: 0.1626090638748242\n",
      "Progress: 50.5% ... Training loss: 0.068 ... Validation loss: 0.174iteration: 5045\n",
      "train_loss: 0.06855762478636242\n",
      "val_loss: 0.17499366629163177\n",
      "Progress: 50.5% ... Training loss: 0.070 ... Validation loss: 0.163iteration: 5046\n",
      "train_loss: 0.07078540511344647\n",
      "val_loss: 0.1630310621390453\n",
      "Progress: 50.5% ... Training loss: 0.085 ... Validation loss: 0.201iteration: 5047\n",
      "train_loss: 0.08545464290207097\n",
      "val_loss: 0.2016404627395446\n",
      "Progress: 50.5% ... Training loss: 0.095 ... Validation loss: 0.158iteration: 5048\n",
      "train_loss: 0.09533921773459172\n",
      "val_loss: 0.15882799294416738\n",
      "Progress: 50.5% ... Training loss: 0.092 ... Validation loss: 0.242iteration: 5049\n",
      "train_loss: 0.09296760075791019\n",
      "val_loss: 0.2424757974556255\n",
      "Progress: 50.5% ... Training loss: 0.081 ... Validation loss: 0.159iteration: 5050\n",
      "train_loss: 0.08199280079080869\n",
      "val_loss: 0.15966483030800466\n",
      "Progress: 50.5% ... Training loss: 0.077 ... Validation loss: 0.219iteration: 5051\n",
      "train_loss: 0.0771729059716195\n",
      "val_loss: 0.21952843316715515\n",
      "Progress: 50.5% ... Training loss: 0.069 ... Validation loss: 0.168iteration: 5052\n",
      "train_loss: 0.0690247966049585\n",
      "val_loss: 0.16848214162587527\n",
      "Progress: 50.5% ... Training loss: 0.069 ... Validation loss: 0.179iteration: 5053\n",
      "train_loss: 0.06965938116160915\n",
      "val_loss: 0.17999694276052255\n",
      "Progress: 50.5% ... Training loss: 0.068 ... Validation loss: 0.176iteration: 5054\n",
      "train_loss: 0.06832005709960863\n",
      "val_loss: 0.17610349657075478\n",
      "Progress: 50.5% ... Training loss: 0.069 ... Validation loss: 0.183iteration: 5055\n",
      "train_loss: 0.06990577339775628\n",
      "val_loss: 0.18370657098861928\n",
      "Progress: 50.6% ... Training loss: 0.069 ... Validation loss: 0.168iteration: 5056\n",
      "train_loss: 0.06932570856654106\n",
      "val_loss: 0.16892140276405082\n",
      "Progress: 50.6% ... Training loss: 0.073 ... Validation loss: 0.175iteration: 5057\n",
      "train_loss: 0.07303777008092253\n",
      "val_loss: 0.1755971675542115\n",
      "Progress: 50.6% ... Training loss: 0.069 ... Validation loss: 0.176iteration: 5058\n",
      "train_loss: 0.06923459744254663\n",
      "val_loss: 0.17630607851538851\n",
      "Progress: 50.6% ... Training loss: 0.076 ... Validation loss: 0.192iteration: 5059\n",
      "train_loss: 0.07610431951649331\n",
      "val_loss: 0.19255752998608738\n",
      "Progress: 50.6% ... Training loss: 0.070 ... Validation loss: 0.171iteration: 5060\n",
      "train_loss: 0.07073480836055207\n",
      "val_loss: 0.17112345349885616\n",
      "Progress: 50.6% ... Training loss: 0.074 ... Validation loss: 0.159iteration: 5061\n",
      "train_loss: 0.07472167762223447\n",
      "val_loss: 0.15934137371015622\n",
      "Progress: 50.6% ... Training loss: 0.070 ... Validation loss: 0.173iteration: 5062\n",
      "train_loss: 0.07026310990627221\n",
      "val_loss: 0.17313681298795952\n",
      "Progress: 50.6% ... Training loss: 0.078 ... Validation loss: 0.158iteration: 5063\n",
      "train_loss: 0.07860795577082931\n",
      "val_loss: 0.15867668518936734\n",
      "Progress: 50.6% ... Training loss: 0.076 ... Validation loss: 0.198iteration: 5064\n",
      "train_loss: 0.0763788258951084\n",
      "val_loss: 0.1980256269737765\n",
      "Progress: 50.6% ... Training loss: 0.072 ... Validation loss: 0.156iteration: 5065\n",
      "train_loss: 0.0728546652976857\n",
      "val_loss: 0.15658105006199297\n",
      "Progress: 50.7% ... Training loss: 0.074 ... Validation loss: 0.183iteration: 5066\n",
      "train_loss: 0.0742331976777758\n",
      "val_loss: 0.18390559475170576\n",
      "Progress: 50.7% ... Training loss: 0.068 ... Validation loss: 0.160iteration: 5067\n",
      "train_loss: 0.06890292443193263\n",
      "val_loss: 0.160996746041181\n",
      "Progress: 50.7% ... Training loss: 0.069 ... Validation loss: 0.175iteration: 5068\n",
      "train_loss: 0.0698923217761547\n",
      "val_loss: 0.17567771946770933\n",
      "Progress: 50.7% ... Training loss: 0.068 ... Validation loss: 0.166iteration: 5069\n",
      "train_loss: 0.06808399469559999\n",
      "val_loss: 0.1668935831710042\n",
      "Progress: 50.7% ... Training loss: 0.070 ... Validation loss: 0.186iteration: 5070\n",
      "train_loss: 0.07001511636546826\n",
      "val_loss: 0.186449482659324\n",
      "Progress: 50.7% ... Training loss: 0.070 ... Validation loss: 0.161iteration: 5071\n",
      "train_loss: 0.07041915406170102\n",
      "val_loss: 0.16166968612457014\n",
      "Progress: 50.7% ... Training loss: 0.071 ... Validation loss: 0.190iteration: 5072\n",
      "train_loss: 0.07185333390897879\n",
      "val_loss: 0.1909010945808216\n",
      "Progress: 50.7% ... Training loss: 0.068 ... Validation loss: 0.171iteration: 5073\n",
      "train_loss: 0.06889754494918489\n",
      "val_loss: 0.17171012195890548\n",
      "Progress: 50.7% ... Training loss: 0.082 ... Validation loss: 0.150iteration: 5074\n",
      "train_loss: 0.08205242992931003\n",
      "val_loss: 0.15099406504066198\n",
      "Progress: 50.8% ... Training loss: 0.075 ... Validation loss: 0.191iteration: 5075\n",
      "train_loss: 0.07501999328230587\n",
      "val_loss: 0.1910605641997803\n",
      "Progress: 50.8% ... Training loss: 0.071 ... Validation loss: 0.156iteration: 5076\n",
      "train_loss: 0.07182095127749816\n",
      "val_loss: 0.1565828177319878\n",
      "Progress: 50.8% ... Training loss: 0.072 ... Validation loss: 0.188iteration: 5077\n",
      "train_loss: 0.0721277770266367\n",
      "val_loss: 0.18894760586402298\n",
      "Progress: 50.8% ... Training loss: 0.068 ... Validation loss: 0.174iteration: 5078\n",
      "train_loss: 0.06883409853074328\n",
      "val_loss: 0.17443779023707867\n",
      "Progress: 50.8% ... Training loss: 0.068 ... Validation loss: 0.165iteration: 5079\n",
      "train_loss: 0.06884128216853677\n",
      "val_loss: 0.16530267569757612\n",
      "Progress: 50.8% ... Training loss: 0.073 ... Validation loss: 0.157iteration: 5080\n",
      "train_loss: 0.07386960644592731\n",
      "val_loss: 0.15763782501172796\n",
      "Progress: 50.8% ... Training loss: 0.074 ... Validation loss: 0.191iteration: 5081\n",
      "train_loss: 0.07427713058894125\n",
      "val_loss: 0.1910713674650221\n",
      "Progress: 50.8% ... Training loss: 0.074 ... Validation loss: 0.163iteration: 5082\n",
      "train_loss: 0.07480942218681866\n",
      "val_loss: 0.1639697855382814\n",
      "Progress: 50.8% ... Training loss: 0.071 ... Validation loss: 0.197iteration: 5083\n",
      "train_loss: 0.07191869984335182\n",
      "val_loss: 0.19727298788058678\n",
      "Progress: 50.8% ... Training loss: 0.078 ... Validation loss: 0.164iteration: 5084\n",
      "train_loss: 0.07880382712977553\n",
      "val_loss: 0.16419384419929284\n",
      "Progress: 50.9% ... Training loss: 0.075 ... Validation loss: 0.216iteration: 5085\n",
      "train_loss: 0.07549037348606243\n",
      "val_loss: 0.21641393657848418\n",
      "Progress: 50.9% ... Training loss: 0.095 ... Validation loss: 0.157iteration: 5086\n",
      "train_loss: 0.09529386279748778\n",
      "val_loss: 0.1571411666065661\n",
      "Progress: 50.9% ... Training loss: 0.093 ... Validation loss: 0.246iteration: 5087\n",
      "train_loss: 0.09395667146935248\n",
      "val_loss: 0.24691658373804662\n",
      "Progress: 50.9% ... Training loss: 0.086 ... Validation loss: 0.152iteration: 5088\n",
      "train_loss: 0.08673013305423904\n",
      "val_loss: 0.1520552255276435\n",
      "Progress: 50.9% ... Training loss: 0.079 ... Validation loss: 0.207iteration: 5089\n",
      "train_loss: 0.07947587697694317\n",
      "val_loss: 0.20742123790825934\n",
      "Progress: 50.9% ... Training loss: 0.068 ... Validation loss: 0.176iteration: 5090\n",
      "train_loss: 0.06839262592081445\n",
      "val_loss: 0.1760229703083515\n",
      "Progress: 50.9% ... Training loss: 0.069 ... Validation loss: 0.188iteration: 5091\n",
      "train_loss: 0.06903139531688895\n",
      "val_loss: 0.1886978548974298\n",
      "Progress: 50.9% ... Training loss: 0.069 ... Validation loss: 0.190iteration: 5092\n",
      "train_loss: 0.06949534121547608\n",
      "val_loss: 0.19042444071565248\n",
      "Progress: 50.9% ... Training loss: 0.071 ... Validation loss: 0.181iteration: 5093\n",
      "train_loss: 0.07145802994736539\n",
      "val_loss: 0.18142682672562715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50.9% ... Training loss: 0.069 ... Validation loss: 0.181iteration: 5094\n",
      "train_loss: 0.06985386090935082\n",
      "val_loss: 0.18185736361242807\n",
      "Progress: 51.0% ... Training loss: 0.070 ... Validation loss: 0.162iteration: 5095\n",
      "train_loss: 0.07073059942106667\n",
      "val_loss: 0.16212445182422167\n",
      "Progress: 51.0% ... Training loss: 0.076 ... Validation loss: 0.217iteration: 5096\n",
      "train_loss: 0.0769220341473779\n",
      "val_loss: 0.21735771044663363\n",
      "Progress: 51.0% ... Training loss: 0.076 ... Validation loss: 0.158iteration: 5097\n",
      "train_loss: 0.07629553868935203\n",
      "val_loss: 0.15871150867849473\n",
      "Progress: 51.0% ... Training loss: 0.076 ... Validation loss: 0.216iteration: 5098\n",
      "train_loss: 0.07644079241974297\n",
      "val_loss: 0.2169467908885031\n",
      "Progress: 51.0% ... Training loss: 0.070 ... Validation loss: 0.167iteration: 5099\n",
      "train_loss: 0.07000887320377554\n",
      "val_loss: 0.16785558570766057\n",
      "Progress: 51.0% ... Training loss: 0.069 ... Validation loss: 0.192iteration: 5100\n",
      "train_loss: 0.06987194364691426\n",
      "val_loss: 0.19297998357963111\n",
      "Progress: 51.0% ... Training loss: 0.072 ... Validation loss: 0.172iteration: 5101\n",
      "train_loss: 0.07244185444567297\n",
      "val_loss: 0.17275844111917402\n",
      "Progress: 51.0% ... Training loss: 0.070 ... Validation loss: 0.204iteration: 5102\n",
      "train_loss: 0.0703879631054362\n",
      "val_loss: 0.2043004725641357\n",
      "Progress: 51.0% ... Training loss: 0.069 ... Validation loss: 0.171iteration: 5103\n",
      "train_loss: 0.06918743491634347\n",
      "val_loss: 0.1718488605085337\n",
      "Progress: 51.0% ... Training loss: 0.070 ... Validation loss: 0.191iteration: 5104\n",
      "train_loss: 0.07028806920642296\n",
      "val_loss: 0.1912288310771985\n",
      "Progress: 51.0% ... Training loss: 0.068 ... Validation loss: 0.164iteration: 5105\n",
      "train_loss: 0.0689623317282482\n",
      "val_loss: 0.1648805006622522\n",
      "Progress: 51.1% ... Training loss: 0.069 ... Validation loss: 0.196iteration: 5106\n",
      "train_loss: 0.06933616003826257\n",
      "val_loss: 0.1960701686827886\n",
      "Progress: 51.1% ... Training loss: 0.069 ... Validation loss: 0.176iteration: 5107\n",
      "train_loss: 0.06942852180113922\n",
      "val_loss: 0.17684313379554778\n",
      "Progress: 51.1% ... Training loss: 0.072 ... Validation loss: 0.202iteration: 5108\n",
      "train_loss: 0.07216722176477303\n",
      "val_loss: 0.2029925962075515\n",
      "Progress: 51.1% ... Training loss: 0.071 ... Validation loss: 0.165iteration: 5109\n",
      "train_loss: 0.07135230939528232\n",
      "val_loss: 0.1656377705259116\n",
      "Progress: 51.1% ... Training loss: 0.078 ... Validation loss: 0.189iteration: 5110\n",
      "train_loss: 0.07841924204556767\n",
      "val_loss: 0.18977098778128618\n",
      "Progress: 51.1% ... Training loss: 0.068 ... Validation loss: 0.176iteration: 5111\n",
      "train_loss: 0.06856846264237408\n",
      "val_loss: 0.1762006473431353\n",
      "Progress: 51.1% ... Training loss: 0.072 ... Validation loss: 0.162iteration: 5112\n",
      "train_loss: 0.07219346057790506\n",
      "val_loss: 0.16233297225249074\n",
      "Progress: 51.1% ... Training loss: 0.087 ... Validation loss: 0.223iteration: 5113\n",
      "train_loss: 0.08772543044475851\n",
      "val_loss: 0.22316343707692346\n",
      "Progress: 51.1% ... Training loss: 0.070 ... Validation loss: 0.156iteration: 5114\n",
      "train_loss: 0.07095134540580599\n",
      "val_loss: 0.15695235274999747\n",
      "Progress: 51.1% ... Training loss: 0.069 ... Validation loss: 0.176iteration: 5115\n",
      "train_loss: 0.06998337394579904\n",
      "val_loss: 0.17680434466970046\n",
      "Progress: 51.2% ... Training loss: 0.071 ... Validation loss: 0.179iteration: 5116\n",
      "train_loss: 0.07128232965254443\n",
      "val_loss: 0.1790706183975387\n",
      "Progress: 51.2% ... Training loss: 0.068 ... Validation loss: 0.166iteration: 5117\n",
      "train_loss: 0.06831463947642873\n",
      "val_loss: 0.16636439736075664\n",
      "Progress: 51.2% ... Training loss: 0.069 ... Validation loss: 0.175iteration: 5118\n",
      "train_loss: 0.06955239930436553\n",
      "val_loss: 0.17588153264793133\n",
      "Progress: 51.2% ... Training loss: 0.070 ... Validation loss: 0.159iteration: 5119\n",
      "train_loss: 0.07008591689035992\n",
      "val_loss: 0.15992896394037207\n",
      "Progress: 51.2% ... Training loss: 0.068 ... Validation loss: 0.173iteration: 5120\n",
      "train_loss: 0.06847264297968242\n",
      "val_loss: 0.17303501998604426\n",
      "Progress: 51.2% ... Training loss: 0.067 ... Validation loss: 0.167iteration: 5121\n",
      "train_loss: 0.06791064519088366\n",
      "val_loss: 0.16781539017416824\n",
      "Progress: 51.2% ... Training loss: 0.068 ... Validation loss: 0.169iteration: 5122\n",
      "train_loss: 0.0687444857531105\n",
      "val_loss: 0.16966232364137634\n",
      "Progress: 51.2% ... Training loss: 0.070 ... Validation loss: 0.191iteration: 5123\n",
      "train_loss: 0.07088250558853629\n",
      "val_loss: 0.19113552696287076\n",
      "Progress: 51.2% ... Training loss: 0.069 ... Validation loss: 0.159iteration: 5124\n",
      "train_loss: 0.06953320191174567\n",
      "val_loss: 0.15922111789329502\n",
      "Progress: 51.2% ... Training loss: 0.069 ... Validation loss: 0.161iteration: 5125\n",
      "train_loss: 0.06932128048576887\n",
      "val_loss: 0.16135726466987144\n",
      "Progress: 51.3% ... Training loss: 0.076 ... Validation loss: 0.180iteration: 5126\n",
      "train_loss: 0.07682938942601969\n",
      "val_loss: 0.1808147283657459\n",
      "Progress: 51.3% ... Training loss: 0.070 ... Validation loss: 0.151iteration: 5127\n",
      "train_loss: 0.07015259488572127\n",
      "val_loss: 0.1518173863590363\n",
      "Progress: 51.3% ... Training loss: 0.069 ... Validation loss: 0.165iteration: 5128\n",
      "train_loss: 0.0696008263729311\n",
      "val_loss: 0.16525002684260412\n",
      "Progress: 51.3% ... Training loss: 0.068 ... Validation loss: 0.168iteration: 5129\n",
      "train_loss: 0.06878724362640523\n",
      "val_loss: 0.16830325851944797\n",
      "Progress: 51.3% ... Training loss: 0.075 ... Validation loss: 0.158iteration: 5130\n",
      "train_loss: 0.07559955574969025\n",
      "val_loss: 0.15827377107437715\n",
      "Progress: 51.3% ... Training loss: 0.075 ... Validation loss: 0.204iteration: 5131\n",
      "train_loss: 0.07510572948038535\n",
      "val_loss: 0.20422508686620372\n",
      "Progress: 51.3% ... Training loss: 0.068 ... Validation loss: 0.172iteration: 5132\n",
      "train_loss: 0.06808882659363803\n",
      "val_loss: 0.17218518914187075\n",
      "Progress: 51.3% ... Training loss: 0.068 ... Validation loss: 0.161iteration: 5133\n",
      "train_loss: 0.06880462425296399\n",
      "val_loss: 0.1612231200027295\n",
      "Progress: 51.3% ... Training loss: 0.067 ... Validation loss: 0.161iteration: 5134\n",
      "train_loss: 0.06792159611391493\n",
      "val_loss: 0.1616202197830744\n",
      "Progress: 51.4% ... Training loss: 0.068 ... Validation loss: 0.171iteration: 5135\n",
      "train_loss: 0.06855943354587418\n",
      "val_loss: 0.1719704686435231\n",
      "Progress: 51.4% ... Training loss: 0.079 ... Validation loss: 0.153iteration: 5136\n",
      "train_loss: 0.07926417240818093\n",
      "val_loss: 0.15385887092310888\n",
      "Progress: 51.4% ... Training loss: 0.076 ... Validation loss: 0.188iteration: 5137\n",
      "train_loss: 0.07610373306663056\n",
      "val_loss: 0.18861002721019618\n",
      "Progress: 51.4% ... Training loss: 0.074 ... Validation loss: 0.152iteration: 5138\n",
      "train_loss: 0.0748320591986714\n",
      "val_loss: 0.152409280695055\n",
      "Progress: 51.4% ... Training loss: 0.077 ... Validation loss: 0.220iteration: 5139\n",
      "train_loss: 0.07790222241790119\n",
      "val_loss: 0.22066116583117976\n",
      "Progress: 51.4% ... Training loss: 0.078 ... Validation loss: 0.159iteration: 5140\n",
      "train_loss: 0.0783919131091053\n",
      "val_loss: 0.15967549182226407\n",
      "Progress: 51.4% ... Training loss: 0.068 ... Validation loss: 0.181iteration: 5141\n",
      "train_loss: 0.06869237801185166\n",
      "val_loss: 0.1816574753073876\n",
      "Progress: 51.4% ... Training loss: 0.069 ... Validation loss: 0.167iteration: 5142\n",
      "train_loss: 0.0690311576089601\n",
      "val_loss: 0.16735584021348646\n",
      "Progress: 51.4% ... Training loss: 0.069 ... Validation loss: 0.176iteration: 5143\n",
      "train_loss: 0.06942842930349145\n",
      "val_loss: 0.17661365929727546\n",
      "Progress: 51.4% ... Training loss: 0.068 ... Validation loss: 0.181iteration: 5144\n",
      "train_loss: 0.06831400293670689\n",
      "val_loss: 0.181871065699835\n",
      "Progress: 51.5% ... Training loss: 0.073 ... Validation loss: 0.152iteration: 5145\n",
      "train_loss: 0.07313915712805462\n",
      "val_loss: 0.15290382707523673\n",
      "Progress: 51.5% ... Training loss: 0.079 ... Validation loss: 0.207iteration: 5146\n",
      "train_loss: 0.07916181108368464\n",
      "val_loss: 0.20703332701167426\n",
      "Progress: 51.5% ... Training loss: 0.075 ... Validation loss: 0.149iteration: 5147\n",
      "train_loss: 0.07543932835106912\n",
      "val_loss: 0.14950651969135642\n",
      "Progress: 51.5% ... Training loss: 0.075 ... Validation loss: 0.197iteration: 5148\n",
      "train_loss: 0.07520749171741434\n",
      "val_loss: 0.19709284921790113\n",
      "Progress: 51.5% ... Training loss: 0.071 ... Validation loss: 0.163iteration: 5149\n",
      "train_loss: 0.07136890951783874\n",
      "val_loss: 0.16385435158978454\n",
      "Progress: 51.5% ... Training loss: 0.075 ... Validation loss: 0.198iteration: 5150\n",
      "train_loss: 0.07538619977028442\n",
      "val_loss: 0.19849347937368056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 51.5% ... Training loss: 0.073 ... Validation loss: 0.156iteration: 5151\n",
      "train_loss: 0.07392775342061127\n",
      "val_loss: 0.15664926730334205\n",
      "Progress: 51.5% ... Training loss: 0.069 ... Validation loss: 0.163iteration: 5152\n",
      "train_loss: 0.06975419336115656\n",
      "val_loss: 0.16317632248302413\n",
      "Progress: 51.5% ... Training loss: 0.067 ... Validation loss: 0.168iteration: 5153\n",
      "train_loss: 0.06790897848352095\n",
      "val_loss: 0.16878065503080247\n",
      "Progress: 51.5% ... Training loss: 0.068 ... Validation loss: 0.163iteration: 5154\n",
      "train_loss: 0.06851900333279501\n",
      "val_loss: 0.1636890259012011\n",
      "Progress: 51.5% ... Training loss: 0.070 ... Validation loss: 0.184iteration: 5155\n",
      "train_loss: 0.07024729725317647\n",
      "val_loss: 0.1845800801177794\n",
      "Progress: 51.6% ... Training loss: 0.072 ... Validation loss: 0.169iteration: 5156\n",
      "train_loss: 0.07229764941519579\n",
      "val_loss: 0.1694976106031656\n",
      "Progress: 51.6% ... Training loss: 0.071 ... Validation loss: 0.201iteration: 5157\n",
      "train_loss: 0.07156218223241148\n",
      "val_loss: 0.201108656337504\n",
      "Progress: 51.6% ... Training loss: 0.068 ... Validation loss: 0.171iteration: 5158\n",
      "train_loss: 0.0683134270981972\n",
      "val_loss: 0.17190598941939722\n",
      "Progress: 51.6% ... Training loss: 0.069 ... Validation loss: 0.191iteration: 5159\n",
      "train_loss: 0.06995038686389232\n",
      "val_loss: 0.19141911968158196\n",
      "Progress: 51.6% ... Training loss: 0.067 ... Validation loss: 0.170iteration: 5160\n",
      "train_loss: 0.06798958028820803\n",
      "val_loss: 0.1700278069708787\n",
      "Progress: 51.6% ... Training loss: 0.069 ... Validation loss: 0.161iteration: 5161\n",
      "train_loss: 0.06906668799423371\n",
      "val_loss: 0.1613122832364695\n",
      "Progress: 51.6% ... Training loss: 0.074 ... Validation loss: 0.173iteration: 5162\n",
      "train_loss: 0.07469830216251468\n",
      "val_loss: 0.17340000109614412\n",
      "Progress: 51.6% ... Training loss: 0.073 ... Validation loss: 0.157iteration: 5163\n",
      "train_loss: 0.07338939919583973\n",
      "val_loss: 0.15738115546109305\n",
      "Progress: 51.6% ... Training loss: 0.076 ... Validation loss: 0.198iteration: 5164\n",
      "train_loss: 0.07610053755163457\n",
      "val_loss: 0.19828453032903195\n",
      "Progress: 51.6% ... Training loss: 0.075 ... Validation loss: 0.153iteration: 5165\n",
      "train_loss: 0.07523420472163718\n",
      "val_loss: 0.15307593066771905\n",
      "Progress: 51.7% ... Training loss: 0.077 ... Validation loss: 0.206iteration: 5166\n",
      "train_loss: 0.07711411550570503\n",
      "val_loss: 0.20657888362711552\n",
      "Progress: 51.7% ... Training loss: 0.075 ... Validation loss: 0.161iteration: 5167\n",
      "train_loss: 0.07543068241915867\n",
      "val_loss: 0.16126600708086244\n",
      "Progress: 51.7% ... Training loss: 0.084 ... Validation loss: 0.220iteration: 5168\n",
      "train_loss: 0.08467002966097781\n",
      "val_loss: 0.2202162263172789\n",
      "Progress: 51.7% ... Training loss: 0.082 ... Validation loss: 0.161iteration: 5169\n",
      "train_loss: 0.08216403170741235\n",
      "val_loss: 0.16118806559678925\n",
      "Progress: 51.7% ... Training loss: 0.075 ... Validation loss: 0.210iteration: 5170\n",
      "train_loss: 0.07505607060871\n",
      "val_loss: 0.21076909309635852\n",
      "Progress: 51.7% ... Training loss: 0.086 ... Validation loss: 0.155iteration: 5171\n",
      "train_loss: 0.08663514082288862\n",
      "val_loss: 0.1554585732749288\n",
      "Progress: 51.7% ... Training loss: 0.079 ... Validation loss: 0.237iteration: 5172\n",
      "train_loss: 0.0795851360402326\n",
      "val_loss: 0.23729425463559042\n",
      "Progress: 51.7% ... Training loss: 0.074 ... Validation loss: 0.163iteration: 5173\n",
      "train_loss: 0.074351733242296\n",
      "val_loss: 0.16370942409535152\n",
      "Progress: 51.7% ... Training loss: 0.071 ... Validation loss: 0.181iteration: 5174\n",
      "train_loss: 0.07140298670537017\n",
      "val_loss: 0.181856561150023\n",
      "Progress: 51.8% ... Training loss: 0.069 ... Validation loss: 0.156iteration: 5175\n",
      "train_loss: 0.06948978158857436\n",
      "val_loss: 0.1563537783515461\n",
      "Progress: 51.8% ... Training loss: 0.069 ... Validation loss: 0.191iteration: 5176\n",
      "train_loss: 0.06957537973889029\n",
      "val_loss: 0.1911945809611138\n",
      "Progress: 51.8% ... Training loss: 0.067 ... Validation loss: 0.172iteration: 5177\n",
      "train_loss: 0.0677661593402507\n",
      "val_loss: 0.1723904361417305\n",
      "Progress: 51.8% ... Training loss: 0.068 ... Validation loss: 0.163iteration: 5178\n",
      "train_loss: 0.06807268253467127\n",
      "val_loss: 0.1630641262330407\n",
      "Progress: 51.8% ... Training loss: 0.077 ... Validation loss: 0.203iteration: 5179\n",
      "train_loss: 0.07766835174235072\n",
      "val_loss: 0.20371859141502136\n",
      "Progress: 51.8% ... Training loss: 0.069 ... Validation loss: 0.161iteration: 5180\n",
      "train_loss: 0.06915314708461748\n",
      "val_loss: 0.1619264966662864\n",
      "Progress: 51.8% ... Training loss: 0.067 ... Validation loss: 0.173iteration: 5181\n",
      "train_loss: 0.06749047232397626\n",
      "val_loss: 0.17309011145378364\n",
      "Progress: 51.8% ... Training loss: 0.069 ... Validation loss: 0.187iteration: 5182\n",
      "train_loss: 0.06998329797105383\n",
      "val_loss: 0.18761344453512688\n",
      "Progress: 51.8% ... Training loss: 0.069 ... Validation loss: 0.160iteration: 5183\n",
      "train_loss: 0.0696109596648476\n",
      "val_loss: 0.16088072510135823\n",
      "Progress: 51.8% ... Training loss: 0.070 ... Validation loss: 0.186iteration: 5184\n",
      "train_loss: 0.07064206774297668\n",
      "val_loss: 0.18621767205129458\n",
      "Progress: 51.9% ... Training loss: 0.076 ... Validation loss: 0.154iteration: 5185\n",
      "train_loss: 0.07694207876555202\n",
      "val_loss: 0.15478468855836588\n",
      "Progress: 51.9% ... Training loss: 0.113 ... Validation loss: 0.264iteration: 5186\n",
      "train_loss: 0.11334755486747117\n",
      "val_loss: 0.26443789831941644\n",
      "Progress: 51.9% ... Training loss: 0.113 ... Validation loss: 0.159iteration: 5187\n",
      "train_loss: 0.11374049473851876\n",
      "val_loss: 0.1597150214740236\n",
      "Progress: 51.9% ... Training loss: 0.093 ... Validation loss: 0.233iteration: 5188\n",
      "train_loss: 0.09396018620183705\n",
      "val_loss: 0.23315573917085114\n",
      "Progress: 51.9% ... Training loss: 0.083 ... Validation loss: 0.152iteration: 5189\n",
      "train_loss: 0.08378046947036166\n",
      "val_loss: 0.15203105963016886\n",
      "Progress: 51.9% ... Training loss: 0.104 ... Validation loss: 0.245iteration: 5190\n",
      "train_loss: 0.10486929407387129\n",
      "val_loss: 0.24516179772687868\n",
      "Progress: 51.9% ... Training loss: 0.084 ... Validation loss: 0.148iteration: 5191\n",
      "train_loss: 0.08445822148434448\n",
      "val_loss: 0.1487605888633604\n",
      "Progress: 51.9% ... Training loss: 0.086 ... Validation loss: 0.233iteration: 5192\n",
      "train_loss: 0.08669272224637956\n",
      "val_loss: 0.23330140096339042\n",
      "Progress: 51.9% ... Training loss: 0.085 ... Validation loss: 0.151iteration: 5193\n",
      "train_loss: 0.08504811706375075\n",
      "val_loss: 0.15187791339543352\n",
      "Progress: 51.9% ... Training loss: 0.082 ... Validation loss: 0.210iteration: 5194\n",
      "train_loss: 0.08289176843349162\n",
      "val_loss: 0.21000711584796514\n",
      "Progress: 52.0% ... Training loss: 0.074 ... Validation loss: 0.154iteration: 5195\n",
      "train_loss: 0.07440257817676886\n",
      "val_loss: 0.15432181754480184\n",
      "Progress: 52.0% ... Training loss: 0.072 ... Validation loss: 0.199iteration: 5196\n",
      "train_loss: 0.07288152075614952\n",
      "val_loss: 0.1992889609630864\n",
      "Progress: 52.0% ... Training loss: 0.068 ... Validation loss: 0.173iteration: 5197\n",
      "train_loss: 0.06826333901481198\n",
      "val_loss: 0.17362297666493748\n",
      "Progress: 52.0% ... Training loss: 0.069 ... Validation loss: 0.173iteration: 5198\n",
      "train_loss: 0.06909389250430203\n",
      "val_loss: 0.1735975641359085\n",
      "Progress: 52.0% ... Training loss: 0.086 ... Validation loss: 0.217iteration: 5199\n",
      "train_loss: 0.08631361431026598\n",
      "val_loss: 0.21749836366425335\n",
      "Progress: 52.0% ... Training loss: 0.088 ... Validation loss: 0.154iteration: 5200\n",
      "train_loss: 0.08898054523302637\n",
      "val_loss: 0.1543890642174938\n",
      "Progress: 52.0% ... Training loss: 0.102 ... Validation loss: 0.238iteration: 5201\n",
      "train_loss: 0.1028734811388268\n",
      "val_loss: 0.23807257231504586\n",
      "Progress: 52.0% ... Training loss: 0.087 ... Validation loss: 0.152iteration: 5202\n",
      "train_loss: 0.08706128198102503\n",
      "val_loss: 0.15243254338082335\n",
      "Progress: 52.0% ... Training loss: 0.076 ... Validation loss: 0.215iteration: 5203\n",
      "train_loss: 0.07660331208587907\n",
      "val_loss: 0.21514934218367124\n",
      "Progress: 52.0% ... Training loss: 0.074 ... Validation loss: 0.166iteration: 5204\n",
      "train_loss: 0.07492597507023738\n",
      "val_loss: 0.1664456886333689\n",
      "Progress: 52.0% ... Training loss: 0.070 ... Validation loss: 0.168iteration: 5205\n",
      "train_loss: 0.07003959750934502\n",
      "val_loss: 0.16864900866448376\n",
      "Progress: 52.1% ... Training loss: 0.071 ... Validation loss: 0.198iteration: 5206\n",
      "train_loss: 0.07112142403438225\n",
      "val_loss: 0.1980426040551003\n",
      "Progress: 52.1% ... Training loss: 0.077 ... Validation loss: 0.158iteration: 5207\n",
      "train_loss: 0.0773963924644487\n",
      "val_loss: 0.15800110583286697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 52.1% ... Training loss: 0.085 ... Validation loss: 0.231iteration: 5208\n",
      "train_loss: 0.08508383500017545\n",
      "val_loss: 0.23165752316597926\n",
      "Progress: 52.1% ... Training loss: 0.089 ... Validation loss: 0.151iteration: 5209\n",
      "train_loss: 0.08905569101871927\n",
      "val_loss: 0.15160565134662066\n",
      "Progress: 52.1% ... Training loss: 0.076 ... Validation loss: 0.209iteration: 5210\n",
      "train_loss: 0.07662484967554231\n",
      "val_loss: 0.20924416695459533\n",
      "Progress: 52.1% ... Training loss: 0.071 ... Validation loss: 0.157iteration: 5211\n",
      "train_loss: 0.07127478803531094\n",
      "val_loss: 0.15742928874160228\n",
      "Progress: 52.1% ... Training loss: 0.068 ... Validation loss: 0.170iteration: 5212\n",
      "train_loss: 0.06872103131226989\n",
      "val_loss: 0.1701050554181122\n",
      "Progress: 52.1% ... Training loss: 0.073 ... Validation loss: 0.160iteration: 5213\n",
      "train_loss: 0.07300514868107594\n",
      "val_loss: 0.16096889467104372\n",
      "Progress: 52.1% ... Training loss: 0.073 ... Validation loss: 0.188iteration: 5214\n",
      "train_loss: 0.07348318772001029\n",
      "val_loss: 0.18891945501724178\n",
      "Progress: 52.1% ... Training loss: 0.071 ... Validation loss: 0.155iteration: 5215\n",
      "train_loss: 0.07196795782426056\n",
      "val_loss: 0.15539640083035194\n",
      "Progress: 52.2% ... Training loss: 0.074 ... Validation loss: 0.179iteration: 5216\n",
      "train_loss: 0.07409441899903883\n",
      "val_loss: 0.17963263244776292\n",
      "Progress: 52.2% ... Training loss: 0.071 ... Validation loss: 0.157iteration: 5217\n",
      "train_loss: 0.07197500576922651\n",
      "val_loss: 0.15797834438640204\n",
      "Progress: 52.2% ... Training loss: 0.070 ... Validation loss: 0.184iteration: 5218\n",
      "train_loss: 0.07065242656310747\n",
      "val_loss: 0.1845645813386475\n",
      "Progress: 52.2% ... Training loss: 0.070 ... Validation loss: 0.159iteration: 5219\n",
      "train_loss: 0.07095397159291206\n",
      "val_loss: 0.15979569478473585\n",
      "Progress: 52.2% ... Training loss: 0.073 ... Validation loss: 0.185iteration: 5220\n",
      "train_loss: 0.07330920778444713\n",
      "val_loss: 0.18535023747546697\n",
      "Progress: 52.2% ... Training loss: 0.073 ... Validation loss: 0.155iteration: 5221\n",
      "train_loss: 0.07364638603580352\n",
      "val_loss: 0.15557570619297892\n",
      "Progress: 52.2% ... Training loss: 0.078 ... Validation loss: 0.196iteration: 5222\n",
      "train_loss: 0.07844770942515895\n",
      "val_loss: 0.19639920442375475\n",
      "Progress: 52.2% ... Training loss: 0.080 ... Validation loss: 0.153iteration: 5223\n",
      "train_loss: 0.08002992755402841\n",
      "val_loss: 0.1531655903600106\n",
      "Progress: 52.2% ... Training loss: 0.068 ... Validation loss: 0.177iteration: 5224\n",
      "train_loss: 0.0681679792797586\n",
      "val_loss: 0.1779009101040681\n",
      "Progress: 52.2% ... Training loss: 0.068 ... Validation loss: 0.166iteration: 5225\n",
      "train_loss: 0.06824130839267144\n",
      "val_loss: 0.16648671737124687\n",
      "Progress: 52.3% ... Training loss: 0.067 ... Validation loss: 0.176iteration: 5226\n",
      "train_loss: 0.06735411288925233\n",
      "val_loss: 0.17654561499271404\n",
      "Progress: 52.3% ... Training loss: 0.068 ... Validation loss: 0.171iteration: 5227\n",
      "train_loss: 0.06826674045470972\n",
      "val_loss: 0.17180951029125002\n",
      "Progress: 52.3% ... Training loss: 0.069 ... Validation loss: 0.172iteration: 5228\n",
      "train_loss: 0.0696449191904455\n",
      "val_loss: 0.17277458799143\n",
      "Progress: 52.3% ... Training loss: 0.068 ... Validation loss: 0.184iteration: 5229\n",
      "train_loss: 0.0681315571933738\n",
      "val_loss: 0.18455713608919028\n",
      "Progress: 52.3% ... Training loss: 0.067 ... Validation loss: 0.176iteration: 5230\n",
      "train_loss: 0.06765639901749733\n",
      "val_loss: 0.1763897461255883\n",
      "Progress: 52.3% ... Training loss: 0.074 ... Validation loss: 0.157iteration: 5231\n",
      "train_loss: 0.07420143214095488\n",
      "val_loss: 0.15715213094676098\n",
      "Progress: 52.3% ... Training loss: 0.072 ... Validation loss: 0.188iteration: 5232\n",
      "train_loss: 0.07207119408070203\n",
      "val_loss: 0.18821074036306434\n",
      "Progress: 52.3% ... Training loss: 0.068 ... Validation loss: 0.159iteration: 5233\n",
      "train_loss: 0.06883255466219027\n",
      "val_loss: 0.15973822329713042\n",
      "Progress: 52.3% ... Training loss: 0.068 ... Validation loss: 0.172iteration: 5234\n",
      "train_loss: 0.06873667825696445\n",
      "val_loss: 0.1727858699783656\n",
      "Progress: 52.4% ... Training loss: 0.068 ... Validation loss: 0.176iteration: 5235\n",
      "train_loss: 0.06842923050251387\n",
      "val_loss: 0.17607480165946116\n",
      "Progress: 52.4% ... Training loss: 0.073 ... Validation loss: 0.157iteration: 5236\n",
      "train_loss: 0.07330951971499715\n",
      "val_loss: 0.1573303180722095\n",
      "Progress: 52.4% ... Training loss: 0.069 ... Validation loss: 0.196iteration: 5237\n",
      "train_loss: 0.06993932017349004\n",
      "val_loss: 0.19698064604136417\n",
      "Progress: 52.4% ... Training loss: 0.068 ... Validation loss: 0.185iteration: 5238\n",
      "train_loss: 0.06864175679487919\n",
      "val_loss: 0.18577799550570095\n",
      "Progress: 52.4% ... Training loss: 0.069 ... Validation loss: 0.176iteration: 5239\n",
      "train_loss: 0.06983681585008798\n",
      "val_loss: 0.17684342607573958\n",
      "Progress: 52.4% ... Training loss: 0.067 ... Validation loss: 0.175iteration: 5240\n",
      "train_loss: 0.06769881392836859\n",
      "val_loss: 0.1751495568943139\n",
      "Progress: 52.4% ... Training loss: 0.068 ... Validation loss: 0.181iteration: 5241\n",
      "train_loss: 0.06800907764171318\n",
      "val_loss: 0.1811665522243706\n",
      "Progress: 52.4% ... Training loss: 0.067 ... Validation loss: 0.166iteration: 5242\n",
      "train_loss: 0.06759291922155287\n",
      "val_loss: 0.16650282785089418\n",
      "Progress: 52.4% ... Training loss: 0.067 ... Validation loss: 0.170iteration: 5243\n",
      "train_loss: 0.06761199210078668\n",
      "val_loss: 0.17011840133233805\n",
      "Progress: 52.4% ... Training loss: 0.073 ... Validation loss: 0.158iteration: 5244\n",
      "train_loss: 0.07341677959806295\n",
      "val_loss: 0.15819275293696214\n",
      "Progress: 52.5% ... Training loss: 0.076 ... Validation loss: 0.203iteration: 5245\n",
      "train_loss: 0.0763301183253868\n",
      "val_loss: 0.20305613506241268\n",
      "Progress: 52.5% ... Training loss: 0.069 ... Validation loss: 0.166iteration: 5246\n",
      "train_loss: 0.06962198514146983\n",
      "val_loss: 0.16643307061459134\n",
      "Progress: 52.5% ... Training loss: 0.068 ... Validation loss: 0.161iteration: 5247\n",
      "train_loss: 0.06820893643507667\n",
      "val_loss: 0.1616308775791089\n",
      "Progress: 52.5% ... Training loss: 0.068 ... Validation loss: 0.170iteration: 5248\n",
      "train_loss: 0.06865952781939681\n",
      "val_loss: 0.17051041573255654\n",
      "Progress: 52.5% ... Training loss: 0.068 ... Validation loss: 0.159iteration: 5249\n",
      "train_loss: 0.06877554741982819\n",
      "val_loss: 0.15954993084100919\n",
      "Progress: 52.5% ... Training loss: 0.069 ... Validation loss: 0.168iteration: 5250\n",
      "train_loss: 0.06917392143491506\n",
      "val_loss: 0.1683272443912684\n",
      "Progress: 52.5% ... Training loss: 0.067 ... Validation loss: 0.168iteration: 5251\n",
      "train_loss: 0.06779615311949301\n",
      "val_loss: 0.16881243168081086\n",
      "Progress: 52.5% ... Training loss: 0.068 ... Validation loss: 0.158iteration: 5252\n",
      "train_loss: 0.06874096749776366\n",
      "val_loss: 0.15890124858992166\n",
      "Progress: 52.5% ... Training loss: 0.067 ... Validation loss: 0.166iteration: 5253\n",
      "train_loss: 0.0678191727435395\n",
      "val_loss: 0.16664806311464717\n",
      "Progress: 52.5% ... Training loss: 0.067 ... Validation loss: 0.164iteration: 5254\n",
      "train_loss: 0.06742069938801645\n",
      "val_loss: 0.16409040451275875\n",
      "Progress: 52.5% ... Training loss: 0.069 ... Validation loss: 0.182iteration: 5255\n",
      "train_loss: 0.0693299260020115\n",
      "val_loss: 0.18233709521530092\n",
      "Progress: 52.6% ... Training loss: 0.067 ... Validation loss: 0.163iteration: 5256\n",
      "train_loss: 0.06758275021948996\n",
      "val_loss: 0.1631072034301897\n",
      "Progress: 52.6% ... Training loss: 0.069 ... Validation loss: 0.166iteration: 5257\n",
      "train_loss: 0.06977342272120444\n",
      "val_loss: 0.16692796362740792\n",
      "Progress: 52.6% ... Training loss: 0.071 ... Validation loss: 0.159iteration: 5258\n",
      "train_loss: 0.07131889327488881\n",
      "val_loss: 0.15957341303713007\n",
      "Progress: 52.6% ... Training loss: 0.072 ... Validation loss: 0.190iteration: 5259\n",
      "train_loss: 0.07299917167784518\n",
      "val_loss: 0.1908709416505682\n",
      "Progress: 52.6% ... Training loss: 0.075 ... Validation loss: 0.153iteration: 5260\n",
      "train_loss: 0.07575798351550252\n",
      "val_loss: 0.15329648990831554\n",
      "Progress: 52.6% ... Training loss: 0.068 ... Validation loss: 0.178iteration: 5261\n",
      "train_loss: 0.06831835927875214\n",
      "val_loss: 0.17858701151609907\n",
      "Progress: 52.6% ... Training loss: 0.085 ... Validation loss: 0.155iteration: 5262\n",
      "train_loss: 0.08581651865261494\n",
      "val_loss: 0.15570132510091914\n",
      "Progress: 52.6% ... Training loss: 0.109 ... Validation loss: 0.261iteration: 5263\n",
      "train_loss: 0.10979834430901958\n",
      "val_loss: 0.26101010314841394\n",
      "Progress: 52.6% ... Training loss: 0.108 ... Validation loss: 0.160iteration: 5264\n",
      "train_loss: 0.10891183166814798\n",
      "val_loss: 0.16048760371199136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 52.6% ... Training loss: 0.102 ... Validation loss: 0.228iteration: 5265\n",
      "train_loss: 0.10286865982592984\n",
      "val_loss: 0.22817313436430714\n",
      "Progress: 52.7% ... Training loss: 0.105 ... Validation loss: 0.162iteration: 5266\n",
      "train_loss: 0.10504720172528294\n",
      "val_loss: 0.16251979263601943\n",
      "Progress: 52.7% ... Training loss: 0.089 ... Validation loss: 0.242iteration: 5267\n",
      "train_loss: 0.08962216056988509\n",
      "val_loss: 0.24295883842731858\n",
      "Progress: 52.7% ... Training loss: 0.080 ... Validation loss: 0.156iteration: 5268\n",
      "train_loss: 0.08043182639415436\n",
      "val_loss: 0.1569327909935299\n",
      "Progress: 52.7% ... Training loss: 0.074 ... Validation loss: 0.205iteration: 5269\n",
      "train_loss: 0.07492150460601847\n",
      "val_loss: 0.2051106480803128\n",
      "Progress: 52.7% ... Training loss: 0.081 ... Validation loss: 0.163iteration: 5270\n",
      "train_loss: 0.08132218995868597\n",
      "val_loss: 0.16302152271706938\n",
      "Progress: 52.7% ... Training loss: 0.080 ... Validation loss: 0.223iteration: 5271\n",
      "train_loss: 0.08083398718001604\n",
      "val_loss: 0.22397673399243273\n",
      "Progress: 52.7% ... Training loss: 0.094 ... Validation loss: 0.157iteration: 5272\n",
      "train_loss: 0.09462870681111328\n",
      "val_loss: 0.1572705997618232\n",
      "Progress: 52.7% ... Training loss: 0.089 ... Validation loss: 0.222iteration: 5273\n",
      "train_loss: 0.08930244647789419\n",
      "val_loss: 0.22285310619002235\n",
      "Progress: 52.7% ... Training loss: 0.081 ... Validation loss: 0.149iteration: 5274\n",
      "train_loss: 0.08192817648410329\n",
      "val_loss: 0.14997268813476883\n",
      "Progress: 52.8% ... Training loss: 0.113 ... Validation loss: 0.256iteration: 5275\n",
      "train_loss: 0.1139798481168706\n",
      "val_loss: 0.256298325923251\n",
      "Progress: 52.8% ... Training loss: 0.157 ... Validation loss: 0.173iteration: 5276\n",
      "train_loss: 0.15745609131145546\n",
      "val_loss: 0.1732775297633321\n",
      "Progress: 52.8% ... Training loss: 0.116 ... Validation loss: 0.309iteration: 5277\n",
      "train_loss: 0.11661649300712025\n",
      "val_loss: 0.309383667540863\n",
      "Progress: 52.8% ... Training loss: 0.084 ... Validation loss: 0.153iteration: 5278\n",
      "train_loss: 0.08438142427202167\n",
      "val_loss: 0.15390551922113296\n",
      "Progress: 52.8% ... Training loss: 0.112 ... Validation loss: 0.261iteration: 5279\n",
      "train_loss: 0.11293769142551685\n",
      "val_loss: 0.2611281481550118\n",
      "Progress: 52.8% ... Training loss: 0.118 ... Validation loss: 0.157iteration: 5280\n",
      "train_loss: 0.1187101700447455\n",
      "val_loss: 0.15756270591713922\n",
      "Progress: 52.8% ... Training loss: 0.095 ... Validation loss: 0.267iteration: 5281\n",
      "train_loss: 0.09586717909330882\n",
      "val_loss: 0.2675287116659341\n",
      "Progress: 52.8% ... Training loss: 0.080 ... Validation loss: 0.159iteration: 5282\n",
      "train_loss: 0.08069473492565685\n",
      "val_loss: 0.15912594801056246\n",
      "Progress: 52.8% ... Training loss: 0.080 ... Validation loss: 0.240iteration: 5283\n",
      "train_loss: 0.08085731148380035\n",
      "val_loss: 0.2403243995878506\n",
      "Progress: 52.8% ... Training loss: 0.082 ... Validation loss: 0.163iteration: 5284\n",
      "train_loss: 0.082645411580031\n",
      "val_loss: 0.1632914206503654\n",
      "Progress: 52.9% ... Training loss: 0.074 ... Validation loss: 0.208iteration: 5285\n",
      "train_loss: 0.07438645242085798\n",
      "val_loss: 0.20833566724460226\n",
      "Progress: 52.9% ... Training loss: 0.069 ... Validation loss: 0.175iteration: 5286\n",
      "train_loss: 0.06922855307706703\n",
      "val_loss: 0.17539359840712782\n",
      "Progress: 52.9% ... Training loss: 0.079 ... Validation loss: 0.230iteration: 5287\n",
      "train_loss: 0.07959678207555058\n",
      "val_loss: 0.230685933741044\n",
      "Progress: 52.9% ... Training loss: 0.093 ... Validation loss: 0.156iteration: 5288\n",
      "train_loss: 0.09380512524243646\n",
      "val_loss: 0.15699685005784367\n",
      "Progress: 52.9% ... Training loss: 0.085 ... Validation loss: 0.215iteration: 5289\n",
      "train_loss: 0.08570833090351981\n",
      "val_loss: 0.21507042056557088\n",
      "Progress: 52.9% ... Training loss: 0.076 ... Validation loss: 0.157iteration: 5290\n",
      "train_loss: 0.0767154779061392\n",
      "val_loss: 0.1576376052613429\n",
      "Progress: 52.9% ... Training loss: 0.077 ... Validation loss: 0.212iteration: 5291\n",
      "train_loss: 0.07716400869779869\n",
      "val_loss: 0.21274217061446135\n",
      "Progress: 52.9% ... Training loss: 0.069 ... Validation loss: 0.156iteration: 5292\n",
      "train_loss: 0.06951066493934265\n",
      "val_loss: 0.1568697373944595\n",
      "Progress: 52.9% ... Training loss: 0.069 ... Validation loss: 0.169iteration: 5293\n",
      "train_loss: 0.0694094292995685\n",
      "val_loss: 0.16967302275364718\n",
      "Progress: 52.9% ... Training loss: 0.072 ... Validation loss: 0.171iteration: 5294\n",
      "train_loss: 0.07256612462496913\n",
      "val_loss: 0.17126312162247154\n",
      "Progress: 53.0% ... Training loss: 0.080 ... Validation loss: 0.149iteration: 5295\n",
      "train_loss: 0.08096439630221357\n",
      "val_loss: 0.14900089986715337\n",
      "Progress: 53.0% ... Training loss: 0.071 ... Validation loss: 0.192iteration: 5296\n",
      "train_loss: 0.07126268098780456\n",
      "val_loss: 0.19261934939840764\n",
      "Progress: 53.0% ... Training loss: 0.071 ... Validation loss: 0.151iteration: 5297\n",
      "train_loss: 0.07103652772843178\n",
      "val_loss: 0.151541231363288\n",
      "Progress: 53.0% ... Training loss: 0.084 ... Validation loss: 0.201iteration: 5298\n",
      "train_loss: 0.08449619685977106\n",
      "val_loss: 0.2010886966088264\n",
      "Progress: 53.0% ... Training loss: 0.078 ... Validation loss: 0.158iteration: 5299\n",
      "train_loss: 0.07869569642557875\n",
      "val_loss: 0.15894592862308587\n",
      "Progress: 53.0% ... Training loss: 0.093 ... Validation loss: 0.238iteration: 5300\n",
      "train_loss: 0.09349809417383793\n",
      "val_loss: 0.23842156988502552\n",
      "Progress: 53.0% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 5301\n",
      "train_loss: 0.08136055261285133\n",
      "val_loss: 0.15467989696887605\n",
      "Progress: 53.0% ... Training loss: 0.074 ... Validation loss: 0.200iteration: 5302\n",
      "train_loss: 0.07405543454204759\n",
      "val_loss: 0.20099459814745563\n",
      "Progress: 53.0% ... Training loss: 0.078 ... Validation loss: 0.155iteration: 5303\n",
      "train_loss: 0.07825775932173552\n",
      "val_loss: 0.15598761105910994\n",
      "Progress: 53.0% ... Training loss: 0.098 ... Validation loss: 0.226iteration: 5304\n",
      "train_loss: 0.09818596106774087\n",
      "val_loss: 0.22629295441982916\n",
      "Progress: 53.0% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 5305\n",
      "train_loss: 0.08126878050944378\n",
      "val_loss: 0.15434957859257808\n",
      "Progress: 53.1% ... Training loss: 0.107 ... Validation loss: 0.251iteration: 5306\n",
      "train_loss: 0.1079599401912736\n",
      "val_loss: 0.2510827733153148\n",
      "Progress: 53.1% ... Training loss: 0.101 ... Validation loss: 0.153iteration: 5307\n",
      "train_loss: 0.10166880181501559\n",
      "val_loss: 0.15349024505966724\n",
      "Progress: 53.1% ... Training loss: 0.087 ... Validation loss: 0.222iteration: 5308\n",
      "train_loss: 0.08721747872317613\n",
      "val_loss: 0.22257881373856508\n",
      "Progress: 53.1% ... Training loss: 0.101 ... Validation loss: 0.153iteration: 5309\n",
      "train_loss: 0.10168475848605645\n",
      "val_loss: 0.15357037429793102\n",
      "Progress: 53.1% ... Training loss: 0.087 ... Validation loss: 0.247iteration: 5310\n",
      "train_loss: 0.08756144182492345\n",
      "val_loss: 0.2470025827175361\n",
      "Progress: 53.1% ... Training loss: 0.072 ... Validation loss: 0.153iteration: 5311\n",
      "train_loss: 0.07250476504522059\n",
      "val_loss: 0.15325072094453163\n",
      "Progress: 53.1% ... Training loss: 0.074 ... Validation loss: 0.214iteration: 5312\n",
      "train_loss: 0.07442910338763449\n",
      "val_loss: 0.2149291300565371\n",
      "Progress: 53.1% ... Training loss: 0.068 ... Validation loss: 0.178iteration: 5313\n",
      "train_loss: 0.06874369757292507\n",
      "val_loss: 0.17831015378720766\n",
      "Progress: 53.1% ... Training loss: 0.068 ... Validation loss: 0.189iteration: 5314\n",
      "train_loss: 0.06813234829034549\n",
      "val_loss: 0.18910956860717704\n",
      "Progress: 53.1% ... Training loss: 0.073 ... Validation loss: 0.170iteration: 5315\n",
      "train_loss: 0.07356915667496051\n",
      "val_loss: 0.1708418333683541\n",
      "Progress: 53.2% ... Training loss: 0.076 ... Validation loss: 0.228iteration: 5316\n",
      "train_loss: 0.07615996198357702\n",
      "val_loss: 0.22898103288445273\n",
      "Progress: 53.2% ... Training loss: 0.072 ... Validation loss: 0.160iteration: 5317\n",
      "train_loss: 0.07263807675394612\n",
      "val_loss: 0.16082285344130232\n",
      "Progress: 53.2% ... Training loss: 0.069 ... Validation loss: 0.196iteration: 5318\n",
      "train_loss: 0.06966892766166786\n",
      "val_loss: 0.19625819183256432\n",
      "Progress: 53.2% ... Training loss: 0.068 ... Validation loss: 0.169iteration: 5319\n",
      "train_loss: 0.06854818320535129\n",
      "val_loss: 0.16953978004052273\n",
      "Progress: 53.2% ... Training loss: 0.070 ... Validation loss: 0.169iteration: 5320\n",
      "train_loss: 0.07009867933788658\n",
      "val_loss: 0.1693285185788393\n",
      "Progress: 53.2% ... Training loss: 0.072 ... Validation loss: 0.208iteration: 5321\n",
      "train_loss: 0.07281719181203174\n",
      "val_loss: 0.20866445564867564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 53.2% ... Training loss: 0.071 ... Validation loss: 0.164iteration: 5322\n",
      "train_loss: 0.0713731241550854\n",
      "val_loss: 0.16416959145540627\n",
      "Progress: 53.2% ... Training loss: 0.068 ... Validation loss: 0.185iteration: 5323\n",
      "train_loss: 0.06829938440193248\n",
      "val_loss: 0.1853589650662106\n",
      "Progress: 53.2% ... Training loss: 0.070 ... Validation loss: 0.160iteration: 5324\n",
      "train_loss: 0.0705117923622972\n",
      "val_loss: 0.16053948502656923\n",
      "Progress: 53.2% ... Training loss: 0.071 ... Validation loss: 0.181iteration: 5325\n",
      "train_loss: 0.07185915109016981\n",
      "val_loss: 0.1814224703065326\n",
      "Progress: 53.3% ... Training loss: 0.067 ... Validation loss: 0.172iteration: 5326\n",
      "train_loss: 0.0678210319436766\n",
      "val_loss: 0.1722626057822692\n",
      "Progress: 53.3% ... Training loss: 0.078 ... Validation loss: 0.153iteration: 5327\n",
      "train_loss: 0.078180256258792\n",
      "val_loss: 0.1536457711670269\n",
      "Progress: 53.3% ... Training loss: 0.072 ... Validation loss: 0.194iteration: 5328\n",
      "train_loss: 0.0727060317448248\n",
      "val_loss: 0.19454289265845207\n",
      "Progress: 53.3% ... Training loss: 0.067 ... Validation loss: 0.161iteration: 5329\n",
      "train_loss: 0.06727372945625626\n",
      "val_loss: 0.16164825292896134\n",
      "Progress: 53.3% ... Training loss: 0.068 ... Validation loss: 0.181iteration: 5330\n",
      "train_loss: 0.06862845003953502\n",
      "val_loss: 0.18184616770090586\n",
      "Progress: 53.3% ... Training loss: 0.080 ... Validation loss: 0.156iteration: 5331\n",
      "train_loss: 0.08024254812707664\n",
      "val_loss: 0.15678092003693456\n",
      "Progress: 53.3% ... Training loss: 0.077 ... Validation loss: 0.221iteration: 5332\n",
      "train_loss: 0.07739550950355023\n",
      "val_loss: 0.22135576716688993\n",
      "Progress: 53.3% ... Training loss: 0.069 ... Validation loss: 0.162iteration: 5333\n",
      "train_loss: 0.06939190566949896\n",
      "val_loss: 0.1629557912167845\n",
      "Progress: 53.3% ... Training loss: 0.067 ... Validation loss: 0.163iteration: 5334\n",
      "train_loss: 0.06749722909700408\n",
      "val_loss: 0.16356177378500858\n",
      "Progress: 53.4% ... Training loss: 0.067 ... Validation loss: 0.171iteration: 5335\n",
      "train_loss: 0.06715523696349122\n",
      "val_loss: 0.1713191994908751\n",
      "Progress: 53.4% ... Training loss: 0.068 ... Validation loss: 0.154iteration: 5336\n",
      "train_loss: 0.0689248314680542\n",
      "val_loss: 0.15453604604402155\n",
      "Progress: 53.4% ... Training loss: 0.066 ... Validation loss: 0.164iteration: 5337\n",
      "train_loss: 0.06688566462701905\n",
      "val_loss: 0.16449803719421904\n",
      "Progress: 53.4% ... Training loss: 0.068 ... Validation loss: 0.158iteration: 5338\n",
      "train_loss: 0.06848359348624253\n",
      "val_loss: 0.15839439791668564\n",
      "Progress: 53.4% ... Training loss: 0.068 ... Validation loss: 0.180iteration: 5339\n",
      "train_loss: 0.06866450373589116\n",
      "val_loss: 0.1805909255107126\n",
      "Progress: 53.4% ... Training loss: 0.072 ... Validation loss: 0.158iteration: 5340\n",
      "train_loss: 0.07221467105773728\n",
      "val_loss: 0.15870078439865434\n",
      "Progress: 53.4% ... Training loss: 0.082 ... Validation loss: 0.223iteration: 5341\n",
      "train_loss: 0.08262791012015154\n",
      "val_loss: 0.22328835767642646\n",
      "Progress: 53.4% ... Training loss: 0.101 ... Validation loss: 0.153iteration: 5342\n",
      "train_loss: 0.10189702783642544\n",
      "val_loss: 0.1536176854394945\n",
      "Progress: 53.4% ... Training loss: 0.103 ... Validation loss: 0.242iteration: 5343\n",
      "train_loss: 0.10360581909331132\n",
      "val_loss: 0.24254571024340715\n",
      "Progress: 53.4% ... Training loss: 0.093 ... Validation loss: 0.148iteration: 5344\n",
      "train_loss: 0.09314636862524975\n",
      "val_loss: 0.1482851539389575\n",
      "Progress: 53.5% ... Training loss: 0.098 ... Validation loss: 0.256iteration: 5345\n",
      "train_loss: 0.09833645235657416\n",
      "val_loss: 0.2562668451193056\n",
      "Progress: 53.5% ... Training loss: 0.073 ... Validation loss: 0.155iteration: 5346\n",
      "train_loss: 0.07303383576584918\n",
      "val_loss: 0.15564460544184078\n",
      "Progress: 53.5% ... Training loss: 0.070 ... Validation loss: 0.190iteration: 5347\n",
      "train_loss: 0.07025077152866323\n",
      "val_loss: 0.19040041735007157\n",
      "Progress: 53.5% ... Training loss: 0.068 ... Validation loss: 0.153iteration: 5348\n",
      "train_loss: 0.06857685908291876\n",
      "val_loss: 0.1534022847598577\n",
      "Progress: 53.5% ... Training loss: 0.068 ... Validation loss: 0.178iteration: 5349\n",
      "train_loss: 0.06852803804086076\n",
      "val_loss: 0.1785022251262593\n",
      "Progress: 53.5% ... Training loss: 0.077 ... Validation loss: 0.149iteration: 5350\n",
      "train_loss: 0.07730844950942058\n",
      "val_loss: 0.14973432634177597\n",
      "Progress: 53.5% ... Training loss: 0.067 ... Validation loss: 0.173iteration: 5351\n",
      "train_loss: 0.06722917376990732\n",
      "val_loss: 0.17338415486343037\n",
      "Progress: 53.5% ... Training loss: 0.070 ... Validation loss: 0.154iteration: 5352\n",
      "train_loss: 0.0709347919433688\n",
      "val_loss: 0.15493083811952613\n",
      "Progress: 53.5% ... Training loss: 0.068 ... Validation loss: 0.176iteration: 5353\n",
      "train_loss: 0.0684842921917038\n",
      "val_loss: 0.1763693693117054\n",
      "Progress: 53.5% ... Training loss: 0.067 ... Validation loss: 0.167iteration: 5354\n",
      "train_loss: 0.06735258052134141\n",
      "val_loss: 0.16727544527070484\n",
      "Progress: 53.5% ... Training loss: 0.069 ... Validation loss: 0.163iteration: 5355\n",
      "train_loss: 0.06969620651181462\n",
      "val_loss: 0.16323305394819526\n",
      "Progress: 53.6% ... Training loss: 0.067 ... Validation loss: 0.169iteration: 5356\n",
      "train_loss: 0.06702070383829481\n",
      "val_loss: 0.16931290192783127\n",
      "Progress: 53.6% ... Training loss: 0.067 ... Validation loss: 0.163iteration: 5357\n",
      "train_loss: 0.06718487771180737\n",
      "val_loss: 0.16341921998115658\n",
      "Progress: 53.6% ... Training loss: 0.074 ... Validation loss: 0.155iteration: 5358\n",
      "train_loss: 0.0746524383969701\n",
      "val_loss: 0.15527665222832712\n",
      "Progress: 53.6% ... Training loss: 0.078 ... Validation loss: 0.207iteration: 5359\n",
      "train_loss: 0.07877645466194742\n",
      "val_loss: 0.20726973591980358\n",
      "Progress: 53.6% ... Training loss: 0.069 ... Validation loss: 0.153iteration: 5360\n",
      "train_loss: 0.06903120417437213\n",
      "val_loss: 0.15357697872123585\n",
      "Progress: 53.6% ... Training loss: 0.068 ... Validation loss: 0.186iteration: 5361\n",
      "train_loss: 0.06885796053015157\n",
      "val_loss: 0.18693738980747093\n",
      "Progress: 53.6% ... Training loss: 0.067 ... Validation loss: 0.168iteration: 5362\n",
      "train_loss: 0.06722066658572808\n",
      "val_loss: 0.16861652830867127\n",
      "Progress: 53.6% ... Training loss: 0.069 ... Validation loss: 0.184iteration: 5363\n",
      "train_loss: 0.06993572833908478\n",
      "val_loss: 0.184161050492941\n",
      "Progress: 53.6% ... Training loss: 0.069 ... Validation loss: 0.154iteration: 5364\n",
      "train_loss: 0.06904435624259343\n",
      "val_loss: 0.1544562248795398\n",
      "Progress: 53.6% ... Training loss: 0.067 ... Validation loss: 0.161iteration: 5365\n",
      "train_loss: 0.06747825821151618\n",
      "val_loss: 0.16175503252582069\n",
      "Progress: 53.7% ... Training loss: 0.068 ... Validation loss: 0.165iteration: 5366\n",
      "train_loss: 0.06858086195984336\n",
      "val_loss: 0.16589462489045578\n",
      "Progress: 53.7% ... Training loss: 0.083 ... Validation loss: 0.152iteration: 5367\n",
      "train_loss: 0.08390559816252752\n",
      "val_loss: 0.1526778820236219\n",
      "Progress: 53.7% ... Training loss: 0.074 ... Validation loss: 0.178iteration: 5368\n",
      "train_loss: 0.07424979188482862\n",
      "val_loss: 0.17876709286068795\n",
      "Progress: 53.7% ... Training loss: 0.086 ... Validation loss: 0.154iteration: 5369\n",
      "train_loss: 0.0862989415728368\n",
      "val_loss: 0.1541100842495741\n",
      "Progress: 53.7% ... Training loss: 0.095 ... Validation loss: 0.242iteration: 5370\n",
      "train_loss: 0.0957668670820258\n",
      "val_loss: 0.24214602620943182\n",
      "Progress: 53.7% ... Training loss: 0.101 ... Validation loss: 0.156iteration: 5371\n",
      "train_loss: 0.1015796819023979\n",
      "val_loss: 0.1562040431417347\n",
      "Progress: 53.7% ... Training loss: 0.083 ... Validation loss: 0.226iteration: 5372\n",
      "train_loss: 0.0834713862042001\n",
      "val_loss: 0.22675936034768893\n",
      "Progress: 53.7% ... Training loss: 0.089 ... Validation loss: 0.154iteration: 5373\n",
      "train_loss: 0.08904540657200943\n",
      "val_loss: 0.1545855634414058\n",
      "Progress: 53.7% ... Training loss: 0.094 ... Validation loss: 0.229iteration: 5374\n",
      "train_loss: 0.09476096891223457\n",
      "val_loss: 0.2298533835737964\n",
      "Progress: 53.8% ... Training loss: 0.117 ... Validation loss: 0.160iteration: 5375\n",
      "train_loss: 0.11767463886118239\n",
      "val_loss: 0.16042263127710285\n",
      "Progress: 53.8% ... Training loss: 0.116 ... Validation loss: 0.268iteration: 5376\n",
      "train_loss: 0.1162663610534739\n",
      "val_loss: 0.2684395056949618\n",
      "Progress: 53.8% ... Training loss: 0.095 ... Validation loss: 0.154iteration: 5377\n",
      "train_loss: 0.09551188542908813\n",
      "val_loss: 0.15430167426281724\n",
      "Progress: 53.8% ... Training loss: 0.092 ... Validation loss: 0.237iteration: 5378\n",
      "train_loss: 0.09262991118033254\n",
      "val_loss: 0.2372529498556705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 53.8% ... Training loss: 0.070 ... Validation loss: 0.155iteration: 5379\n",
      "train_loss: 0.07002635922681985\n",
      "val_loss: 0.15570518626841903\n",
      "Progress: 53.8% ... Training loss: 0.076 ... Validation loss: 0.182iteration: 5380\n",
      "train_loss: 0.07693500956017653\n",
      "val_loss: 0.18245374834269765\n",
      "Progress: 53.8% ... Training loss: 0.078 ... Validation loss: 0.155iteration: 5381\n",
      "train_loss: 0.07898645394924203\n",
      "val_loss: 0.15566994473076534\n",
      "Progress: 53.8% ... Training loss: 0.086 ... Validation loss: 0.208iteration: 5382\n",
      "train_loss: 0.08616159595191551\n",
      "val_loss: 0.20857195151972488\n",
      "Progress: 53.8% ... Training loss: 0.075 ... Validation loss: 0.154iteration: 5383\n",
      "train_loss: 0.07584326043099766\n",
      "val_loss: 0.154099292407367\n",
      "Progress: 53.8% ... Training loss: 0.100 ... Validation loss: 0.230iteration: 5384\n",
      "train_loss: 0.10008033497356517\n",
      "val_loss: 0.23083951935023295\n",
      "Progress: 53.9% ... Training loss: 0.089 ... Validation loss: 0.153iteration: 5385\n",
      "train_loss: 0.08907099728399957\n",
      "val_loss: 0.15368296579113067\n",
      "Progress: 53.9% ... Training loss: 0.070 ... Validation loss: 0.189iteration: 5386\n",
      "train_loss: 0.07088542163382705\n",
      "val_loss: 0.18925198378037322\n",
      "Progress: 53.9% ... Training loss: 0.071 ... Validation loss: 0.161iteration: 5387\n",
      "train_loss: 0.0715785305681447\n",
      "val_loss: 0.161810141258558\n",
      "Progress: 53.9% ... Training loss: 0.077 ... Validation loss: 0.220iteration: 5388\n",
      "train_loss: 0.07717905831712189\n",
      "val_loss: 0.22021389781445208\n",
      "Progress: 53.9% ... Training loss: 0.074 ... Validation loss: 0.158iteration: 5389\n",
      "train_loss: 0.07492416462539847\n",
      "val_loss: 0.15805273123873076\n",
      "Progress: 53.9% ... Training loss: 0.087 ... Validation loss: 0.209iteration: 5390\n",
      "train_loss: 0.08744657230530656\n",
      "val_loss: 0.20969757530826202\n",
      "Progress: 53.9% ... Training loss: 0.074 ... Validation loss: 0.152iteration: 5391\n",
      "train_loss: 0.07442391383218179\n",
      "val_loss: 0.15281041535732054\n",
      "Progress: 53.9% ... Training loss: 0.070 ... Validation loss: 0.182iteration: 5392\n",
      "train_loss: 0.0705263832500311\n",
      "val_loss: 0.18242044276830321\n",
      "Progress: 53.9% ... Training loss: 0.070 ... Validation loss: 0.158iteration: 5393\n",
      "train_loss: 0.07093925214442355\n",
      "val_loss: 0.15839248030605477\n",
      "Progress: 53.9% ... Training loss: 0.076 ... Validation loss: 0.209iteration: 5394\n",
      "train_loss: 0.07634884859476272\n",
      "val_loss: 0.2090575465921481\n",
      "Progress: 54.0% ... Training loss: 0.071 ... Validation loss: 0.158iteration: 5395\n",
      "train_loss: 0.0711563644311164\n",
      "val_loss: 0.15812108613096137\n",
      "Progress: 54.0% ... Training loss: 0.076 ... Validation loss: 0.193iteration: 5396\n",
      "train_loss: 0.07677987645714286\n",
      "val_loss: 0.19381662198311428\n",
      "Progress: 54.0% ... Training loss: 0.075 ... Validation loss: 0.152iteration: 5397\n",
      "train_loss: 0.0753512826739167\n",
      "val_loss: 0.15266430656730345\n",
      "Progress: 54.0% ... Training loss: 0.069 ... Validation loss: 0.185iteration: 5398\n",
      "train_loss: 0.06921439368173285\n",
      "val_loss: 0.18592984052956557\n",
      "Progress: 54.0% ... Training loss: 0.067 ... Validation loss: 0.174iteration: 5399\n",
      "train_loss: 0.06700236335744728\n",
      "val_loss: 0.17450163897998142\n",
      "Progress: 54.0% ... Training loss: 0.067 ... Validation loss: 0.171iteration: 5400\n",
      "train_loss: 0.06778763223905876\n",
      "val_loss: 0.17145298750703594\n",
      "Progress: 54.0% ... Training loss: 0.067 ... Validation loss: 0.177iteration: 5401\n",
      "train_loss: 0.06772539675570982\n",
      "val_loss: 0.17757184719262367\n",
      "Progress: 54.0% ... Training loss: 0.066 ... Validation loss: 0.170iteration: 5402\n",
      "train_loss: 0.06663858465392769\n",
      "val_loss: 0.17001221359855778\n",
      "Progress: 54.0% ... Training loss: 0.067 ... Validation loss: 0.162iteration: 5403\n",
      "train_loss: 0.06730246104105159\n",
      "val_loss: 0.16233304518046027\n",
      "Progress: 54.0% ... Training loss: 0.069 ... Validation loss: 0.187iteration: 5404\n",
      "train_loss: 0.06948363596216633\n",
      "val_loss: 0.18779051469867034\n",
      "Progress: 54.0% ... Training loss: 0.070 ... Validation loss: 0.156iteration: 5405\n",
      "train_loss: 0.07094210883244491\n",
      "val_loss: 0.15679231372874466\n",
      "Progress: 54.1% ... Training loss: 0.082 ... Validation loss: 0.188iteration: 5406\n",
      "train_loss: 0.08217359467881521\n",
      "val_loss: 0.18855797393813767\n",
      "Progress: 54.1% ... Training loss: 0.070 ... Validation loss: 0.149iteration: 5407\n",
      "train_loss: 0.07028443489202119\n",
      "val_loss: 0.1499062238488002\n",
      "Progress: 54.1% ... Training loss: 0.071 ... Validation loss: 0.190iteration: 5408\n",
      "train_loss: 0.07129226341501273\n",
      "val_loss: 0.19051183713206502\n",
      "Progress: 54.1% ... Training loss: 0.066 ... Validation loss: 0.164iteration: 5409\n",
      "train_loss: 0.06673552856348723\n",
      "val_loss: 0.16426479852221332\n",
      "Progress: 54.1% ... Training loss: 0.066 ... Validation loss: 0.166iteration: 5410\n",
      "train_loss: 0.06699922385614729\n",
      "val_loss: 0.16682414678603738\n",
      "Progress: 54.1% ... Training loss: 0.073 ... Validation loss: 0.204iteration: 5411\n",
      "train_loss: 0.07385786311286875\n",
      "val_loss: 0.20410207838135286\n",
      "Progress: 54.1% ... Training loss: 0.086 ... Validation loss: 0.156iteration: 5412\n",
      "train_loss: 0.08610073384242435\n",
      "val_loss: 0.15669891339339184\n",
      "Progress: 54.1% ... Training loss: 0.085 ... Validation loss: 0.230iteration: 5413\n",
      "train_loss: 0.0856511079003249\n",
      "val_loss: 0.23053889855502596\n",
      "Progress: 54.1% ... Training loss: 0.111 ... Validation loss: 0.163iteration: 5414\n",
      "train_loss: 0.1111286819663478\n",
      "val_loss: 0.1633848097505239\n",
      "Progress: 54.1% ... Training loss: 0.085 ... Validation loss: 0.214iteration: 5415\n",
      "train_loss: 0.0859215555130471\n",
      "val_loss: 0.21442944490119428\n",
      "Progress: 54.2% ... Training loss: 0.088 ... Validation loss: 0.152iteration: 5416\n",
      "train_loss: 0.08859470631581585\n",
      "val_loss: 0.1524859497070466\n",
      "Progress: 54.2% ... Training loss: 0.086 ... Validation loss: 0.216iteration: 5417\n",
      "train_loss: 0.08670739660683313\n",
      "val_loss: 0.2166945821106723\n",
      "Progress: 54.2% ... Training loss: 0.086 ... Validation loss: 0.151iteration: 5418\n",
      "train_loss: 0.0861840930665484\n",
      "val_loss: 0.15169361007952156\n",
      "Progress: 54.2% ... Training loss: 0.075 ... Validation loss: 0.194iteration: 5419\n",
      "train_loss: 0.0759673406285786\n",
      "val_loss: 0.19456562458551335\n",
      "Progress: 54.2% ... Training loss: 0.073 ... Validation loss: 0.154iteration: 5420\n",
      "train_loss: 0.07305083385705566\n",
      "val_loss: 0.15499704500308628\n",
      "Progress: 54.2% ... Training loss: 0.068 ... Validation loss: 0.178iteration: 5421\n",
      "train_loss: 0.0682216337303101\n",
      "val_loss: 0.1783111078749478\n",
      "Progress: 54.2% ... Training loss: 0.073 ... Validation loss: 0.156iteration: 5422\n",
      "train_loss: 0.07322568430727874\n",
      "val_loss: 0.15688749611406477\n",
      "Progress: 54.2% ... Training loss: 0.071 ... Validation loss: 0.169iteration: 5423\n",
      "train_loss: 0.07100743051150547\n",
      "val_loss: 0.16917367298987446\n",
      "Progress: 54.2% ... Training loss: 0.067 ... Validation loss: 0.157iteration: 5424\n",
      "train_loss: 0.067501027110138\n",
      "val_loss: 0.1573577978839821\n",
      "Progress: 54.2% ... Training loss: 0.066 ... Validation loss: 0.166iteration: 5425\n",
      "train_loss: 0.06671726849688979\n",
      "val_loss: 0.16676284518665357\n",
      "Progress: 54.3% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 5426\n",
      "train_loss: 0.06624906052647868\n",
      "val_loss: 0.171054308439798\n",
      "Progress: 54.3% ... Training loss: 0.067 ... Validation loss: 0.162iteration: 5427\n",
      "train_loss: 0.06785244459536749\n",
      "val_loss: 0.1624090705634691\n",
      "Progress: 54.3% ... Training loss: 0.068 ... Validation loss: 0.180iteration: 5428\n",
      "train_loss: 0.06884229865251143\n",
      "val_loss: 0.1808195703820281\n",
      "Progress: 54.3% ... Training loss: 0.067 ... Validation loss: 0.157iteration: 5429\n",
      "train_loss: 0.06743981784981723\n",
      "val_loss: 0.15750131450670415\n",
      "Progress: 54.3% ... Training loss: 0.067 ... Validation loss: 0.171iteration: 5430\n",
      "train_loss: 0.06739876273461276\n",
      "val_loss: 0.17119907546013088\n",
      "Progress: 54.3% ... Training loss: 0.068 ... Validation loss: 0.154iteration: 5431\n",
      "train_loss: 0.06824289132320127\n",
      "val_loss: 0.15434641318744668\n",
      "Progress: 54.3% ... Training loss: 0.071 ... Validation loss: 0.181iteration: 5432\n",
      "train_loss: 0.0711152683780146\n",
      "val_loss: 0.18166120937904778\n",
      "Progress: 54.3% ... Training loss: 0.068 ... Validation loss: 0.167iteration: 5433\n",
      "train_loss: 0.0681515686853687\n",
      "val_loss: 0.16725619696770413\n",
      "Progress: 54.3% ... Training loss: 0.075 ... Validation loss: 0.197iteration: 5434\n",
      "train_loss: 0.07507738526622615\n",
      "val_loss: 0.19797375432179\n",
      "Progress: 54.4% ... Training loss: 0.079 ... Validation loss: 0.156iteration: 5435\n",
      "train_loss: 0.07922758515801538\n",
      "val_loss: 0.15669438740845204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 54.4% ... Training loss: 0.068 ... Validation loss: 0.182iteration: 5436\n",
      "train_loss: 0.0687666203882675\n",
      "val_loss: 0.182525017393126\n",
      "Progress: 54.4% ... Training loss: 0.067 ... Validation loss: 0.160iteration: 5437\n",
      "train_loss: 0.06758538557244\n",
      "val_loss: 0.16011575692257868\n",
      "Progress: 54.4% ... Training loss: 0.066 ... Validation loss: 0.164iteration: 5438\n",
      "train_loss: 0.06646952061866074\n",
      "val_loss: 0.1641286325444069\n",
      "Progress: 54.4% ... Training loss: 0.066 ... Validation loss: 0.167iteration: 5439\n",
      "train_loss: 0.06642107559307571\n",
      "val_loss: 0.16711405883716998\n",
      "Progress: 54.4% ... Training loss: 0.067 ... Validation loss: 0.170iteration: 5440\n",
      "train_loss: 0.06731428642660701\n",
      "val_loss: 0.1706748318457853\n",
      "Progress: 54.4% ... Training loss: 0.067 ... Validation loss: 0.161iteration: 5441\n",
      "train_loss: 0.0670051535001202\n",
      "val_loss: 0.16126762434730293\n",
      "Progress: 54.4% ... Training loss: 0.071 ... Validation loss: 0.174iteration: 5442\n",
      "train_loss: 0.0715458834157428\n",
      "val_loss: 0.1749581789569841\n",
      "Progress: 54.4% ... Training loss: 0.067 ... Validation loss: 0.152iteration: 5443\n",
      "train_loss: 0.06758028800714454\n",
      "val_loss: 0.1526267567562099\n",
      "Progress: 54.4% ... Training loss: 0.066 ... Validation loss: 0.160iteration: 5444\n",
      "train_loss: 0.06685321724267469\n",
      "val_loss: 0.16079359833623197\n",
      "Progress: 54.5% ... Training loss: 0.066 ... Validation loss: 0.166iteration: 5445\n",
      "train_loss: 0.06639624001601084\n",
      "val_loss: 0.16690163375087977\n",
      "Progress: 54.5% ... Training loss: 0.074 ... Validation loss: 0.149iteration: 5446\n",
      "train_loss: 0.07491219044311874\n",
      "val_loss: 0.1498735338543741\n",
      "Progress: 54.5% ... Training loss: 0.066 ... Validation loss: 0.173iteration: 5447\n",
      "train_loss: 0.06655222026018609\n",
      "val_loss: 0.17353496112392994\n",
      "Progress: 54.5% ... Training loss: 0.067 ... Validation loss: 0.167iteration: 5448\n",
      "train_loss: 0.06748760203798417\n",
      "val_loss: 0.1672298404098876\n",
      "Progress: 54.5% ... Training loss: 0.082 ... Validation loss: 0.217iteration: 5449\n",
      "train_loss: 0.08291955279881483\n",
      "val_loss: 0.21762447373548222\n",
      "Progress: 54.5% ... Training loss: 0.069 ... Validation loss: 0.152iteration: 5450\n",
      "train_loss: 0.06925169684929222\n",
      "val_loss: 0.15214096097046803\n",
      "Progress: 54.5% ... Training loss: 0.068 ... Validation loss: 0.183iteration: 5451\n",
      "train_loss: 0.06845355364268885\n",
      "val_loss: 0.18333773080183302\n",
      "Progress: 54.5% ... Training loss: 0.069 ... Validation loss: 0.166iteration: 5452\n",
      "train_loss: 0.06971580561944121\n",
      "val_loss: 0.16678500067662191\n",
      "Progress: 54.5% ... Training loss: 0.067 ... Validation loss: 0.181iteration: 5453\n",
      "train_loss: 0.06700388411141134\n",
      "val_loss: 0.18166489219609103\n",
      "Progress: 54.5% ... Training loss: 0.068 ... Validation loss: 0.172iteration: 5454\n",
      "train_loss: 0.06847632979562496\n",
      "val_loss: 0.172832802933012\n",
      "Progress: 54.5% ... Training loss: 0.066 ... Validation loss: 0.175iteration: 5455\n",
      "train_loss: 0.06639052010187349\n",
      "val_loss: 0.17573210122146912\n",
      "Progress: 54.6% ... Training loss: 0.069 ... Validation loss: 0.178iteration: 5456\n",
      "train_loss: 0.06940384181035604\n",
      "val_loss: 0.17823389947933185\n",
      "Progress: 54.6% ... Training loss: 0.078 ... Validation loss: 0.151iteration: 5457\n",
      "train_loss: 0.07828215197444882\n",
      "val_loss: 0.15139567418406394\n",
      "Progress: 54.6% ... Training loss: 0.091 ... Validation loss: 0.233iteration: 5458\n",
      "train_loss: 0.09131600971777845\n",
      "val_loss: 0.23373993406993007\n",
      "Progress: 54.6% ... Training loss: 0.123 ... Validation loss: 0.156iteration: 5459\n",
      "train_loss: 0.1235121617414017\n",
      "val_loss: 0.15687165956076957\n",
      "Progress: 54.6% ... Training loss: 0.149 ... Validation loss: 0.326iteration: 5460\n",
      "train_loss: 0.14912794102171745\n",
      "val_loss: 0.32652703334510363\n",
      "Progress: 54.6% ... Training loss: 0.107 ... Validation loss: 0.154iteration: 5461\n",
      "train_loss: 0.10723624543610481\n",
      "val_loss: 0.15407032244761049\n",
      "Progress: 54.6% ... Training loss: 0.080 ... Validation loss: 0.222iteration: 5462\n",
      "train_loss: 0.0801741556857036\n",
      "val_loss: 0.22213381146270617\n",
      "Progress: 54.6% ... Training loss: 0.071 ... Validation loss: 0.151iteration: 5463\n",
      "train_loss: 0.07191918422380636\n",
      "val_loss: 0.15194801981205114\n",
      "Progress: 54.6% ... Training loss: 0.071 ... Validation loss: 0.202iteration: 5464\n",
      "train_loss: 0.07109424654528916\n",
      "val_loss: 0.20214935469151513\n",
      "Progress: 54.6% ... Training loss: 0.088 ... Validation loss: 0.150iteration: 5465\n",
      "train_loss: 0.08802279786872982\n",
      "val_loss: 0.15040501869019848\n",
      "Progress: 54.7% ... Training loss: 0.076 ... Validation loss: 0.210iteration: 5466\n",
      "train_loss: 0.07617526031122551\n",
      "val_loss: 0.21016664998035636\n",
      "Progress: 54.7% ... Training loss: 0.067 ... Validation loss: 0.164iteration: 5467\n",
      "train_loss: 0.06757111857085303\n",
      "val_loss: 0.16490825835452025\n",
      "Progress: 54.7% ... Training loss: 0.067 ... Validation loss: 0.173iteration: 5468\n",
      "train_loss: 0.06791286139242336\n",
      "val_loss: 0.17332088153376207\n",
      "Progress: 54.7% ... Training loss: 0.067 ... Validation loss: 0.161iteration: 5469\n",
      "train_loss: 0.06770979797803438\n",
      "val_loss: 0.1610172311638589\n",
      "Progress: 54.7% ... Training loss: 0.066 ... Validation loss: 0.181iteration: 5470\n",
      "train_loss: 0.06683861494737102\n",
      "val_loss: 0.18153561277399383\n",
      "Progress: 54.7% ... Training loss: 0.066 ... Validation loss: 0.183iteration: 5471\n",
      "train_loss: 0.06641883348222477\n",
      "val_loss: 0.18317623153232446\n",
      "Progress: 54.7% ... Training loss: 0.067 ... Validation loss: 0.191iteration: 5472\n",
      "train_loss: 0.06705575044931085\n",
      "val_loss: 0.19102354027697935\n",
      "Progress: 54.7% ... Training loss: 0.067 ... Validation loss: 0.169iteration: 5473\n",
      "train_loss: 0.06763309819509354\n",
      "val_loss: 0.16972114759293958\n",
      "Progress: 54.7% ... Training loss: 0.066 ... Validation loss: 0.185iteration: 5474\n",
      "train_loss: 0.06646985330371029\n",
      "val_loss: 0.18543820902742772\n",
      "Progress: 54.8% ... Training loss: 0.076 ... Validation loss: 0.205iteration: 5475\n",
      "train_loss: 0.07633904824323652\n",
      "val_loss: 0.2056761311985221\n",
      "Progress: 54.8% ... Training loss: 0.083 ... Validation loss: 0.155iteration: 5476\n",
      "train_loss: 0.08378666911057268\n",
      "val_loss: 0.15520140314698538\n",
      "Progress: 54.8% ... Training loss: 0.067 ... Validation loss: 0.183iteration: 5477\n",
      "train_loss: 0.0672720681663668\n",
      "val_loss: 0.18301583849275352\n",
      "Progress: 54.8% ... Training loss: 0.070 ... Validation loss: 0.154iteration: 5478\n",
      "train_loss: 0.07011642508498525\n",
      "val_loss: 0.15470457440813798\n",
      "Progress: 54.8% ... Training loss: 0.076 ... Validation loss: 0.209iteration: 5479\n",
      "train_loss: 0.07630298798444447\n",
      "val_loss: 0.20923171598587173\n",
      "Progress: 54.8% ... Training loss: 0.075 ... Validation loss: 0.153iteration: 5480\n",
      "train_loss: 0.07585995467551912\n",
      "val_loss: 0.1534133147157461\n",
      "Progress: 54.8% ... Training loss: 0.069 ... Validation loss: 0.180iteration: 5481\n",
      "train_loss: 0.06961038984667428\n",
      "val_loss: 0.1804771706529391\n",
      "Progress: 54.8% ... Training loss: 0.076 ... Validation loss: 0.152iteration: 5482\n",
      "train_loss: 0.07645413709883868\n",
      "val_loss: 0.15299833286890954\n",
      "Progress: 54.8% ... Training loss: 0.071 ... Validation loss: 0.200iteration: 5483\n",
      "train_loss: 0.07141551065412309\n",
      "val_loss: 0.2008490415080291\n",
      "Progress: 54.8% ... Training loss: 0.075 ... Validation loss: 0.148iteration: 5484\n",
      "train_loss: 0.0759453129631675\n",
      "val_loss: 0.1488332652367255\n",
      "Progress: 54.9% ... Training loss: 0.069 ... Validation loss: 0.172iteration: 5485\n",
      "train_loss: 0.06951730370376565\n",
      "val_loss: 0.17267410097669042\n",
      "Progress: 54.9% ... Training loss: 0.068 ... Validation loss: 0.166iteration: 5486\n",
      "train_loss: 0.06893907302848085\n",
      "val_loss: 0.16636442277122432\n",
      "Progress: 54.9% ... Training loss: 0.072 ... Validation loss: 0.199iteration: 5487\n",
      "train_loss: 0.07292922473266678\n",
      "val_loss: 0.19900639172199394\n",
      "Progress: 54.9% ... Training loss: 0.067 ... Validation loss: 0.157iteration: 5488\n",
      "train_loss: 0.0673817526788148\n",
      "val_loss: 0.1571588739752636\n",
      "Progress: 54.9% ... Training loss: 0.067 ... Validation loss: 0.173iteration: 5489\n",
      "train_loss: 0.06748515290015195\n",
      "val_loss: 0.17336700986196324\n",
      "Progress: 54.9% ... Training loss: 0.068 ... Validation loss: 0.167iteration: 5490\n",
      "train_loss: 0.06866999862878406\n",
      "val_loss: 0.16788478659918585\n",
      "Progress: 54.9% ... Training loss: 0.067 ... Validation loss: 0.158iteration: 5491\n",
      "train_loss: 0.06742643084601554\n",
      "val_loss: 0.1585795331407743\n",
      "Progress: 54.9% ... Training loss: 0.067 ... Validation loss: 0.182iteration: 5492\n",
      "train_loss: 0.06765666546510307\n",
      "val_loss: 0.1825089018553465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 54.9% ... Training loss: 0.072 ... Validation loss: 0.157iteration: 5493\n",
      "train_loss: 0.07209379926870475\n",
      "val_loss: 0.15748540900100222\n",
      "Progress: 54.9% ... Training loss: 0.069 ... Validation loss: 0.179iteration: 5494\n",
      "train_loss: 0.06968040046368096\n",
      "val_loss: 0.1799315480409243\n",
      "Progress: 55.0% ... Training loss: 0.079 ... Validation loss: 0.155iteration: 5495\n",
      "train_loss: 0.07973641890011915\n",
      "val_loss: 0.15547603072246402\n",
      "Progress: 55.0% ... Training loss: 0.099 ... Validation loss: 0.249iteration: 5496\n",
      "train_loss: 0.09914862338016088\n",
      "val_loss: 0.2490042145449552\n",
      "Progress: 55.0% ... Training loss: 0.116 ... Validation loss: 0.161iteration: 5497\n",
      "train_loss: 0.11694056045386032\n",
      "val_loss: 0.161572907365051\n",
      "Progress: 55.0% ... Training loss: 0.079 ... Validation loss: 0.207iteration: 5498\n",
      "train_loss: 0.07906428897098065\n",
      "val_loss: 0.20740738247611798\n",
      "Progress: 55.0% ... Training loss: 0.066 ... Validation loss: 0.161iteration: 5499\n",
      "train_loss: 0.06690345101368911\n",
      "val_loss: 0.16165834867335496\n",
      "Progress: 55.0% ... Training loss: 0.068 ... Validation loss: 0.158iteration: 5500\n",
      "train_loss: 0.06835010261437935\n",
      "val_loss: 0.15871275584528896\n",
      "Progress: 55.0% ... Training loss: 0.066 ... Validation loss: 0.173iteration: 5501\n",
      "train_loss: 0.06645153108047003\n",
      "val_loss: 0.17303769024414398\n",
      "Progress: 55.0% ... Training loss: 0.070 ... Validation loss: 0.172iteration: 5502\n",
      "train_loss: 0.07051013318419949\n",
      "val_loss: 0.17236506340169855\n",
      "Progress: 55.0% ... Training loss: 0.066 ... Validation loss: 0.181iteration: 5503\n",
      "train_loss: 0.06695978543080218\n",
      "val_loss: 0.18145534654232406\n",
      "Progress: 55.0% ... Training loss: 0.069 ... Validation loss: 0.169iteration: 5504\n",
      "train_loss: 0.06934231493118005\n",
      "val_loss: 0.1696405243440029\n",
      "Progress: 55.0% ... Training loss: 0.071 ... Validation loss: 0.165iteration: 5505\n",
      "train_loss: 0.07164761407792869\n",
      "val_loss: 0.16529250134738044\n",
      "Progress: 55.1% ... Training loss: 0.074 ... Validation loss: 0.196iteration: 5506\n",
      "train_loss: 0.07486684708976203\n",
      "val_loss: 0.196419581454951\n",
      "Progress: 55.1% ... Training loss: 0.091 ... Validation loss: 0.155iteration: 5507\n",
      "train_loss: 0.09147665136944766\n",
      "val_loss: 0.1558443852374021\n",
      "Progress: 55.1% ... Training loss: 0.112 ... Validation loss: 0.251iteration: 5508\n",
      "train_loss: 0.11296890906080849\n",
      "val_loss: 0.2513439748971373\n",
      "Progress: 55.1% ... Training loss: 0.128 ... Validation loss: 0.168iteration: 5509\n",
      "train_loss: 0.12864278764755877\n",
      "val_loss: 0.16856860848469093\n",
      "Progress: 55.1% ... Training loss: 0.099 ... Validation loss: 0.259iteration: 5510\n",
      "train_loss: 0.09904880889353557\n",
      "val_loss: 0.2595666112345683\n",
      "Progress: 55.1% ... Training loss: 0.110 ... Validation loss: 0.160iteration: 5511\n",
      "train_loss: 0.11093174144973213\n",
      "val_loss: 0.16036791750654722\n",
      "Progress: 55.1% ... Training loss: 0.120 ... Validation loss: 0.226iteration: 5512\n",
      "train_loss: 0.12012836373721497\n",
      "val_loss: 0.22680633182405607\n",
      "Progress: 55.1% ... Training loss: 0.077 ... Validation loss: 0.150iteration: 5513\n",
      "train_loss: 0.0772125863497765\n",
      "val_loss: 0.15055917909496813\n",
      "Progress: 55.1% ... Training loss: 0.068 ... Validation loss: 0.163iteration: 5514\n",
      "train_loss: 0.06873640973110416\n",
      "val_loss: 0.1631270873229508\n",
      "Progress: 55.1% ... Training loss: 0.066 ... Validation loss: 0.158iteration: 5515\n",
      "train_loss: 0.06695498893162348\n",
      "val_loss: 0.1586004943498064\n",
      "Progress: 55.2% ... Training loss: 0.066 ... Validation loss: 0.159iteration: 5516\n",
      "train_loss: 0.06634849646897766\n",
      "val_loss: 0.15951771966311543\n",
      "Progress: 55.2% ... Training loss: 0.067 ... Validation loss: 0.171iteration: 5517\n",
      "train_loss: 0.0670332611057242\n",
      "val_loss: 0.17180451840408276\n",
      "Progress: 55.2% ... Training loss: 0.073 ... Validation loss: 0.149iteration: 5518\n",
      "train_loss: 0.07342653210605586\n",
      "val_loss: 0.1493527849837629\n",
      "Progress: 55.2% ... Training loss: 0.075 ... Validation loss: 0.209iteration: 5519\n",
      "train_loss: 0.07566934819415887\n",
      "val_loss: 0.20921642808496013\n",
      "Progress: 55.2% ... Training loss: 0.067 ... Validation loss: 0.166iteration: 5520\n",
      "train_loss: 0.06750455566598947\n",
      "val_loss: 0.16610694077686883\n",
      "Progress: 55.2% ... Training loss: 0.070 ... Validation loss: 0.190iteration: 5521\n",
      "train_loss: 0.07009200928482101\n",
      "val_loss: 0.19052863646325274\n",
      "Progress: 55.2% ... Training loss: 0.069 ... Validation loss: 0.156iteration: 5522\n",
      "train_loss: 0.0694352594694954\n",
      "val_loss: 0.1566201879169305\n",
      "Progress: 55.2% ... Training loss: 0.069 ... Validation loss: 0.190iteration: 5523\n",
      "train_loss: 0.06947683947354909\n",
      "val_loss: 0.1905439849897564\n",
      "Progress: 55.2% ... Training loss: 0.069 ... Validation loss: 0.169iteration: 5524\n",
      "train_loss: 0.06906868481548259\n",
      "val_loss: 0.16928054430825473\n",
      "Progress: 55.2% ... Training loss: 0.068 ... Validation loss: 0.195iteration: 5525\n",
      "train_loss: 0.06896410576635537\n",
      "val_loss: 0.19504734531819515\n",
      "Progress: 55.3% ... Training loss: 0.068 ... Validation loss: 0.159iteration: 5526\n",
      "train_loss: 0.06892200677247362\n",
      "val_loss: 0.15993962692318117\n",
      "Progress: 55.3% ... Training loss: 0.072 ... Validation loss: 0.192iteration: 5527\n",
      "train_loss: 0.07278946691166947\n",
      "val_loss: 0.19213746994737962\n",
      "Progress: 55.3% ... Training loss: 0.075 ... Validation loss: 0.150iteration: 5528\n",
      "train_loss: 0.075383431448228\n",
      "val_loss: 0.15049030129819568\n",
      "Progress: 55.3% ... Training loss: 0.069 ... Validation loss: 0.180iteration: 5529\n",
      "train_loss: 0.06924202406710331\n",
      "val_loss: 0.1801206642834745\n",
      "Progress: 55.3% ... Training loss: 0.070 ... Validation loss: 0.163iteration: 5530\n",
      "train_loss: 0.07049745506320772\n",
      "val_loss: 0.163702398669422\n",
      "Progress: 55.3% ... Training loss: 0.075 ... Validation loss: 0.218iteration: 5531\n",
      "train_loss: 0.0751580402229052\n",
      "val_loss: 0.218460021779492\n",
      "Progress: 55.3% ... Training loss: 0.084 ... Validation loss: 0.153iteration: 5532\n",
      "train_loss: 0.0843102725475471\n",
      "val_loss: 0.15397686602560057\n",
      "Progress: 55.3% ... Training loss: 0.078 ... Validation loss: 0.229iteration: 5533\n",
      "train_loss: 0.07854480593083989\n",
      "val_loss: 0.22976826065956257\n",
      "Progress: 55.3% ... Training loss: 0.068 ... Validation loss: 0.162iteration: 5534\n",
      "train_loss: 0.06831082964702652\n",
      "val_loss: 0.16243506834548027\n",
      "Progress: 55.4% ... Training loss: 0.066 ... Validation loss: 0.170iteration: 5535\n",
      "train_loss: 0.06665280143885964\n",
      "val_loss: 0.17088808600045455\n",
      "Progress: 55.4% ... Training loss: 0.067 ... Validation loss: 0.165iteration: 5536\n",
      "train_loss: 0.06790470912811215\n",
      "val_loss: 0.16581438177631158\n",
      "Progress: 55.4% ... Training loss: 0.067 ... Validation loss: 0.183iteration: 5537\n",
      "train_loss: 0.06745788397354514\n",
      "val_loss: 0.1831687069905346\n",
      "Progress: 55.4% ... Training loss: 0.071 ... Validation loss: 0.190iteration: 5538\n",
      "train_loss: 0.0713912604299942\n",
      "val_loss: 0.19081724020608004\n",
      "Progress: 55.4% ... Training loss: 0.070 ... Validation loss: 0.152iteration: 5539\n",
      "train_loss: 0.07024225513112169\n",
      "val_loss: 0.152489150224753\n",
      "Progress: 55.4% ... Training loss: 0.066 ... Validation loss: 0.178iteration: 5540\n",
      "train_loss: 0.06682129824919747\n",
      "val_loss: 0.17898376454155518\n",
      "Progress: 55.4% ... Training loss: 0.065 ... Validation loss: 0.173iteration: 5541\n",
      "train_loss: 0.0659209804825675\n",
      "val_loss: 0.1739437959828426\n",
      "Progress: 55.4% ... Training loss: 0.069 ... Validation loss: 0.155iteration: 5542\n",
      "train_loss: 0.06915030240432003\n",
      "val_loss: 0.15599085281586722\n",
      "Progress: 55.4% ... Training loss: 0.067 ... Validation loss: 0.159iteration: 5543\n",
      "train_loss: 0.06744512926452614\n",
      "val_loss: 0.15950531465659165\n",
      "Progress: 55.4% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 5544\n",
      "train_loss: 0.06650853872451405\n",
      "val_loss: 0.17153094160893528\n",
      "Progress: 55.5% ... Training loss: 0.067 ... Validation loss: 0.161iteration: 5545\n",
      "train_loss: 0.06770586476483274\n",
      "val_loss: 0.16151834121840528\n",
      "Progress: 55.5% ... Training loss: 0.067 ... Validation loss: 0.172iteration: 5546\n",
      "train_loss: 0.06733217003571508\n",
      "val_loss: 0.17202083775102145\n",
      "Progress: 55.5% ... Training loss: 0.066 ... Validation loss: 0.158iteration: 5547\n",
      "train_loss: 0.0669320716586925\n",
      "val_loss: 0.15868359317955447\n",
      "Progress: 55.5% ... Training loss: 0.072 ... Validation loss: 0.192iteration: 5548\n",
      "train_loss: 0.07283586045081013\n",
      "val_loss: 0.1922369037124082\n",
      "Progress: 55.5% ... Training loss: 0.072 ... Validation loss: 0.157iteration: 5549\n",
      "train_loss: 0.0722920120460273\n",
      "val_loss: 0.15736490533182557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 55.5% ... Training loss: 0.085 ... Validation loss: 0.216iteration: 5550\n",
      "train_loss: 0.0852244340934488\n",
      "val_loss: 0.21664071612084612\n",
      "Progress: 55.5% ... Training loss: 0.078 ... Validation loss: 0.152iteration: 5551\n",
      "train_loss: 0.07810437633251353\n",
      "val_loss: 0.15262376708505426\n",
      "Progress: 55.5% ... Training loss: 0.066 ... Validation loss: 0.185iteration: 5552\n",
      "train_loss: 0.06687862459503986\n",
      "val_loss: 0.18547864477993856\n",
      "Progress: 55.5% ... Training loss: 0.067 ... Validation loss: 0.165iteration: 5553\n",
      "train_loss: 0.06729050664523616\n",
      "val_loss: 0.16506477417797683\n",
      "Progress: 55.5% ... Training loss: 0.085 ... Validation loss: 0.214iteration: 5554\n",
      "train_loss: 0.0857385248832075\n",
      "val_loss: 0.21459352238526547\n",
      "Progress: 55.5% ... Training loss: 0.077 ... Validation loss: 0.155iteration: 5555\n",
      "train_loss: 0.0773627596931562\n",
      "val_loss: 0.15588195297993043\n",
      "Progress: 55.6% ... Training loss: 0.072 ... Validation loss: 0.199iteration: 5556\n",
      "train_loss: 0.07254915749007301\n",
      "val_loss: 0.19947091077729823\n",
      "Progress: 55.6% ... Training loss: 0.102 ... Validation loss: 0.151iteration: 5557\n",
      "train_loss: 0.10263774763024713\n",
      "val_loss: 0.1510313798110074\n",
      "Progress: 55.6% ... Training loss: 0.096 ... Validation loss: 0.249iteration: 5558\n",
      "train_loss: 0.09642665846263326\n",
      "val_loss: 0.24912335858433615\n",
      "Progress: 55.6% ... Training loss: 0.079 ... Validation loss: 0.148iteration: 5559\n",
      "train_loss: 0.07973978268362274\n",
      "val_loss: 0.14809132758539623\n",
      "Progress: 55.6% ... Training loss: 0.104 ... Validation loss: 0.250iteration: 5560\n",
      "train_loss: 0.1048395877830639\n",
      "val_loss: 0.2509833113227621\n",
      "Progress: 55.6% ... Training loss: 0.082 ... Validation loss: 0.151iteration: 5561\n",
      "train_loss: 0.08292271072622466\n",
      "val_loss: 0.1510690663438698\n",
      "Progress: 55.6% ... Training loss: 0.086 ... Validation loss: 0.230iteration: 5562\n",
      "train_loss: 0.08654883981310924\n",
      "val_loss: 0.23004649772679622\n",
      "Progress: 55.6% ... Training loss: 0.085 ... Validation loss: 0.147iteration: 5563\n",
      "train_loss: 0.08537207395153139\n",
      "val_loss: 0.1471337380120198\n",
      "Progress: 55.6% ... Training loss: 0.087 ... Validation loss: 0.227iteration: 5564\n",
      "train_loss: 0.08737630184728004\n",
      "val_loss: 0.2273248825691152\n",
      "Progress: 55.6% ... Training loss: 0.110 ... Validation loss: 0.155iteration: 5565\n",
      "train_loss: 0.11024953646599799\n",
      "val_loss: 0.15501810566236415\n",
      "Progress: 55.7% ... Training loss: 0.117 ... Validation loss: 0.306iteration: 5566\n",
      "train_loss: 0.1176791843220917\n",
      "val_loss: 0.30659504010252425\n",
      "Progress: 55.7% ... Training loss: 0.107 ... Validation loss: 0.151iteration: 5567\n",
      "train_loss: 0.10707702370730862\n",
      "val_loss: 0.15126137210023888\n",
      "Progress: 55.7% ... Training loss: 0.083 ... Validation loss: 0.222iteration: 5568\n",
      "train_loss: 0.08343589992711639\n",
      "val_loss: 0.22289594638924545\n",
      "Progress: 55.7% ... Training loss: 0.071 ... Validation loss: 0.155iteration: 5569\n",
      "train_loss: 0.07126786257685228\n",
      "val_loss: 0.1558040441144738\n",
      "Progress: 55.7% ... Training loss: 0.072 ... Validation loss: 0.201iteration: 5570\n",
      "train_loss: 0.07288869433004384\n",
      "val_loss: 0.20135665812451123\n",
      "Progress: 55.7% ... Training loss: 0.067 ... Validation loss: 0.177iteration: 5571\n",
      "train_loss: 0.06746837963200647\n",
      "val_loss: 0.1775124076643707\n",
      "Progress: 55.7% ... Training loss: 0.070 ... Validation loss: 0.188iteration: 5572\n",
      "train_loss: 0.07026862848355664\n",
      "val_loss: 0.18837220401473095\n",
      "Progress: 55.7% ... Training loss: 0.070 ... Validation loss: 0.159iteration: 5573\n",
      "train_loss: 0.07068764578535983\n",
      "val_loss: 0.1599208705023623\n",
      "Progress: 55.7% ... Training loss: 0.073 ... Validation loss: 0.225iteration: 5574\n",
      "train_loss: 0.0738238128544574\n",
      "val_loss: 0.2255584421955573\n",
      "Progress: 55.8% ... Training loss: 0.073 ... Validation loss: 0.162iteration: 5575\n",
      "train_loss: 0.07361646344062057\n",
      "val_loss: 0.16213364408485104\n",
      "Progress: 55.8% ... Training loss: 0.071 ... Validation loss: 0.202iteration: 5576\n",
      "train_loss: 0.071833113785551\n",
      "val_loss: 0.2020726658614975\n",
      "Progress: 55.8% ... Training loss: 0.071 ... Validation loss: 0.161iteration: 5577\n",
      "train_loss: 0.07156197122214679\n",
      "val_loss: 0.16113684100590026\n",
      "Progress: 55.8% ... Training loss: 0.068 ... Validation loss: 0.199iteration: 5578\n",
      "train_loss: 0.06857287887742304\n",
      "val_loss: 0.19971876382944898\n",
      "Progress: 55.8% ... Training loss: 0.069 ... Validation loss: 0.169iteration: 5579\n",
      "train_loss: 0.06907008963901816\n",
      "val_loss: 0.1693442633426362\n",
      "Progress: 55.8% ... Training loss: 0.078 ... Validation loss: 0.219iteration: 5580\n",
      "train_loss: 0.07811056122646978\n",
      "val_loss: 0.21929164449397506\n",
      "Progress: 55.8% ... Training loss: 0.072 ... Validation loss: 0.164iteration: 5581\n",
      "train_loss: 0.07279908965821345\n",
      "val_loss: 0.16429409025965264\n",
      "Progress: 55.8% ... Training loss: 0.076 ... Validation loss: 0.222iteration: 5582\n",
      "train_loss: 0.07675573188134066\n",
      "val_loss: 0.22222218486215872\n",
      "Progress: 55.8% ... Training loss: 0.069 ... Validation loss: 0.161iteration: 5583\n",
      "train_loss: 0.06932538216117229\n",
      "val_loss: 0.16195287868808927\n",
      "Progress: 55.8% ... Training loss: 0.067 ... Validation loss: 0.186iteration: 5584\n",
      "train_loss: 0.06730774308986945\n",
      "val_loss: 0.1866777492590317\n",
      "Progress: 55.9% ... Training loss: 0.067 ... Validation loss: 0.165iteration: 5585\n",
      "train_loss: 0.06758794018780515\n",
      "val_loss: 0.16504481754967965\n",
      "Progress: 55.9% ... Training loss: 0.075 ... Validation loss: 0.193iteration: 5586\n",
      "train_loss: 0.07542967771756721\n",
      "val_loss: 0.1936578011936539\n",
      "Progress: 55.9% ... Training loss: 0.066 ... Validation loss: 0.183iteration: 5587\n",
      "train_loss: 0.0663704664086745\n",
      "val_loss: 0.1836567165998151\n",
      "Progress: 55.9% ... Training loss: 0.065 ... Validation loss: 0.183iteration: 5588\n",
      "train_loss: 0.0657062460505884\n",
      "val_loss: 0.18370993148117098\n",
      "Progress: 55.9% ... Training loss: 0.066 ... Validation loss: 0.169iteration: 5589\n",
      "train_loss: 0.06644370465301604\n",
      "val_loss: 0.169391715910097\n",
      "Progress: 55.9% ... Training loss: 0.066 ... Validation loss: 0.182iteration: 5590\n",
      "train_loss: 0.06645141799913327\n",
      "val_loss: 0.18261206336559657\n",
      "Progress: 55.9% ... Training loss: 0.065 ... Validation loss: 0.181iteration: 5591\n",
      "train_loss: 0.06575322361315061\n",
      "val_loss: 0.18189530567003098\n",
      "Progress: 55.9% ... Training loss: 0.066 ... Validation loss: 0.182iteration: 5592\n",
      "train_loss: 0.06646085930619175\n",
      "val_loss: 0.1828165551750196\n",
      "Progress: 55.9% ... Training loss: 0.069 ... Validation loss: 0.163iteration: 5593\n",
      "train_loss: 0.06915783972661291\n",
      "val_loss: 0.16317004136389737\n",
      "Progress: 55.9% ... Training loss: 0.068 ... Validation loss: 0.193iteration: 5594\n",
      "train_loss: 0.06832105451185237\n",
      "val_loss: 0.19380128216606587\n",
      "Progress: 56.0% ... Training loss: 0.068 ... Validation loss: 0.158iteration: 5595\n",
      "train_loss: 0.06822818393982293\n",
      "val_loss: 0.1580212307240456\n",
      "Progress: 56.0% ... Training loss: 0.067 ... Validation loss: 0.191iteration: 5596\n",
      "train_loss: 0.06712491204776212\n",
      "val_loss: 0.19169215069355738\n",
      "Progress: 56.0% ... Training loss: 0.069 ... Validation loss: 0.162iteration: 5597\n",
      "train_loss: 0.06971093012303742\n",
      "val_loss: 0.16239559273382548\n",
      "Progress: 56.0% ... Training loss: 0.066 ... Validation loss: 0.172iteration: 5598\n",
      "train_loss: 0.06635529617385322\n",
      "val_loss: 0.1725933471115839\n",
      "Progress: 56.0% ... Training loss: 0.065 ... Validation loss: 0.174iteration: 5599\n",
      "train_loss: 0.0656317949959495\n",
      "val_loss: 0.1745794512230062\n",
      "Progress: 56.0% ... Training loss: 0.065 ... Validation loss: 0.164iteration: 5600\n",
      "train_loss: 0.065804620883197\n",
      "val_loss: 0.16492844556711392\n",
      "Progress: 56.0% ... Training loss: 0.069 ... Validation loss: 0.163iteration: 5601\n",
      "train_loss: 0.06916160718643713\n",
      "val_loss: 0.16392305374962757\n",
      "Progress: 56.0% ... Training loss: 0.070 ... Validation loss: 0.185iteration: 5602\n",
      "train_loss: 0.07039674884585286\n",
      "val_loss: 0.18558644610133754\n",
      "Progress: 56.0% ... Training loss: 0.071 ... Validation loss: 0.166iteration: 5603\n",
      "train_loss: 0.07144078372846982\n",
      "val_loss: 0.16629363669051062\n",
      "Progress: 56.0% ... Training loss: 0.068 ... Validation loss: 0.208iteration: 5604\n",
      "train_loss: 0.06897802129688783\n",
      "val_loss: 0.20806258841879222\n",
      "Progress: 56.0% ... Training loss: 0.065 ... Validation loss: 0.183iteration: 5605\n",
      "train_loss: 0.06546339323973653\n",
      "val_loss: 0.18359074506265816\n",
      "Progress: 56.1% ... Training loss: 0.065 ... Validation loss: 0.182iteration: 5606\n",
      "train_loss: 0.06589642149772842\n",
      "val_loss: 0.1820634401427164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 56.1% ... Training loss: 0.068 ... Validation loss: 0.162iteration: 5607\n",
      "train_loss: 0.06826029202793446\n",
      "val_loss: 0.16298244537997772\n",
      "Progress: 56.1% ... Training loss: 0.066 ... Validation loss: 0.181iteration: 5608\n",
      "train_loss: 0.06697795284405936\n",
      "val_loss: 0.18173641713902763\n",
      "Progress: 56.1% ... Training loss: 0.066 ... Validation loss: 0.165iteration: 5609\n",
      "train_loss: 0.06695108452286676\n",
      "val_loss: 0.16598482876248483\n",
      "Progress: 56.1% ... Training loss: 0.067 ... Validation loss: 0.188iteration: 5610\n",
      "train_loss: 0.06743117342950726\n",
      "val_loss: 0.1882490257788176\n",
      "Progress: 56.1% ... Training loss: 0.073 ... Validation loss: 0.160iteration: 5611\n",
      "train_loss: 0.07340251222676702\n",
      "val_loss: 0.16075580252350793\n",
      "Progress: 56.1% ... Training loss: 0.084 ... Validation loss: 0.217iteration: 5612\n",
      "train_loss: 0.0840019585351402\n",
      "val_loss: 0.21738068230763205\n",
      "Progress: 56.1% ... Training loss: 0.107 ... Validation loss: 0.159iteration: 5613\n",
      "train_loss: 0.1070467105558606\n",
      "val_loss: 0.15910415252045126\n",
      "Progress: 56.1% ... Training loss: 0.085 ... Validation loss: 0.238iteration: 5614\n",
      "train_loss: 0.0851152806197357\n",
      "val_loss: 0.2380663562977603\n",
      "Progress: 56.1% ... Training loss: 0.078 ... Validation loss: 0.156iteration: 5615\n",
      "train_loss: 0.07865504029613304\n",
      "val_loss: 0.15645994199166458\n",
      "Progress: 56.2% ... Training loss: 0.078 ... Validation loss: 0.233iteration: 5616\n",
      "train_loss: 0.07892427604768865\n",
      "val_loss: 0.23331603009652827\n",
      "Progress: 56.2% ... Training loss: 0.092 ... Validation loss: 0.154iteration: 5617\n",
      "train_loss: 0.09227987862549625\n",
      "val_loss: 0.15429339649776277\n",
      "Progress: 56.2% ... Training loss: 0.090 ... Validation loss: 0.242iteration: 5618\n",
      "train_loss: 0.09028254870696878\n",
      "val_loss: 0.2423103033541954\n",
      "Progress: 56.2% ... Training loss: 0.084 ... Validation loss: 0.157iteration: 5619\n",
      "train_loss: 0.08464304031340367\n",
      "val_loss: 0.15788487195119033\n",
      "Progress: 56.2% ... Training loss: 0.077 ... Validation loss: 0.215iteration: 5620\n",
      "train_loss: 0.07700113389398257\n",
      "val_loss: 0.21525164412377498\n",
      "Progress: 56.2% ... Training loss: 0.078 ... Validation loss: 0.159iteration: 5621\n",
      "train_loss: 0.07834332981870111\n",
      "val_loss: 0.15917534100378403\n",
      "Progress: 56.2% ... Training loss: 0.074 ... Validation loss: 0.213iteration: 5622\n",
      "train_loss: 0.07437961130648778\n",
      "val_loss: 0.21399560110226679\n",
      "Progress: 56.2% ... Training loss: 0.084 ... Validation loss: 0.156iteration: 5623\n",
      "train_loss: 0.08474168521139001\n",
      "val_loss: 0.15654806171459448\n",
      "Progress: 56.2% ... Training loss: 0.108 ... Validation loss: 0.269iteration: 5624\n",
      "train_loss: 0.1084217414863816\n",
      "val_loss: 0.2698355606817477\n",
      "Progress: 56.2% ... Training loss: 0.093 ... Validation loss: 0.156iteration: 5625\n",
      "train_loss: 0.09336354222803118\n",
      "val_loss: 0.15678294718803884\n",
      "Progress: 56.3% ... Training loss: 0.091 ... Validation loss: 0.249iteration: 5626\n",
      "train_loss: 0.09119862718150612\n",
      "val_loss: 0.24963067314192836\n",
      "Progress: 56.3% ... Training loss: 0.085 ... Validation loss: 0.160iteration: 5627\n",
      "train_loss: 0.08538683333398935\n",
      "val_loss: 0.16032147013132847\n",
      "Progress: 56.3% ... Training loss: 0.091 ... Validation loss: 0.237iteration: 5628\n",
      "train_loss: 0.09130977936740156\n",
      "val_loss: 0.23794878600814393\n",
      "Progress: 56.3% ... Training loss: 0.123 ... Validation loss: 0.157iteration: 5629\n",
      "train_loss: 0.12373010185037514\n",
      "val_loss: 0.15723246156421042\n",
      "Progress: 56.3% ... Training loss: 0.087 ... Validation loss: 0.263iteration: 5630\n",
      "train_loss: 0.08759186751544899\n",
      "val_loss: 0.2635512213972154\n",
      "Progress: 56.3% ... Training loss: 0.077 ... Validation loss: 0.157iteration: 5631\n",
      "train_loss: 0.0771973199770907\n",
      "val_loss: 0.15733671470696386\n",
      "Progress: 56.3% ... Training loss: 0.070 ... Validation loss: 0.198iteration: 5632\n",
      "train_loss: 0.07070754843891831\n",
      "val_loss: 0.1987063251801449\n",
      "Progress: 56.3% ... Training loss: 0.071 ... Validation loss: 0.155iteration: 5633\n",
      "train_loss: 0.07189785700409063\n",
      "val_loss: 0.15509601041392773\n",
      "Progress: 56.3% ... Training loss: 0.066 ... Validation loss: 0.180iteration: 5634\n",
      "train_loss: 0.06645621771241471\n",
      "val_loss: 0.1806153684015981\n",
      "Progress: 56.4% ... Training loss: 0.067 ... Validation loss: 0.158iteration: 5635\n",
      "train_loss: 0.06765432102261495\n",
      "val_loss: 0.15885999390556538\n",
      "Progress: 56.4% ... Training loss: 0.068 ... Validation loss: 0.159iteration: 5636\n",
      "train_loss: 0.06805940039232702\n",
      "val_loss: 0.1592807845273929\n",
      "Progress: 56.4% ... Training loss: 0.071 ... Validation loss: 0.205iteration: 5637\n",
      "train_loss: 0.07100143608441166\n",
      "val_loss: 0.2050004260483926\n",
      "Progress: 56.4% ... Training loss: 0.065 ... Validation loss: 0.171iteration: 5638\n",
      "train_loss: 0.0659491877822991\n",
      "val_loss: 0.17192131150823897\n",
      "Progress: 56.4% ... Training loss: 0.066 ... Validation loss: 0.167iteration: 5639\n",
      "train_loss: 0.06639732829635428\n",
      "val_loss: 0.16721001864496274\n",
      "Progress: 56.4% ... Training loss: 0.066 ... Validation loss: 0.173iteration: 5640\n",
      "train_loss: 0.06638078520922487\n",
      "val_loss: 0.17339494391465218\n",
      "Progress: 56.4% ... Training loss: 0.065 ... Validation loss: 0.166iteration: 5641\n",
      "train_loss: 0.06578125159526077\n",
      "val_loss: 0.1669436640360085\n",
      "Progress: 56.4% ... Training loss: 0.070 ... Validation loss: 0.153iteration: 5642\n",
      "train_loss: 0.0701265106378756\n",
      "val_loss: 0.15366025602573358\n",
      "Progress: 56.4% ... Training loss: 0.067 ... Validation loss: 0.187iteration: 5643\n",
      "train_loss: 0.06785056499516144\n",
      "val_loss: 0.18720916366651427\n",
      "Progress: 56.4% ... Training loss: 0.066 ... Validation loss: 0.174iteration: 5644\n",
      "train_loss: 0.06668982885424725\n",
      "val_loss: 0.17436429699805767\n",
      "Progress: 56.5% ... Training loss: 0.066 ... Validation loss: 0.160iteration: 5645\n",
      "train_loss: 0.0668691186603366\n",
      "val_loss: 0.16078839907506673\n",
      "Progress: 56.5% ... Training loss: 0.068 ... Validation loss: 0.174iteration: 5646\n",
      "train_loss: 0.06836795083522514\n",
      "val_loss: 0.17409356015056568\n",
      "Progress: 56.5% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 5647\n",
      "train_loss: 0.06570103365194994\n",
      "val_loss: 0.15873552733498494\n",
      "Progress: 56.5% ... Training loss: 0.066 ... Validation loss: 0.172iteration: 5648\n",
      "train_loss: 0.06602596028120483\n",
      "val_loss: 0.1728571195513167\n",
      "Progress: 56.5% ... Training loss: 0.065 ... Validation loss: 0.163iteration: 5649\n",
      "train_loss: 0.06592125883290595\n",
      "val_loss: 0.1633034171155297\n",
      "Progress: 56.5% ... Training loss: 0.066 ... Validation loss: 0.168iteration: 5650\n",
      "train_loss: 0.06672386168145852\n",
      "val_loss: 0.16873878040126106\n",
      "Progress: 56.5% ... Training loss: 0.073 ... Validation loss: 0.148iteration: 5651\n",
      "train_loss: 0.07354224982225036\n",
      "val_loss: 0.14849152453630204\n",
      "Progress: 56.5% ... Training loss: 0.102 ... Validation loss: 0.234iteration: 5652\n",
      "train_loss: 0.10264954386389612\n",
      "val_loss: 0.23444153342308516\n",
      "Progress: 56.5% ... Training loss: 0.103 ... Validation loss: 0.152iteration: 5653\n",
      "train_loss: 0.10315858566728471\n",
      "val_loss: 0.15236617728912683\n",
      "Progress: 56.5% ... Training loss: 0.080 ... Validation loss: 0.204iteration: 5654\n",
      "train_loss: 0.08025356503355333\n",
      "val_loss: 0.20466719424724408\n",
      "Progress: 56.5% ... Training loss: 0.073 ... Validation loss: 0.147iteration: 5655\n",
      "train_loss: 0.07398872652093759\n",
      "val_loss: 0.14760283598466467\n",
      "Progress: 56.6% ... Training loss: 0.068 ... Validation loss: 0.182iteration: 5656\n",
      "train_loss: 0.06858075513439818\n",
      "val_loss: 0.18298550559023788\n",
      "Progress: 56.6% ... Training loss: 0.066 ... Validation loss: 0.163iteration: 5657\n",
      "train_loss: 0.06615265483566292\n",
      "val_loss: 0.16341962651751313\n",
      "Progress: 56.6% ... Training loss: 0.066 ... Validation loss: 0.174iteration: 5658\n",
      "train_loss: 0.06609288144255109\n",
      "val_loss: 0.17485166180815018\n",
      "Progress: 56.6% ... Training loss: 0.066 ... Validation loss: 0.178iteration: 5659\n",
      "train_loss: 0.066891431266925\n",
      "val_loss: 0.178010909025872\n",
      "Progress: 56.6% ... Training loss: 0.066 ... Validation loss: 0.164iteration: 5660\n",
      "train_loss: 0.06691411847356891\n",
      "val_loss: 0.16462055786863442\n",
      "Progress: 56.6% ... Training loss: 0.078 ... Validation loss: 0.148iteration: 5661\n",
      "train_loss: 0.07857354945437443\n",
      "val_loss: 0.1485846878614786\n",
      "Progress: 56.6% ... Training loss: 0.072 ... Validation loss: 0.204iteration: 5662\n",
      "train_loss: 0.07203766349876631\n",
      "val_loss: 0.2044657363865743\n",
      "Progress: 56.6% ... Training loss: 0.068 ... Validation loss: 0.156iteration: 5663\n",
      "train_loss: 0.06896935385630622\n",
      "val_loss: 0.1561171752934403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 56.6% ... Training loss: 0.069 ... Validation loss: 0.185iteration: 5664\n",
      "train_loss: 0.06919433337828419\n",
      "val_loss: 0.18572431153122362\n",
      "Progress: 56.6% ... Training loss: 0.066 ... Validation loss: 0.156iteration: 5665\n",
      "train_loss: 0.06620122321088569\n",
      "val_loss: 0.15691411726800647\n",
      "Progress: 56.7% ... Training loss: 0.065 ... Validation loss: 0.163iteration: 5666\n",
      "train_loss: 0.06536741782565328\n",
      "val_loss: 0.1634280508190019\n",
      "Progress: 56.7% ... Training loss: 0.066 ... Validation loss: 0.161iteration: 5667\n",
      "train_loss: 0.06675231726270191\n",
      "val_loss: 0.16140127181429784\n",
      "Progress: 56.7% ... Training loss: 0.070 ... Validation loss: 0.189iteration: 5668\n",
      "train_loss: 0.07090708483757595\n",
      "val_loss: 0.18980679769311404\n",
      "Progress: 56.7% ... Training loss: 0.067 ... Validation loss: 0.152iteration: 5669\n",
      "train_loss: 0.06778491779192615\n",
      "val_loss: 0.15260468130078986\n",
      "Progress: 56.7% ... Training loss: 0.065 ... Validation loss: 0.163iteration: 5670\n",
      "train_loss: 0.06593426966549056\n",
      "val_loss: 0.1630132334441253\n",
      "Progress: 56.7% ... Training loss: 0.066 ... Validation loss: 0.151iteration: 5671\n",
      "train_loss: 0.06665776324767408\n",
      "val_loss: 0.15196851914852458\n",
      "Progress: 56.7% ... Training loss: 0.066 ... Validation loss: 0.168iteration: 5672\n",
      "train_loss: 0.0667980639722157\n",
      "val_loss: 0.16803300599207094\n",
      "Progress: 56.7% ... Training loss: 0.065 ... Validation loss: 0.167iteration: 5673\n",
      "train_loss: 0.06565953379187678\n",
      "val_loss: 0.1672889388265367\n",
      "Progress: 56.7% ... Training loss: 0.066 ... Validation loss: 0.173iteration: 5674\n",
      "train_loss: 0.06681640156320968\n",
      "val_loss: 0.17379820081980593\n",
      "Progress: 56.8% ... Training loss: 0.073 ... Validation loss: 0.152iteration: 5675\n",
      "train_loss: 0.0730373187944598\n",
      "val_loss: 0.1527129541653052\n",
      "Progress: 56.8% ... Training loss: 0.075 ... Validation loss: 0.195iteration: 5676\n",
      "train_loss: 0.07594081110113056\n",
      "val_loss: 0.19579003790580268\n",
      "Progress: 56.8% ... Training loss: 0.072 ... Validation loss: 0.150iteration: 5677\n",
      "train_loss: 0.07281732691550968\n",
      "val_loss: 0.15059685512331608\n",
      "Progress: 56.8% ... Training loss: 0.072 ... Validation loss: 0.195iteration: 5678\n",
      "train_loss: 0.07232097180452422\n",
      "val_loss: 0.19573665511331645\n",
      "Progress: 56.8% ... Training loss: 0.074 ... Validation loss: 0.147iteration: 5679\n",
      "train_loss: 0.07440631167588446\n",
      "val_loss: 0.14747752149213433\n",
      "Progress: 56.8% ... Training loss: 0.082 ... Validation loss: 0.208iteration: 5680\n",
      "train_loss: 0.08234164795594705\n",
      "val_loss: 0.20881384984470464\n",
      "Progress: 56.8% ... Training loss: 0.087 ... Validation loss: 0.149iteration: 5681\n",
      "train_loss: 0.0879987495789397\n",
      "val_loss: 0.14949001842243495\n",
      "Progress: 56.8% ... Training loss: 0.073 ... Validation loss: 0.193iteration: 5682\n",
      "train_loss: 0.07346972838789624\n",
      "val_loss: 0.1934059786458142\n",
      "Progress: 56.8% ... Training loss: 0.072 ... Validation loss: 0.148iteration: 5683\n",
      "train_loss: 0.07257528109461363\n",
      "val_loss: 0.14806724715496322\n",
      "Progress: 56.8% ... Training loss: 0.075 ... Validation loss: 0.198iteration: 5684\n",
      "train_loss: 0.07519987580096459\n",
      "val_loss: 0.1989288004040376\n",
      "Progress: 56.9% ... Training loss: 0.074 ... Validation loss: 0.149iteration: 5685\n",
      "train_loss: 0.07447936205502532\n",
      "val_loss: 0.14929332004671397\n",
      "Progress: 56.9% ... Training loss: 0.069 ... Validation loss: 0.186iteration: 5686\n",
      "train_loss: 0.06934334428107188\n",
      "val_loss: 0.18644253267756813\n",
      "Progress: 56.9% ... Training loss: 0.068 ... Validation loss: 0.152iteration: 5687\n",
      "train_loss: 0.0688436050292535\n",
      "val_loss: 0.15257312105612816\n",
      "Progress: 56.9% ... Training loss: 0.070 ... Validation loss: 0.178iteration: 5688\n",
      "train_loss: 0.07014376525165221\n",
      "val_loss: 0.17893812162104325\n",
      "Progress: 56.9% ... Training loss: 0.066 ... Validation loss: 0.156iteration: 5689\n",
      "train_loss: 0.06683947808924337\n",
      "val_loss: 0.156324226314796\n",
      "Progress: 56.9% ... Training loss: 0.069 ... Validation loss: 0.191iteration: 5690\n",
      "train_loss: 0.06903703477003852\n",
      "val_loss: 0.19123478499237598\n",
      "Progress: 56.9% ... Training loss: 0.076 ... Validation loss: 0.153iteration: 5691\n",
      "train_loss: 0.07608286582186438\n",
      "val_loss: 0.15328646165982596\n",
      "Progress: 56.9% ... Training loss: 0.082 ... Validation loss: 0.208iteration: 5692\n",
      "train_loss: 0.08266206957144476\n",
      "val_loss: 0.20871208325544124\n",
      "Progress: 56.9% ... Training loss: 0.069 ... Validation loss: 0.158iteration: 5693\n",
      "train_loss: 0.06942414387561087\n",
      "val_loss: 0.15898700414962683\n",
      "Progress: 56.9% ... Training loss: 0.067 ... Validation loss: 0.187iteration: 5694\n",
      "train_loss: 0.0678543023835252\n",
      "val_loss: 0.18762643566380208\n",
      "Progress: 57.0% ... Training loss: 0.065 ... Validation loss: 0.161iteration: 5695\n",
      "train_loss: 0.06571576829080589\n",
      "val_loss: 0.16129052817518785\n",
      "Progress: 57.0% ... Training loss: 0.068 ... Validation loss: 0.156iteration: 5696\n",
      "train_loss: 0.06819376317548449\n",
      "val_loss: 0.15669683436688533\n",
      "Progress: 57.0% ... Training loss: 0.074 ... Validation loss: 0.182iteration: 5697\n",
      "train_loss: 0.0745181004227944\n",
      "val_loss: 0.18299041376885544\n",
      "Progress: 57.0% ... Training loss: 0.067 ... Validation loss: 0.160iteration: 5698\n",
      "train_loss: 0.06787867012091245\n",
      "val_loss: 0.1609898174544096\n",
      "Progress: 57.0% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 5699\n",
      "train_loss: 0.06622251032228973\n",
      "val_loss: 0.17166803600106587\n",
      "Progress: 57.0% ... Training loss: 0.067 ... Validation loss: 0.158iteration: 5700\n",
      "train_loss: 0.0672785777378974\n",
      "val_loss: 0.15806690509531657\n",
      "Progress: 57.0% ... Training loss: 0.091 ... Validation loss: 0.213iteration: 5701\n",
      "train_loss: 0.0918401701892073\n",
      "val_loss: 0.21379899687566192\n",
      "Progress: 57.0% ... Training loss: 0.093 ... Validation loss: 0.155iteration: 5702\n",
      "train_loss: 0.09304854410835936\n",
      "val_loss: 0.15593286020209918\n",
      "Progress: 57.0% ... Training loss: 0.081 ... Validation loss: 0.223iteration: 5703\n",
      "train_loss: 0.08181695022034653\n",
      "val_loss: 0.2230882415335843\n",
      "Progress: 57.0% ... Training loss: 0.076 ... Validation loss: 0.154iteration: 5704\n",
      "train_loss: 0.07675062144546929\n",
      "val_loss: 0.15453211584132168\n",
      "Progress: 57.0% ... Training loss: 0.066 ... Validation loss: 0.177iteration: 5705\n",
      "train_loss: 0.06686487758970769\n",
      "val_loss: 0.17767529845074684\n",
      "Progress: 57.1% ... Training loss: 0.066 ... Validation loss: 0.178iteration: 5706\n",
      "train_loss: 0.06606785569557592\n",
      "val_loss: 0.17881592087556908\n",
      "Progress: 57.1% ... Training loss: 0.070 ... Validation loss: 0.185iteration: 5707\n",
      "train_loss: 0.07077042331913708\n",
      "val_loss: 0.18567396703162883\n",
      "Progress: 57.1% ... Training loss: 0.068 ... Validation loss: 0.156iteration: 5708\n",
      "train_loss: 0.06873314050382284\n",
      "val_loss: 0.1568023082449641\n",
      "Progress: 57.1% ... Training loss: 0.074 ... Validation loss: 0.202iteration: 5709\n",
      "train_loss: 0.07496016730232921\n",
      "val_loss: 0.20295492496920767\n",
      "Progress: 57.1% ... Training loss: 0.084 ... Validation loss: 0.155iteration: 5710\n",
      "train_loss: 0.08482948667547982\n",
      "val_loss: 0.15572478899826975\n",
      "Progress: 57.1% ... Training loss: 0.079 ... Validation loss: 0.213iteration: 5711\n",
      "train_loss: 0.07934749051666175\n",
      "val_loss: 0.21378736166672077\n",
      "Progress: 57.1% ... Training loss: 0.075 ... Validation loss: 0.155iteration: 5712\n",
      "train_loss: 0.07507290424146533\n",
      "val_loss: 0.15583500992542543\n",
      "Progress: 57.1% ... Training loss: 0.071 ... Validation loss: 0.193iteration: 5713\n",
      "train_loss: 0.0717603348235088\n",
      "val_loss: 0.19342661781715742\n",
      "Progress: 57.1% ... Training loss: 0.090 ... Validation loss: 0.155iteration: 5714\n",
      "train_loss: 0.09093092770661955\n",
      "val_loss: 0.15542190707810155\n",
      "Progress: 57.1% ... Training loss: 0.073 ... Validation loss: 0.193iteration: 5715\n",
      "train_loss: 0.07313440522871753\n",
      "val_loss: 0.1935976913129036\n",
      "Progress: 57.2% ... Training loss: 0.067 ... Validation loss: 0.158iteration: 5716\n",
      "train_loss: 0.06743450023621714\n",
      "val_loss: 0.15822272262032536\n",
      "Progress: 57.2% ... Training loss: 0.065 ... Validation loss: 0.175iteration: 5717\n",
      "train_loss: 0.06562108137871825\n",
      "val_loss: 0.17506431783548804\n",
      "Progress: 57.2% ... Training loss: 0.067 ... Validation loss: 0.157iteration: 5718\n",
      "train_loss: 0.06761234902397516\n",
      "val_loss: 0.15708882688121487\n",
      "Progress: 57.2% ... Training loss: 0.067 ... Validation loss: 0.176iteration: 5719\n",
      "train_loss: 0.0677718045382216\n",
      "val_loss: 0.1769282865590124\n",
      "Progress: 57.2% ... Training loss: 0.066 ... Validation loss: 0.154iteration: 5720\n",
      "train_loss: 0.06636590192088204\n",
      "val_loss: 0.15465618057053174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 57.2% ... Training loss: 0.067 ... Validation loss: 0.177iteration: 5721\n",
      "train_loss: 0.06730836937192015\n",
      "val_loss: 0.17761757684988982\n",
      "Progress: 57.2% ... Training loss: 0.065 ... Validation loss: 0.159iteration: 5722\n",
      "train_loss: 0.06597365562111128\n",
      "val_loss: 0.15918540674115403\n",
      "Progress: 57.2% ... Training loss: 0.068 ... Validation loss: 0.185iteration: 5723\n",
      "train_loss: 0.068130832506744\n",
      "val_loss: 0.1853212459863591\n",
      "Progress: 57.2% ... Training loss: 0.066 ... Validation loss: 0.170iteration: 5724\n",
      "train_loss: 0.06683170759045823\n",
      "val_loss: 0.1704269969813703\n",
      "Progress: 57.2% ... Training loss: 0.067 ... Validation loss: 0.167iteration: 5725\n",
      "train_loss: 0.06737670666498989\n",
      "val_loss: 0.1671668279268596\n",
      "Progress: 57.3% ... Training loss: 0.065 ... Validation loss: 0.161iteration: 5726\n",
      "train_loss: 0.06567454214935374\n",
      "val_loss: 0.16118206098019894\n",
      "Progress: 57.3% ... Training loss: 0.067 ... Validation loss: 0.179iteration: 5727\n",
      "train_loss: 0.06706826730136381\n",
      "val_loss: 0.17950250778196752\n",
      "Progress: 57.3% ... Training loss: 0.067 ... Validation loss: 0.156iteration: 5728\n",
      "train_loss: 0.06783496484739962\n",
      "val_loss: 0.15626640593390104\n",
      "Progress: 57.3% ... Training loss: 0.068 ... Validation loss: 0.185iteration: 5729\n",
      "train_loss: 0.06809211735897301\n",
      "val_loss: 0.1856143905731428\n",
      "Progress: 57.3% ... Training loss: 0.071 ... Validation loss: 0.162iteration: 5730\n",
      "train_loss: 0.07140238723295746\n",
      "val_loss: 0.1620389890870562\n",
      "Progress: 57.3% ... Training loss: 0.070 ... Validation loss: 0.192iteration: 5731\n",
      "train_loss: 0.07087236860713203\n",
      "val_loss: 0.19296954124139734\n",
      "Progress: 57.3% ... Training loss: 0.067 ... Validation loss: 0.160iteration: 5732\n",
      "train_loss: 0.06711366587093734\n",
      "val_loss: 0.16053973240480093\n",
      "Progress: 57.3% ... Training loss: 0.066 ... Validation loss: 0.179iteration: 5733\n",
      "train_loss: 0.06607721281443277\n",
      "val_loss: 0.17977940039417384\n",
      "Progress: 57.3% ... Training loss: 0.066 ... Validation loss: 0.160iteration: 5734\n",
      "train_loss: 0.06671635588186137\n",
      "val_loss: 0.16053318600250174\n",
      "Progress: 57.4% ... Training loss: 0.065 ... Validation loss: 0.178iteration: 5735\n",
      "train_loss: 0.0656691043714384\n",
      "val_loss: 0.17828039383865874\n",
      "Progress: 57.4% ... Training loss: 0.068 ... Validation loss: 0.154iteration: 5736\n",
      "train_loss: 0.06896837356608367\n",
      "val_loss: 0.1541340365254812\n",
      "Progress: 57.4% ... Training loss: 0.069 ... Validation loss: 0.192iteration: 5737\n",
      "train_loss: 0.06953209910219074\n",
      "val_loss: 0.19244534796724078\n",
      "Progress: 57.4% ... Training loss: 0.077 ... Validation loss: 0.155iteration: 5738\n",
      "train_loss: 0.07726607744856281\n",
      "val_loss: 0.15535117778486932\n",
      "Progress: 57.4% ... Training loss: 0.076 ... Validation loss: 0.207iteration: 5739\n",
      "train_loss: 0.07670392472967405\n",
      "val_loss: 0.20783319606413528\n",
      "Progress: 57.4% ... Training loss: 0.077 ... Validation loss: 0.163iteration: 5740\n",
      "train_loss: 0.0775154164612023\n",
      "val_loss: 0.1631929333026916\n",
      "Progress: 57.4% ... Training loss: 0.082 ... Validation loss: 0.219iteration: 5741\n",
      "train_loss: 0.08255838168733787\n",
      "val_loss: 0.21901920386339593\n",
      "Progress: 57.4% ... Training loss: 0.080 ... Validation loss: 0.162iteration: 5742\n",
      "train_loss: 0.08077078208433067\n",
      "val_loss: 0.16241478241014196\n",
      "Progress: 57.4% ... Training loss: 0.070 ... Validation loss: 0.207iteration: 5743\n",
      "train_loss: 0.07046732159926007\n",
      "val_loss: 0.20710976342830206\n",
      "Progress: 57.4% ... Training loss: 0.069 ... Validation loss: 0.194iteration: 5744\n",
      "train_loss: 0.06993241056502526\n",
      "val_loss: 0.19423447376138953\n",
      "Progress: 57.5% ... Training loss: 0.068 ... Validation loss: 0.195iteration: 5745\n",
      "train_loss: 0.06846883056304799\n",
      "val_loss: 0.1950194960666894\n",
      "Progress: 57.5% ... Training loss: 0.075 ... Validation loss: 0.157iteration: 5746\n",
      "train_loss: 0.07542551459607033\n",
      "val_loss: 0.15755973772953116\n",
      "Progress: 57.5% ... Training loss: 0.090 ... Validation loss: 0.239iteration: 5747\n",
      "train_loss: 0.09083314869663797\n",
      "val_loss: 0.23952533324551567\n",
      "Progress: 57.5% ... Training loss: 0.078 ... Validation loss: 0.151iteration: 5748\n",
      "train_loss: 0.0788409677829724\n",
      "val_loss: 0.15130484115005283\n",
      "Progress: 57.5% ... Training loss: 0.070 ... Validation loss: 0.185iteration: 5749\n",
      "train_loss: 0.07070931438815213\n",
      "val_loss: 0.18581964754275523\n",
      "Progress: 57.5% ... Training loss: 0.066 ... Validation loss: 0.166iteration: 5750\n",
      "train_loss: 0.06655004188890257\n",
      "val_loss: 0.1661249547981439\n",
      "Progress: 57.5% ... Training loss: 0.066 ... Validation loss: 0.163iteration: 5751\n",
      "train_loss: 0.06610115274651292\n",
      "val_loss: 0.16341159412862385\n",
      "Progress: 57.5% ... Training loss: 0.064 ... Validation loss: 0.175iteration: 5752\n",
      "train_loss: 0.06499696851370916\n",
      "val_loss: 0.17584440422273337\n",
      "Progress: 57.5% ... Training loss: 0.064 ... Validation loss: 0.171iteration: 5753\n",
      "train_loss: 0.06448908972549386\n",
      "val_loss: 0.1712870417198504\n",
      "Progress: 57.5% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 5754\n",
      "train_loss: 0.06657447383417786\n",
      "val_loss: 0.17132701713363577\n",
      "Progress: 57.5% ... Training loss: 0.073 ... Validation loss: 0.154iteration: 5755\n",
      "train_loss: 0.07351828257718926\n",
      "val_loss: 0.15435545906966064\n",
      "Progress: 57.6% ... Training loss: 0.068 ... Validation loss: 0.202iteration: 5756\n",
      "train_loss: 0.06877871973031097\n",
      "val_loss: 0.20213752862908493\n",
      "Progress: 57.6% ... Training loss: 0.072 ... Validation loss: 0.165iteration: 5757\n",
      "train_loss: 0.07280905969946383\n",
      "val_loss: 0.1656846486553613\n",
      "Progress: 57.6% ... Training loss: 0.070 ... Validation loss: 0.203iteration: 5758\n",
      "train_loss: 0.07034234393704265\n",
      "val_loss: 0.2030814071462622\n",
      "Progress: 57.6% ... Training loss: 0.066 ... Validation loss: 0.156iteration: 5759\n",
      "train_loss: 0.0666949359541622\n",
      "val_loss: 0.15651122077163182\n",
      "Progress: 57.6% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 5760\n",
      "train_loss: 0.06473643083396041\n",
      "val_loss: 0.17340047364981842\n",
      "Progress: 57.6% ... Training loss: 0.070 ... Validation loss: 0.163iteration: 5761\n",
      "train_loss: 0.07034607311712156\n",
      "val_loss: 0.16364912471906093\n",
      "Progress: 57.6% ... Training loss: 0.085 ... Validation loss: 0.238iteration: 5762\n",
      "train_loss: 0.08529593620038922\n",
      "val_loss: 0.23866101475934215\n",
      "Progress: 57.6% ... Training loss: 0.093 ... Validation loss: 0.149iteration: 5763\n",
      "train_loss: 0.0937357798752129\n",
      "val_loss: 0.1493198039592569\n",
      "Progress: 57.6% ... Training loss: 0.085 ... Validation loss: 0.210iteration: 5764\n",
      "train_loss: 0.08506272482804472\n",
      "val_loss: 0.21007551709507852\n",
      "Progress: 57.6% ... Training loss: 0.071 ... Validation loss: 0.154iteration: 5765\n",
      "train_loss: 0.07172291534692485\n",
      "val_loss: 0.15437592916475815\n",
      "Progress: 57.7% ... Training loss: 0.069 ... Validation loss: 0.186iteration: 5766\n",
      "train_loss: 0.06937242670807302\n",
      "val_loss: 0.18600319966310833\n",
      "Progress: 57.7% ... Training loss: 0.072 ... Validation loss: 0.152iteration: 5767\n",
      "train_loss: 0.07225518186778696\n",
      "val_loss: 0.15214459802003288\n",
      "Progress: 57.7% ... Training loss: 0.074 ... Validation loss: 0.195iteration: 5768\n",
      "train_loss: 0.07418174749879047\n",
      "val_loss: 0.1951299177190659\n",
      "Progress: 57.7% ... Training loss: 0.076 ... Validation loss: 0.152iteration: 5769\n",
      "train_loss: 0.07685287138243282\n",
      "val_loss: 0.15218337287099307\n",
      "Progress: 57.7% ... Training loss: 0.069 ... Validation loss: 0.197iteration: 5770\n",
      "train_loss: 0.06993445233881258\n",
      "val_loss: 0.19711827380342464\n",
      "Progress: 57.7% ... Training loss: 0.068 ... Validation loss: 0.152iteration: 5771\n",
      "train_loss: 0.06892724143881038\n",
      "val_loss: 0.15240720194646842\n",
      "Progress: 57.7% ... Training loss: 0.065 ... Validation loss: 0.176iteration: 5772\n",
      "train_loss: 0.06573290801526838\n",
      "val_loss: 0.1761040382408524\n",
      "Progress: 57.7% ... Training loss: 0.065 ... Validation loss: 0.165iteration: 5773\n",
      "train_loss: 0.06516621960278295\n",
      "val_loss: 0.165290873346072\n",
      "Progress: 57.7% ... Training loss: 0.065 ... Validation loss: 0.181iteration: 5774\n",
      "train_loss: 0.06540639109100702\n",
      "val_loss: 0.18164449331584223\n",
      "Progress: 57.8% ... Training loss: 0.066 ... Validation loss: 0.173iteration: 5775\n",
      "train_loss: 0.06661110813236334\n",
      "val_loss: 0.17379932221566366\n",
      "Progress: 57.8% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 5776\n",
      "train_loss: 0.06618459371105212\n",
      "val_loss: 0.1719166008466844\n",
      "Progress: 57.8% ... Training loss: 0.066 ... Validation loss: 0.153iteration: 5777\n",
      "train_loss: 0.06692261137519746\n",
      "val_loss: 0.15340345439578382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 57.8% ... Training loss: 0.066 ... Validation loss: 0.180iteration: 5778\n",
      "train_loss: 0.06613271248456853\n",
      "val_loss: 0.1808465742872824\n",
      "Progress: 57.8% ... Training loss: 0.064 ... Validation loss: 0.167iteration: 5779\n",
      "train_loss: 0.06452402471688871\n",
      "val_loss: 0.16727159524036186\n",
      "Progress: 57.8% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 5780\n",
      "train_loss: 0.06494678213184901\n",
      "val_loss: 0.17054996593722618\n",
      "Progress: 57.8% ... Training loss: 0.067 ... Validation loss: 0.154iteration: 5781\n",
      "train_loss: 0.06758521575465767\n",
      "val_loss: 0.1548799507920413\n",
      "Progress: 57.8% ... Training loss: 0.065 ... Validation loss: 0.164iteration: 5782\n",
      "train_loss: 0.06548919062085383\n",
      "val_loss: 0.1644255302233404\n",
      "Progress: 57.8% ... Training loss: 0.067 ... Validation loss: 0.182iteration: 5783\n",
      "train_loss: 0.06722788791364602\n",
      "val_loss: 0.18250305057290528\n",
      "Progress: 57.8% ... Training loss: 0.067 ... Validation loss: 0.164iteration: 5784\n",
      "train_loss: 0.06741946584205781\n",
      "val_loss: 0.16495712126079382\n",
      "Progress: 57.9% ... Training loss: 0.092 ... Validation loss: 0.211iteration: 5785\n",
      "train_loss: 0.09205968960931826\n",
      "val_loss: 0.21196911310268232\n",
      "Progress: 57.9% ... Training loss: 0.075 ... Validation loss: 0.148iteration: 5786\n",
      "train_loss: 0.0751442403011755\n",
      "val_loss: 0.14875896638589295\n",
      "Progress: 57.9% ... Training loss: 0.087 ... Validation loss: 0.206iteration: 5787\n",
      "train_loss: 0.0875102653334418\n",
      "val_loss: 0.20639476864868556\n",
      "Progress: 57.9% ... Training loss: 0.084 ... Validation loss: 0.152iteration: 5788\n",
      "train_loss: 0.08473898096566353\n",
      "val_loss: 0.15261412091778523\n",
      "Progress: 57.9% ... Training loss: 0.100 ... Validation loss: 0.229iteration: 5789\n",
      "train_loss: 0.10041595251810942\n",
      "val_loss: 0.22951490766089513\n",
      "Progress: 57.9% ... Training loss: 0.077 ... Validation loss: 0.150iteration: 5790\n",
      "train_loss: 0.07736899911056476\n",
      "val_loss: 0.15097246464477568\n",
      "Progress: 57.9% ... Training loss: 0.090 ... Validation loss: 0.240iteration: 5791\n",
      "train_loss: 0.09000545920473631\n",
      "val_loss: 0.2403383030902117\n",
      "Progress: 57.9% ... Training loss: 0.081 ... Validation loss: 0.156iteration: 5792\n",
      "train_loss: 0.0817693232919735\n",
      "val_loss: 0.15696459417704292\n",
      "Progress: 57.9% ... Training loss: 0.081 ... Validation loss: 0.202iteration: 5793\n",
      "train_loss: 0.08191558685458097\n",
      "val_loss: 0.20251099009507917\n",
      "Progress: 57.9% ... Training loss: 0.100 ... Validation loss: 0.158iteration: 5794\n",
      "train_loss: 0.10049484058880058\n",
      "val_loss: 0.1585179074712723\n",
      "Progress: 58.0% ... Training loss: 0.077 ... Validation loss: 0.204iteration: 5795\n",
      "train_loss: 0.07774532779052226\n",
      "val_loss: 0.20496936273647617\n",
      "Progress: 58.0% ... Training loss: 0.082 ... Validation loss: 0.154iteration: 5796\n",
      "train_loss: 0.08205557246204813\n",
      "val_loss: 0.1548068123157501\n",
      "Progress: 58.0% ... Training loss: 0.093 ... Validation loss: 0.226iteration: 5797\n",
      "train_loss: 0.09336093160678516\n",
      "val_loss: 0.2269112770520107\n",
      "Progress: 58.0% ... Training loss: 0.079 ... Validation loss: 0.150iteration: 5798\n",
      "train_loss: 0.07972062020120431\n",
      "val_loss: 0.15013658404794736\n",
      "Progress: 58.0% ... Training loss: 0.070 ... Validation loss: 0.186iteration: 5799\n",
      "train_loss: 0.07029977355675042\n",
      "val_loss: 0.18690758704928317\n",
      "Progress: 58.0% ... Training loss: 0.066 ... Validation loss: 0.159iteration: 5800\n",
      "train_loss: 0.06644794456705994\n",
      "val_loss: 0.1590562119958202\n",
      "Progress: 58.0% ... Training loss: 0.066 ... Validation loss: 0.180iteration: 5801\n",
      "train_loss: 0.06658075217276573\n",
      "val_loss: 0.18011300752867815\n",
      "Progress: 58.0% ... Training loss: 0.070 ... Validation loss: 0.155iteration: 5802\n",
      "train_loss: 0.07091262675985363\n",
      "val_loss: 0.15567352267176973\n",
      "Progress: 58.0% ... Training loss: 0.065 ... Validation loss: 0.172iteration: 5803\n",
      "train_loss: 0.06579278932337322\n",
      "val_loss: 0.17208688263525987\n",
      "Progress: 58.0% ... Training loss: 0.069 ... Validation loss: 0.156iteration: 5804\n",
      "train_loss: 0.06941723566219989\n",
      "val_loss: 0.15609620063093305\n",
      "Progress: 58.0% ... Training loss: 0.066 ... Validation loss: 0.173iteration: 5805\n",
      "train_loss: 0.0664646905723339\n",
      "val_loss: 0.17380989402012495\n",
      "Progress: 58.1% ... Training loss: 0.068 ... Validation loss: 0.158iteration: 5806\n",
      "train_loss: 0.06899434835393124\n",
      "val_loss: 0.15803185969367298\n",
      "Progress: 58.1% ... Training loss: 0.073 ... Validation loss: 0.205iteration: 5807\n",
      "train_loss: 0.07336124786941634\n",
      "val_loss: 0.20564437933944132\n",
      "Progress: 58.1% ... Training loss: 0.085 ... Validation loss: 0.158iteration: 5808\n",
      "train_loss: 0.08561431765079268\n",
      "val_loss: 0.1581823876196208\n",
      "Progress: 58.1% ... Training loss: 0.086 ... Validation loss: 0.233iteration: 5809\n",
      "train_loss: 0.08660720455488671\n",
      "val_loss: 0.23354961062383256\n",
      "Progress: 58.1% ... Training loss: 0.101 ... Validation loss: 0.156iteration: 5810\n",
      "train_loss: 0.1017201168561761\n",
      "val_loss: 0.15640216780980823\n",
      "Progress: 58.1% ... Training loss: 0.100 ... Validation loss: 0.255iteration: 5811\n",
      "train_loss: 0.1005327286188982\n",
      "val_loss: 0.2557719080479617\n",
      "Progress: 58.1% ... Training loss: 0.078 ... Validation loss: 0.152iteration: 5812\n",
      "train_loss: 0.07835275411203989\n",
      "val_loss: 0.15270448453328178\n",
      "Progress: 58.1% ... Training loss: 0.069 ... Validation loss: 0.196iteration: 5813\n",
      "train_loss: 0.06988440897654434\n",
      "val_loss: 0.1964311617959833\n",
      "Progress: 58.1% ... Training loss: 0.075 ... Validation loss: 0.160iteration: 5814\n",
      "train_loss: 0.07522546621458226\n",
      "val_loss: 0.16026076617994325\n",
      "Progress: 58.1% ... Training loss: 0.118 ... Validation loss: 0.271iteration: 5815\n",
      "train_loss: 0.11839941167495692\n",
      "val_loss: 0.2713025886546952\n",
      "Progress: 58.2% ... Training loss: 0.091 ... Validation loss: 0.159iteration: 5816\n",
      "train_loss: 0.0912814716151662\n",
      "val_loss: 0.15937925348739768\n",
      "Progress: 58.2% ... Training loss: 0.083 ... Validation loss: 0.258iteration: 5817\n",
      "train_loss: 0.08377214197769822\n",
      "val_loss: 0.25828329708657954\n",
      "Progress: 58.2% ... Training loss: 0.067 ... Validation loss: 0.168iteration: 5818\n",
      "train_loss: 0.06736056299736548\n",
      "val_loss: 0.16887623507042765\n",
      "Progress: 58.2% ... Training loss: 0.065 ... Validation loss: 0.193iteration: 5819\n",
      "train_loss: 0.06530324179552494\n",
      "val_loss: 0.1932441168876295\n",
      "Progress: 58.2% ... Training loss: 0.065 ... Validation loss: 0.197iteration: 5820\n",
      "train_loss: 0.06595320978714275\n",
      "val_loss: 0.1979097299721556\n",
      "Progress: 58.2% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 5821\n",
      "train_loss: 0.06604152445927312\n",
      "val_loss: 0.1710111926729266\n",
      "Progress: 58.2% ... Training loss: 0.066 ... Validation loss: 0.170iteration: 5822\n",
      "train_loss: 0.06614566492951773\n",
      "val_loss: 0.17088491339399695\n",
      "Progress: 58.2% ... Training loss: 0.066 ... Validation loss: 0.172iteration: 5823\n",
      "train_loss: 0.06646548508805326\n",
      "val_loss: 0.17257221105473414\n",
      "Progress: 58.2% ... Training loss: 0.065 ... Validation loss: 0.179iteration: 5824\n",
      "train_loss: 0.06564819797861553\n",
      "val_loss: 0.17927931818492268\n",
      "Progress: 58.2% ... Training loss: 0.064 ... Validation loss: 0.185iteration: 5825\n",
      "train_loss: 0.06459992731758003\n",
      "val_loss: 0.18552051822302057\n",
      "Progress: 58.3% ... Training loss: 0.064 ... Validation loss: 0.187iteration: 5826\n",
      "train_loss: 0.0648120604524875\n",
      "val_loss: 0.187970876638399\n",
      "Progress: 58.3% ... Training loss: 0.064 ... Validation loss: 0.194iteration: 5827\n",
      "train_loss: 0.06494160652022991\n",
      "val_loss: 0.19428495104729218\n",
      "Progress: 58.3% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 5828\n",
      "train_loss: 0.0645963691682785\n",
      "val_loss: 0.17847750473531762\n",
      "Progress: 58.3% ... Training loss: 0.064 ... Validation loss: 0.180iteration: 5829\n",
      "train_loss: 0.06471657691331727\n",
      "val_loss: 0.1809398236539035\n",
      "Progress: 58.3% ... Training loss: 0.068 ... Validation loss: 0.174iteration: 5830\n",
      "train_loss: 0.06805450838221197\n",
      "val_loss: 0.1749706165615973\n",
      "Progress: 58.3% ... Training loss: 0.067 ... Validation loss: 0.205iteration: 5831\n",
      "train_loss: 0.06744247752001717\n",
      "val_loss: 0.20545550560740747\n",
      "Progress: 58.3% ... Training loss: 0.066 ... Validation loss: 0.182iteration: 5832\n",
      "train_loss: 0.0661156678259512\n",
      "val_loss: 0.18284567647215802\n",
      "Progress: 58.3% ... Training loss: 0.064 ... Validation loss: 0.184iteration: 5833\n",
      "train_loss: 0.06492527134448671\n",
      "val_loss: 0.18429019999328813\n",
      "Progress: 58.3% ... Training loss: 0.067 ... Validation loss: 0.171iteration: 5834\n",
      "train_loss: 0.06736999845342012\n",
      "val_loss: 0.17114665840627105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 58.4% ... Training loss: 0.083 ... Validation loss: 0.207iteration: 5835\n",
      "train_loss: 0.08369506159211876\n",
      "val_loss: 0.2073055955546188\n",
      "Progress: 58.4% ... Training loss: 0.092 ... Validation loss: 0.156iteration: 5836\n",
      "train_loss: 0.09272058300905199\n",
      "val_loss: 0.156148983536249\n",
      "Progress: 58.4% ... Training loss: 0.076 ... Validation loss: 0.216iteration: 5837\n",
      "train_loss: 0.07628033930566676\n",
      "val_loss: 0.21692168789966124\n",
      "Progress: 58.4% ... Training loss: 0.094 ... Validation loss: 0.161iteration: 5838\n",
      "train_loss: 0.09498696312603963\n",
      "val_loss: 0.16153032860667987\n",
      "Progress: 58.4% ... Training loss: 0.080 ... Validation loss: 0.212iteration: 5839\n",
      "train_loss: 0.0806647767383503\n",
      "val_loss: 0.21295039610284483\n",
      "Progress: 58.4% ... Training loss: 0.073 ... Validation loss: 0.159iteration: 5840\n",
      "train_loss: 0.07373145634917502\n",
      "val_loss: 0.15981267509123703\n",
      "Progress: 58.4% ... Training loss: 0.065 ... Validation loss: 0.173iteration: 5841\n",
      "train_loss: 0.06523301148056138\n",
      "val_loss: 0.17399663752676106\n",
      "Progress: 58.4% ... Training loss: 0.078 ... Validation loss: 0.218iteration: 5842\n",
      "train_loss: 0.07867920407582368\n",
      "val_loss: 0.21834382884204895\n",
      "Progress: 58.4% ... Training loss: 0.072 ... Validation loss: 0.157iteration: 5843\n",
      "train_loss: 0.072623429336963\n",
      "val_loss: 0.15754726078858125\n",
      "Progress: 58.4% ... Training loss: 0.065 ... Validation loss: 0.180iteration: 5844\n",
      "train_loss: 0.06587442173238009\n",
      "val_loss: 0.18078503966423567\n",
      "Progress: 58.5% ... Training loss: 0.067 ... Validation loss: 0.162iteration: 5845\n",
      "train_loss: 0.06711134561571681\n",
      "val_loss: 0.1620801794583677\n",
      "Progress: 58.5% ... Training loss: 0.068 ... Validation loss: 0.200iteration: 5846\n",
      "train_loss: 0.0684345460372014\n",
      "val_loss: 0.2005051152369481\n",
      "Progress: 58.5% ... Training loss: 0.081 ... Validation loss: 0.153iteration: 5847\n",
      "train_loss: 0.08154697782769707\n",
      "val_loss: 0.15328365322732665\n",
      "Progress: 58.5% ... Training loss: 0.078 ... Validation loss: 0.208iteration: 5848\n",
      "train_loss: 0.07802797428406906\n",
      "val_loss: 0.2086631857705579\n",
      "Progress: 58.5% ... Training loss: 0.070 ... Validation loss: 0.156iteration: 5849\n",
      "train_loss: 0.07035033215271731\n",
      "val_loss: 0.15603523459538934\n",
      "Progress: 58.5% ... Training loss: 0.070 ... Validation loss: 0.196iteration: 5850\n",
      "train_loss: 0.07072461642105485\n",
      "val_loss: 0.19666708914057282\n",
      "Progress: 58.5% ... Training loss: 0.066 ... Validation loss: 0.159iteration: 5851\n",
      "train_loss: 0.06635441630330988\n",
      "val_loss: 0.15917359958858127\n",
      "Progress: 58.5% ... Training loss: 0.066 ... Validation loss: 0.172iteration: 5852\n",
      "train_loss: 0.06653201849698981\n",
      "val_loss: 0.17243634773592786\n",
      "Progress: 58.5% ... Training loss: 0.066 ... Validation loss: 0.164iteration: 5853\n",
      "train_loss: 0.0664836191316787\n",
      "val_loss: 0.16499112655462359\n",
      "Progress: 58.5% ... Training loss: 0.070 ... Validation loss: 0.201iteration: 5854\n",
      "train_loss: 0.0703727709151317\n",
      "val_loss: 0.201424568352346\n",
      "Progress: 58.5% ... Training loss: 0.064 ... Validation loss: 0.163iteration: 5855\n",
      "train_loss: 0.06445471604143325\n",
      "val_loss: 0.16383564005592485\n",
      "Progress: 58.6% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 5856\n",
      "train_loss: 0.06452681441430883\n",
      "val_loss: 0.17819140880448808\n",
      "Progress: 58.6% ... Training loss: 0.064 ... Validation loss: 0.169iteration: 5857\n",
      "train_loss: 0.06438825746652417\n",
      "val_loss: 0.1690233466036694\n",
      "Progress: 58.6% ... Training loss: 0.065 ... Validation loss: 0.171iteration: 5858\n",
      "train_loss: 0.06598861321689947\n",
      "val_loss: 0.17163663072167118\n",
      "Progress: 58.6% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 5859\n",
      "train_loss: 0.06494799814369967\n",
      "val_loss: 0.17314215169123268\n",
      "Progress: 58.6% ... Training loss: 0.066 ... Validation loss: 0.180iteration: 5860\n",
      "train_loss: 0.06618272099121107\n",
      "val_loss: 0.1800039578046491\n",
      "Progress: 58.6% ... Training loss: 0.065 ... Validation loss: 0.160iteration: 5861\n",
      "train_loss: 0.06544749383495604\n",
      "val_loss: 0.16026777650897725\n",
      "Progress: 58.6% ... Training loss: 0.066 ... Validation loss: 0.187iteration: 5862\n",
      "train_loss: 0.06666684338416409\n",
      "val_loss: 0.18724763623831575\n",
      "Progress: 58.6% ... Training loss: 0.065 ... Validation loss: 0.172iteration: 5863\n",
      "train_loss: 0.06578646183688049\n",
      "val_loss: 0.17261350669355285\n",
      "Progress: 58.6% ... Training loss: 0.080 ... Validation loss: 0.215iteration: 5864\n",
      "train_loss: 0.08090499507069607\n",
      "val_loss: 0.21565498581804327\n",
      "Progress: 58.6% ... Training loss: 0.104 ... Validation loss: 0.161iteration: 5865\n",
      "train_loss: 0.10413013429897082\n",
      "val_loss: 0.16175113733160168\n",
      "Progress: 58.7% ... Training loss: 0.086 ... Validation loss: 0.222iteration: 5866\n",
      "train_loss: 0.0861469427249517\n",
      "val_loss: 0.22282084845068795\n",
      "Progress: 58.7% ... Training loss: 0.075 ... Validation loss: 0.153iteration: 5867\n",
      "train_loss: 0.07571960168801893\n",
      "val_loss: 0.15346148644283156\n",
      "Progress: 58.7% ... Training loss: 0.085 ... Validation loss: 0.229iteration: 5868\n",
      "train_loss: 0.08526268765081649\n",
      "val_loss: 0.22938601979144785\n",
      "Progress: 58.7% ... Training loss: 0.079 ... Validation loss: 0.156iteration: 5869\n",
      "train_loss: 0.07994269492397804\n",
      "val_loss: 0.15628162762039186\n",
      "Progress: 58.7% ... Training loss: 0.083 ... Validation loss: 0.232iteration: 5870\n",
      "train_loss: 0.0838024959340827\n",
      "val_loss: 0.23273771291672365\n",
      "Progress: 58.7% ... Training loss: 0.068 ... Validation loss: 0.153iteration: 5871\n",
      "train_loss: 0.0681913743595128\n",
      "val_loss: 0.15342077030778561\n",
      "Progress: 58.7% ... Training loss: 0.067 ... Validation loss: 0.175iteration: 5872\n",
      "train_loss: 0.06705869466455998\n",
      "val_loss: 0.17580850232035378\n",
      "Progress: 58.7% ... Training loss: 0.065 ... Validation loss: 0.160iteration: 5873\n",
      "train_loss: 0.06527086575867769\n",
      "val_loss: 0.16056295662204453\n",
      "Progress: 58.7% ... Training loss: 0.070 ... Validation loss: 0.190iteration: 5874\n",
      "train_loss: 0.07060735439715404\n",
      "val_loss: 0.19076254581208799\n",
      "Progress: 58.8% ... Training loss: 0.068 ... Validation loss: 0.159iteration: 5875\n",
      "train_loss: 0.06801140239108568\n",
      "val_loss: 0.1596279443972558\n",
      "Progress: 58.8% ... Training loss: 0.064 ... Validation loss: 0.171iteration: 5876\n",
      "train_loss: 0.06453619074843439\n",
      "val_loss: 0.1714982133703108\n",
      "Progress: 58.8% ... Training loss: 0.065 ... Validation loss: 0.173iteration: 5877\n",
      "train_loss: 0.06527758510605516\n",
      "val_loss: 0.17354295946786472\n",
      "Progress: 58.8% ... Training loss: 0.067 ... Validation loss: 0.153iteration: 5878\n",
      "train_loss: 0.06735130381983842\n",
      "val_loss: 0.1531021004592841\n",
      "Progress: 58.8% ... Training loss: 0.067 ... Validation loss: 0.153iteration: 5879\n",
      "train_loss: 0.06760192471575752\n",
      "val_loss: 0.15365232132236561\n",
      "Progress: 58.8% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 5880\n",
      "train_loss: 0.06470503035492851\n",
      "val_loss: 0.17620598097586798\n",
      "Progress: 58.8% ... Training loss: 0.072 ... Validation loss: 0.159iteration: 5881\n",
      "train_loss: 0.07299107319970369\n",
      "val_loss: 0.15907388454338325\n",
      "Progress: 58.8% ... Training loss: 0.076 ... Validation loss: 0.207iteration: 5882\n",
      "train_loss: 0.0767482987186001\n",
      "val_loss: 0.20745148396300697\n",
      "Progress: 58.8% ... Training loss: 0.073 ... Validation loss: 0.157iteration: 5883\n",
      "train_loss: 0.07302692497514662\n",
      "val_loss: 0.15733168937508898\n",
      "Progress: 58.8% ... Training loss: 0.064 ... Validation loss: 0.181iteration: 5884\n",
      "train_loss: 0.06471802674401615\n",
      "val_loss: 0.18137525056222079\n",
      "Progress: 58.9% ... Training loss: 0.069 ... Validation loss: 0.159iteration: 5885\n",
      "train_loss: 0.06971344567929785\n",
      "val_loss: 0.15952860933160798\n",
      "Progress: 58.9% ... Training loss: 0.064 ... Validation loss: 0.172iteration: 5886\n",
      "train_loss: 0.06449581633625917\n",
      "val_loss: 0.1728714892614686\n",
      "Progress: 58.9% ... Training loss: 0.065 ... Validation loss: 0.175iteration: 5887\n",
      "train_loss: 0.06511105983486257\n",
      "val_loss: 0.17520533619344794\n",
      "Progress: 58.9% ... Training loss: 0.070 ... Validation loss: 0.188iteration: 5888\n",
      "train_loss: 0.07004703511171316\n",
      "val_loss: 0.1881681424597377\n",
      "Progress: 58.9% ... Training loss: 0.083 ... Validation loss: 0.156iteration: 5889\n",
      "train_loss: 0.0832332934646915\n",
      "val_loss: 0.15692332032243125\n",
      "Progress: 58.9% ... Training loss: 0.095 ... Validation loss: 0.226iteration: 5890\n",
      "train_loss: 0.0950328093422313\n",
      "val_loss: 0.2267756607048401\n",
      "Progress: 58.9% ... Training loss: 0.105 ... Validation loss: 0.157iteration: 5891\n",
      "train_loss: 0.1051673408520472\n",
      "val_loss: 0.1575776089287266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 58.9% ... Training loss: 0.107 ... Validation loss: 0.264iteration: 5892\n",
      "train_loss: 0.10740115414934405\n",
      "val_loss: 0.2641653363757678\n",
      "Progress: 58.9% ... Training loss: 0.076 ... Validation loss: 0.152iteration: 5893\n",
      "train_loss: 0.07691470560073786\n",
      "val_loss: 0.15254146956599512\n",
      "Progress: 58.9% ... Training loss: 0.073 ... Validation loss: 0.198iteration: 5894\n",
      "train_loss: 0.07366709411060454\n",
      "val_loss: 0.19878728124047157\n",
      "Progress: 59.0% ... Training loss: 0.075 ... Validation loss: 0.155iteration: 5895\n",
      "train_loss: 0.07541972465810447\n",
      "val_loss: 0.15506733886116764\n",
      "Progress: 59.0% ... Training loss: 0.076 ... Validation loss: 0.202iteration: 5896\n",
      "train_loss: 0.07613223698731934\n",
      "val_loss: 0.20226789305332057\n",
      "Progress: 59.0% ... Training loss: 0.065 ... Validation loss: 0.159iteration: 5897\n",
      "train_loss: 0.06592505610998618\n",
      "val_loss: 0.15996894678124385\n",
      "Progress: 59.0% ... Training loss: 0.068 ... Validation loss: 0.183iteration: 5898\n",
      "train_loss: 0.06874613065665221\n",
      "val_loss: 0.18346162015881115\n",
      "Progress: 59.0% ... Training loss: 0.072 ... Validation loss: 0.153iteration: 5899\n",
      "train_loss: 0.07295097337974174\n",
      "val_loss: 0.15393424975652742\n",
      "Progress: 59.0% ... Training loss: 0.065 ... Validation loss: 0.183iteration: 5900\n",
      "train_loss: 0.06528439185643041\n",
      "val_loss: 0.18373933529248518\n",
      "Progress: 59.0% ... Training loss: 0.065 ... Validation loss: 0.167iteration: 5901\n",
      "train_loss: 0.06587994480347388\n",
      "val_loss: 0.16795673781248424\n",
      "Progress: 59.0% ... Training loss: 0.064 ... Validation loss: 0.175iteration: 5902\n",
      "train_loss: 0.06430908784466983\n",
      "val_loss: 0.1752566195206086\n",
      "Progress: 59.0% ... Training loss: 0.064 ... Validation loss: 0.168iteration: 5903\n",
      "train_loss: 0.06432744496892862\n",
      "val_loss: 0.16817988547048496\n",
      "Progress: 59.0% ... Training loss: 0.067 ... Validation loss: 0.187iteration: 5904\n",
      "train_loss: 0.06750010570749228\n",
      "val_loss: 0.1877505416131848\n",
      "Progress: 59.0% ... Training loss: 0.064 ... Validation loss: 0.168iteration: 5905\n",
      "train_loss: 0.06454337975218068\n",
      "val_loss: 0.1684945589776475\n",
      "Progress: 59.1% ... Training loss: 0.064 ... Validation loss: 0.165iteration: 5906\n",
      "train_loss: 0.06457212713806401\n",
      "val_loss: 0.1658914352279806\n",
      "Progress: 59.1% ... Training loss: 0.066 ... Validation loss: 0.182iteration: 5907\n",
      "train_loss: 0.06609840518502297\n",
      "val_loss: 0.18268039615169498\n",
      "Progress: 59.1% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 5908\n",
      "train_loss: 0.06461561546083577\n",
      "val_loss: 0.17630533147450314\n",
      "Progress: 59.1% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 5909\n",
      "train_loss: 0.06492306907382618\n",
      "val_loss: 0.17349405440552298\n",
      "Progress: 59.1% ... Training loss: 0.065 ... Validation loss: 0.174iteration: 5910\n",
      "train_loss: 0.06573277179750198\n",
      "val_loss: 0.17496547994981215\n",
      "Progress: 59.1% ... Training loss: 0.066 ... Validation loss: 0.167iteration: 5911\n",
      "train_loss: 0.06684653966100049\n",
      "val_loss: 0.16769921339435592\n",
      "Progress: 59.1% ... Training loss: 0.073 ... Validation loss: 0.207iteration: 5912\n",
      "train_loss: 0.07303228662675222\n",
      "val_loss: 0.20725262504778547\n",
      "Progress: 59.1% ... Training loss: 0.073 ... Validation loss: 0.157iteration: 5913\n",
      "train_loss: 0.07338724272718954\n",
      "val_loss: 0.15709102826412444\n",
      "Progress: 59.1% ... Training loss: 0.069 ... Validation loss: 0.194iteration: 5914\n",
      "train_loss: 0.06962757863833488\n",
      "val_loss: 0.19484364046542185\n",
      "Progress: 59.1% ... Training loss: 0.074 ... Validation loss: 0.155iteration: 5915\n",
      "train_loss: 0.07473257145025942\n",
      "val_loss: 0.15569678965573822\n",
      "Progress: 59.2% ... Training loss: 0.072 ... Validation loss: 0.195iteration: 5916\n",
      "train_loss: 0.07203016870553251\n",
      "val_loss: 0.19511096182948917\n",
      "Progress: 59.2% ... Training loss: 0.064 ... Validation loss: 0.174iteration: 5917\n",
      "train_loss: 0.06473773359946522\n",
      "val_loss: 0.1748697118956456\n",
      "Progress: 59.2% ... Training loss: 0.065 ... Validation loss: 0.174iteration: 5918\n",
      "train_loss: 0.06505846439712533\n",
      "val_loss: 0.17490666493076745\n",
      "Progress: 59.2% ... Training loss: 0.069 ... Validation loss: 0.170iteration: 5919\n",
      "train_loss: 0.06969102670701922\n",
      "val_loss: 0.17034585583754225\n",
      "Progress: 59.2% ... Training loss: 0.066 ... Validation loss: 0.185iteration: 5920\n",
      "train_loss: 0.0668452861239899\n",
      "val_loss: 0.18565102270154524\n",
      "Progress: 59.2% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 5921\n",
      "train_loss: 0.06473782253688112\n",
      "val_loss: 0.17026165231020568\n",
      "Progress: 59.2% ... Training loss: 0.065 ... Validation loss: 0.161iteration: 5922\n",
      "train_loss: 0.0652496588210646\n",
      "val_loss: 0.16146402890856362\n",
      "Progress: 59.2% ... Training loss: 0.071 ... Validation loss: 0.151iteration: 5923\n",
      "train_loss: 0.07127250706706374\n",
      "val_loss: 0.1515349747306833\n",
      "Progress: 59.2% ... Training loss: 0.079 ... Validation loss: 0.202iteration: 5924\n",
      "train_loss: 0.0797165627715481\n",
      "val_loss: 0.20282436844265553\n",
      "Progress: 59.2% ... Training loss: 0.087 ... Validation loss: 0.154iteration: 5925\n",
      "train_loss: 0.0876054289719924\n",
      "val_loss: 0.1545731379307828\n",
      "Progress: 59.3% ... Training loss: 0.125 ... Validation loss: 0.269iteration: 5926\n",
      "train_loss: 0.12514698249411219\n",
      "val_loss: 0.26940726746880966\n",
      "Progress: 59.3% ... Training loss: 0.119 ... Validation loss: 0.160iteration: 5927\n",
      "train_loss: 0.11958788222151756\n",
      "val_loss: 0.16088954102203148\n",
      "Progress: 59.3% ... Training loss: 0.085 ... Validation loss: 0.232iteration: 5928\n",
      "train_loss: 0.08598934283043588\n",
      "val_loss: 0.23271534872099037\n",
      "Progress: 59.3% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 5929\n",
      "train_loss: 0.08187090342094111\n",
      "val_loss: 0.15420074375713913\n",
      "Progress: 59.3% ... Training loss: 0.073 ... Validation loss: 0.201iteration: 5930\n",
      "train_loss: 0.07321824229094896\n",
      "val_loss: 0.20192654226193435\n",
      "Progress: 59.3% ... Training loss: 0.112 ... Validation loss: 0.165iteration: 5931\n",
      "train_loss: 0.11247336289251435\n",
      "val_loss: 0.16593697340084979\n",
      "Progress: 59.3% ... Training loss: 0.101 ... Validation loss: 0.254iteration: 5932\n",
      "train_loss: 0.10132953684023055\n",
      "val_loss: 0.25417440785570344\n",
      "Progress: 59.3% ... Training loss: 0.123 ... Validation loss: 0.165iteration: 5933\n",
      "train_loss: 0.1238209351728666\n",
      "val_loss: 0.16585328420183013\n",
      "Progress: 59.3% ... Training loss: 0.176 ... Validation loss: 0.311iteration: 5934\n",
      "train_loss: 0.176053128898502\n",
      "val_loss: 0.31141732763555896\n",
      "Progress: 59.4% ... Training loss: 0.149 ... Validation loss: 0.177iteration: 5935\n",
      "train_loss: 0.1492332517624972\n",
      "val_loss: 0.17781175701611565\n",
      "Progress: 59.4% ... Training loss: 0.122 ... Validation loss: 0.269iteration: 5936\n",
      "train_loss: 0.12252113009370263\n",
      "val_loss: 0.26916018207263287\n",
      "Progress: 59.4% ... Training loss: 0.097 ... Validation loss: 0.161iteration: 5937\n",
      "train_loss: 0.09737785886481536\n",
      "val_loss: 0.16190786980810132\n",
      "Progress: 59.4% ... Training loss: 0.093 ... Validation loss: 0.228iteration: 5938\n",
      "train_loss: 0.09387836555443806\n",
      "val_loss: 0.22880231662015982\n",
      "Progress: 59.4% ... Training loss: 0.084 ... Validation loss: 0.158iteration: 5939\n",
      "train_loss: 0.08408006214333663\n",
      "val_loss: 0.1585672697516397\n",
      "Progress: 59.4% ... Training loss: 0.077 ... Validation loss: 0.219iteration: 5940\n",
      "train_loss: 0.07710564314407546\n",
      "val_loss: 0.21926914765954342\n",
      "Progress: 59.4% ... Training loss: 0.080 ... Validation loss: 0.161iteration: 5941\n",
      "train_loss: 0.08077381410466783\n",
      "val_loss: 0.1613612950927741\n",
      "Progress: 59.4% ... Training loss: 0.071 ... Validation loss: 0.196iteration: 5942\n",
      "train_loss: 0.07142306386003087\n",
      "val_loss: 0.19635081993026182\n",
      "Progress: 59.4% ... Training loss: 0.079 ... Validation loss: 0.159iteration: 5943\n",
      "train_loss: 0.07981177717679663\n",
      "val_loss: 0.15965058103407104\n",
      "Progress: 59.4% ... Training loss: 0.082 ... Validation loss: 0.224iteration: 5944\n",
      "train_loss: 0.08255933888079871\n",
      "val_loss: 0.22491205851081528\n",
      "Progress: 59.5% ... Training loss: 0.102 ... Validation loss: 0.164iteration: 5945\n",
      "train_loss: 0.10206645379011424\n",
      "val_loss: 0.16423839879397414\n",
      "Progress: 59.5% ... Training loss: 0.108 ... Validation loss: 0.269iteration: 5946\n",
      "train_loss: 0.10866425258549967\n",
      "val_loss: 0.26930548118402925\n",
      "Progress: 59.5% ... Training loss: 0.117 ... Validation loss: 0.161iteration: 5947\n",
      "train_loss: 0.11704220104425159\n",
      "val_loss: 0.16166667150284278\n",
      "Progress: 59.5% ... Training loss: 0.088 ... Validation loss: 0.227iteration: 5948\n",
      "train_loss: 0.08890197381871687\n",
      "val_loss: 0.22778302221607502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 59.5% ... Training loss: 0.103 ... Validation loss: 0.159iteration: 5949\n",
      "train_loss: 0.103361619754093\n",
      "val_loss: 0.15901698934497718\n",
      "Progress: 59.5% ... Training loss: 0.081 ... Validation loss: 0.209iteration: 5950\n",
      "train_loss: 0.08192225717364793\n",
      "val_loss: 0.20927305165132826\n",
      "Progress: 59.5% ... Training loss: 0.073 ... Validation loss: 0.154iteration: 5951\n",
      "train_loss: 0.07373534774166522\n",
      "val_loss: 0.15472953219438987\n",
      "Progress: 59.5% ... Training loss: 0.072 ... Validation loss: 0.200iteration: 5952\n",
      "train_loss: 0.07269076540160242\n",
      "val_loss: 0.20074940957957357\n",
      "Progress: 59.5% ... Training loss: 0.086 ... Validation loss: 0.155iteration: 5953\n",
      "train_loss: 0.08639122301492712\n",
      "val_loss: 0.15548647667388188\n",
      "Progress: 59.5% ... Training loss: 0.075 ... Validation loss: 0.212iteration: 5954\n",
      "train_loss: 0.07596613367301534\n",
      "val_loss: 0.2122590112673798\n",
      "Progress: 59.5% ... Training loss: 0.093 ... Validation loss: 0.155iteration: 5955\n",
      "train_loss: 0.09352185588326561\n",
      "val_loss: 0.1559457476048584\n",
      "Progress: 59.6% ... Training loss: 0.073 ... Validation loss: 0.203iteration: 5956\n",
      "train_loss: 0.07383552709087689\n",
      "val_loss: 0.20331384328395896\n",
      "Progress: 59.6% ... Training loss: 0.072 ... Validation loss: 0.159iteration: 5957\n",
      "train_loss: 0.07261031873958862\n",
      "val_loss: 0.1597589753003262\n",
      "Progress: 59.6% ... Training loss: 0.068 ... Validation loss: 0.187iteration: 5958\n",
      "train_loss: 0.06868906239654109\n",
      "val_loss: 0.1876772418368002\n",
      "Progress: 59.6% ... Training loss: 0.071 ... Validation loss: 0.154iteration: 5959\n",
      "train_loss: 0.0711945507335431\n",
      "val_loss: 0.15409482503850588\n",
      "Progress: 59.6% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 5960\n",
      "train_loss: 0.0644834331613899\n",
      "val_loss: 0.1702906958004898\n",
      "Progress: 59.6% ... Training loss: 0.064 ... Validation loss: 0.167iteration: 5961\n",
      "train_loss: 0.06404897664955182\n",
      "val_loss: 0.16714108726225782\n",
      "Progress: 59.6% ... Training loss: 0.063 ... Validation loss: 0.172iteration: 5962\n",
      "train_loss: 0.06390176182719835\n",
      "val_loss: 0.17294105779920316\n",
      "Progress: 59.6% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 5963\n",
      "train_loss: 0.06442007091025553\n",
      "val_loss: 0.17653995692203744\n",
      "Progress: 59.6% ... Training loss: 0.064 ... Validation loss: 0.159iteration: 5964\n",
      "train_loss: 0.06481358104009236\n",
      "val_loss: 0.1594460941819537\n",
      "Progress: 59.6% ... Training loss: 0.064 ... Validation loss: 0.161iteration: 5965\n",
      "train_loss: 0.06496028248623997\n",
      "val_loss: 0.16177779151272667\n",
      "Progress: 59.7% ... Training loss: 0.064 ... Validation loss: 0.167iteration: 5966\n",
      "train_loss: 0.06418193855768653\n",
      "val_loss: 0.1674613111656076\n",
      "Progress: 59.7% ... Training loss: 0.070 ... Validation loss: 0.158iteration: 5967\n",
      "train_loss: 0.07081279685898245\n",
      "val_loss: 0.1582444607042975\n",
      "Progress: 59.7% ... Training loss: 0.073 ... Validation loss: 0.204iteration: 5968\n",
      "train_loss: 0.07392476767317332\n",
      "val_loss: 0.20472340965617367\n",
      "Progress: 59.7% ... Training loss: 0.081 ... Validation loss: 0.149iteration: 5969\n",
      "train_loss: 0.08190779835705148\n",
      "val_loss: 0.14957616838104956\n",
      "Progress: 59.7% ... Training loss: 0.066 ... Validation loss: 0.182iteration: 5970\n",
      "train_loss: 0.06635799644748028\n",
      "val_loss: 0.18239172970183667\n",
      "Progress: 59.7% ... Training loss: 0.066 ... Validation loss: 0.169iteration: 5971\n",
      "train_loss: 0.0660667903475868\n",
      "val_loss: 0.16949467743724642\n",
      "Progress: 59.7% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 5972\n",
      "train_loss: 0.06485224466046409\n",
      "val_loss: 0.17097762887098655\n",
      "Progress: 59.7% ... Training loss: 0.065 ... Validation loss: 0.165iteration: 5973\n",
      "train_loss: 0.06563616655949492\n",
      "val_loss: 0.16537746421935257\n",
      "Progress: 59.7% ... Training loss: 0.064 ... Validation loss: 0.161iteration: 5974\n",
      "train_loss: 0.0648497447914564\n",
      "val_loss: 0.16131336598392876\n",
      "Progress: 59.8% ... Training loss: 0.063 ... Validation loss: 0.170iteration: 5975\n",
      "train_loss: 0.06388475936256129\n",
      "val_loss: 0.17074859298196673\n",
      "Progress: 59.8% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 5976\n",
      "train_loss: 0.06402419629983509\n",
      "val_loss: 0.17605672381924053\n",
      "Progress: 59.8% ... Training loss: 0.073 ... Validation loss: 0.193iteration: 5977\n",
      "train_loss: 0.07386637592894153\n",
      "val_loss: 0.19354395192094695\n",
      "Progress: 59.8% ... Training loss: 0.076 ... Validation loss: 0.157iteration: 5978\n",
      "train_loss: 0.07673022753449395\n",
      "val_loss: 0.15790178190359389\n",
      "Progress: 59.8% ... Training loss: 0.074 ... Validation loss: 0.216iteration: 5979\n",
      "train_loss: 0.07432620067749594\n",
      "val_loss: 0.2167974252865789\n",
      "Progress: 59.8% ... Training loss: 0.071 ... Validation loss: 0.161iteration: 5980\n",
      "train_loss: 0.07101889377140039\n",
      "val_loss: 0.16131764085731148\n",
      "Progress: 59.8% ... Training loss: 0.073 ... Validation loss: 0.202iteration: 5981\n",
      "train_loss: 0.0734300444917746\n",
      "val_loss: 0.20236789456524412\n",
      "Progress: 59.8% ... Training loss: 0.065 ... Validation loss: 0.162iteration: 5982\n",
      "train_loss: 0.06556808037667665\n",
      "val_loss: 0.16203523279546606\n",
      "Progress: 59.8% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 5983\n",
      "train_loss: 0.06480319738831587\n",
      "val_loss: 0.17811151993302918\n",
      "Progress: 59.8% ... Training loss: 0.064 ... Validation loss: 0.172iteration: 5984\n",
      "train_loss: 0.06470476754352253\n",
      "val_loss: 0.1721647219785805\n",
      "Progress: 59.9% ... Training loss: 0.065 ... Validation loss: 0.188iteration: 5985\n",
      "train_loss: 0.06528419089121484\n",
      "val_loss: 0.18827642859189808\n",
      "Progress: 59.9% ... Training loss: 0.072 ... Validation loss: 0.169iteration: 5986\n",
      "train_loss: 0.07229706479831245\n",
      "val_loss: 0.16930501170681217\n",
      "Progress: 59.9% ... Training loss: 0.069 ... Validation loss: 0.206iteration: 5987\n",
      "train_loss: 0.0693595529229553\n",
      "val_loss: 0.20659143300700705\n",
      "Progress: 59.9% ... Training loss: 0.084 ... Validation loss: 0.159iteration: 5988\n",
      "train_loss: 0.08499845270751309\n",
      "val_loss: 0.15936118818280123\n",
      "Progress: 59.9% ... Training loss: 0.110 ... Validation loss: 0.270iteration: 5989\n",
      "train_loss: 0.11066570434924523\n",
      "val_loss: 0.27048204276320953\n",
      "Progress: 59.9% ... Training loss: 0.079 ... Validation loss: 0.160iteration: 5990\n",
      "train_loss: 0.07944491730378603\n",
      "val_loss: 0.16049180373999947\n",
      "Progress: 59.9% ... Training loss: 0.072 ... Validation loss: 0.212iteration: 5991\n",
      "train_loss: 0.07210206989085303\n",
      "val_loss: 0.21290908609421\n",
      "Progress: 59.9% ... Training loss: 0.067 ... Validation loss: 0.168iteration: 5992\n",
      "train_loss: 0.06749491232119588\n",
      "val_loss: 0.16802173644950644\n",
      "Progress: 59.9% ... Training loss: 0.071 ... Validation loss: 0.214iteration: 5993\n",
      "train_loss: 0.07129370376451274\n",
      "val_loss: 0.21450080337172459\n",
      "Progress: 59.9% ... Training loss: 0.071 ... Validation loss: 0.172iteration: 5994\n",
      "train_loss: 0.07199872194259445\n",
      "val_loss: 0.17282655281165749\n",
      "Progress: 60.0% ... Training loss: 0.065 ... Validation loss: 0.193iteration: 5995\n",
      "train_loss: 0.06577235672108445\n",
      "val_loss: 0.19342601320322936\n",
      "Progress: 60.0% ... Training loss: 0.064 ... Validation loss: 0.181iteration: 5996\n",
      "train_loss: 0.06455872141937502\n",
      "val_loss: 0.18123713295663116\n",
      "Progress: 60.0% ... Training loss: 0.066 ... Validation loss: 0.184iteration: 5997\n",
      "train_loss: 0.06667066135224209\n",
      "val_loss: 0.18406749882186138\n",
      "Progress: 60.0% ... Training loss: 0.071 ... Validation loss: 0.221iteration: 5998\n",
      "train_loss: 0.07103766443283306\n",
      "val_loss: 0.22156520775682678\n",
      "Progress: 60.0% ... Training loss: 0.064 ... Validation loss: 0.183iteration: 5999\n",
      "train_loss: 0.06429963652274036\n",
      "val_loss: 0.18328986596425184\n",
      "Progress: 60.0% ... Training loss: 0.065 ... Validation loss: 0.192iteration: 6000\n",
      "train_loss: 0.06523174604280874\n",
      "val_loss: 0.19279671413114707\n",
      "Progress: 60.0% ... Training loss: 0.065 ... Validation loss: 0.179iteration: 6001\n",
      "train_loss: 0.06550263335532713\n",
      "val_loss: 0.1797321219874234\n",
      "Progress: 60.0% ... Training loss: 0.065 ... Validation loss: 0.187iteration: 6002\n",
      "train_loss: 0.06513825999799973\n",
      "val_loss: 0.18738728930422915\n",
      "Progress: 60.0% ... Training loss: 0.065 ... Validation loss: 0.193iteration: 6003\n",
      "train_loss: 0.06538205719300526\n",
      "val_loss: 0.19395140768617353\n",
      "Progress: 60.0% ... Training loss: 0.067 ... Validation loss: 0.168iteration: 6004\n",
      "train_loss: 0.06782949335893969\n",
      "val_loss: 0.16809019340396697\n",
      "Progress: 60.0% ... Training loss: 0.065 ... Validation loss: 0.186iteration: 6005\n",
      "train_loss: 0.06535405480434504\n",
      "val_loss: 0.1860360521058365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 60.1% ... Training loss: 0.071 ... Validation loss: 0.165iteration: 6006\n",
      "train_loss: 0.07123837974954852\n",
      "val_loss: 0.1659877615781651\n",
      "Progress: 60.1% ... Training loss: 0.064 ... Validation loss: 0.191iteration: 6007\n",
      "train_loss: 0.06460576041474622\n",
      "val_loss: 0.19114598429269364\n",
      "Progress: 60.1% ... Training loss: 0.064 ... Validation loss: 0.167iteration: 6008\n",
      "train_loss: 0.06437280303744852\n",
      "val_loss: 0.16724179195311276\n",
      "Progress: 60.1% ... Training loss: 0.066 ... Validation loss: 0.164iteration: 6009\n",
      "train_loss: 0.0669098219941544\n",
      "val_loss: 0.16466676943371222\n",
      "Progress: 60.1% ... Training loss: 0.070 ... Validation loss: 0.193iteration: 6010\n",
      "train_loss: 0.07071262812463218\n",
      "val_loss: 0.19390133402882176\n",
      "Progress: 60.1% ... Training loss: 0.074 ... Validation loss: 0.157iteration: 6011\n",
      "train_loss: 0.07486433938475759\n",
      "val_loss: 0.15703566730190993\n",
      "Progress: 60.1% ... Training loss: 0.075 ... Validation loss: 0.197iteration: 6012\n",
      "train_loss: 0.07542802662983476\n",
      "val_loss: 0.1974508979160628\n",
      "Progress: 60.1% ... Training loss: 0.069 ... Validation loss: 0.162iteration: 6013\n",
      "train_loss: 0.0691024335959571\n",
      "val_loss: 0.16209699795810031\n",
      "Progress: 60.1% ... Training loss: 0.067 ... Validation loss: 0.182iteration: 6014\n",
      "train_loss: 0.067112196192559\n",
      "val_loss: 0.18259304251945407\n",
      "Progress: 60.1% ... Training loss: 0.066 ... Validation loss: 0.162iteration: 6015\n",
      "train_loss: 0.06624820344332026\n",
      "val_loss: 0.1626158197745125\n",
      "Progress: 60.2% ... Training loss: 0.065 ... Validation loss: 0.166iteration: 6016\n",
      "train_loss: 0.065200372449051\n",
      "val_loss: 0.16666228991309467\n",
      "Progress: 60.2% ... Training loss: 0.065 ... Validation loss: 0.185iteration: 6017\n",
      "train_loss: 0.06504478936334902\n",
      "val_loss: 0.18572170406464955\n",
      "Progress: 60.2% ... Training loss: 0.070 ... Validation loss: 0.168iteration: 6018\n",
      "train_loss: 0.07031334582340557\n",
      "val_loss: 0.1680389234523657\n",
      "Progress: 60.2% ... Training loss: 0.084 ... Validation loss: 0.224iteration: 6019\n",
      "train_loss: 0.08424593177846171\n",
      "val_loss: 0.224380216227887\n",
      "Progress: 60.2% ... Training loss: 0.073 ... Validation loss: 0.155iteration: 6020\n",
      "train_loss: 0.07366850555463209\n",
      "val_loss: 0.1559135259675817\n",
      "Progress: 60.2% ... Training loss: 0.076 ... Validation loss: 0.196iteration: 6021\n",
      "train_loss: 0.07667240056334414\n",
      "val_loss: 0.19667236886650113\n",
      "Progress: 60.2% ... Training loss: 0.067 ... Validation loss: 0.169iteration: 6022\n",
      "train_loss: 0.06740576588452095\n",
      "val_loss: 0.16926183371888548\n",
      "Progress: 60.2% ... Training loss: 0.069 ... Validation loss: 0.192iteration: 6023\n",
      "train_loss: 0.0698090996357691\n",
      "val_loss: 0.1921332054318981\n",
      "Progress: 60.2% ... Training loss: 0.068 ... Validation loss: 0.162iteration: 6024\n",
      "train_loss: 0.06882402746219067\n",
      "val_loss: 0.16289536828696027\n",
      "Progress: 60.2% ... Training loss: 0.064 ... Validation loss: 0.183iteration: 6025\n",
      "train_loss: 0.0641098096162508\n",
      "val_loss: 0.18333059806008487\n",
      "Progress: 60.3% ... Training loss: 0.064 ... Validation loss: 0.175iteration: 6026\n",
      "train_loss: 0.064413544429767\n",
      "val_loss: 0.17572545218984653\n",
      "Progress: 60.3% ... Training loss: 0.065 ... Validation loss: 0.167iteration: 6027\n",
      "train_loss: 0.06562121346859195\n",
      "val_loss: 0.1676835466811778\n",
      "Progress: 60.3% ... Training loss: 0.065 ... Validation loss: 0.175iteration: 6028\n",
      "train_loss: 0.06562898108416408\n",
      "val_loss: 0.17597173865572163\n",
      "Progress: 60.3% ... Training loss: 0.064 ... Validation loss: 0.168iteration: 6029\n",
      "train_loss: 0.06422424801191924\n",
      "val_loss: 0.16830136225267903\n",
      "Progress: 60.3% ... Training loss: 0.066 ... Validation loss: 0.170iteration: 6030\n",
      "train_loss: 0.06627726176145723\n",
      "val_loss: 0.1700664624036368\n",
      "Progress: 60.3% ... Training loss: 0.070 ... Validation loss: 0.205iteration: 6031\n",
      "train_loss: 0.07007489994414991\n",
      "val_loss: 0.2055395783276328\n",
      "Progress: 60.3% ... Training loss: 0.072 ... Validation loss: 0.160iteration: 6032\n",
      "train_loss: 0.07258391114840604\n",
      "val_loss: 0.1606451193077964\n",
      "Progress: 60.3% ... Training loss: 0.069 ... Validation loss: 0.206iteration: 6033\n",
      "train_loss: 0.06909298780954894\n",
      "val_loss: 0.20647832792950463\n",
      "Progress: 60.3% ... Training loss: 0.066 ... Validation loss: 0.164iteration: 6034\n",
      "train_loss: 0.06661430102191353\n",
      "val_loss: 0.1642178304018903\n",
      "Progress: 60.4% ... Training loss: 0.074 ... Validation loss: 0.207iteration: 6035\n",
      "train_loss: 0.07463265975223914\n",
      "val_loss: 0.20786721697684504\n",
      "Progress: 60.4% ... Training loss: 0.071 ... Validation loss: 0.160iteration: 6036\n",
      "train_loss: 0.07167146787520015\n",
      "val_loss: 0.16089983030414787\n",
      "Progress: 60.4% ... Training loss: 0.077 ... Validation loss: 0.218iteration: 6037\n",
      "train_loss: 0.07724059040338756\n",
      "val_loss: 0.21863217374030725\n",
      "Progress: 60.4% ... Training loss: 0.067 ... Validation loss: 0.173iteration: 6038\n",
      "train_loss: 0.06788962142318236\n",
      "val_loss: 0.17320255634574577\n",
      "Progress: 60.4% ... Training loss: 0.064 ... Validation loss: 0.186iteration: 6039\n",
      "train_loss: 0.06468515528844099\n",
      "val_loss: 0.18662725931558566\n",
      "Progress: 60.4% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 6040\n",
      "train_loss: 0.06457220010995372\n",
      "val_loss: 0.17334138329537027\n",
      "Progress: 60.4% ... Training loss: 0.065 ... Validation loss: 0.186iteration: 6041\n",
      "train_loss: 0.0650401619914589\n",
      "val_loss: 0.18671705918917247\n",
      "Progress: 60.4% ... Training loss: 0.064 ... Validation loss: 0.187iteration: 6042\n",
      "train_loss: 0.06424710217826127\n",
      "val_loss: 0.18733961713630864\n",
      "Progress: 60.4% ... Training loss: 0.064 ... Validation loss: 0.191iteration: 6043\n",
      "train_loss: 0.06465389148289649\n",
      "val_loss: 0.19177631272688578\n",
      "Progress: 60.4% ... Training loss: 0.083 ... Validation loss: 0.166iteration: 6044\n",
      "train_loss: 0.0830135512586594\n",
      "val_loss: 0.16661067179290698\n",
      "Progress: 60.5% ... Training loss: 0.073 ... Validation loss: 0.213iteration: 6045\n",
      "train_loss: 0.07355222883138278\n",
      "val_loss: 0.2133172263733714\n",
      "Progress: 60.5% ... Training loss: 0.085 ... Validation loss: 0.156iteration: 6046\n",
      "train_loss: 0.08529423247895272\n",
      "val_loss: 0.15641490134322808\n",
      "Progress: 60.5% ... Training loss: 0.105 ... Validation loss: 0.279iteration: 6047\n",
      "train_loss: 0.10535618219148188\n",
      "val_loss: 0.27936709714151325\n",
      "Progress: 60.5% ... Training loss: 0.118 ... Validation loss: 0.165iteration: 6048\n",
      "train_loss: 0.11830376432358843\n",
      "val_loss: 0.16546951483060346\n",
      "Progress: 60.5% ... Training loss: 0.092 ... Validation loss: 0.271iteration: 6049\n",
      "train_loss: 0.09292738677616093\n",
      "val_loss: 0.27158447651596057\n",
      "Progress: 60.5% ... Training loss: 0.093 ... Validation loss: 0.161iteration: 6050\n",
      "train_loss: 0.09380304262097566\n",
      "val_loss: 0.16145432654900144\n",
      "Progress: 60.5% ... Training loss: 0.086 ... Validation loss: 0.251iteration: 6051\n",
      "train_loss: 0.08638309967548427\n",
      "val_loss: 0.25182417866993495\n",
      "Progress: 60.5% ... Training loss: 0.108 ... Validation loss: 0.160iteration: 6052\n",
      "train_loss: 0.1086975835224636\n",
      "val_loss: 0.1600022379251243\n",
      "Progress: 60.5% ... Training loss: 0.104 ... Validation loss: 0.295iteration: 6053\n",
      "train_loss: 0.10474075145339738\n",
      "val_loss: 0.2955157682955939\n",
      "Progress: 60.5% ... Training loss: 0.085 ... Validation loss: 0.160iteration: 6054\n",
      "train_loss: 0.08569013867854522\n",
      "val_loss: 0.16063016702888375\n",
      "Progress: 60.5% ... Training loss: 0.080 ... Validation loss: 0.241iteration: 6055\n",
      "train_loss: 0.08067606007280322\n",
      "val_loss: 0.241366962853354\n",
      "Progress: 60.6% ... Training loss: 0.078 ... Validation loss: 0.156iteration: 6056\n",
      "train_loss: 0.07872420612074245\n",
      "val_loss: 0.15681999431913993\n",
      "Progress: 60.6% ... Training loss: 0.083 ... Validation loss: 0.253iteration: 6057\n",
      "train_loss: 0.08357769898025094\n",
      "val_loss: 0.2532359324569254\n",
      "Progress: 60.6% ... Training loss: 0.066 ... Validation loss: 0.170iteration: 6058\n",
      "train_loss: 0.06677393635104337\n",
      "val_loss: 0.17043866042816877\n",
      "Progress: 60.6% ... Training loss: 0.077 ... Validation loss: 0.218iteration: 6059\n",
      "train_loss: 0.07757106139627319\n",
      "val_loss: 0.21899200234978086\n",
      "Progress: 60.6% ... Training loss: 0.078 ... Validation loss: 0.160iteration: 6060\n",
      "train_loss: 0.07848132662911589\n",
      "val_loss: 0.16086588259168302\n",
      "Progress: 60.6% ... Training loss: 0.091 ... Validation loss: 0.244iteration: 6061\n",
      "train_loss: 0.09153496115163173\n",
      "val_loss: 0.24493189498335768\n",
      "Progress: 60.6% ... Training loss: 0.072 ... Validation loss: 0.158iteration: 6062\n",
      "train_loss: 0.07225943895518508\n",
      "val_loss: 0.15838965774993327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 60.6% ... Training loss: 0.069 ... Validation loss: 0.214iteration: 6063\n",
      "train_loss: 0.06945798472622723\n",
      "val_loss: 0.21437339050620663\n",
      "Progress: 60.6% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 6064\n",
      "train_loss: 0.06366153709505552\n",
      "val_loss: 0.17838676499505596\n",
      "Progress: 60.6% ... Training loss: 0.076 ... Validation loss: 0.156iteration: 6065\n",
      "train_loss: 0.07681996269371973\n",
      "val_loss: 0.15697258405271078\n",
      "Progress: 60.7% ... Training loss: 0.090 ... Validation loss: 0.246iteration: 6066\n",
      "train_loss: 0.09017804069922014\n",
      "val_loss: 0.2468513227363177\n",
      "Progress: 60.7% ... Training loss: 0.080 ... Validation loss: 0.151iteration: 6067\n",
      "train_loss: 0.08069316332608671\n",
      "val_loss: 0.15134975636462986\n",
      "Progress: 60.7% ... Training loss: 0.070 ... Validation loss: 0.203iteration: 6068\n",
      "train_loss: 0.07036581623772928\n",
      "val_loss: 0.2033032697057482\n",
      "Progress: 60.7% ... Training loss: 0.067 ... Validation loss: 0.163iteration: 6069\n",
      "train_loss: 0.06795400401509286\n",
      "val_loss: 0.16368359642040398\n",
      "Progress: 60.7% ... Training loss: 0.078 ... Validation loss: 0.198iteration: 6070\n",
      "train_loss: 0.07840901298261894\n",
      "val_loss: 0.19814737741137284\n",
      "Progress: 60.7% ... Training loss: 0.066 ... Validation loss: 0.166iteration: 6071\n",
      "train_loss: 0.0668637766703766\n",
      "val_loss: 0.16653046926263573\n",
      "Progress: 60.7% ... Training loss: 0.085 ... Validation loss: 0.231iteration: 6072\n",
      "train_loss: 0.0853279331627137\n",
      "val_loss: 0.2318847912023957\n",
      "Progress: 60.7% ... Training loss: 0.081 ... Validation loss: 0.150iteration: 6073\n",
      "train_loss: 0.08111179572686784\n",
      "val_loss: 0.150839831413168\n",
      "Progress: 60.7% ... Training loss: 0.064 ... Validation loss: 0.187iteration: 6074\n",
      "train_loss: 0.06484942055262839\n",
      "val_loss: 0.18743876611827787\n",
      "Progress: 60.8% ... Training loss: 0.063 ... Validation loss: 0.171iteration: 6075\n",
      "train_loss: 0.06338367855990529\n",
      "val_loss: 0.1718542096190164\n",
      "Progress: 60.8% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 6076\n",
      "train_loss: 0.06389789509473223\n",
      "val_loss: 0.17482509569698257\n",
      "Progress: 60.8% ... Training loss: 0.064 ... Validation loss: 0.163iteration: 6077\n",
      "train_loss: 0.06449715854714644\n",
      "val_loss: 0.1635870119626967\n",
      "Progress: 60.8% ... Training loss: 0.064 ... Validation loss: 0.164iteration: 6078\n",
      "train_loss: 0.06456070516618707\n",
      "val_loss: 0.16454767164275397\n",
      "Progress: 60.8% ... Training loss: 0.065 ... Validation loss: 0.161iteration: 6079\n",
      "train_loss: 0.06504327171963714\n",
      "val_loss: 0.16124842549825055\n",
      "Progress: 60.8% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 6080\n",
      "train_loss: 0.06427549636827382\n",
      "val_loss: 0.17672096315386257\n",
      "Progress: 60.8% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 6081\n",
      "train_loss: 0.07062123180758183\n",
      "val_loss: 0.15011538524784773\n",
      "Progress: 60.8% ... Training loss: 0.064 ... Validation loss: 0.179iteration: 6082\n",
      "train_loss: 0.06433684593237589\n",
      "val_loss: 0.1793904792242689\n",
      "Progress: 60.8% ... Training loss: 0.065 ... Validation loss: 0.178iteration: 6083\n",
      "train_loss: 0.06578172925243811\n",
      "val_loss: 0.17847598117983757\n",
      "Progress: 60.8% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 6084\n",
      "train_loss: 0.06587664449029061\n",
      "val_loss: 0.1584944834929001\n",
      "Progress: 60.9% ... Training loss: 0.081 ... Validation loss: 0.215iteration: 6085\n",
      "train_loss: 0.08165418545398784\n",
      "val_loss: 0.21597202973001156\n",
      "Progress: 60.9% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 6086\n",
      "train_loss: 0.08188318440096407\n",
      "val_loss: 0.15432509225368524\n",
      "Progress: 60.9% ... Training loss: 0.083 ... Validation loss: 0.215iteration: 6087\n",
      "train_loss: 0.08355496727221891\n",
      "val_loss: 0.21539413501692653\n",
      "Progress: 60.9% ... Training loss: 0.071 ... Validation loss: 0.155iteration: 6088\n",
      "train_loss: 0.07108589442652814\n",
      "val_loss: 0.15514567097931117\n",
      "Progress: 60.9% ... Training loss: 0.093 ... Validation loss: 0.238iteration: 6089\n",
      "train_loss: 0.09363652221008571\n",
      "val_loss: 0.23872841229615813\n",
      "Progress: 60.9% ... Training loss: 0.076 ... Validation loss: 0.151iteration: 6090\n",
      "train_loss: 0.07609577825378566\n",
      "val_loss: 0.15131339087830875\n",
      "Progress: 60.9% ... Training loss: 0.064 ... Validation loss: 0.174iteration: 6091\n",
      "train_loss: 0.0642422247921628\n",
      "val_loss: 0.17471246270598387\n",
      "Progress: 60.9% ... Training loss: 0.063 ... Validation loss: 0.169iteration: 6092\n",
      "train_loss: 0.06374291416275485\n",
      "val_loss: 0.16951208418036195\n",
      "Progress: 60.9% ... Training loss: 0.064 ... Validation loss: 0.160iteration: 6093\n",
      "train_loss: 0.06479575965675853\n",
      "val_loss: 0.16097807694681757\n",
      "Progress: 60.9% ... Training loss: 0.070 ... Validation loss: 0.155iteration: 6094\n",
      "train_loss: 0.07073038100621305\n",
      "val_loss: 0.1558464445476205\n",
      "Progress: 61.0% ... Training loss: 0.065 ... Validation loss: 0.162iteration: 6095\n",
      "train_loss: 0.06580772867316168\n",
      "val_loss: 0.162887213502374\n",
      "Progress: 61.0% ... Training loss: 0.063 ... Validation loss: 0.172iteration: 6096\n",
      "train_loss: 0.06358784882676083\n",
      "val_loss: 0.17201818875285693\n",
      "Progress: 61.0% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 6097\n",
      "train_loss: 0.06471021524246234\n",
      "val_loss: 0.17696246291080298\n",
      "Progress: 61.0% ... Training loss: 0.073 ... Validation loss: 0.153iteration: 6098\n",
      "train_loss: 0.0734126216600268\n",
      "val_loss: 0.15384915903853094\n",
      "Progress: 61.0% ... Training loss: 0.078 ... Validation loss: 0.214iteration: 6099\n",
      "train_loss: 0.07810429752010645\n",
      "val_loss: 0.21423721165515763\n",
      "Progress: 61.0% ... Training loss: 0.078 ... Validation loss: 0.153iteration: 6100\n",
      "train_loss: 0.07838095849190693\n",
      "val_loss: 0.15369182704524043\n",
      "Progress: 61.0% ... Training loss: 0.082 ... Validation loss: 0.214iteration: 6101\n",
      "train_loss: 0.08266582702227358\n",
      "val_loss: 0.2149574589678674\n",
      "Progress: 61.0% ... Training loss: 0.073 ... Validation loss: 0.156iteration: 6102\n",
      "train_loss: 0.07366326918730551\n",
      "val_loss: 0.15603495486869953\n",
      "Progress: 61.0% ... Training loss: 0.073 ... Validation loss: 0.199iteration: 6103\n",
      "train_loss: 0.07319957390716654\n",
      "val_loss: 0.19941940689618828\n",
      "Progress: 61.0% ... Training loss: 0.071 ... Validation loss: 0.156iteration: 6104\n",
      "train_loss: 0.07163781396065659\n",
      "val_loss: 0.15641653149769394\n",
      "Progress: 61.0% ... Training loss: 0.070 ... Validation loss: 0.194iteration: 6105\n",
      "train_loss: 0.07036683709205545\n",
      "val_loss: 0.19444920375874367\n",
      "Progress: 61.1% ... Training loss: 0.063 ... Validation loss: 0.168iteration: 6106\n",
      "train_loss: 0.06355415512131574\n",
      "val_loss: 0.16847957989838275\n",
      "Progress: 61.1% ... Training loss: 0.063 ... Validation loss: 0.159iteration: 6107\n",
      "train_loss: 0.06386741963287686\n",
      "val_loss: 0.15969760710062542\n",
      "Progress: 61.1% ... Training loss: 0.064 ... Validation loss: 0.168iteration: 6108\n",
      "train_loss: 0.0641826451200203\n",
      "val_loss: 0.1685209355181311\n",
      "Progress: 61.1% ... Training loss: 0.066 ... Validation loss: 0.153iteration: 6109\n",
      "train_loss: 0.06611673931196464\n",
      "val_loss: 0.1539313900644275\n",
      "Progress: 61.1% ... Training loss: 0.071 ... Validation loss: 0.184iteration: 6110\n",
      "train_loss: 0.0719967003651788\n",
      "val_loss: 0.1846218715902456\n",
      "Progress: 61.1% ... Training loss: 0.077 ... Validation loss: 0.150iteration: 6111\n",
      "train_loss: 0.07728201503681553\n",
      "val_loss: 0.15061896774099548\n",
      "Progress: 61.1% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 6112\n",
      "train_loss: 0.06452484147178723\n",
      "val_loss: 0.17623861008444952\n",
      "Progress: 61.1% ... Training loss: 0.065 ... Validation loss: 0.155iteration: 6113\n",
      "train_loss: 0.06549616559240298\n",
      "val_loss: 0.15502549564021936\n",
      "Progress: 61.1% ... Training loss: 0.065 ... Validation loss: 0.163iteration: 6114\n",
      "train_loss: 0.06572213388297153\n",
      "val_loss: 0.16374013414995392\n",
      "Progress: 61.1% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 6115\n",
      "train_loss: 0.06330890595402627\n",
      "val_loss: 0.16179091228774656\n",
      "Progress: 61.2% ... Training loss: 0.065 ... Validation loss: 0.169iteration: 6116\n",
      "train_loss: 0.06513191897526543\n",
      "val_loss: 0.16967200711331318\n",
      "Progress: 61.2% ... Training loss: 0.064 ... Validation loss: 0.163iteration: 6117\n",
      "train_loss: 0.06486826941388277\n",
      "val_loss: 0.16337128853183486\n",
      "Progress: 61.2% ... Training loss: 0.066 ... Validation loss: 0.155iteration: 6118\n",
      "train_loss: 0.0665072937728539\n",
      "val_loss: 0.15518262271362143\n",
      "Progress: 61.2% ... Training loss: 0.078 ... Validation loss: 0.210iteration: 6119\n",
      "train_loss: 0.07876881782431022\n",
      "val_loss: 0.21014525514288335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 61.2% ... Training loss: 0.082 ... Validation loss: 0.150iteration: 6120\n",
      "train_loss: 0.08232620671736915\n",
      "val_loss: 0.15007254504755602\n",
      "Progress: 61.2% ... Training loss: 0.076 ... Validation loss: 0.219iteration: 6121\n",
      "train_loss: 0.07631230130114884\n",
      "val_loss: 0.2199987363869574\n",
      "Progress: 61.2% ... Training loss: 0.085 ... Validation loss: 0.152iteration: 6122\n",
      "train_loss: 0.0854922996942408\n",
      "val_loss: 0.1524561689060692\n",
      "Progress: 61.2% ... Training loss: 0.074 ... Validation loss: 0.210iteration: 6123\n",
      "train_loss: 0.07461806412963828\n",
      "val_loss: 0.2108880856354984\n",
      "Progress: 61.2% ... Training loss: 0.076 ... Validation loss: 0.154iteration: 6124\n",
      "train_loss: 0.07695078918763117\n",
      "val_loss: 0.15411203171018323\n",
      "Progress: 61.2% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 6125\n",
      "train_loss: 0.06413633296587795\n",
      "val_loss: 0.1783312779168254\n",
      "Progress: 61.3% ... Training loss: 0.064 ... Validation loss: 0.165iteration: 6126\n",
      "train_loss: 0.06483709668548938\n",
      "val_loss: 0.1659394090752954\n",
      "Progress: 61.3% ... Training loss: 0.064 ... Validation loss: 0.180iteration: 6127\n",
      "train_loss: 0.06432915175237745\n",
      "val_loss: 0.18020188412050603\n",
      "Progress: 61.3% ... Training loss: 0.063 ... Validation loss: 0.165iteration: 6128\n",
      "train_loss: 0.06356748724599402\n",
      "val_loss: 0.1658815940120963\n",
      "Progress: 61.3% ... Training loss: 0.071 ... Validation loss: 0.153iteration: 6129\n",
      "train_loss: 0.07118230202271039\n",
      "val_loss: 0.15394907049747342\n",
      "Progress: 61.3% ... Training loss: 0.063 ... Validation loss: 0.171iteration: 6130\n",
      "train_loss: 0.06335123882272989\n",
      "val_loss: 0.1711687253943857\n",
      "Progress: 61.3% ... Training loss: 0.063 ... Validation loss: 0.158iteration: 6131\n",
      "train_loss: 0.06347969548255455\n",
      "val_loss: 0.15897567673933558\n",
      "Progress: 61.3% ... Training loss: 0.063 ... Validation loss: 0.180iteration: 6132\n",
      "train_loss: 0.06372707331984898\n",
      "val_loss: 0.1808995734924731\n",
      "Progress: 61.3% ... Training loss: 0.071 ... Validation loss: 0.163iteration: 6133\n",
      "train_loss: 0.07122249189395814\n",
      "val_loss: 0.16319095423565408\n",
      "Progress: 61.3% ... Training loss: 0.068 ... Validation loss: 0.204iteration: 6134\n",
      "train_loss: 0.06827664548573066\n",
      "val_loss: 0.204892533224304\n",
      "Progress: 61.4% ... Training loss: 0.069 ... Validation loss: 0.155iteration: 6135\n",
      "train_loss: 0.06908840519953237\n",
      "val_loss: 0.15585233426690137\n",
      "Progress: 61.4% ... Training loss: 0.068 ... Validation loss: 0.197iteration: 6136\n",
      "train_loss: 0.06878828418363105\n",
      "val_loss: 0.19706831628772098\n",
      "Progress: 61.4% ... Training loss: 0.070 ... Validation loss: 0.157iteration: 6137\n",
      "train_loss: 0.07068477817310947\n",
      "val_loss: 0.15759035920579345\n",
      "Progress: 61.4% ... Training loss: 0.074 ... Validation loss: 0.195iteration: 6138\n",
      "train_loss: 0.07482000943317327\n",
      "val_loss: 0.19558997818793644\n",
      "Progress: 61.4% ... Training loss: 0.073 ... Validation loss: 0.155iteration: 6139\n",
      "train_loss: 0.0734108485596185\n",
      "val_loss: 0.1555463286567609\n",
      "Progress: 61.4% ... Training loss: 0.069 ... Validation loss: 0.196iteration: 6140\n",
      "train_loss: 0.0696159526976844\n",
      "val_loss: 0.19679130475525516\n",
      "Progress: 61.4% ... Training loss: 0.079 ... Validation loss: 0.153iteration: 6141\n",
      "train_loss: 0.07916133542564835\n",
      "val_loss: 0.153361537626975\n",
      "Progress: 61.4% ... Training loss: 0.072 ... Validation loss: 0.197iteration: 6142\n",
      "train_loss: 0.07275556097296301\n",
      "val_loss: 0.19748012147411273\n",
      "Progress: 61.4% ... Training loss: 0.066 ... Validation loss: 0.159iteration: 6143\n",
      "train_loss: 0.06637114268585206\n",
      "val_loss: 0.15959589907616978\n",
      "Progress: 61.4% ... Training loss: 0.064 ... Validation loss: 0.163iteration: 6144\n",
      "train_loss: 0.06411365092919542\n",
      "val_loss: 0.16344854200697967\n",
      "Progress: 61.5% ... Training loss: 0.063 ... Validation loss: 0.160iteration: 6145\n",
      "train_loss: 0.06386301909258495\n",
      "val_loss: 0.16075939159876987\n",
      "Progress: 61.5% ... Training loss: 0.065 ... Validation loss: 0.161iteration: 6146\n",
      "train_loss: 0.06508185128115022\n",
      "val_loss: 0.1614680051256255\n",
      "Progress: 61.5% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 6147\n",
      "train_loss: 0.06568616585448375\n",
      "val_loss: 0.15887924316977112\n",
      "Progress: 61.5% ... Training loss: 0.070 ... Validation loss: 0.182iteration: 6148\n",
      "train_loss: 0.07044588852748052\n",
      "val_loss: 0.1826760822591994\n",
      "Progress: 61.5% ... Training loss: 0.064 ... Validation loss: 0.165iteration: 6149\n",
      "train_loss: 0.06489583940788304\n",
      "val_loss: 0.16535642326198574\n",
      "Progress: 61.5% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 6150\n",
      "train_loss: 0.06420728917815244\n",
      "val_loss: 0.1708549928915587\n",
      "Progress: 61.5% ... Training loss: 0.071 ... Validation loss: 0.181iteration: 6151\n",
      "train_loss: 0.0711075982933169\n",
      "val_loss: 0.1814251731728358\n",
      "Progress: 61.5% ... Training loss: 0.080 ... Validation loss: 0.156iteration: 6152\n",
      "train_loss: 0.08076561891867537\n",
      "val_loss: 0.15672925368724575\n",
      "Progress: 61.5% ... Training loss: 0.081 ... Validation loss: 0.221iteration: 6153\n",
      "train_loss: 0.0816728651711259\n",
      "val_loss: 0.22191849675613073\n",
      "Progress: 61.5% ... Training loss: 0.086 ... Validation loss: 0.159iteration: 6154\n",
      "train_loss: 0.08690467425055592\n",
      "val_loss: 0.15916674335001418\n",
      "Progress: 61.5% ... Training loss: 0.105 ... Validation loss: 0.221iteration: 6155\n",
      "train_loss: 0.10572971929575714\n",
      "val_loss: 0.2211549577033301\n",
      "Progress: 61.6% ... Training loss: 0.105 ... Validation loss: 0.168iteration: 6156\n",
      "train_loss: 0.10509204072981516\n",
      "val_loss: 0.16867220985221767\n",
      "Progress: 61.6% ... Training loss: 0.107 ... Validation loss: 0.238iteration: 6157\n",
      "train_loss: 0.10736853295415698\n",
      "val_loss: 0.23895379576230932\n",
      "Progress: 61.6% ... Training loss: 0.086 ... Validation loss: 0.155iteration: 6158\n",
      "train_loss: 0.08657709217026698\n",
      "val_loss: 0.15535974567762925\n",
      "Progress: 61.6% ... Training loss: 0.107 ... Validation loss: 0.248iteration: 6159\n",
      "train_loss: 0.10766982312701626\n",
      "val_loss: 0.2480027015018196\n",
      "Progress: 61.6% ... Training loss: 0.091 ... Validation loss: 0.150iteration: 6160\n",
      "train_loss: 0.09111338419711847\n",
      "val_loss: 0.1500904283592255\n",
      "Progress: 61.6% ... Training loss: 0.082 ... Validation loss: 0.220iteration: 6161\n",
      "train_loss: 0.08207411315804419\n",
      "val_loss: 0.22002765042346334\n",
      "Progress: 61.6% ... Training loss: 0.072 ... Validation loss: 0.160iteration: 6162\n",
      "train_loss: 0.07213153803984136\n",
      "val_loss: 0.16061560604378441\n",
      "Progress: 61.6% ... Training loss: 0.068 ... Validation loss: 0.205iteration: 6163\n",
      "train_loss: 0.0683458814075151\n",
      "val_loss: 0.20589600153336193\n",
      "Progress: 61.6% ... Training loss: 0.069 ... Validation loss: 0.160iteration: 6164\n",
      "train_loss: 0.069981134692999\n",
      "val_loss: 0.16026233721527625\n",
      "Progress: 61.6% ... Training loss: 0.070 ... Validation loss: 0.218iteration: 6165\n",
      "train_loss: 0.07060346641345491\n",
      "val_loss: 0.21893795883480172\n",
      "Progress: 61.7% ... Training loss: 0.069 ... Validation loss: 0.162iteration: 6166\n",
      "train_loss: 0.06934997113725805\n",
      "val_loss: 0.16210347933581187\n",
      "Progress: 61.7% ... Training loss: 0.063 ... Validation loss: 0.167iteration: 6167\n",
      "train_loss: 0.06396894192915359\n",
      "val_loss: 0.16779854712026274\n",
      "Progress: 61.7% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 6168\n",
      "train_loss: 0.06453014390948784\n",
      "val_loss: 0.17627267779109357\n",
      "Progress: 61.7% ... Training loss: 0.063 ... Validation loss: 0.168iteration: 6169\n",
      "train_loss: 0.06338474622825745\n",
      "val_loss: 0.16837817013503248\n",
      "Progress: 61.7% ... Training loss: 0.067 ... Validation loss: 0.186iteration: 6170\n",
      "train_loss: 0.06789901523968324\n",
      "val_loss: 0.18660084557540033\n",
      "Progress: 61.7% ... Training loss: 0.067 ... Validation loss: 0.156iteration: 6171\n",
      "train_loss: 0.06702737174625453\n",
      "val_loss: 0.15608509606331408\n",
      "Progress: 61.7% ... Training loss: 0.085 ... Validation loss: 0.214iteration: 6172\n",
      "train_loss: 0.08576915503724268\n",
      "val_loss: 0.21439136102098982\n",
      "Progress: 61.7% ... Training loss: 0.101 ... Validation loss: 0.157iteration: 6173\n",
      "train_loss: 0.10152998063418649\n",
      "val_loss: 0.157144085636133\n",
      "Progress: 61.7% ... Training loss: 0.111 ... Validation loss: 0.279iteration: 6174\n",
      "train_loss: 0.11115092897253225\n",
      "val_loss: 0.27901555926330585\n",
      "Progress: 61.8% ... Training loss: 0.109 ... Validation loss: 0.154iteration: 6175\n",
      "train_loss: 0.10939332773207185\n",
      "val_loss: 0.1549622464951317\n",
      "Progress: 61.8% ... Training loss: 0.103 ... Validation loss: 0.265iteration: 6176\n",
      "train_loss: 0.10331600532449081\n",
      "val_loss: 0.2657614737358992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 61.8% ... Training loss: 0.110 ... Validation loss: 0.153iteration: 6177\n",
      "train_loss: 0.11039833093779299\n",
      "val_loss: 0.15381708600143687\n",
      "Progress: 61.8% ... Training loss: 0.099 ... Validation loss: 0.238iteration: 6178\n",
      "train_loss: 0.09959198325612416\n",
      "val_loss: 0.23843438030924\n",
      "Progress: 61.8% ... Training loss: 0.117 ... Validation loss: 0.163iteration: 6179\n",
      "train_loss: 0.1174860597185418\n",
      "val_loss: 0.16327756790918285\n",
      "Progress: 61.8% ... Training loss: 0.142 ... Validation loss: 0.290iteration: 6180\n",
      "train_loss: 0.14293231052971178\n",
      "val_loss: 0.2908125655966645\n",
      "Progress: 61.8% ... Training loss: 0.133 ... Validation loss: 0.172iteration: 6181\n",
      "train_loss: 0.13363428117475393\n",
      "val_loss: 0.17213078013803684\n",
      "Progress: 61.8% ... Training loss: 0.092 ... Validation loss: 0.252iteration: 6182\n",
      "train_loss: 0.09281740682098776\n",
      "val_loss: 0.25293068698699855\n",
      "Progress: 61.8% ... Training loss: 0.073 ... Validation loss: 0.147iteration: 6183\n",
      "train_loss: 0.07394283438848213\n",
      "val_loss: 0.14749266895422117\n",
      "Progress: 61.8% ... Training loss: 0.074 ... Validation loss: 0.197iteration: 6184\n",
      "train_loss: 0.0740910387285194\n",
      "val_loss: 0.19749850188098822\n",
      "Progress: 61.9% ... Training loss: 0.068 ... Validation loss: 0.153iteration: 6185\n",
      "train_loss: 0.06884699343157069\n",
      "val_loss: 0.15314875755270618\n",
      "Progress: 61.9% ... Training loss: 0.064 ... Validation loss: 0.167iteration: 6186\n",
      "train_loss: 0.0644662322990663\n",
      "val_loss: 0.1679467939269905\n",
      "Progress: 61.9% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 6187\n",
      "train_loss: 0.06354985527140936\n",
      "val_loss: 0.166032318511293\n",
      "Progress: 61.9% ... Training loss: 0.065 ... Validation loss: 0.178iteration: 6188\n",
      "train_loss: 0.06503125614372937\n",
      "val_loss: 0.17898877320162676\n",
      "Progress: 61.9% ... Training loss: 0.064 ... Validation loss: 0.171iteration: 6189\n",
      "train_loss: 0.06474703729890592\n",
      "val_loss: 0.1715918844867868\n",
      "Progress: 61.9% ... Training loss: 0.064 ... Validation loss: 0.168iteration: 6190\n",
      "train_loss: 0.06403123984999716\n",
      "val_loss: 0.16813849360724664\n",
      "Progress: 61.9% ... Training loss: 0.068 ... Validation loss: 0.154iteration: 6191\n",
      "train_loss: 0.06829279485914172\n",
      "val_loss: 0.15491570984145123\n",
      "Progress: 61.9% ... Training loss: 0.064 ... Validation loss: 0.162iteration: 6192\n",
      "train_loss: 0.0646858566100081\n",
      "val_loss: 0.1620965923562319\n",
      "Progress: 61.9% ... Training loss: 0.063 ... Validation loss: 0.159iteration: 6193\n",
      "train_loss: 0.0634547693111321\n",
      "val_loss: 0.1593995801413558\n",
      "Progress: 61.9% ... Training loss: 0.063 ... Validation loss: 0.168iteration: 6194\n",
      "train_loss: 0.06362014889545226\n",
      "val_loss: 0.16819030590946987\n",
      "Progress: 62.0% ... Training loss: 0.063 ... Validation loss: 0.164iteration: 6195\n",
      "train_loss: 0.06352585919744562\n",
      "val_loss: 0.16400690758481343\n",
      "Progress: 62.0% ... Training loss: 0.066 ... Validation loss: 0.174iteration: 6196\n",
      "train_loss: 0.0663785621381963\n",
      "val_loss: 0.17473568375857076\n",
      "Progress: 62.0% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 6197\n",
      "train_loss: 0.06496877255204947\n",
      "val_loss: 0.15616525691883\n",
      "Progress: 62.0% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 6198\n",
      "train_loss: 0.06353913968221528\n",
      "val_loss: 0.17401440555034772\n",
      "Progress: 62.0% ... Training loss: 0.062 ... Validation loss: 0.171iteration: 6199\n",
      "train_loss: 0.06298348663145553\n",
      "val_loss: 0.1712324750643241\n",
      "Progress: 62.0% ... Training loss: 0.063 ... Validation loss: 0.165iteration: 6200\n",
      "train_loss: 0.0631743614978222\n",
      "val_loss: 0.1654547259751468\n",
      "Progress: 62.0% ... Training loss: 0.064 ... Validation loss: 0.188iteration: 6201\n",
      "train_loss: 0.06469776727333611\n",
      "val_loss: 0.18812920102028935\n",
      "Progress: 62.0% ... Training loss: 0.066 ... Validation loss: 0.163iteration: 6202\n",
      "train_loss: 0.06673099824244123\n",
      "val_loss: 0.16336582975865976\n",
      "Progress: 62.0% ... Training loss: 0.063 ... Validation loss: 0.180iteration: 6203\n",
      "train_loss: 0.06351997712948386\n",
      "val_loss: 0.1803602188581691\n",
      "Progress: 62.0% ... Training loss: 0.065 ... Validation loss: 0.165iteration: 6204\n",
      "train_loss: 0.0658830775228613\n",
      "val_loss: 0.16520181654818258\n",
      "Progress: 62.0% ... Training loss: 0.064 ... Validation loss: 0.183iteration: 6205\n",
      "train_loss: 0.06477154462670238\n",
      "val_loss: 0.18312733357283306\n",
      "Progress: 62.1% ... Training loss: 0.064 ... Validation loss: 0.167iteration: 6206\n",
      "train_loss: 0.0648303028744843\n",
      "val_loss: 0.16793899156835615\n",
      "Progress: 62.1% ... Training loss: 0.063 ... Validation loss: 0.173iteration: 6207\n",
      "train_loss: 0.06329760417135782\n",
      "val_loss: 0.17392547677311485\n",
      "Progress: 62.1% ... Training loss: 0.063 ... Validation loss: 0.175iteration: 6208\n",
      "train_loss: 0.06330979110709467\n",
      "val_loss: 0.17562271690367307\n",
      "Progress: 62.1% ... Training loss: 0.063 ... Validation loss: 0.179iteration: 6209\n",
      "train_loss: 0.06322779288531774\n",
      "val_loss: 0.17907501723907657\n",
      "Progress: 62.1% ... Training loss: 0.067 ... Validation loss: 0.157iteration: 6210\n",
      "train_loss: 0.0675148944874755\n",
      "val_loss: 0.1578038229671679\n",
      "Progress: 62.1% ... Training loss: 0.064 ... Validation loss: 0.183iteration: 6211\n",
      "train_loss: 0.06475573722765895\n",
      "val_loss: 0.18387434014670173\n",
      "Progress: 62.1% ... Training loss: 0.064 ... Validation loss: 0.189iteration: 6212\n",
      "train_loss: 0.06464454178192258\n",
      "val_loss: 0.18983973158842382\n",
      "Progress: 62.1% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 6213\n",
      "train_loss: 0.06660711040568597\n",
      "val_loss: 0.17126950986430994\n",
      "Progress: 62.1% ... Training loss: 0.064 ... Validation loss: 0.192iteration: 6214\n",
      "train_loss: 0.06410485719992406\n",
      "val_loss: 0.19250331751210367\n",
      "Progress: 62.1% ... Training loss: 0.067 ... Validation loss: 0.175iteration: 6215\n",
      "train_loss: 0.0676417383227401\n",
      "val_loss: 0.17538470295271347\n",
      "Progress: 62.2% ... Training loss: 0.067 ... Validation loss: 0.199iteration: 6216\n",
      "train_loss: 0.0670010377409608\n",
      "val_loss: 0.1996628053543088\n",
      "Progress: 62.2% ... Training loss: 0.063 ... Validation loss: 0.167iteration: 6217\n",
      "train_loss: 0.06378182478162044\n",
      "val_loss: 0.16776756960778208\n",
      "Progress: 62.2% ... Training loss: 0.063 ... Validation loss: 0.186iteration: 6218\n",
      "train_loss: 0.0639095646921237\n",
      "val_loss: 0.18618026095577148\n",
      "Progress: 62.2% ... Training loss: 0.064 ... Validation loss: 0.199iteration: 6219\n",
      "train_loss: 0.06490678032768515\n",
      "val_loss: 0.19968620663638298\n",
      "Progress: 62.2% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 6220\n",
      "train_loss: 0.0631967990856217\n",
      "val_loss: 0.174869609356166\n",
      "Progress: 62.2% ... Training loss: 0.068 ... Validation loss: 0.219iteration: 6221\n",
      "train_loss: 0.06864576788050014\n",
      "val_loss: 0.21986285749634946\n",
      "Progress: 62.2% ... Training loss: 0.066 ... Validation loss: 0.162iteration: 6222\n",
      "train_loss: 0.06689667973570258\n",
      "val_loss: 0.16288682333414778\n",
      "Progress: 62.2% ... Training loss: 0.069 ... Validation loss: 0.199iteration: 6223\n",
      "train_loss: 0.06928215939352915\n",
      "val_loss: 0.1992505072867104\n",
      "Progress: 62.2% ... Training loss: 0.077 ... Validation loss: 0.158iteration: 6224\n",
      "train_loss: 0.07747011751661997\n",
      "val_loss: 0.15876387579065782\n",
      "Progress: 62.2% ... Training loss: 0.081 ... Validation loss: 0.247iteration: 6225\n",
      "train_loss: 0.08193747621586152\n",
      "val_loss: 0.24738780669456628\n",
      "Progress: 62.3% ... Training loss: 0.081 ... Validation loss: 0.163iteration: 6226\n",
      "train_loss: 0.08171670310263052\n",
      "val_loss: 0.16384900598917157\n",
      "Progress: 62.3% ... Training loss: 0.079 ... Validation loss: 0.250iteration: 6227\n",
      "train_loss: 0.07933207897274794\n",
      "val_loss: 0.25017723959837745\n",
      "Progress: 62.3% ... Training loss: 0.076 ... Validation loss: 0.166iteration: 6228\n",
      "train_loss: 0.07692243556039846\n",
      "val_loss: 0.1663818466297373\n",
      "Progress: 62.3% ... Training loss: 0.081 ... Validation loss: 0.257iteration: 6229\n",
      "train_loss: 0.08134655148307295\n",
      "val_loss: 0.2571297022551374\n",
      "Progress: 62.3% ... Training loss: 0.071 ... Validation loss: 0.163iteration: 6230\n",
      "train_loss: 0.07109573994070509\n",
      "val_loss: 0.16310608178743552\n",
      "Progress: 62.3% ... Training loss: 0.063 ... Validation loss: 0.202iteration: 6231\n",
      "train_loss: 0.06399278162775894\n",
      "val_loss: 0.20275785586817807\n",
      "Progress: 62.3% ... Training loss: 0.063 ... Validation loss: 0.171iteration: 6232\n",
      "train_loss: 0.06373928823269864\n",
      "val_loss: 0.1713082380366091\n",
      "Progress: 62.3% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 6233\n",
      "train_loss: 0.06393865791759452\n",
      "val_loss: 0.16651021015791925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 62.3% ... Training loss: 0.065 ... Validation loss: 0.183iteration: 6234\n",
      "train_loss: 0.06533317946990021\n",
      "val_loss: 0.18334702563996896\n",
      "Progress: 62.4% ... Training loss: 0.062 ... Validation loss: 0.177iteration: 6235\n",
      "train_loss: 0.06268780023970097\n",
      "val_loss: 0.17753093606614548\n",
      "Progress: 62.4% ... Training loss: 0.064 ... Validation loss: 0.163iteration: 6236\n",
      "train_loss: 0.06408559228305044\n",
      "val_loss: 0.163349855034423\n",
      "Progress: 62.4% ... Training loss: 0.064 ... Validation loss: 0.177iteration: 6237\n",
      "train_loss: 0.0640127178447614\n",
      "val_loss: 0.17723287462083687\n",
      "Progress: 62.4% ... Training loss: 0.062 ... Validation loss: 0.181iteration: 6238\n",
      "train_loss: 0.06279528117923343\n",
      "val_loss: 0.18101050977541702\n",
      "Progress: 62.4% ... Training loss: 0.063 ... Validation loss: 0.186iteration: 6239\n",
      "train_loss: 0.06355741525405313\n",
      "val_loss: 0.1868304814485714\n",
      "Progress: 62.4% ... Training loss: 0.063 ... Validation loss: 0.184iteration: 6240\n",
      "train_loss: 0.063617063586472\n",
      "val_loss: 0.1843284085028648\n",
      "Progress: 62.4% ... Training loss: 0.065 ... Validation loss: 0.173iteration: 6241\n",
      "train_loss: 0.06585915531273244\n",
      "val_loss: 0.17332648268986564\n",
      "Progress: 62.4% ... Training loss: 0.075 ... Validation loss: 0.214iteration: 6242\n",
      "train_loss: 0.07567858130162433\n",
      "val_loss: 0.2146865868547867\n",
      "Progress: 62.4% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 6243\n",
      "train_loss: 0.06380686798857131\n",
      "val_loss: 0.16109814706266232\n",
      "Progress: 62.4% ... Training loss: 0.063 ... Validation loss: 0.167iteration: 6244\n",
      "train_loss: 0.06316433977448806\n",
      "val_loss: 0.16728380238314194\n",
      "Progress: 62.5% ... Training loss: 0.063 ... Validation loss: 0.168iteration: 6245\n",
      "train_loss: 0.06374416949369908\n",
      "val_loss: 0.1687790170138544\n",
      "Progress: 62.5% ... Training loss: 0.063 ... Validation loss: 0.185iteration: 6246\n",
      "train_loss: 0.06363581131434837\n",
      "val_loss: 0.1857519257363977\n",
      "Progress: 62.5% ... Training loss: 0.069 ... Validation loss: 0.159iteration: 6247\n",
      "train_loss: 0.06904470790483182\n",
      "val_loss: 0.15997682846444286\n",
      "Progress: 62.5% ... Training loss: 0.069 ... Validation loss: 0.215iteration: 6248\n",
      "train_loss: 0.0699852629635738\n",
      "val_loss: 0.21572533053909804\n",
      "Progress: 62.5% ... Training loss: 0.065 ... Validation loss: 0.169iteration: 6249\n",
      "train_loss: 0.0653051541739321\n",
      "val_loss: 0.16921050877580485\n",
      "Progress: 62.5% ... Training loss: 0.069 ... Validation loss: 0.213iteration: 6250\n",
      "train_loss: 0.06904966080562995\n",
      "val_loss: 0.21350975668809344\n",
      "Progress: 62.5% ... Training loss: 0.067 ... Validation loss: 0.167iteration: 6251\n",
      "train_loss: 0.06749258729384343\n",
      "val_loss: 0.16786079967123688\n",
      "Progress: 62.5% ... Training loss: 0.062 ... Validation loss: 0.173iteration: 6252\n",
      "train_loss: 0.06269288733028779\n",
      "val_loss: 0.17367811644454112\n",
      "Progress: 62.5% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 6253\n",
      "train_loss: 0.06447819897684932\n",
      "val_loss: 0.17378144170415688\n",
      "Progress: 62.5% ... Training loss: 0.067 ... Validation loss: 0.193iteration: 6254\n",
      "train_loss: 0.06751552833241711\n",
      "val_loss: 0.19322283582338254\n",
      "Progress: 62.5% ... Training loss: 0.064 ... Validation loss: 0.167iteration: 6255\n",
      "train_loss: 0.06465821276702383\n",
      "val_loss: 0.16745194631160146\n",
      "Progress: 62.6% ... Training loss: 0.065 ... Validation loss: 0.191iteration: 6256\n",
      "train_loss: 0.06512898706526868\n",
      "val_loss: 0.19169422165540656\n",
      "Progress: 62.6% ... Training loss: 0.062 ... Validation loss: 0.182iteration: 6257\n",
      "train_loss: 0.06296368527998888\n",
      "val_loss: 0.1823707181837339\n",
      "Progress: 62.6% ... Training loss: 0.067 ... Validation loss: 0.162iteration: 6258\n",
      "train_loss: 0.0676621560015777\n",
      "val_loss: 0.16258981037912718\n",
      "Progress: 62.6% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 6259\n",
      "train_loss: 0.06309467153185959\n",
      "val_loss: 0.18268202476380907\n",
      "Progress: 62.6% ... Training loss: 0.064 ... Validation loss: 0.164iteration: 6260\n",
      "train_loss: 0.06403773492753796\n",
      "val_loss: 0.16498816995277477\n",
      "Progress: 62.6% ... Training loss: 0.063 ... Validation loss: 0.180iteration: 6261\n",
      "train_loss: 0.06347929232601232\n",
      "val_loss: 0.18064408097304666\n",
      "Progress: 62.6% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 6262\n",
      "train_loss: 0.06437031685705127\n",
      "val_loss: 0.1567764639865631\n",
      "Progress: 62.6% ... Training loss: 0.066 ... Validation loss: 0.189iteration: 6263\n",
      "train_loss: 0.06680846989294208\n",
      "val_loss: 0.18982706438209773\n",
      "Progress: 62.6% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 6264\n",
      "train_loss: 0.0641247618999572\n",
      "val_loss: 0.1780415916759628\n",
      "Progress: 62.6% ... Training loss: 0.068 ... Validation loss: 0.199iteration: 6265\n",
      "train_loss: 0.06883364034520915\n",
      "val_loss: 0.19986257790525425\n",
      "Progress: 62.7% ... Training loss: 0.073 ... Validation loss: 0.155iteration: 6266\n",
      "train_loss: 0.07309228940966202\n",
      "val_loss: 0.15529648832865867\n",
      "Progress: 62.7% ... Training loss: 0.076 ... Validation loss: 0.202iteration: 6267\n",
      "train_loss: 0.07690188821025476\n",
      "val_loss: 0.20239617727606793\n",
      "Progress: 62.7% ... Training loss: 0.071 ... Validation loss: 0.150iteration: 6268\n",
      "train_loss: 0.07152234208384653\n",
      "val_loss: 0.15016126161713467\n",
      "Progress: 62.7% ... Training loss: 0.069 ... Validation loss: 0.189iteration: 6269\n",
      "train_loss: 0.06956175545851284\n",
      "val_loss: 0.18965253374963748\n",
      "Progress: 62.7% ... Training loss: 0.075 ... Validation loss: 0.153iteration: 6270\n",
      "train_loss: 0.07514774912082885\n",
      "val_loss: 0.15315891995796477\n",
      "Progress: 62.7% ... Training loss: 0.065 ... Validation loss: 0.179iteration: 6271\n",
      "train_loss: 0.06501657809843044\n",
      "val_loss: 0.17945114448968782\n",
      "Progress: 62.7% ... Training loss: 0.064 ... Validation loss: 0.157iteration: 6272\n",
      "train_loss: 0.06452215590097067\n",
      "val_loss: 0.1576329549856356\n",
      "Progress: 62.7% ... Training loss: 0.064 ... Validation loss: 0.166iteration: 6273\n",
      "train_loss: 0.06405574031484504\n",
      "val_loss: 0.16647370645737147\n",
      "Progress: 62.7% ... Training loss: 0.065 ... Validation loss: 0.173iteration: 6274\n",
      "train_loss: 0.06579943654234889\n",
      "val_loss: 0.1737549119995225\n",
      "Progress: 62.8% ... Training loss: 0.064 ... Validation loss: 0.152iteration: 6275\n",
      "train_loss: 0.06496449710612477\n",
      "val_loss: 0.15252233045738853\n",
      "Progress: 62.8% ... Training loss: 0.064 ... Validation loss: 0.179iteration: 6276\n",
      "train_loss: 0.06493660284196184\n",
      "val_loss: 0.17921120743054628\n",
      "Progress: 62.8% ... Training loss: 0.064 ... Validation loss: 0.175iteration: 6277\n",
      "train_loss: 0.06489248109305787\n",
      "val_loss: 0.17518076413162867\n",
      "Progress: 62.8% ... Training loss: 0.068 ... Validation loss: 0.182iteration: 6278\n",
      "train_loss: 0.06839151704638824\n",
      "val_loss: 0.1824428652332662\n",
      "Progress: 62.8% ... Training loss: 0.071 ... Validation loss: 0.158iteration: 6279\n",
      "train_loss: 0.07178246943256408\n",
      "val_loss: 0.15845508022395005\n",
      "Progress: 62.8% ... Training loss: 0.068 ... Validation loss: 0.177iteration: 6280\n",
      "train_loss: 0.06874086250439475\n",
      "val_loss: 0.17701137073137183\n",
      "Progress: 62.8% ... Training loss: 0.068 ... Validation loss: 0.149iteration: 6281\n",
      "train_loss: 0.06827618423582332\n",
      "val_loss: 0.1498880428551975\n",
      "Progress: 62.8% ... Training loss: 0.065 ... Validation loss: 0.179iteration: 6282\n",
      "train_loss: 0.06560012503800794\n",
      "val_loss: 0.17970682227627602\n",
      "Progress: 62.8% ... Training loss: 0.072 ... Validation loss: 0.155iteration: 6283\n",
      "train_loss: 0.07253399412634055\n",
      "val_loss: 0.1556555178234386\n",
      "Progress: 62.8% ... Training loss: 0.068 ... Validation loss: 0.173iteration: 6284\n",
      "train_loss: 0.06815986314255122\n",
      "val_loss: 0.17363232998773687\n",
      "Progress: 62.9% ... Training loss: 0.063 ... Validation loss: 0.176iteration: 6285\n",
      "train_loss: 0.06395066729649454\n",
      "val_loss: 0.17613035867218516\n",
      "Progress: 62.9% ... Training loss: 0.063 ... Validation loss: 0.171iteration: 6286\n",
      "train_loss: 0.06398693661669466\n",
      "val_loss: 0.17161001508598253\n",
      "Progress: 62.9% ... Training loss: 0.064 ... Validation loss: 0.160iteration: 6287\n",
      "train_loss: 0.06444394547090937\n",
      "val_loss: 0.160613744144803\n",
      "Progress: 62.9% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 6288\n",
      "train_loss: 0.06359387077056143\n",
      "val_loss: 0.17442505929620303\n",
      "Progress: 62.9% ... Training loss: 0.065 ... Validation loss: 0.173iteration: 6289\n",
      "train_loss: 0.06518514494481452\n",
      "val_loss: 0.1733313193066966\n",
      "Progress: 62.9% ... Training loss: 0.063 ... Validation loss: 0.160iteration: 6290\n",
      "train_loss: 0.06361290030483006\n",
      "val_loss: 0.1603208171449903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 62.9% ... Training loss: 0.065 ... Validation loss: 0.174iteration: 6291\n",
      "train_loss: 0.06504163359399809\n",
      "val_loss: 0.174508365852676\n",
      "Progress: 62.9% ... Training loss: 0.066 ... Validation loss: 0.154iteration: 6292\n",
      "train_loss: 0.06624992773939127\n",
      "val_loss: 0.15441883550416072\n",
      "Progress: 62.9% ... Training loss: 0.075 ... Validation loss: 0.198iteration: 6293\n",
      "train_loss: 0.07587795010634772\n",
      "val_loss: 0.1987547617632318\n",
      "Progress: 62.9% ... Training loss: 0.077 ... Validation loss: 0.152iteration: 6294\n",
      "train_loss: 0.07799763189205484\n",
      "val_loss: 0.15231477859748827\n",
      "Progress: 63.0% ... Training loss: 0.073 ... Validation loss: 0.189iteration: 6295\n",
      "train_loss: 0.07312428823579809\n",
      "val_loss: 0.18984286354363533\n",
      "Progress: 63.0% ... Training loss: 0.082 ... Validation loss: 0.156iteration: 6296\n",
      "train_loss: 0.08238819541454302\n",
      "val_loss: 0.15617618535956224\n",
      "Progress: 63.0% ... Training loss: 0.092 ... Validation loss: 0.226iteration: 6297\n",
      "train_loss: 0.09220947280290645\n",
      "val_loss: 0.2266660938486901\n",
      "Progress: 63.0% ... Training loss: 0.077 ... Validation loss: 0.151iteration: 6298\n",
      "train_loss: 0.07739212930113389\n",
      "val_loss: 0.1516135855254677\n",
      "Progress: 63.0% ... Training loss: 0.067 ... Validation loss: 0.185iteration: 6299\n",
      "train_loss: 0.06775872972904734\n",
      "val_loss: 0.18548550396255944\n",
      "Progress: 63.0% ... Training loss: 0.062 ... Validation loss: 0.166iteration: 6300\n",
      "train_loss: 0.06256581589357837\n",
      "val_loss: 0.16628527448773292\n",
      "Progress: 63.0% ... Training loss: 0.063 ... Validation loss: 0.164iteration: 6301\n",
      "train_loss: 0.06333121083893932\n",
      "val_loss: 0.1642645903150303\n",
      "Progress: 63.0% ... Training loss: 0.066 ... Validation loss: 0.176iteration: 6302\n",
      "train_loss: 0.0664412328377872\n",
      "val_loss: 0.17622846361218178\n",
      "Progress: 63.0% ... Training loss: 0.070 ... Validation loss: 0.160iteration: 6303\n",
      "train_loss: 0.07063190088672446\n",
      "val_loss: 0.1606823901137789\n",
      "Progress: 63.0% ... Training loss: 0.091 ... Validation loss: 0.225iteration: 6304\n",
      "train_loss: 0.09106042703166452\n",
      "val_loss: 0.22500453989801927\n",
      "Progress: 63.0% ... Training loss: 0.086 ... Validation loss: 0.153iteration: 6305\n",
      "train_loss: 0.0862282476796198\n",
      "val_loss: 0.1534705954113028\n",
      "Progress: 63.1% ... Training loss: 0.103 ... Validation loss: 0.205iteration: 6306\n",
      "train_loss: 0.10354260335969047\n",
      "val_loss: 0.205793209557272\n",
      "Progress: 63.1% ... Training loss: 0.101 ... Validation loss: 0.161iteration: 6307\n",
      "train_loss: 0.1013873766477722\n",
      "val_loss: 0.16182618152031192\n",
      "Progress: 63.1% ... Training loss: 0.089 ... Validation loss: 0.228iteration: 6308\n",
      "train_loss: 0.0897682912484541\n",
      "val_loss: 0.2286720333587352\n",
      "Progress: 63.1% ... Training loss: 0.078 ... Validation loss: 0.153iteration: 6309\n",
      "train_loss: 0.07894763510972375\n",
      "val_loss: 0.15351259018284896\n",
      "Progress: 63.1% ... Training loss: 0.074 ... Validation loss: 0.202iteration: 6310\n",
      "train_loss: 0.07482118795981516\n",
      "val_loss: 0.2029722246114313\n",
      "Progress: 63.1% ... Training loss: 0.080 ... Validation loss: 0.152iteration: 6311\n",
      "train_loss: 0.08059751445644156\n",
      "val_loss: 0.15277291947761984\n",
      "Progress: 63.1% ... Training loss: 0.082 ... Validation loss: 0.226iteration: 6312\n",
      "train_loss: 0.0823788465328841\n",
      "val_loss: 0.22619105328479736\n",
      "Progress: 63.1% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 6313\n",
      "train_loss: 0.07025739170893862\n",
      "val_loss: 0.15082814512633297\n",
      "Progress: 63.1% ... Training loss: 0.074 ... Validation loss: 0.194iteration: 6314\n",
      "train_loss: 0.07429787350572534\n",
      "val_loss: 0.19432022453049297\n",
      "Progress: 63.1% ... Training loss: 0.069 ... Validation loss: 0.169iteration: 6315\n",
      "train_loss: 0.06973100988040053\n",
      "val_loss: 0.16928174473588048\n",
      "Progress: 63.2% ... Training loss: 0.065 ... Validation loss: 0.192iteration: 6316\n",
      "train_loss: 0.0655407849029893\n",
      "val_loss: 0.19227879198405126\n",
      "Progress: 63.2% ... Training loss: 0.068 ... Validation loss: 0.149iteration: 6317\n",
      "train_loss: 0.06807819544230044\n",
      "val_loss: 0.1493277999091147\n",
      "Progress: 63.2% ... Training loss: 0.073 ... Validation loss: 0.185iteration: 6318\n",
      "train_loss: 0.07320034583183242\n",
      "val_loss: 0.18524818239288493\n",
      "Progress: 63.2% ... Training loss: 0.075 ... Validation loss: 0.144iteration: 6319\n",
      "train_loss: 0.07526548713649171\n",
      "val_loss: 0.1440971300857142\n",
      "Progress: 63.2% ... Training loss: 0.079 ... Validation loss: 0.199iteration: 6320\n",
      "train_loss: 0.07994384657955314\n",
      "val_loss: 0.199364555010726\n",
      "Progress: 63.2% ... Training loss: 0.063 ... Validation loss: 0.150iteration: 6321\n",
      "train_loss: 0.0637313686878162\n",
      "val_loss: 0.15041526509732023\n",
      "Progress: 63.2% ... Training loss: 0.066 ... Validation loss: 0.153iteration: 6322\n",
      "train_loss: 0.06656210471543055\n",
      "val_loss: 0.15331406726039748\n",
      "Progress: 63.2% ... Training loss: 0.070 ... Validation loss: 0.171iteration: 6323\n",
      "train_loss: 0.07010120630584395\n",
      "val_loss: 0.17129730260201695\n",
      "Progress: 63.2% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 6324\n",
      "train_loss: 0.0635790406808445\n",
      "val_loss: 0.1562796593856781\n",
      "Progress: 63.2% ... Training loss: 0.064 ... Validation loss: 0.152iteration: 6325\n",
      "train_loss: 0.06462268175018793\n",
      "val_loss: 0.15273487552501466\n",
      "Progress: 63.3% ... Training loss: 0.065 ... Validation loss: 0.168iteration: 6326\n",
      "train_loss: 0.06537898487617709\n",
      "val_loss: 0.1688795443006283\n",
      "Progress: 63.3% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 6327\n",
      "train_loss: 0.06377191848927904\n",
      "val_loss: 0.15603388677186622\n",
      "Progress: 63.3% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 6328\n",
      "train_loss: 0.06523643974602875\n",
      "val_loss: 0.1523672685717042\n",
      "Progress: 63.3% ... Training loss: 0.064 ... Validation loss: 0.166iteration: 6329\n",
      "train_loss: 0.06471426926612636\n",
      "val_loss: 0.16604349452929565\n",
      "Progress: 63.3% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 6330\n",
      "train_loss: 0.06341425323134736\n",
      "val_loss: 0.1577060832105485\n",
      "Progress: 63.3% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 6331\n",
      "train_loss: 0.06338263427181995\n",
      "val_loss: 0.15757854977818295\n",
      "Progress: 63.3% ... Training loss: 0.063 ... Validation loss: 0.159iteration: 6332\n",
      "train_loss: 0.0635389452069337\n",
      "val_loss: 0.15977998712593747\n",
      "Progress: 63.3% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 6333\n",
      "train_loss: 0.06305668127461446\n",
      "val_loss: 0.1666781660193607\n",
      "Progress: 63.3% ... Training loss: 0.065 ... Validation loss: 0.157iteration: 6334\n",
      "train_loss: 0.06582460498191434\n",
      "val_loss: 0.15777889900439016\n",
      "Progress: 63.4% ... Training loss: 0.064 ... Validation loss: 0.168iteration: 6335\n",
      "train_loss: 0.06405255741636724\n",
      "val_loss: 0.16876320525677754\n",
      "Progress: 63.4% ... Training loss: 0.064 ... Validation loss: 0.163iteration: 6336\n",
      "train_loss: 0.06486958414095857\n",
      "val_loss: 0.16393822138967395\n",
      "Progress: 63.4% ... Training loss: 0.066 ... Validation loss: 0.176iteration: 6337\n",
      "train_loss: 0.06640614188396006\n",
      "val_loss: 0.1769132319340534\n",
      "Progress: 63.4% ... Training loss: 0.063 ... Validation loss: 0.170iteration: 6338\n",
      "train_loss: 0.06376692009204762\n",
      "val_loss: 0.1703030208561235\n",
      "Progress: 63.4% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 6339\n",
      "train_loss: 0.06328104628098961\n",
      "val_loss: 0.16138179386598941\n",
      "Progress: 63.4% ... Training loss: 0.070 ... Validation loss: 0.192iteration: 6340\n",
      "train_loss: 0.0709131465555095\n",
      "val_loss: 0.1922030480793263\n",
      "Progress: 63.4% ... Training loss: 0.083 ... Validation loss: 0.144iteration: 6341\n",
      "train_loss: 0.08343904534581843\n",
      "val_loss: 0.14463418443022605\n",
      "Progress: 63.4% ... Training loss: 0.066 ... Validation loss: 0.190iteration: 6342\n",
      "train_loss: 0.06687817987319886\n",
      "val_loss: 0.19085817116061216\n",
      "Progress: 63.4% ... Training loss: 0.067 ... Validation loss: 0.157iteration: 6343\n",
      "train_loss: 0.06753609657072213\n",
      "val_loss: 0.15797588507429097\n",
      "Progress: 63.4% ... Training loss: 0.063 ... Validation loss: 0.183iteration: 6344\n",
      "train_loss: 0.06369223772578053\n",
      "val_loss: 0.18330019829912586\n",
      "Progress: 63.5% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 6345\n",
      "train_loss: 0.06324851992767262\n",
      "val_loss: 0.18270693256838685\n",
      "Progress: 63.5% ... Training loss: 0.062 ... Validation loss: 0.174iteration: 6346\n",
      "train_loss: 0.06270991598758469\n",
      "val_loss: 0.17413446257647477\n",
      "Progress: 63.5% ... Training loss: 0.076 ... Validation loss: 0.146iteration: 6347\n",
      "train_loss: 0.07612372165516479\n",
      "val_loss: 0.14645667534463827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 63.5% ... Training loss: 0.081 ... Validation loss: 0.212iteration: 6348\n",
      "train_loss: 0.081185943872856\n",
      "val_loss: 0.2121424481718901\n",
      "Progress: 63.5% ... Training loss: 0.067 ... Validation loss: 0.150iteration: 6349\n",
      "train_loss: 0.06771847854604914\n",
      "val_loss: 0.15047448967162677\n",
      "Progress: 63.5% ... Training loss: 0.069 ... Validation loss: 0.192iteration: 6350\n",
      "train_loss: 0.06926287254291083\n",
      "val_loss: 0.19222672090817272\n",
      "Progress: 63.5% ... Training loss: 0.071 ... Validation loss: 0.151iteration: 6351\n",
      "train_loss: 0.07181708309564655\n",
      "val_loss: 0.15128367546267457\n",
      "Progress: 63.5% ... Training loss: 0.071 ... Validation loss: 0.204iteration: 6352\n",
      "train_loss: 0.07100936539282388\n",
      "val_loss: 0.2045949232422352\n",
      "Progress: 63.5% ... Training loss: 0.064 ... Validation loss: 0.165iteration: 6353\n",
      "train_loss: 0.06468741255716343\n",
      "val_loss: 0.16536414929037505\n",
      "Progress: 63.5% ... Training loss: 0.068 ... Validation loss: 0.208iteration: 6354\n",
      "train_loss: 0.0685935119838579\n",
      "val_loss: 0.20860304845043798\n",
      "Progress: 63.5% ... Training loss: 0.066 ... Validation loss: 0.162iteration: 6355\n",
      "train_loss: 0.06623890202132239\n",
      "val_loss: 0.16210400479264056\n",
      "Progress: 63.6% ... Training loss: 0.076 ... Validation loss: 0.219iteration: 6356\n",
      "train_loss: 0.07651864917330597\n",
      "val_loss: 0.2193155903589163\n",
      "Progress: 63.6% ... Training loss: 0.088 ... Validation loss: 0.154iteration: 6357\n",
      "train_loss: 0.08815049420292172\n",
      "val_loss: 0.1547727461199662\n",
      "Progress: 63.6% ... Training loss: 0.100 ... Validation loss: 0.238iteration: 6358\n",
      "train_loss: 0.10071570375083963\n",
      "val_loss: 0.2382124805575743\n",
      "Progress: 63.6% ... Training loss: 0.105 ... Validation loss: 0.152iteration: 6359\n",
      "train_loss: 0.10518740628762334\n",
      "val_loss: 0.1527158714757624\n",
      "Progress: 63.6% ... Training loss: 0.097 ... Validation loss: 0.251iteration: 6360\n",
      "train_loss: 0.09740922329106626\n",
      "val_loss: 0.2517305313375861\n",
      "Progress: 63.6% ... Training loss: 0.096 ... Validation loss: 0.152iteration: 6361\n",
      "train_loss: 0.09628473978369567\n",
      "val_loss: 0.15295583583009234\n",
      "Progress: 63.6% ... Training loss: 0.119 ... Validation loss: 0.288iteration: 6362\n",
      "train_loss: 0.11965273581713258\n",
      "val_loss: 0.2885034298642127\n",
      "Progress: 63.6% ... Training loss: 0.087 ... Validation loss: 0.148iteration: 6363\n",
      "train_loss: 0.08724695697213158\n",
      "val_loss: 0.14860525031133714\n",
      "Progress: 63.6% ... Training loss: 0.073 ... Validation loss: 0.224iteration: 6364\n",
      "train_loss: 0.07301419614635987\n",
      "val_loss: 0.2248955962568552\n",
      "Progress: 63.6% ... Training loss: 0.063 ... Validation loss: 0.172iteration: 6365\n",
      "train_loss: 0.06378399546725355\n",
      "val_loss: 0.17210993584659195\n",
      "Progress: 63.7% ... Training loss: 0.062 ... Validation loss: 0.177iteration: 6366\n",
      "train_loss: 0.06272425238315564\n",
      "val_loss: 0.17705715300382335\n",
      "Progress: 63.7% ... Training loss: 0.066 ... Validation loss: 0.162iteration: 6367\n",
      "train_loss: 0.06694185357690896\n",
      "val_loss: 0.1621704983412053\n",
      "Progress: 63.7% ... Training loss: 0.062 ... Validation loss: 0.174iteration: 6368\n",
      "train_loss: 0.0629079006642309\n",
      "val_loss: 0.17457067152006975\n",
      "Progress: 63.7% ... Training loss: 0.066 ... Validation loss: 0.158iteration: 6369\n",
      "train_loss: 0.0665023834074802\n",
      "val_loss: 0.1581633990705548\n",
      "Progress: 63.7% ... Training loss: 0.063 ... Validation loss: 0.177iteration: 6370\n",
      "train_loss: 0.06321116151051602\n",
      "val_loss: 0.17750782212164284\n",
      "Progress: 63.7% ... Training loss: 0.065 ... Validation loss: 0.182iteration: 6371\n",
      "train_loss: 0.06507604864899073\n",
      "val_loss: 0.18266372657167118\n",
      "Progress: 63.7% ... Training loss: 0.068 ... Validation loss: 0.154iteration: 6372\n",
      "train_loss: 0.06821895906321357\n",
      "val_loss: 0.15462716593179193\n",
      "Progress: 63.7% ... Training loss: 0.068 ... Validation loss: 0.198iteration: 6373\n",
      "train_loss: 0.06821694281676481\n",
      "val_loss: 0.19889698028793082\n",
      "Progress: 63.7% ... Training loss: 0.071 ... Validation loss: 0.155iteration: 6374\n",
      "train_loss: 0.07156310575251544\n",
      "val_loss: 0.15575382709577934\n",
      "Progress: 63.8% ... Training loss: 0.068 ... Validation loss: 0.199iteration: 6375\n",
      "train_loss: 0.06856348010613302\n",
      "val_loss: 0.19947139022261026\n",
      "Progress: 63.8% ... Training loss: 0.063 ... Validation loss: 0.159iteration: 6376\n",
      "train_loss: 0.06329152176457935\n",
      "val_loss: 0.15915867849029391\n",
      "Progress: 63.8% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 6377\n",
      "train_loss: 0.06244285792312745\n",
      "val_loss: 0.17268990834532097\n",
      "Progress: 63.8% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 6378\n",
      "train_loss: 0.06369387891216526\n",
      "val_loss: 0.17821926285764045\n",
      "Progress: 63.8% ... Training loss: 0.073 ... Validation loss: 0.179iteration: 6379\n",
      "train_loss: 0.07374218801588059\n",
      "val_loss: 0.17929020584485417\n",
      "Progress: 63.8% ... Training loss: 0.082 ... Validation loss: 0.150iteration: 6380\n",
      "train_loss: 0.08267652502840908\n",
      "val_loss: 0.15070418961547485\n",
      "Progress: 63.8% ... Training loss: 0.084 ... Validation loss: 0.233iteration: 6381\n",
      "train_loss: 0.08419144934587047\n",
      "val_loss: 0.2330707522691892\n",
      "Progress: 63.8% ... Training loss: 0.074 ... Validation loss: 0.155iteration: 6382\n",
      "train_loss: 0.07418140641278269\n",
      "val_loss: 0.15558346095378214\n",
      "Progress: 63.8% ... Training loss: 0.073 ... Validation loss: 0.207iteration: 6383\n",
      "train_loss: 0.07354359801220613\n",
      "val_loss: 0.20752793607174036\n",
      "Progress: 63.8% ... Training loss: 0.070 ... Validation loss: 0.160iteration: 6384\n",
      "train_loss: 0.07069046291926699\n",
      "val_loss: 0.16015905451247275\n",
      "Progress: 63.9% ... Training loss: 0.069 ... Validation loss: 0.212iteration: 6385\n",
      "train_loss: 0.06987851674426532\n",
      "val_loss: 0.21229190611184423\n",
      "Progress: 63.9% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 6386\n",
      "train_loss: 0.06462053731252634\n",
      "val_loss: 0.1709916810206727\n",
      "Progress: 63.9% ... Training loss: 0.066 ... Validation loss: 0.205iteration: 6387\n",
      "train_loss: 0.06663083186538683\n",
      "val_loss: 0.20507929929687085\n",
      "Progress: 63.9% ... Training loss: 0.064 ... Validation loss: 0.161iteration: 6388\n",
      "train_loss: 0.06423051235449444\n",
      "val_loss: 0.1619079539656364\n",
      "Progress: 63.9% ... Training loss: 0.063 ... Validation loss: 0.179iteration: 6389\n",
      "train_loss: 0.0630921783599436\n",
      "val_loss: 0.1792582918763948\n",
      "Progress: 63.9% ... Training loss: 0.064 ... Validation loss: 0.174iteration: 6390\n",
      "train_loss: 0.06426961558855634\n",
      "val_loss: 0.17439062847704365\n",
      "Progress: 63.9% ... Training loss: 0.063 ... Validation loss: 0.162iteration: 6391\n",
      "train_loss: 0.06324628294714232\n",
      "val_loss: 0.16241435254465086\n",
      "Progress: 63.9% ... Training loss: 0.068 ... Validation loss: 0.190iteration: 6392\n",
      "train_loss: 0.06802454278904277\n",
      "val_loss: 0.19007101323002817\n",
      "Progress: 63.9% ... Training loss: 0.066 ... Validation loss: 0.159iteration: 6393\n",
      "train_loss: 0.06631249385075695\n",
      "val_loss: 0.1598486682910779\n",
      "Progress: 63.9% ... Training loss: 0.064 ... Validation loss: 0.175iteration: 6394\n",
      "train_loss: 0.06479869329353487\n",
      "val_loss: 0.17591824994409605\n",
      "Progress: 64.0% ... Training loss: 0.063 ... Validation loss: 0.164iteration: 6395\n",
      "train_loss: 0.06316959196328943\n",
      "val_loss: 0.1645083124228597\n",
      "Progress: 64.0% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 6396\n",
      "train_loss: 0.06325346533129432\n",
      "val_loss: 0.17437867906269836\n",
      "Progress: 64.0% ... Training loss: 0.077 ... Validation loss: 0.157iteration: 6397\n",
      "train_loss: 0.07768653111309236\n",
      "val_loss: 0.15734871780227358\n",
      "Progress: 64.0% ... Training loss: 0.085 ... Validation loss: 0.249iteration: 6398\n",
      "train_loss: 0.08555822078144054\n",
      "val_loss: 0.24965479073695063\n",
      "Progress: 64.0% ... Training loss: 0.099 ... Validation loss: 0.153iteration: 6399\n",
      "train_loss: 0.09970186264476898\n",
      "val_loss: 0.1534722961121921\n",
      "Progress: 64.0% ... Training loss: 0.064 ... Validation loss: 0.201iteration: 6400\n",
      "train_loss: 0.06498071988298111\n",
      "val_loss: 0.2017234485091356\n",
      "Progress: 64.0% ... Training loss: 0.063 ... Validation loss: 0.169iteration: 6401\n",
      "train_loss: 0.06369428061144593\n",
      "val_loss: 0.16959563530525376\n",
      "Progress: 64.0% ... Training loss: 0.063 ... Validation loss: 0.183iteration: 6402\n",
      "train_loss: 0.06353923036720585\n",
      "val_loss: 0.18358442015281848\n",
      "Progress: 64.0% ... Training loss: 0.062 ... Validation loss: 0.179iteration: 6403\n",
      "train_loss: 0.06259758732672191\n",
      "val_loss: 0.1796015927511872\n",
      "Progress: 64.0% ... Training loss: 0.064 ... Validation loss: 0.165iteration: 6404\n",
      "train_loss: 0.0644630093332644\n",
      "val_loss: 0.16542563528009122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 64.0% ... Training loss: 0.065 ... Validation loss: 0.192iteration: 6405\n",
      "train_loss: 0.06505978837208798\n",
      "val_loss: 0.19244195546333454\n",
      "Progress: 64.1% ... Training loss: 0.066 ... Validation loss: 0.166iteration: 6406\n",
      "train_loss: 0.06695427848766589\n",
      "val_loss: 0.16660380407803857\n",
      "Progress: 64.1% ... Training loss: 0.065 ... Validation loss: 0.191iteration: 6407\n",
      "train_loss: 0.065592456967887\n",
      "val_loss: 0.1914003577611711\n",
      "Progress: 64.1% ... Training loss: 0.072 ... Validation loss: 0.155iteration: 6408\n",
      "train_loss: 0.07229754441006664\n",
      "val_loss: 0.1559426078571618\n",
      "Progress: 64.1% ... Training loss: 0.063 ... Validation loss: 0.189iteration: 6409\n",
      "train_loss: 0.06349458836593214\n",
      "val_loss: 0.1893812056323465\n",
      "Progress: 64.1% ... Training loss: 0.062 ... Validation loss: 0.177iteration: 6410\n",
      "train_loss: 0.06285995911906476\n",
      "val_loss: 0.17722092258066263\n",
      "Progress: 64.1% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 6411\n",
      "train_loss: 0.06293039495518989\n",
      "val_loss: 0.18308323723889666\n",
      "Progress: 64.1% ... Training loss: 0.067 ... Validation loss: 0.158iteration: 6412\n",
      "train_loss: 0.06765912625270736\n",
      "val_loss: 0.1585619779249009\n",
      "Progress: 64.1% ... Training loss: 0.068 ... Validation loss: 0.205iteration: 6413\n",
      "train_loss: 0.06875804695186992\n",
      "val_loss: 0.20530645429891672\n",
      "Progress: 64.1% ... Training loss: 0.085 ... Validation loss: 0.150iteration: 6414\n",
      "train_loss: 0.08513045160094293\n",
      "val_loss: 0.15051578316605266\n",
      "Progress: 64.2% ... Training loss: 0.083 ... Validation loss: 0.221iteration: 6415\n",
      "train_loss: 0.08394270672631836\n",
      "val_loss: 0.22113453774561573\n",
      "Progress: 64.2% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 6416\n",
      "train_loss: 0.0704464782786033\n",
      "val_loss: 0.1508545454683199\n",
      "Progress: 64.2% ... Training loss: 0.075 ... Validation loss: 0.190iteration: 6417\n",
      "train_loss: 0.0750665558915003\n",
      "val_loss: 0.1906581580367781\n",
      "Progress: 64.2% ... Training loss: 0.080 ... Validation loss: 0.147iteration: 6418\n",
      "train_loss: 0.08030907845810546\n",
      "val_loss: 0.14762054802536212\n",
      "Progress: 64.2% ... Training loss: 0.073 ... Validation loss: 0.210iteration: 6419\n",
      "train_loss: 0.07391745280488929\n",
      "val_loss: 0.21076098975884733\n",
      "Progress: 64.2% ... Training loss: 0.086 ... Validation loss: 0.151iteration: 6420\n",
      "train_loss: 0.08625063337190651\n",
      "val_loss: 0.15166972867929007\n",
      "Progress: 64.2% ... Training loss: 0.071 ... Validation loss: 0.206iteration: 6421\n",
      "train_loss: 0.0715105756520368\n",
      "val_loss: 0.20681239716162034\n",
      "Progress: 64.2% ... Training loss: 0.079 ... Validation loss: 0.151iteration: 6422\n",
      "train_loss: 0.0794469143166566\n",
      "val_loss: 0.1512248522749497\n",
      "Progress: 64.2% ... Training loss: 0.073 ... Validation loss: 0.215iteration: 6423\n",
      "train_loss: 0.07334872223538624\n",
      "val_loss: 0.21527180774425203\n",
      "Progress: 64.2% ... Training loss: 0.074 ... Validation loss: 0.164iteration: 6424\n",
      "train_loss: 0.074778084512291\n",
      "val_loss: 0.16444667823195092\n",
      "Progress: 64.2% ... Training loss: 0.075 ... Validation loss: 0.230iteration: 6425\n",
      "train_loss: 0.07523310384025715\n",
      "val_loss: 0.23075256321985055\n",
      "Progress: 64.3% ... Training loss: 0.064 ... Validation loss: 0.166iteration: 6426\n",
      "train_loss: 0.06475981241548055\n",
      "val_loss: 0.16661540054122542\n",
      "Progress: 64.3% ... Training loss: 0.062 ... Validation loss: 0.171iteration: 6427\n",
      "train_loss: 0.06266365962998556\n",
      "val_loss: 0.17152346949916572\n",
      "Progress: 64.3% ... Training loss: 0.062 ... Validation loss: 0.171iteration: 6428\n",
      "train_loss: 0.06252247951765294\n",
      "val_loss: 0.17144006931642153\n",
      "Progress: 64.3% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 6429\n",
      "train_loss: 0.06270310089174372\n",
      "val_loss: 0.17269045673077368\n",
      "Progress: 64.3% ... Training loss: 0.067 ... Validation loss: 0.187iteration: 6430\n",
      "train_loss: 0.06710180326385942\n",
      "val_loss: 0.18796289522960496\n",
      "Progress: 64.3% ... Training loss: 0.074 ... Validation loss: 0.156iteration: 6431\n",
      "train_loss: 0.07445448363719996\n",
      "val_loss: 0.15639692820006232\n",
      "Progress: 64.3% ... Training loss: 0.064 ... Validation loss: 0.183iteration: 6432\n",
      "train_loss: 0.0645835350827014\n",
      "val_loss: 0.1831058725945749\n",
      "Progress: 64.3% ... Training loss: 0.064 ... Validation loss: 0.157iteration: 6433\n",
      "train_loss: 0.06400056026853833\n",
      "val_loss: 0.15707409779080303\n",
      "Progress: 64.3% ... Training loss: 0.065 ... Validation loss: 0.169iteration: 6434\n",
      "train_loss: 0.06560816714618234\n",
      "val_loss: 0.1697683027764711\n",
      "Progress: 64.3% ... Training loss: 0.079 ... Validation loss: 0.158iteration: 6435\n",
      "train_loss: 0.0790174755017796\n",
      "val_loss: 0.1582523206355331\n",
      "Progress: 64.4% ... Training loss: 0.073 ... Validation loss: 0.222iteration: 6436\n",
      "train_loss: 0.07356478993938886\n",
      "val_loss: 0.22250831216582556\n",
      "Progress: 64.4% ... Training loss: 0.072 ... Validation loss: 0.156iteration: 6437\n",
      "train_loss: 0.07225901877780408\n",
      "val_loss: 0.1564915477389618\n",
      "Progress: 64.4% ... Training loss: 0.068 ... Validation loss: 0.190iteration: 6438\n",
      "train_loss: 0.06814232446324991\n",
      "val_loss: 0.19011570138913342\n",
      "Progress: 64.4% ... Training loss: 0.076 ... Validation loss: 0.150iteration: 6439\n",
      "train_loss: 0.07643105921178872\n",
      "val_loss: 0.15062076082677878\n",
      "Progress: 64.4% ... Training loss: 0.067 ... Validation loss: 0.201iteration: 6440\n",
      "train_loss: 0.06703643883436627\n",
      "val_loss: 0.2011970395248412\n",
      "Progress: 64.4% ... Training loss: 0.062 ... Validation loss: 0.175iteration: 6441\n",
      "train_loss: 0.06269469641148355\n",
      "val_loss: 0.17599255415681517\n",
      "Progress: 64.4% ... Training loss: 0.063 ... Validation loss: 0.168iteration: 6442\n",
      "train_loss: 0.06342370985100497\n",
      "val_loss: 0.1685872494056643\n",
      "Progress: 64.4% ... Training loss: 0.073 ... Validation loss: 0.204iteration: 6443\n",
      "train_loss: 0.07366031427729756\n",
      "val_loss: 0.20464146108440268\n",
      "Progress: 64.4% ... Training loss: 0.072 ... Validation loss: 0.159iteration: 6444\n",
      "train_loss: 0.07236109793730897\n",
      "val_loss: 0.15974339292717815\n",
      "Progress: 64.5% ... Training loss: 0.073 ... Validation loss: 0.221iteration: 6445\n",
      "train_loss: 0.0731640738237094\n",
      "val_loss: 0.22173299399034035\n",
      "Progress: 64.5% ... Training loss: 0.077 ... Validation loss: 0.159iteration: 6446\n",
      "train_loss: 0.07760455671583497\n",
      "val_loss: 0.15936841317267395\n",
      "Progress: 64.5% ... Training loss: 0.066 ... Validation loss: 0.195iteration: 6447\n",
      "train_loss: 0.06616170503971613\n",
      "val_loss: 0.19578668134006894\n",
      "Progress: 64.5% ... Training loss: 0.069 ... Validation loss: 0.157iteration: 6448\n",
      "train_loss: 0.06967364440061974\n",
      "val_loss: 0.15790173330381474\n",
      "Progress: 64.5% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 6449\n",
      "train_loss: 0.06371010373476453\n",
      "val_loss: 0.1820304672428037\n",
      "Progress: 64.5% ... Training loss: 0.065 ... Validation loss: 0.154iteration: 6450\n",
      "train_loss: 0.06501563210170151\n",
      "val_loss: 0.1545803036941311\n",
      "Progress: 64.5% ... Training loss: 0.067 ... Validation loss: 0.187iteration: 6451\n",
      "train_loss: 0.06735602882130165\n",
      "val_loss: 0.18764183684837207\n",
      "Progress: 64.5% ... Training loss: 0.067 ... Validation loss: 0.152iteration: 6452\n",
      "train_loss: 0.06741303365693203\n",
      "val_loss: 0.15254487917996215\n",
      "Progress: 64.5% ... Training loss: 0.063 ... Validation loss: 0.172iteration: 6453\n",
      "train_loss: 0.0630229448341713\n",
      "val_loss: 0.17290716511774537\n",
      "Progress: 64.5% ... Training loss: 0.062 ... Validation loss: 0.170iteration: 6454\n",
      "train_loss: 0.06265447395327935\n",
      "val_loss: 0.1708436750295446\n",
      "Progress: 64.5% ... Training loss: 0.063 ... Validation loss: 0.176iteration: 6455\n",
      "train_loss: 0.06321480494219504\n",
      "val_loss: 0.17693031315949054\n",
      "Progress: 64.6% ... Training loss: 0.066 ... Validation loss: 0.155iteration: 6456\n",
      "train_loss: 0.06690589633400922\n",
      "val_loss: 0.15583402830719134\n",
      "Progress: 64.6% ... Training loss: 0.066 ... Validation loss: 0.189iteration: 6457\n",
      "train_loss: 0.06686247828519089\n",
      "val_loss: 0.18930311971808675\n",
      "Progress: 64.6% ... Training loss: 0.075 ... Validation loss: 0.154iteration: 6458\n",
      "train_loss: 0.07542078893428504\n",
      "val_loss: 0.1548044437023261\n",
      "Progress: 64.6% ... Training loss: 0.093 ... Validation loss: 0.242iteration: 6459\n",
      "train_loss: 0.09307931856517972\n",
      "val_loss: 0.24235269564794687\n",
      "Progress: 64.6% ... Training loss: 0.105 ... Validation loss: 0.162iteration: 6460\n",
      "train_loss: 0.10561341231973952\n",
      "val_loss: 0.16230452616866176\n",
      "Progress: 64.6% ... Training loss: 0.105 ... Validation loss: 0.278iteration: 6461\n",
      "train_loss: 0.10579535520526605\n",
      "val_loss: 0.2787753106418087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 64.6% ... Training loss: 0.095 ... Validation loss: 0.154iteration: 6462\n",
      "train_loss: 0.09528726441367129\n",
      "val_loss: 0.15494703895816764\n",
      "Progress: 64.6% ... Training loss: 0.074 ... Validation loss: 0.217iteration: 6463\n",
      "train_loss: 0.07463927733162654\n",
      "val_loss: 0.21745002283203807\n",
      "Progress: 64.6% ... Training loss: 0.069 ... Validation loss: 0.153iteration: 6464\n",
      "train_loss: 0.06945779941796616\n",
      "val_loss: 0.15394681534862012\n",
      "Progress: 64.7% ... Training loss: 0.073 ... Validation loss: 0.208iteration: 6465\n",
      "train_loss: 0.07378569453474965\n",
      "val_loss: 0.2082278465241375\n",
      "Progress: 64.7% ... Training loss: 0.073 ... Validation loss: 0.156iteration: 6466\n",
      "train_loss: 0.07325746081855686\n",
      "val_loss: 0.15636799555918046\n",
      "Progress: 64.7% ... Training loss: 0.068 ... Validation loss: 0.195iteration: 6467\n",
      "train_loss: 0.06802857019706177\n",
      "val_loss: 0.19504945374798355\n",
      "Progress: 64.7% ... Training loss: 0.063 ... Validation loss: 0.165iteration: 6468\n",
      "train_loss: 0.06357189361672409\n",
      "val_loss: 0.16591464179500323\n",
      "Progress: 64.7% ... Training loss: 0.070 ... Validation loss: 0.205iteration: 6469\n",
      "train_loss: 0.0709576928330402\n",
      "val_loss: 0.20532228868498137\n",
      "Progress: 64.7% ... Training loss: 0.065 ... Validation loss: 0.157iteration: 6470\n",
      "train_loss: 0.06577542656361153\n",
      "val_loss: 0.1573330969461959\n",
      "Progress: 64.7% ... Training loss: 0.063 ... Validation loss: 0.176iteration: 6471\n",
      "train_loss: 0.06344506887104075\n",
      "val_loss: 0.17687824828501628\n",
      "Progress: 64.7% ... Training loss: 0.064 ... Validation loss: 0.166iteration: 6472\n",
      "train_loss: 0.0646603149328779\n",
      "val_loss: 0.16602677644990593\n",
      "Progress: 64.7% ... Training loss: 0.063 ... Validation loss: 0.170iteration: 6473\n",
      "train_loss: 0.06377330942415788\n",
      "val_loss: 0.17046536086841704\n",
      "Progress: 64.7% ... Training loss: 0.066 ... Validation loss: 0.159iteration: 6474\n",
      "train_loss: 0.06629784276617888\n",
      "val_loss: 0.15907220834974511\n",
      "Progress: 64.8% ... Training loss: 0.064 ... Validation loss: 0.179iteration: 6475\n",
      "train_loss: 0.06455288799748239\n",
      "val_loss: 0.17987094064247305\n",
      "Progress: 64.8% ... Training loss: 0.067 ... Validation loss: 0.162iteration: 6476\n",
      "train_loss: 0.06713137349301469\n",
      "val_loss: 0.16203461834021368\n",
      "Progress: 64.8% ... Training loss: 0.066 ... Validation loss: 0.186iteration: 6477\n",
      "train_loss: 0.06625945011041728\n",
      "val_loss: 0.18623175770567973\n",
      "Progress: 64.8% ... Training loss: 0.070 ... Validation loss: 0.151iteration: 6478\n",
      "train_loss: 0.07037489166449784\n",
      "val_loss: 0.15150730024222797\n",
      "Progress: 64.8% ... Training loss: 0.073 ... Validation loss: 0.217iteration: 6479\n",
      "train_loss: 0.07343168661437943\n",
      "val_loss: 0.21779633095911646\n",
      "Progress: 64.8% ... Training loss: 0.073 ... Validation loss: 0.150iteration: 6480\n",
      "train_loss: 0.0731810174648859\n",
      "val_loss: 0.15098169074447937\n",
      "Progress: 64.8% ... Training loss: 0.075 ... Validation loss: 0.198iteration: 6481\n",
      "train_loss: 0.07526604738755646\n",
      "val_loss: 0.19855211519952723\n",
      "Progress: 64.8% ... Training loss: 0.086 ... Validation loss: 0.152iteration: 6482\n",
      "train_loss: 0.08629754308097828\n",
      "val_loss: 0.15236868050091326\n",
      "Progress: 64.8% ... Training loss: 0.081 ... Validation loss: 0.231iteration: 6483\n",
      "train_loss: 0.08135860283756743\n",
      "val_loss: 0.2311864205469087\n",
      "Progress: 64.8% ... Training loss: 0.108 ... Validation loss: 0.154iteration: 6484\n",
      "train_loss: 0.10815991648032271\n",
      "val_loss: 0.15431501170080283\n",
      "Progress: 64.8% ... Training loss: 0.090 ... Validation loss: 0.234iteration: 6485\n",
      "train_loss: 0.09091765385253375\n",
      "val_loss: 0.23414011039860835\n",
      "Progress: 64.9% ... Training loss: 0.072 ... Validation loss: 0.150iteration: 6486\n",
      "train_loss: 0.07200967360325426\n",
      "val_loss: 0.1504238963472737\n",
      "Progress: 64.9% ... Training loss: 0.068 ... Validation loss: 0.203iteration: 6487\n",
      "train_loss: 0.06869875113063663\n",
      "val_loss: 0.20375304118804766\n",
      "Progress: 64.9% ... Training loss: 0.074 ... Validation loss: 0.153iteration: 6488\n",
      "train_loss: 0.07414963968343546\n",
      "val_loss: 0.15305340580669893\n",
      "Progress: 64.9% ... Training loss: 0.063 ... Validation loss: 0.176iteration: 6489\n",
      "train_loss: 0.06303780586927664\n",
      "val_loss: 0.1766945900095392\n",
      "Progress: 64.9% ... Training loss: 0.063 ... Validation loss: 0.181iteration: 6490\n",
      "train_loss: 0.06342175275533461\n",
      "val_loss: 0.1815053259558188\n",
      "Progress: 64.9% ... Training loss: 0.063 ... Validation loss: 0.175iteration: 6491\n",
      "train_loss: 0.06359538099146757\n",
      "val_loss: 0.17569529202355294\n",
      "Progress: 64.9% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 6492\n",
      "train_loss: 0.06284057522986773\n",
      "val_loss: 0.16512375679227817\n",
      "Progress: 64.9% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 6493\n",
      "train_loss: 0.06599217810526618\n",
      "val_loss: 0.15203146549653443\n",
      "Progress: 64.9% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 6494\n",
      "train_loss: 0.06343475288271448\n",
      "val_loss: 0.16612782406229995\n",
      "Progress: 65.0% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 6495\n",
      "train_loss: 0.06285922618914487\n",
      "val_loss: 0.17235745642645328\n",
      "Progress: 65.0% ... Training loss: 0.063 ... Validation loss: 0.171iteration: 6496\n",
      "train_loss: 0.06330960852427252\n",
      "val_loss: 0.17189189426459042\n",
      "Progress: 65.0% ... Training loss: 0.062 ... Validation loss: 0.186iteration: 6497\n",
      "train_loss: 0.06262608196956088\n",
      "val_loss: 0.18679585951797498\n",
      "Progress: 65.0% ... Training loss: 0.063 ... Validation loss: 0.189iteration: 6498\n",
      "train_loss: 0.06377551741728857\n",
      "val_loss: 0.1895118236230483\n",
      "Progress: 65.0% ... Training loss: 0.068 ... Validation loss: 0.147iteration: 6499\n",
      "train_loss: 0.06830482693290509\n",
      "val_loss: 0.14763957112094447\n",
      "Progress: 65.0% ... Training loss: 0.062 ... Validation loss: 0.176iteration: 6500\n",
      "train_loss: 0.06289112467908471\n",
      "val_loss: 0.17622879401612473\n",
      "Progress: 65.0% ... Training loss: 0.064 ... Validation loss: 0.165iteration: 6501\n",
      "train_loss: 0.06434062919863741\n",
      "val_loss: 0.16508460257299934\n",
      "Progress: 65.0% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 6502\n",
      "train_loss: 0.06275256927789315\n",
      "val_loss: 0.17260087917210282\n",
      "Progress: 65.0% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 6503\n",
      "train_loss: 0.06372405371154634\n",
      "val_loss: 0.1619039881929358\n",
      "Progress: 65.0% ... Training loss: 0.062 ... Validation loss: 0.171iteration: 6504\n",
      "train_loss: 0.06288102976278077\n",
      "val_loss: 0.1710047706122363\n",
      "Progress: 65.0% ... Training loss: 0.064 ... Validation loss: 0.164iteration: 6505\n",
      "train_loss: 0.06465479959223226\n",
      "val_loss: 0.1643201303310658\n",
      "Progress: 65.1% ... Training loss: 0.071 ... Validation loss: 0.207iteration: 6506\n",
      "train_loss: 0.07119375990007884\n",
      "val_loss: 0.20774528704801767\n",
      "Progress: 65.1% ... Training loss: 0.064 ... Validation loss: 0.164iteration: 6507\n",
      "train_loss: 0.06446385703962369\n",
      "val_loss: 0.16463532200677095\n",
      "Progress: 65.1% ... Training loss: 0.066 ... Validation loss: 0.204iteration: 6508\n",
      "train_loss: 0.06691991205568629\n",
      "val_loss: 0.20414527230025908\n",
      "Progress: 65.1% ... Training loss: 0.063 ... Validation loss: 0.162iteration: 6509\n",
      "train_loss: 0.06399693785997389\n",
      "val_loss: 0.16277986864936878\n",
      "Progress: 65.1% ... Training loss: 0.064 ... Validation loss: 0.181iteration: 6510\n",
      "train_loss: 0.06434456879935958\n",
      "val_loss: 0.18154443930852382\n",
      "Progress: 65.1% ... Training loss: 0.069 ... Validation loss: 0.150iteration: 6511\n",
      "train_loss: 0.06919636322690499\n",
      "val_loss: 0.15076502550823412\n",
      "Progress: 65.1% ... Training loss: 0.067 ... Validation loss: 0.189iteration: 6512\n",
      "train_loss: 0.06751762434430085\n",
      "val_loss: 0.18979635897207128\n",
      "Progress: 65.1% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 6513\n",
      "train_loss: 0.06229191128606654\n",
      "val_loss: 0.17233238931829664\n",
      "Progress: 65.1% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 6514\n",
      "train_loss: 0.06423417822647856\n",
      "val_loss: 0.15668500025240306\n",
      "Progress: 65.2% ... Training loss: 0.075 ... Validation loss: 0.208iteration: 6515\n",
      "train_loss: 0.07539474811354492\n",
      "val_loss: 0.20855532108500505\n",
      "Progress: 65.2% ... Training loss: 0.066 ... Validation loss: 0.157iteration: 6516\n",
      "train_loss: 0.06617799647758968\n",
      "val_loss: 0.15747884730397932\n",
      "Progress: 65.2% ... Training loss: 0.062 ... Validation loss: 0.174iteration: 6517\n",
      "train_loss: 0.06263575619636029\n",
      "val_loss: 0.17419325194838228\n",
      "Progress: 65.2% ... Training loss: 0.062 ... Validation loss: 0.168iteration: 6518\n",
      "train_loss: 0.062144083681395575\n",
      "val_loss: 0.16806452601042063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 65.2% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 6519\n",
      "train_loss: 0.061885286657394595\n",
      "val_loss: 0.16898702724345585\n",
      "Progress: 65.2% ... Training loss: 0.067 ... Validation loss: 0.192iteration: 6520\n",
      "train_loss: 0.06767921965455213\n",
      "val_loss: 0.19263910951699753\n",
      "Progress: 65.2% ... Training loss: 0.075 ... Validation loss: 0.154iteration: 6521\n",
      "train_loss: 0.07563867790371602\n",
      "val_loss: 0.15476573333472704\n",
      "Progress: 65.2% ... Training loss: 0.073 ... Validation loss: 0.217iteration: 6522\n",
      "train_loss: 0.07369032987281245\n",
      "val_loss: 0.21745773666904694\n",
      "Progress: 65.2% ... Training loss: 0.089 ... Validation loss: 0.157iteration: 6523\n",
      "train_loss: 0.08912936433495168\n",
      "val_loss: 0.1577695546334082\n",
      "Progress: 65.2% ... Training loss: 0.080 ... Validation loss: 0.234iteration: 6524\n",
      "train_loss: 0.08087106262872164\n",
      "val_loss: 0.23421389562880965\n",
      "Progress: 65.2% ... Training loss: 0.078 ... Validation loss: 0.160iteration: 6525\n",
      "train_loss: 0.0785218725031903\n",
      "val_loss: 0.1600396079467282\n",
      "Progress: 65.3% ... Training loss: 0.076 ... Validation loss: 0.228iteration: 6526\n",
      "train_loss: 0.07691059092035345\n",
      "val_loss: 0.22836236873159346\n",
      "Progress: 65.3% ... Training loss: 0.085 ... Validation loss: 0.158iteration: 6527\n",
      "train_loss: 0.08529735111869269\n",
      "val_loss: 0.15839886174948012\n",
      "Progress: 65.3% ... Training loss: 0.094 ... Validation loss: 0.241iteration: 6528\n",
      "train_loss: 0.09406906787708984\n",
      "val_loss: 0.24198313103890143\n",
      "Progress: 65.3% ... Training loss: 0.079 ... Validation loss: 0.152iteration: 6529\n",
      "train_loss: 0.07939262981931892\n",
      "val_loss: 0.15290093496925516\n",
      "Progress: 65.3% ... Training loss: 0.111 ... Validation loss: 0.257iteration: 6530\n",
      "train_loss: 0.11132825976886296\n",
      "val_loss: 0.25798878908017364\n",
      "Progress: 65.3% ... Training loss: 0.083 ... Validation loss: 0.148iteration: 6531\n",
      "train_loss: 0.08350286300567349\n",
      "val_loss: 0.14888488531753957\n",
      "Progress: 65.3% ... Training loss: 0.076 ... Validation loss: 0.204iteration: 6532\n",
      "train_loss: 0.07613632810178025\n",
      "val_loss: 0.2040216233326864\n",
      "Progress: 65.3% ... Training loss: 0.081 ... Validation loss: 0.153iteration: 6533\n",
      "train_loss: 0.08160390381934897\n",
      "val_loss: 0.15348473889393952\n",
      "Progress: 65.3% ... Training loss: 0.067 ... Validation loss: 0.190iteration: 6534\n",
      "train_loss: 0.06729282658723053\n",
      "val_loss: 0.1902104477500309\n",
      "Progress: 65.3% ... Training loss: 0.077 ... Validation loss: 0.156iteration: 6535\n",
      "train_loss: 0.07797116598880198\n",
      "val_loss: 0.15631879322128656\n",
      "Progress: 65.4% ... Training loss: 0.070 ... Validation loss: 0.215iteration: 6536\n",
      "train_loss: 0.07001540742601971\n",
      "val_loss: 0.21575651887633768\n",
      "Progress: 65.4% ... Training loss: 0.071 ... Validation loss: 0.157iteration: 6537\n",
      "train_loss: 0.07124312230727003\n",
      "val_loss: 0.15728671272833378\n",
      "Progress: 65.4% ... Training loss: 0.069 ... Validation loss: 0.214iteration: 6538\n",
      "train_loss: 0.0694256168396809\n",
      "val_loss: 0.2143882779001373\n",
      "Progress: 65.4% ... Training loss: 0.065 ... Validation loss: 0.161iteration: 6539\n",
      "train_loss: 0.06518592325137035\n",
      "val_loss: 0.1619411749321767\n",
      "Progress: 65.4% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 6540\n",
      "train_loss: 0.062445779306232914\n",
      "val_loss: 0.1720808022314897\n",
      "Progress: 65.4% ... Training loss: 0.062 ... Validation loss: 0.170iteration: 6541\n",
      "train_loss: 0.06229341235635024\n",
      "val_loss: 0.1705810957344197\n",
      "Progress: 65.4% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 6542\n",
      "train_loss: 0.0655270294293877\n",
      "val_loss: 0.15861527015578614\n",
      "Progress: 65.4% ... Training loss: 0.068 ... Validation loss: 0.209iteration: 6543\n",
      "train_loss: 0.0684036733606837\n",
      "val_loss: 0.2092638849783151\n",
      "Progress: 65.4% ... Training loss: 0.063 ... Validation loss: 0.175iteration: 6544\n",
      "train_loss: 0.06362585324821109\n",
      "val_loss: 0.17578419046012617\n",
      "Progress: 65.5% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 6545\n",
      "train_loss: 0.06189323754814956\n",
      "val_loss: 0.171833949599716\n",
      "Progress: 65.5% ... Training loss: 0.081 ... Validation loss: 0.202iteration: 6546\n",
      "train_loss: 0.08108780609626841\n",
      "val_loss: 0.20274221051005417\n",
      "Progress: 65.5% ... Training loss: 0.078 ... Validation loss: 0.142iteration: 6547\n",
      "train_loss: 0.07874730290676711\n",
      "val_loss: 0.14286290000260699\n",
      "Progress: 65.5% ... Training loss: 0.074 ... Validation loss: 0.196iteration: 6548\n",
      "train_loss: 0.07474459798119659\n",
      "val_loss: 0.19663946716036962\n",
      "Progress: 65.5% ... Training loss: 0.084 ... Validation loss: 0.150iteration: 6549\n",
      "train_loss: 0.084306590285993\n",
      "val_loss: 0.1506224605232383\n",
      "Progress: 65.5% ... Training loss: 0.070 ... Validation loss: 0.213iteration: 6550\n",
      "train_loss: 0.07039597437282501\n",
      "val_loss: 0.21389197887829117\n",
      "Progress: 65.5% ... Training loss: 0.086 ... Validation loss: 0.150iteration: 6551\n",
      "train_loss: 0.08623419456338366\n",
      "val_loss: 0.15046857798844027\n",
      "Progress: 65.5% ... Training loss: 0.094 ... Validation loss: 0.217iteration: 6552\n",
      "train_loss: 0.09469343711648553\n",
      "val_loss: 0.21712735552586512\n",
      "Progress: 65.5% ... Training loss: 0.119 ... Validation loss: 0.157iteration: 6553\n",
      "train_loss: 0.11933292394793003\n",
      "val_loss: 0.15718873306847284\n",
      "Progress: 65.5% ... Training loss: 0.170 ... Validation loss: 0.342iteration: 6554\n",
      "train_loss: 0.1708629797268309\n",
      "val_loss: 0.3426819575786846\n",
      "Progress: 65.5% ... Training loss: 0.124 ... Validation loss: 0.164iteration: 6555\n",
      "train_loss: 0.12446431627831017\n",
      "val_loss: 0.16449387968220694\n",
      "Progress: 65.6% ... Training loss: 0.111 ... Validation loss: 0.272iteration: 6556\n",
      "train_loss: 0.11172711360784673\n",
      "val_loss: 0.2729165293967195\n",
      "Progress: 65.6% ... Training loss: 0.075 ... Validation loss: 0.158iteration: 6557\n",
      "train_loss: 0.07542891660588183\n",
      "val_loss: 0.1584786826953194\n",
      "Progress: 65.6% ... Training loss: 0.070 ... Validation loss: 0.198iteration: 6558\n",
      "train_loss: 0.07038769860375457\n",
      "val_loss: 0.19874651760402884\n",
      "Progress: 65.6% ... Training loss: 0.065 ... Validation loss: 0.155iteration: 6559\n",
      "train_loss: 0.06507386938968802\n",
      "val_loss: 0.15555289158623373\n",
      "Progress: 65.6% ... Training loss: 0.063 ... Validation loss: 0.180iteration: 6560\n",
      "train_loss: 0.06316738234223594\n",
      "val_loss: 0.18030885685019657\n",
      "Progress: 65.6% ... Training loss: 0.065 ... Validation loss: 0.164iteration: 6561\n",
      "train_loss: 0.06591267286264538\n",
      "val_loss: 0.16421069259918292\n",
      "Progress: 65.6% ... Training loss: 0.067 ... Validation loss: 0.203iteration: 6562\n",
      "train_loss: 0.06789926523162117\n",
      "val_loss: 0.20303812597539195\n",
      "Progress: 65.6% ... Training loss: 0.070 ... Validation loss: 0.154iteration: 6563\n",
      "train_loss: 0.07021809036856777\n",
      "val_loss: 0.15467726974896712\n",
      "Progress: 65.6% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 6564\n",
      "train_loss: 0.06272564145944932\n",
      "val_loss: 0.18337188177003277\n",
      "Progress: 65.7% ... Training loss: 0.064 ... Validation loss: 0.190iteration: 6565\n",
      "train_loss: 0.06441700060720909\n",
      "val_loss: 0.19029091013904478\n",
      "Progress: 65.7% ... Training loss: 0.068 ... Validation loss: 0.163iteration: 6566\n",
      "train_loss: 0.06837062881732998\n",
      "val_loss: 0.16394395309516485\n",
      "Progress: 65.7% ... Training loss: 0.077 ... Validation loss: 0.217iteration: 6567\n",
      "train_loss: 0.07735137348397317\n",
      "val_loss: 0.2175608161138565\n",
      "Progress: 65.7% ... Training loss: 0.074 ... Validation loss: 0.155iteration: 6568\n",
      "train_loss: 0.07461831605876258\n",
      "val_loss: 0.15518621557080373\n",
      "Progress: 65.7% ... Training loss: 0.082 ... Validation loss: 0.248iteration: 6569\n",
      "train_loss: 0.08204005610486541\n",
      "val_loss: 0.24815478871860638\n",
      "Progress: 65.7% ... Training loss: 0.081 ... Validation loss: 0.162iteration: 6570\n",
      "train_loss: 0.08197532279955605\n",
      "val_loss: 0.16247750291911633\n",
      "Progress: 65.7% ... Training loss: 0.077 ... Validation loss: 0.240iteration: 6571\n",
      "train_loss: 0.07710006813159621\n",
      "val_loss: 0.24030047242293548\n",
      "Progress: 65.7% ... Training loss: 0.077 ... Validation loss: 0.161iteration: 6572\n",
      "train_loss: 0.07745609518092082\n",
      "val_loss: 0.16182110351552173\n",
      "Progress: 65.7% ... Training loss: 0.102 ... Validation loss: 0.256iteration: 6573\n",
      "train_loss: 0.10238619379325567\n",
      "val_loss: 0.25611245601471694\n",
      "Progress: 65.7% ... Training loss: 0.081 ... Validation loss: 0.166iteration: 6574\n",
      "train_loss: 0.08155265432808723\n",
      "val_loss: 0.16627116745089277\n",
      "Progress: 65.8% ... Training loss: 0.069 ... Validation loss: 0.225iteration: 6575\n",
      "train_loss: 0.06903252947705214\n",
      "val_loss: 0.22588813619267256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 65.8% ... Training loss: 0.067 ... Validation loss: 0.161iteration: 6576\n",
      "train_loss: 0.06747796646377625\n",
      "val_loss: 0.1618033456853594\n",
      "Progress: 65.8% ... Training loss: 0.065 ... Validation loss: 0.200iteration: 6577\n",
      "train_loss: 0.06561386623433524\n",
      "val_loss: 0.20016804107089878\n",
      "Progress: 65.8% ... Training loss: 0.063 ... Validation loss: 0.167iteration: 6578\n",
      "train_loss: 0.06324347857367853\n",
      "val_loss: 0.16759457693044444\n",
      "Progress: 65.8% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 6579\n",
      "train_loss: 0.06312780678288106\n",
      "val_loss: 0.18287214260893958\n",
      "Progress: 65.8% ... Training loss: 0.063 ... Validation loss: 0.164iteration: 6580\n",
      "train_loss: 0.063494146911782\n",
      "val_loss: 0.16415555733565812\n",
      "Progress: 65.8% ... Training loss: 0.064 ... Validation loss: 0.183iteration: 6581\n",
      "train_loss: 0.06484238820307309\n",
      "val_loss: 0.18317327243824116\n",
      "Progress: 65.8% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 6582\n",
      "train_loss: 0.06207185994371404\n",
      "val_loss: 0.17295817873935673\n",
      "Progress: 65.8% ... Training loss: 0.067 ... Validation loss: 0.163iteration: 6583\n",
      "train_loss: 0.06707036602195844\n",
      "val_loss: 0.16342845267069098\n",
      "Progress: 65.8% ... Training loss: 0.070 ... Validation loss: 0.200iteration: 6584\n",
      "train_loss: 0.07039898058464615\n",
      "val_loss: 0.20071913667692975\n",
      "Progress: 65.8% ... Training loss: 0.070 ... Validation loss: 0.153iteration: 6585\n",
      "train_loss: 0.07028409073303958\n",
      "val_loss: 0.15304135443680217\n",
      "Progress: 65.9% ... Training loss: 0.066 ... Validation loss: 0.194iteration: 6586\n",
      "train_loss: 0.06640522142378283\n",
      "val_loss: 0.19488532384560395\n",
      "Progress: 65.9% ... Training loss: 0.064 ... Validation loss: 0.165iteration: 6587\n",
      "train_loss: 0.06420077714422344\n",
      "val_loss: 0.1656004889028259\n",
      "Progress: 65.9% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 6588\n",
      "train_loss: 0.06336756025502704\n",
      "val_loss: 0.17813589307160413\n",
      "Progress: 65.9% ... Training loss: 0.066 ... Validation loss: 0.162iteration: 6589\n",
      "train_loss: 0.06693473729489972\n",
      "val_loss: 0.16250341263064427\n",
      "Progress: 65.9% ... Training loss: 0.072 ... Validation loss: 0.213iteration: 6590\n",
      "train_loss: 0.0726523360240231\n",
      "val_loss: 0.21350921660847227\n",
      "Progress: 65.9% ... Training loss: 0.071 ... Validation loss: 0.161iteration: 6591\n",
      "train_loss: 0.07185318554340246\n",
      "val_loss: 0.16106800966205628\n",
      "Progress: 65.9% ... Training loss: 0.065 ... Validation loss: 0.198iteration: 6592\n",
      "train_loss: 0.06582407912556393\n",
      "val_loss: 0.19874322379487325\n",
      "Progress: 65.9% ... Training loss: 0.068 ... Validation loss: 0.166iteration: 6593\n",
      "train_loss: 0.06821237412683007\n",
      "val_loss: 0.16626963255400734\n",
      "Progress: 65.9% ... Training loss: 0.064 ... Validation loss: 0.198iteration: 6594\n",
      "train_loss: 0.06453914899371621\n",
      "val_loss: 0.19847434992351703\n",
      "Progress: 66.0% ... Training loss: 0.082 ... Validation loss: 0.152iteration: 6595\n",
      "train_loss: 0.0826332088775249\n",
      "val_loss: 0.1523750651392004\n",
      "Progress: 66.0% ... Training loss: 0.074 ... Validation loss: 0.213iteration: 6596\n",
      "train_loss: 0.07408136774175192\n",
      "val_loss: 0.21392472855904107\n",
      "Progress: 66.0% ... Training loss: 0.065 ... Validation loss: 0.160iteration: 6597\n",
      "train_loss: 0.065882639239911\n",
      "val_loss: 0.16005595241516907\n",
      "Progress: 66.0% ... Training loss: 0.065 ... Validation loss: 0.181iteration: 6598\n",
      "train_loss: 0.06562100510425355\n",
      "val_loss: 0.18179916684661224\n",
      "Progress: 66.0% ... Training loss: 0.063 ... Validation loss: 0.162iteration: 6599\n",
      "train_loss: 0.06332305781594595\n",
      "val_loss: 0.1629200092623548\n",
      "Progress: 66.0% ... Training loss: 0.066 ... Validation loss: 0.189iteration: 6600\n",
      "train_loss: 0.06694926223487431\n",
      "val_loss: 0.18964133241585798\n",
      "Progress: 66.0% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 6601\n",
      "train_loss: 0.06558555815481354\n",
      "val_loss: 0.1525245286372151\n",
      "Progress: 66.0% ... Training loss: 0.063 ... Validation loss: 0.158iteration: 6602\n",
      "train_loss: 0.06333486432088917\n",
      "val_loss: 0.15828120951908062\n",
      "Progress: 66.0% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 6603\n",
      "train_loss: 0.06197266617966491\n",
      "val_loss: 0.16335177926389896\n",
      "Progress: 66.0% ... Training loss: 0.062 ... Validation loss: 0.178iteration: 6604\n",
      "train_loss: 0.062176550441671884\n",
      "val_loss: 0.17823932593098743\n",
      "Progress: 66.0% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 6605\n",
      "train_loss: 0.0625492231200729\n",
      "val_loss: 0.1653470956073628\n",
      "Progress: 66.1% ... Training loss: 0.063 ... Validation loss: 0.190iteration: 6606\n",
      "train_loss: 0.06347490230868903\n",
      "val_loss: 0.19060170623092673\n",
      "Progress: 66.1% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 6607\n",
      "train_loss: 0.06197419025856594\n",
      "val_loss: 0.1622502451767051\n",
      "Progress: 66.1% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 6608\n",
      "train_loss: 0.06354764442224307\n",
      "val_loss: 0.1744446486389492\n",
      "Progress: 66.1% ... Training loss: 0.073 ... Validation loss: 0.161iteration: 6609\n",
      "train_loss: 0.0737456979366715\n",
      "val_loss: 0.16148104856688136\n",
      "Progress: 66.1% ... Training loss: 0.070 ... Validation loss: 0.191iteration: 6610\n",
      "train_loss: 0.07016006180990013\n",
      "val_loss: 0.19110031972340288\n",
      "Progress: 66.1% ... Training loss: 0.064 ... Validation loss: 0.159iteration: 6611\n",
      "train_loss: 0.06420846800370328\n",
      "val_loss: 0.1594024478102377\n",
      "Progress: 66.1% ... Training loss: 0.067 ... Validation loss: 0.189iteration: 6612\n",
      "train_loss: 0.0672059905118459\n",
      "val_loss: 0.18984872679882558\n",
      "Progress: 66.1% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 6613\n",
      "train_loss: 0.063208770731323\n",
      "val_loss: 0.16105648968903216\n",
      "Progress: 66.1% ... Training loss: 0.062 ... Validation loss: 0.179iteration: 6614\n",
      "train_loss: 0.0628547489484687\n",
      "val_loss: 0.17911286453556466\n",
      "Progress: 66.2% ... Training loss: 0.066 ... Validation loss: 0.170iteration: 6615\n",
      "train_loss: 0.06655519308920914\n",
      "val_loss: 0.17074225060365267\n",
      "Progress: 66.2% ... Training loss: 0.061 ... Validation loss: 0.172iteration: 6616\n",
      "train_loss: 0.06186077659284162\n",
      "val_loss: 0.17279082846548954\n",
      "Progress: 66.2% ... Training loss: 0.067 ... Validation loss: 0.148iteration: 6617\n",
      "train_loss: 0.06773209094591938\n",
      "val_loss: 0.14892085725346949\n",
      "Progress: 66.2% ... Training loss: 0.062 ... Validation loss: 0.174iteration: 6618\n",
      "train_loss: 0.06264427608727315\n",
      "val_loss: 0.17464552699132105\n",
      "Progress: 66.2% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 6619\n",
      "train_loss: 0.061544976183059624\n",
      "val_loss: 0.1620739937428088\n",
      "Progress: 66.2% ... Training loss: 0.063 ... Validation loss: 0.177iteration: 6620\n",
      "train_loss: 0.06395922707489686\n",
      "val_loss: 0.17738199861392545\n",
      "Progress: 66.2% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 6621\n",
      "train_loss: 0.06194601631851598\n",
      "val_loss: 0.1597053872885654\n",
      "Progress: 66.2% ... Training loss: 0.063 ... Validation loss: 0.173iteration: 6622\n",
      "train_loss: 0.06335407976154118\n",
      "val_loss: 0.17391054518733656\n",
      "Progress: 66.2% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 6623\n",
      "train_loss: 0.06181741091681156\n",
      "val_loss: 0.16280539056987717\n",
      "Progress: 66.2% ... Training loss: 0.062 ... Validation loss: 0.169iteration: 6624\n",
      "train_loss: 0.06280274742380827\n",
      "val_loss: 0.1692755954829633\n",
      "Progress: 66.2% ... Training loss: 0.062 ... Validation loss: 0.152iteration: 6625\n",
      "train_loss: 0.0627071711451444\n",
      "val_loss: 0.15208864886324452\n",
      "Progress: 66.3% ... Training loss: 0.064 ... Validation loss: 0.168iteration: 6626\n",
      "train_loss: 0.06463323222880474\n",
      "val_loss: 0.1688582307930406\n",
      "Progress: 66.3% ... Training loss: 0.075 ... Validation loss: 0.156iteration: 6627\n",
      "train_loss: 0.07526847570336201\n",
      "val_loss: 0.156925037893581\n",
      "Progress: 66.3% ... Training loss: 0.079 ... Validation loss: 0.207iteration: 6628\n",
      "train_loss: 0.07987575951712161\n",
      "val_loss: 0.20750223669764986\n",
      "Progress: 66.3% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 6629\n",
      "train_loss: 0.07098166178798958\n",
      "val_loss: 0.15071219527281177\n",
      "Progress: 66.3% ... Training loss: 0.077 ... Validation loss: 0.214iteration: 6630\n",
      "train_loss: 0.07715153439566484\n",
      "val_loss: 0.21498092964368126\n",
      "Progress: 66.3% ... Training loss: 0.067 ... Validation loss: 0.170iteration: 6631\n",
      "train_loss: 0.0673636397976752\n",
      "val_loss: 0.17004740420840447\n",
      "Progress: 66.3% ... Training loss: 0.066 ... Validation loss: 0.158iteration: 6632\n",
      "train_loss: 0.06633051984885321\n",
      "val_loss: 0.1587206423025054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 66.3% ... Training loss: 0.063 ... Validation loss: 0.163iteration: 6633\n",
      "train_loss: 0.0638876409411581\n",
      "val_loss: 0.16353699813661138\n",
      "Progress: 66.3% ... Training loss: 0.065 ... Validation loss: 0.163iteration: 6634\n",
      "train_loss: 0.06558687211698348\n",
      "val_loss: 0.16380438880183673\n",
      "Progress: 66.3% ... Training loss: 0.067 ... Validation loss: 0.217iteration: 6635\n",
      "train_loss: 0.0670245641953203\n",
      "val_loss: 0.21769957793353428\n",
      "Progress: 66.4% ... Training loss: 0.064 ... Validation loss: 0.160iteration: 6636\n",
      "train_loss: 0.06412301663921394\n",
      "val_loss: 0.16000219964326318\n",
      "Progress: 66.4% ... Training loss: 0.069 ... Validation loss: 0.203iteration: 6637\n",
      "train_loss: 0.06926362114168395\n",
      "val_loss: 0.20348390340273464\n",
      "Progress: 66.4% ... Training loss: 0.070 ... Validation loss: 0.151iteration: 6638\n",
      "train_loss: 0.07052574188446611\n",
      "val_loss: 0.15158768844985795\n",
      "Progress: 66.4% ... Training loss: 0.072 ... Validation loss: 0.202iteration: 6639\n",
      "train_loss: 0.07209510239287584\n",
      "val_loss: 0.20236952235698422\n",
      "Progress: 66.4% ... Training loss: 0.064 ... Validation loss: 0.151iteration: 6640\n",
      "train_loss: 0.06488469437532675\n",
      "val_loss: 0.151866471555433\n",
      "Progress: 66.4% ... Training loss: 0.068 ... Validation loss: 0.187iteration: 6641\n",
      "train_loss: 0.06863150350712159\n",
      "val_loss: 0.18776809605895975\n",
      "Progress: 66.4% ... Training loss: 0.068 ... Validation loss: 0.154iteration: 6642\n",
      "train_loss: 0.06860651948426248\n",
      "val_loss: 0.1546401406302686\n",
      "Progress: 66.4% ... Training loss: 0.072 ... Validation loss: 0.199iteration: 6643\n",
      "train_loss: 0.07220105317794791\n",
      "val_loss: 0.19956316420897777\n",
      "Progress: 66.4% ... Training loss: 0.077 ... Validation loss: 0.152iteration: 6644\n",
      "train_loss: 0.07757807860528763\n",
      "val_loss: 0.15206743107648876\n",
      "Progress: 66.5% ... Training loss: 0.086 ... Validation loss: 0.223iteration: 6645\n",
      "train_loss: 0.08666457522605867\n",
      "val_loss: 0.2238304854199661\n",
      "Progress: 66.5% ... Training loss: 0.096 ... Validation loss: 0.150iteration: 6646\n",
      "train_loss: 0.09662234572498425\n",
      "val_loss: 0.15053560413011532\n",
      "Progress: 66.5% ... Training loss: 0.099 ... Validation loss: 0.250iteration: 6647\n",
      "train_loss: 0.09980346794149614\n",
      "val_loss: 0.2500862462221924\n",
      "Progress: 66.5% ... Training loss: 0.084 ... Validation loss: 0.154iteration: 6648\n",
      "train_loss: 0.08413147235341557\n",
      "val_loss: 0.15492203873735227\n",
      "Progress: 66.5% ... Training loss: 0.073 ... Validation loss: 0.220iteration: 6649\n",
      "train_loss: 0.07353651418165827\n",
      "val_loss: 0.2206909809743903\n",
      "Progress: 66.5% ... Training loss: 0.080 ... Validation loss: 0.146iteration: 6650\n",
      "train_loss: 0.08033447703175471\n",
      "val_loss: 0.14672261700965836\n",
      "Progress: 66.5% ... Training loss: 0.080 ... Validation loss: 0.231iteration: 6651\n",
      "train_loss: 0.08032880555965519\n",
      "val_loss: 0.23158958856396986\n",
      "Progress: 66.5% ... Training loss: 0.067 ... Validation loss: 0.151iteration: 6652\n",
      "train_loss: 0.06700236263800521\n",
      "val_loss: 0.1517414626239504\n",
      "Progress: 66.5% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 6653\n",
      "train_loss: 0.062192475953955065\n",
      "val_loss: 0.16501063771721125\n",
      "Progress: 66.5% ... Training loss: 0.063 ... Validation loss: 0.159iteration: 6654\n",
      "train_loss: 0.06380488589411806\n",
      "val_loss: 0.15968077927527868\n",
      "Progress: 66.5% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 6655\n",
      "train_loss: 0.06432476972788474\n",
      "val_loss: 0.17643219022598555\n",
      "Progress: 66.6% ... Training loss: 0.063 ... Validation loss: 0.176iteration: 6656\n",
      "train_loss: 0.0631764837797475\n",
      "val_loss: 0.17698981289416432\n",
      "Progress: 66.6% ... Training loss: 0.062 ... Validation loss: 0.163iteration: 6657\n",
      "train_loss: 0.06280105215052241\n",
      "val_loss: 0.16395624986212115\n",
      "Progress: 66.6% ... Training loss: 0.062 ... Validation loss: 0.157iteration: 6658\n",
      "train_loss: 0.0622016003726151\n",
      "val_loss: 0.15788961003348104\n",
      "Progress: 66.6% ... Training loss: 0.062 ... Validation loss: 0.163iteration: 6659\n",
      "train_loss: 0.06240698564421574\n",
      "val_loss: 0.1631975484541233\n",
      "Progress: 66.6% ... Training loss: 0.062 ... Validation loss: 0.155iteration: 6660\n",
      "train_loss: 0.06228934054623028\n",
      "val_loss: 0.15516001932771464\n",
      "Progress: 66.6% ... Training loss: 0.062 ... Validation loss: 0.166iteration: 6661\n",
      "train_loss: 0.062421926817845884\n",
      "val_loss: 0.1664852882326706\n",
      "Progress: 66.6% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 6662\n",
      "train_loss: 0.061386727442808105\n",
      "val_loss: 0.1650604757258086\n",
      "Progress: 66.6% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 6663\n",
      "train_loss: 0.06509282146204538\n",
      "val_loss: 0.15225136902977876\n",
      "Progress: 66.6% ... Training loss: 0.061 ... Validation loss: 0.164iteration: 6664\n",
      "train_loss: 0.061275361186702\n",
      "val_loss: 0.16419884898581963\n",
      "Progress: 66.7% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 6665\n",
      "train_loss: 0.06193615639449031\n",
      "val_loss: 0.16522156849059802\n",
      "Progress: 66.7% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 6666\n",
      "train_loss: 0.06233783102475698\n",
      "val_loss: 0.16523822780458036\n",
      "Progress: 66.7% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 6667\n",
      "train_loss: 0.06118850893277653\n",
      "val_loss: 0.16698132665686272\n",
      "Progress: 66.7% ... Training loss: 0.068 ... Validation loss: 0.181iteration: 6668\n",
      "train_loss: 0.06890327142435372\n",
      "val_loss: 0.18113107329965344\n",
      "Progress: 66.7% ... Training loss: 0.065 ... Validation loss: 0.151iteration: 6669\n",
      "train_loss: 0.06512790708121272\n",
      "val_loss: 0.15186576190382955\n",
      "Progress: 66.7% ... Training loss: 0.066 ... Validation loss: 0.184iteration: 6670\n",
      "train_loss: 0.06661841856182099\n",
      "val_loss: 0.18448582432797184\n",
      "Progress: 66.7% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 6671\n",
      "train_loss: 0.06163800884984663\n",
      "val_loss: 0.16252817925770294\n",
      "Progress: 66.7% ... Training loss: 0.070 ... Validation loss: 0.194iteration: 6672\n",
      "train_loss: 0.07010375800816124\n",
      "val_loss: 0.19466673760905817\n",
      "Progress: 66.7% ... Training loss: 0.064 ... Validation loss: 0.155iteration: 6673\n",
      "train_loss: 0.06442742291339534\n",
      "val_loss: 0.1553616239228664\n",
      "Progress: 66.7% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 6674\n",
      "train_loss: 0.06170063913128497\n",
      "val_loss: 0.1600806261959459\n",
      "Progress: 66.8% ... Training loss: 0.063 ... Validation loss: 0.177iteration: 6675\n",
      "train_loss: 0.06318524942009449\n",
      "val_loss: 0.17723884504208462\n",
      "Progress: 66.8% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 6676\n",
      "train_loss: 0.06184399135666121\n",
      "val_loss: 0.17134869933539484\n",
      "Progress: 66.8% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 6677\n",
      "train_loss: 0.06160638473432499\n",
      "val_loss: 0.16636149515621237\n",
      "Progress: 66.8% ... Training loss: 0.065 ... Validation loss: 0.178iteration: 6678\n",
      "train_loss: 0.06523846835100718\n",
      "val_loss: 0.17873616189850258\n",
      "Progress: 66.8% ... Training loss: 0.072 ... Validation loss: 0.155iteration: 6679\n",
      "train_loss: 0.07245531190562587\n",
      "val_loss: 0.15513342539977823\n",
      "Progress: 66.8% ... Training loss: 0.068 ... Validation loss: 0.193iteration: 6680\n",
      "train_loss: 0.06837430686132223\n",
      "val_loss: 0.19337413953505952\n",
      "Progress: 66.8% ... Training loss: 0.070 ... Validation loss: 0.158iteration: 6681\n",
      "train_loss: 0.07087427174484116\n",
      "val_loss: 0.15875076068449534\n",
      "Progress: 66.8% ... Training loss: 0.062 ... Validation loss: 0.182iteration: 6682\n",
      "train_loss: 0.06266385796832713\n",
      "val_loss: 0.1826532090762487\n",
      "Progress: 66.8% ... Training loss: 0.066 ... Validation loss: 0.162iteration: 6683\n",
      "train_loss: 0.06618874236403081\n",
      "val_loss: 0.1627734396452039\n",
      "Progress: 66.8% ... Training loss: 0.065 ... Validation loss: 0.189iteration: 6684\n",
      "train_loss: 0.06577185304892665\n",
      "val_loss: 0.18905238501758068\n",
      "Progress: 66.8% ... Training loss: 0.072 ... Validation loss: 0.152iteration: 6685\n",
      "train_loss: 0.07298659313435178\n",
      "val_loss: 0.15268232953982136\n",
      "Progress: 66.9% ... Training loss: 0.089 ... Validation loss: 0.236iteration: 6686\n",
      "train_loss: 0.08960102679736819\n",
      "val_loss: 0.23674412112056725\n",
      "Progress: 66.9% ... Training loss: 0.070 ... Validation loss: 0.151iteration: 6687\n",
      "train_loss: 0.070516961801003\n",
      "val_loss: 0.15152863638360495\n",
      "Progress: 66.9% ... Training loss: 0.064 ... Validation loss: 0.182iteration: 6688\n",
      "train_loss: 0.06458991779709804\n",
      "val_loss: 0.1822985181981949\n",
      "Progress: 66.9% ... Training loss: 0.064 ... Validation loss: 0.160iteration: 6689\n",
      "train_loss: 0.06490492957068271\n",
      "val_loss: 0.16008135817279318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 66.9% ... Training loss: 0.064 ... Validation loss: 0.189iteration: 6690\n",
      "train_loss: 0.06465341092197509\n",
      "val_loss: 0.1890975805178653\n",
      "Progress: 66.9% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 6691\n",
      "train_loss: 0.0618460081579286\n",
      "val_loss: 0.17320764569553926\n",
      "Progress: 66.9% ... Training loss: 0.062 ... Validation loss: 0.178iteration: 6692\n",
      "train_loss: 0.0627761739132524\n",
      "val_loss: 0.17834061450319483\n",
      "Progress: 66.9% ... Training loss: 0.062 ... Validation loss: 0.168iteration: 6693\n",
      "train_loss: 0.06232605432411351\n",
      "val_loss: 0.16880220687265407\n",
      "Progress: 66.9% ... Training loss: 0.064 ... Validation loss: 0.190iteration: 6694\n",
      "train_loss: 0.0647113649851096\n",
      "val_loss: 0.19026923788252426\n",
      "Progress: 67.0% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 6695\n",
      "train_loss: 0.06407615308792847\n",
      "val_loss: 0.17827601543539287\n",
      "Progress: 67.0% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 6696\n",
      "train_loss: 0.06199273096113476\n",
      "val_loss: 0.16604644116819622\n",
      "Progress: 67.0% ... Training loss: 0.070 ... Validation loss: 0.188iteration: 6697\n",
      "train_loss: 0.07048871860910724\n",
      "val_loss: 0.18862863929949655\n",
      "Progress: 67.0% ... Training loss: 0.086 ... Validation loss: 0.160iteration: 6698\n",
      "train_loss: 0.08678726146609084\n",
      "val_loss: 0.1601587125959278\n",
      "Progress: 67.0% ... Training loss: 0.088 ... Validation loss: 0.234iteration: 6699\n",
      "train_loss: 0.0885632531952875\n",
      "val_loss: 0.23406894115962362\n",
      "Progress: 67.0% ... Training loss: 0.075 ... Validation loss: 0.152iteration: 6700\n",
      "train_loss: 0.07535020619981134\n",
      "val_loss: 0.1528952839615247\n",
      "Progress: 67.0% ... Training loss: 0.082 ... Validation loss: 0.225iteration: 6701\n",
      "train_loss: 0.08201722076936578\n",
      "val_loss: 0.22585571641172322\n",
      "Progress: 67.0% ... Training loss: 0.078 ... Validation loss: 0.152iteration: 6702\n",
      "train_loss: 0.07815017298494138\n",
      "val_loss: 0.152805191228693\n",
      "Progress: 67.0% ... Training loss: 0.078 ... Validation loss: 0.221iteration: 6703\n",
      "train_loss: 0.07884751348385809\n",
      "val_loss: 0.22114759092132524\n",
      "Progress: 67.0% ... Training loss: 0.081 ... Validation loss: 0.155iteration: 6704\n",
      "train_loss: 0.08143475636593196\n",
      "val_loss: 0.15599830398733805\n",
      "Progress: 67.0% ... Training loss: 0.065 ... Validation loss: 0.188iteration: 6705\n",
      "train_loss: 0.06585692523097314\n",
      "val_loss: 0.18893139622601152\n",
      "Progress: 67.1% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 6706\n",
      "train_loss: 0.06165154588901481\n",
      "val_loss: 0.16626679679768472\n",
      "Progress: 67.1% ... Training loss: 0.064 ... Validation loss: 0.174iteration: 6707\n",
      "train_loss: 0.06479619929726013\n",
      "val_loss: 0.17463594490332263\n",
      "Progress: 67.1% ... Training loss: 0.065 ... Validation loss: 0.153iteration: 6708\n",
      "train_loss: 0.06573152839453189\n",
      "val_loss: 0.15375046578617102\n",
      "Progress: 67.1% ... Training loss: 0.071 ... Validation loss: 0.203iteration: 6709\n",
      "train_loss: 0.07125506587300255\n",
      "val_loss: 0.20333362554299142\n",
      "Progress: 67.1% ... Training loss: 0.066 ... Validation loss: 0.158iteration: 6710\n",
      "train_loss: 0.0661662045125781\n",
      "val_loss: 0.1587084791598832\n",
      "Progress: 67.1% ... Training loss: 0.064 ... Validation loss: 0.199iteration: 6711\n",
      "train_loss: 0.06407128403039963\n",
      "val_loss: 0.19916670513911722\n",
      "Progress: 67.1% ... Training loss: 0.062 ... Validation loss: 0.175iteration: 6712\n",
      "train_loss: 0.06250604980656868\n",
      "val_loss: 0.17517570055178042\n",
      "Progress: 67.1% ... Training loss: 0.062 ... Validation loss: 0.157iteration: 6713\n",
      "train_loss: 0.06234543939806865\n",
      "val_loss: 0.15795235821675402\n",
      "Progress: 67.1% ... Training loss: 0.068 ... Validation loss: 0.195iteration: 6714\n",
      "train_loss: 0.06891625956548333\n",
      "val_loss: 0.1952777492382197\n",
      "Progress: 67.2% ... Training loss: 0.070 ... Validation loss: 0.155iteration: 6715\n",
      "train_loss: 0.07086791867558481\n",
      "val_loss: 0.15553466665112858\n",
      "Progress: 67.2% ... Training loss: 0.078 ... Validation loss: 0.229iteration: 6716\n",
      "train_loss: 0.07826302472887417\n",
      "val_loss: 0.2299293601736687\n",
      "Progress: 67.2% ... Training loss: 0.069 ... Validation loss: 0.153iteration: 6717\n",
      "train_loss: 0.06957464058328118\n",
      "val_loss: 0.15327549266021417\n",
      "Progress: 67.2% ... Training loss: 0.075 ... Validation loss: 0.211iteration: 6718\n",
      "train_loss: 0.0752238645361706\n",
      "val_loss: 0.21120600135204404\n",
      "Progress: 67.2% ... Training loss: 0.073 ... Validation loss: 0.148iteration: 6719\n",
      "train_loss: 0.07389436925501931\n",
      "val_loss: 0.1480878086475708\n",
      "Progress: 67.2% ... Training loss: 0.069 ... Validation loss: 0.187iteration: 6720\n",
      "train_loss: 0.06984113948475483\n",
      "val_loss: 0.1870742258467911\n",
      "Progress: 67.2% ... Training loss: 0.068 ... Validation loss: 0.150iteration: 6721\n",
      "train_loss: 0.06854642957354387\n",
      "val_loss: 0.15086125528884403\n",
      "Progress: 67.2% ... Training loss: 0.068 ... Validation loss: 0.178iteration: 6722\n",
      "train_loss: 0.06806198178959993\n",
      "val_loss: 0.17867534605180044\n",
      "Progress: 67.2% ... Training loss: 0.067 ... Validation loss: 0.155iteration: 6723\n",
      "train_loss: 0.06781422480526064\n",
      "val_loss: 0.15530898927755316\n",
      "Progress: 67.2% ... Training loss: 0.061 ... Validation loss: 0.174iteration: 6724\n",
      "train_loss: 0.061623077990318985\n",
      "val_loss: 0.1741606763756381\n",
      "Progress: 67.2% ... Training loss: 0.061 ... Validation loss: 0.164iteration: 6725\n",
      "train_loss: 0.061696204922066254\n",
      "val_loss: 0.16405568228366196\n",
      "Progress: 67.3% ... Training loss: 0.062 ... Validation loss: 0.174iteration: 6726\n",
      "train_loss: 0.06275163444699798\n",
      "val_loss: 0.1742730110501016\n",
      "Progress: 67.3% ... Training loss: 0.063 ... Validation loss: 0.186iteration: 6727\n",
      "train_loss: 0.06366253671539178\n",
      "val_loss: 0.18629499868694244\n",
      "Progress: 67.3% ... Training loss: 0.071 ... Validation loss: 0.154iteration: 6728\n",
      "train_loss: 0.07127454389706278\n",
      "val_loss: 0.15438491570092178\n",
      "Progress: 67.3% ... Training loss: 0.076 ... Validation loss: 0.197iteration: 6729\n",
      "train_loss: 0.07640437367075763\n",
      "val_loss: 0.19717236034720764\n",
      "Progress: 67.3% ... Training loss: 0.072 ... Validation loss: 0.146iteration: 6730\n",
      "train_loss: 0.07299589131018855\n",
      "val_loss: 0.14663110594387796\n",
      "Progress: 67.3% ... Training loss: 0.069 ... Validation loss: 0.179iteration: 6731\n",
      "train_loss: 0.06935017774280162\n",
      "val_loss: 0.17957676961340158\n",
      "Progress: 67.3% ... Training loss: 0.066 ... Validation loss: 0.148iteration: 6732\n",
      "train_loss: 0.06699470589058659\n",
      "val_loss: 0.1487169421383453\n",
      "Progress: 67.3% ... Training loss: 0.069 ... Validation loss: 0.176iteration: 6733\n",
      "train_loss: 0.0692061846809272\n",
      "val_loss: 0.1769917609727656\n",
      "Progress: 67.3% ... Training loss: 0.062 ... Validation loss: 0.161iteration: 6734\n",
      "train_loss: 0.06224729542492465\n",
      "val_loss: 0.16108168770525297\n",
      "Progress: 67.3% ... Training loss: 0.062 ... Validation loss: 0.159iteration: 6735\n",
      "train_loss: 0.06255960717602503\n",
      "val_loss: 0.15911827072685078\n",
      "Progress: 67.4% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 6736\n",
      "train_loss: 0.06189911411288111\n",
      "val_loss: 0.1624462912031735\n",
      "Progress: 67.4% ... Training loss: 0.063 ... Validation loss: 0.179iteration: 6737\n",
      "train_loss: 0.0635764100799891\n",
      "val_loss: 0.1791505092751821\n",
      "Progress: 67.4% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 6738\n",
      "train_loss: 0.06203078787109448\n",
      "val_loss: 0.16536578532149224\n",
      "Progress: 67.4% ... Training loss: 0.061 ... Validation loss: 0.178iteration: 6739\n",
      "train_loss: 0.06190309547512947\n",
      "val_loss: 0.17828950013266245\n",
      "Progress: 67.4% ... Training loss: 0.062 ... Validation loss: 0.178iteration: 6740\n",
      "train_loss: 0.06242605636386974\n",
      "val_loss: 0.17813410598061366\n",
      "Progress: 67.4% ... Training loss: 0.062 ... Validation loss: 0.180iteration: 6741\n",
      "train_loss: 0.06259424540534982\n",
      "val_loss: 0.18065469008421223\n",
      "Progress: 67.4% ... Training loss: 0.062 ... Validation loss: 0.185iteration: 6742\n",
      "train_loss: 0.06274173509185063\n",
      "val_loss: 0.18554642181179792\n",
      "Progress: 67.4% ... Training loss: 0.062 ... Validation loss: 0.158iteration: 6743\n",
      "train_loss: 0.06269247071160522\n",
      "val_loss: 0.15884261364503247\n",
      "Progress: 67.4% ... Training loss: 0.070 ... Validation loss: 0.198iteration: 6744\n",
      "train_loss: 0.07009998450154073\n",
      "val_loss: 0.19812384673773395\n",
      "Progress: 67.5% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 6745\n",
      "train_loss: 0.06544886889574826\n",
      "val_loss: 0.15277222186214334\n",
      "Progress: 67.5% ... Training loss: 0.062 ... Validation loss: 0.160iteration: 6746\n",
      "train_loss: 0.06220194645248872\n",
      "val_loss: 0.1607501157482932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 67.5% ... Training loss: 0.062 ... Validation loss: 0.168iteration: 6747\n",
      "train_loss: 0.06202331949140005\n",
      "val_loss: 0.16896115182349827\n",
      "Progress: 67.5% ... Training loss: 0.062 ... Validation loss: 0.163iteration: 6748\n",
      "train_loss: 0.06225777915654898\n",
      "val_loss: 0.16351507432104023\n",
      "Progress: 67.5% ... Training loss: 0.076 ... Validation loss: 0.200iteration: 6749\n",
      "train_loss: 0.0761068263157841\n",
      "val_loss: 0.2008555262060507\n",
      "Progress: 67.5% ... Training loss: 0.088 ... Validation loss: 0.150iteration: 6750\n",
      "train_loss: 0.08882683295418205\n",
      "val_loss: 0.1507426424630197\n",
      "Progress: 67.5% ... Training loss: 0.083 ... Validation loss: 0.219iteration: 6751\n",
      "train_loss: 0.08337183977396233\n",
      "val_loss: 0.21900618136671055\n",
      "Progress: 67.5% ... Training loss: 0.096 ... Validation loss: 0.154iteration: 6752\n",
      "train_loss: 0.09636312729172092\n",
      "val_loss: 0.154196940625543\n",
      "Progress: 67.5% ... Training loss: 0.099 ... Validation loss: 0.250iteration: 6753\n",
      "train_loss: 0.09934523394359454\n",
      "val_loss: 0.25059977798580924\n",
      "Progress: 67.5% ... Training loss: 0.092 ... Validation loss: 0.158iteration: 6754\n",
      "train_loss: 0.09275076250286896\n",
      "val_loss: 0.15893533534013665\n",
      "Progress: 67.5% ... Training loss: 0.092 ... Validation loss: 0.243iteration: 6755\n",
      "train_loss: 0.09210261695785911\n",
      "val_loss: 0.24308401449775935\n",
      "Progress: 67.6% ... Training loss: 0.082 ... Validation loss: 0.156iteration: 6756\n",
      "train_loss: 0.08233065980865303\n",
      "val_loss: 0.1562717137002815\n",
      "Progress: 67.6% ... Training loss: 0.073 ... Validation loss: 0.195iteration: 6757\n",
      "train_loss: 0.07313180210768457\n",
      "val_loss: 0.1953021844923423\n",
      "Progress: 67.6% ... Training loss: 0.072 ... Validation loss: 0.154iteration: 6758\n",
      "train_loss: 0.07243718240177051\n",
      "val_loss: 0.15430376982302832\n",
      "Progress: 67.6% ... Training loss: 0.096 ... Validation loss: 0.225iteration: 6759\n",
      "train_loss: 0.09606285917794982\n",
      "val_loss: 0.22568015014658382\n",
      "Progress: 67.6% ... Training loss: 0.087 ... Validation loss: 0.153iteration: 6760\n",
      "train_loss: 0.08736846428983455\n",
      "val_loss: 0.1535328473080589\n",
      "Progress: 67.6% ... Training loss: 0.074 ... Validation loss: 0.221iteration: 6761\n",
      "train_loss: 0.07457112212172085\n",
      "val_loss: 0.2214859223970032\n",
      "Progress: 67.6% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 6762\n",
      "train_loss: 0.06523122064941725\n",
      "val_loss: 0.15867011143932022\n",
      "Progress: 67.6% ... Training loss: 0.063 ... Validation loss: 0.184iteration: 6763\n",
      "train_loss: 0.06321105864985177\n",
      "val_loss: 0.18412374957165709\n",
      "Progress: 67.6% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 6764\n",
      "train_loss: 0.0651711686048623\n",
      "val_loss: 0.15804861763590256\n",
      "Progress: 67.7% ... Training loss: 0.077 ... Validation loss: 0.231iteration: 6765\n",
      "train_loss: 0.07776490005430815\n",
      "val_loss: 0.2315202172521377\n",
      "Progress: 67.7% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 6766\n",
      "train_loss: 0.06301110161467807\n",
      "val_loss: 0.16642308985446744\n",
      "Progress: 67.7% ... Training loss: 0.063 ... Validation loss: 0.187iteration: 6767\n",
      "train_loss: 0.06337271325664591\n",
      "val_loss: 0.18703241897891282\n",
      "Progress: 67.7% ... Training loss: 0.063 ... Validation loss: 0.191iteration: 6768\n",
      "train_loss: 0.06352874441026363\n",
      "val_loss: 0.19165398398298564\n",
      "Progress: 67.7% ... Training loss: 0.071 ... Validation loss: 0.157iteration: 6769\n",
      "train_loss: 0.07190417431396068\n",
      "val_loss: 0.15707654075284805\n",
      "Progress: 67.7% ... Training loss: 0.062 ... Validation loss: 0.181iteration: 6770\n",
      "train_loss: 0.062109949098968435\n",
      "val_loss: 0.1813673700909445\n",
      "Progress: 67.7% ... Training loss: 0.064 ... Validation loss: 0.177iteration: 6771\n",
      "train_loss: 0.06423371175871408\n",
      "val_loss: 0.17776663654888486\n",
      "Progress: 67.7% ... Training loss: 0.061 ... Validation loss: 0.178iteration: 6772\n",
      "train_loss: 0.061898480446671844\n",
      "val_loss: 0.17839298694082742\n",
      "Progress: 67.7% ... Training loss: 0.066 ... Validation loss: 0.200iteration: 6773\n",
      "train_loss: 0.06608054170938212\n",
      "val_loss: 0.20029655735242075\n",
      "Progress: 67.7% ... Training loss: 0.062 ... Validation loss: 0.160iteration: 6774\n",
      "train_loss: 0.062180823372594375\n",
      "val_loss: 0.16010939539442487\n",
      "Progress: 67.8% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 6775\n",
      "train_loss: 0.061428504916623126\n",
      "val_loss: 0.16525336195831802\n",
      "Progress: 67.8% ... Training loss: 0.063 ... Validation loss: 0.152iteration: 6776\n",
      "train_loss: 0.06334640983607936\n",
      "val_loss: 0.15287758668829154\n",
      "Progress: 67.8% ... Training loss: 0.061 ... Validation loss: 0.175iteration: 6777\n",
      "train_loss: 0.06197756228496759\n",
      "val_loss: 0.1753786852484374\n",
      "Progress: 67.8% ... Training loss: 0.062 ... Validation loss: 0.168iteration: 6778\n",
      "train_loss: 0.0625429182036459\n",
      "val_loss: 0.16865535963049053\n",
      "Progress: 67.8% ... Training loss: 0.061 ... Validation loss: 0.167iteration: 6779\n",
      "train_loss: 0.061971766145599856\n",
      "val_loss: 0.16795864609996605\n",
      "Progress: 67.8% ... Training loss: 0.062 ... Validation loss: 0.155iteration: 6780\n",
      "train_loss: 0.06288213876244811\n",
      "val_loss: 0.1558571740411855\n",
      "Progress: 67.8% ... Training loss: 0.062 ... Validation loss: 0.163iteration: 6781\n",
      "train_loss: 0.06278361206580911\n",
      "val_loss: 0.16354779462946667\n",
      "Progress: 67.8% ... Training loss: 0.062 ... Validation loss: 0.173iteration: 6782\n",
      "train_loss: 0.06235199064790484\n",
      "val_loss: 0.17308954156361456\n",
      "Progress: 67.8% ... Training loss: 0.065 ... Validation loss: 0.151iteration: 6783\n",
      "train_loss: 0.06551362795540494\n",
      "val_loss: 0.1513120625975906\n",
      "Progress: 67.8% ... Training loss: 0.076 ... Validation loss: 0.190iteration: 6784\n",
      "train_loss: 0.07678350289829693\n",
      "val_loss: 0.190898103860393\n",
      "Progress: 67.8% ... Training loss: 0.062 ... Validation loss: 0.157iteration: 6785\n",
      "train_loss: 0.062429440777769976\n",
      "val_loss: 0.15732897403364066\n",
      "Progress: 67.9% ... Training loss: 0.064 ... Validation loss: 0.166iteration: 6786\n",
      "train_loss: 0.0640375558348231\n",
      "val_loss: 0.1664663364473301\n",
      "Progress: 67.9% ... Training loss: 0.062 ... Validation loss: 0.171iteration: 6787\n",
      "train_loss: 0.062004481818805766\n",
      "val_loss: 0.17167685401600863\n",
      "Progress: 67.9% ... Training loss: 0.068 ... Validation loss: 0.152iteration: 6788\n",
      "train_loss: 0.06854801727342871\n",
      "val_loss: 0.15225798206472418\n",
      "Progress: 67.9% ... Training loss: 0.067 ... Validation loss: 0.192iteration: 6789\n",
      "train_loss: 0.06759690433597243\n",
      "val_loss: 0.19298790222098855\n",
      "Progress: 67.9% ... Training loss: 0.067 ... Validation loss: 0.155iteration: 6790\n",
      "train_loss: 0.06766957292569682\n",
      "val_loss: 0.1558568348964025\n",
      "Progress: 67.9% ... Training loss: 0.063 ... Validation loss: 0.173iteration: 6791\n",
      "train_loss: 0.06332163644184492\n",
      "val_loss: 0.17305032231824288\n",
      "Progress: 67.9% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 6792\n",
      "train_loss: 0.061945118844057706\n",
      "val_loss: 0.17397758867755192\n",
      "Progress: 67.9% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 6793\n",
      "train_loss: 0.06151691794767232\n",
      "val_loss: 0.1596367427700284\n",
      "Progress: 67.9% ... Training loss: 0.063 ... Validation loss: 0.175iteration: 6794\n",
      "train_loss: 0.06339181455424857\n",
      "val_loss: 0.17565412808347647\n",
      "Progress: 68.0% ... Training loss: 0.062 ... Validation loss: 0.170iteration: 6795\n",
      "train_loss: 0.06250903619160837\n",
      "val_loss: 0.17080729734200029\n",
      "Progress: 68.0% ... Training loss: 0.067 ... Validation loss: 0.179iteration: 6796\n",
      "train_loss: 0.0670017173106168\n",
      "val_loss: 0.17988536456798673\n",
      "Progress: 68.0% ... Training loss: 0.066 ... Validation loss: 0.152iteration: 6797\n",
      "train_loss: 0.06669580803712782\n",
      "val_loss: 0.15298881708518147\n",
      "Progress: 68.0% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 6798\n",
      "train_loss: 0.0635440291061522\n",
      "val_loss: 0.1820852734416538\n",
      "Progress: 68.0% ... Training loss: 0.062 ... Validation loss: 0.159iteration: 6799\n",
      "train_loss: 0.06212030924145539\n",
      "val_loss: 0.15965263626077333\n",
      "Progress: 68.0% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 6800\n",
      "train_loss: 0.06199889194124249\n",
      "val_loss: 0.17031566749977026\n",
      "Progress: 68.0% ... Training loss: 0.067 ... Validation loss: 0.191iteration: 6801\n",
      "train_loss: 0.06738773382875571\n",
      "val_loss: 0.19170372413026276\n",
      "Progress: 68.0% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 6802\n",
      "train_loss: 0.06385958767093598\n",
      "val_loss: 0.1611816800717831\n",
      "Progress: 68.0% ... Training loss: 0.064 ... Validation loss: 0.187iteration: 6803\n",
      "train_loss: 0.06402944957536251\n",
      "val_loss: 0.1878152552412072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 68.0% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 6804\n",
      "train_loss: 0.06198594256657608\n",
      "val_loss: 0.1689412752489921\n",
      "Progress: 68.0% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 6805\n",
      "train_loss: 0.06174589712712146\n",
      "val_loss: 0.17164213780848236\n",
      "Progress: 68.1% ... Training loss: 0.063 ... Validation loss: 0.187iteration: 6806\n",
      "train_loss: 0.0637779909384992\n",
      "val_loss: 0.18791409260444725\n",
      "Progress: 68.1% ... Training loss: 0.070 ... Validation loss: 0.160iteration: 6807\n",
      "train_loss: 0.07079417762800273\n",
      "val_loss: 0.1602464008429897\n",
      "Progress: 68.1% ... Training loss: 0.065 ... Validation loss: 0.186iteration: 6808\n",
      "train_loss: 0.06524158662007871\n",
      "val_loss: 0.18667333821570736\n",
      "Progress: 68.1% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 6809\n",
      "train_loss: 0.06592091124000457\n",
      "val_loss: 0.15883190267171277\n",
      "Progress: 68.1% ... Training loss: 0.062 ... Validation loss: 0.163iteration: 6810\n",
      "train_loss: 0.06240257367375937\n",
      "val_loss: 0.16331763795793325\n",
      "Progress: 68.1% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 6811\n",
      "train_loss: 0.061769643960435286\n",
      "val_loss: 0.17079607915267686\n",
      "Progress: 68.1% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 6812\n",
      "train_loss: 0.06143871125520017\n",
      "val_loss: 0.1631744178300085\n",
      "Progress: 68.1% ... Training loss: 0.064 ... Validation loss: 0.181iteration: 6813\n",
      "train_loss: 0.06473417159620011\n",
      "val_loss: 0.18119941865381542\n",
      "Progress: 68.1% ... Training loss: 0.073 ... Validation loss: 0.155iteration: 6814\n",
      "train_loss: 0.07333200916522217\n",
      "val_loss: 0.15589185817380344\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.186iteration: 6815\n",
      "train_loss: 0.06498600639927378\n",
      "val_loss: 0.18657923858006253\n",
      "Progress: 68.2% ... Training loss: 0.065 ... Validation loss: 0.153iteration: 6816\n",
      "train_loss: 0.06571622220992832\n",
      "val_loss: 0.1530184419592759\n",
      "Progress: 68.2% ... Training loss: 0.063 ... Validation loss: 0.183iteration: 6817\n",
      "train_loss: 0.0637304378739942\n",
      "val_loss: 0.18381150218778752\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.181iteration: 6818\n",
      "train_loss: 0.0641684279827796\n",
      "val_loss: 0.18196877944643378\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.150iteration: 6819\n",
      "train_loss: 0.06424872251173186\n",
      "val_loss: 0.1505484434324876\n",
      "Progress: 68.2% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 6820\n",
      "train_loss: 0.061367538075753146\n",
      "val_loss: 0.16874897958994842\n",
      "Progress: 68.2% ... Training loss: 0.066 ... Validation loss: 0.157iteration: 6821\n",
      "train_loss: 0.06624160535021963\n",
      "val_loss: 0.15719593429391387\n",
      "Progress: 68.2% ... Training loss: 0.062 ... Validation loss: 0.166iteration: 6822\n",
      "train_loss: 0.06298742075406143\n",
      "val_loss: 0.16666481117144893\n",
      "Progress: 68.2% ... Training loss: 0.069 ... Validation loss: 0.152iteration: 6823\n",
      "train_loss: 0.06908423981664864\n",
      "val_loss: 0.1521603695846232\n",
      "Progress: 68.2% ... Training loss: 0.064 ... Validation loss: 0.185iteration: 6824\n",
      "train_loss: 0.06481174217671226\n",
      "val_loss: 0.18583294827013339\n",
      "Progress: 68.2% ... Training loss: 0.062 ... Validation loss: 0.166iteration: 6825\n",
      "train_loss: 0.06221628116839434\n",
      "val_loss: 0.16683772758131077\n",
      "Progress: 68.3% ... Training loss: 0.065 ... Validation loss: 0.155iteration: 6826\n",
      "train_loss: 0.06577680058672071\n",
      "val_loss: 0.15597884409774004\n",
      "Progress: 68.3% ... Training loss: 0.062 ... Validation loss: 0.173iteration: 6827\n",
      "train_loss: 0.06259589882460291\n",
      "val_loss: 0.1739850719067973\n",
      "Progress: 68.3% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 6828\n",
      "train_loss: 0.0616028276040358\n",
      "val_loss: 0.15962702896548508\n",
      "Progress: 68.3% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 6829\n",
      "train_loss: 0.06177957019779831\n",
      "val_loss: 0.16369046145784616\n",
      "Progress: 68.3% ... Training loss: 0.063 ... Validation loss: 0.147iteration: 6830\n",
      "train_loss: 0.0637801141782823\n",
      "val_loss: 0.14729511196183612\n",
      "Progress: 68.3% ... Training loss: 0.065 ... Validation loss: 0.169iteration: 6831\n",
      "train_loss: 0.06570067458141662\n",
      "val_loss: 0.16937589385207197\n",
      "Progress: 68.3% ... Training loss: 0.064 ... Validation loss: 0.148iteration: 6832\n",
      "train_loss: 0.06432348379849309\n",
      "val_loss: 0.14844066627640423\n",
      "Progress: 68.3% ... Training loss: 0.069 ... Validation loss: 0.177iteration: 6833\n",
      "train_loss: 0.06956105494918378\n",
      "val_loss: 0.17747625762117347\n",
      "Progress: 68.3% ... Training loss: 0.066 ... Validation loss: 0.145iteration: 6834\n",
      "train_loss: 0.06657574724205174\n",
      "val_loss: 0.14567067687469298\n",
      "Progress: 68.3% ... Training loss: 0.072 ... Validation loss: 0.182iteration: 6835\n",
      "train_loss: 0.07259017128140512\n",
      "val_loss: 0.1828733094401933\n",
      "Progress: 68.4% ... Training loss: 0.080 ... Validation loss: 0.150iteration: 6836\n",
      "train_loss: 0.08078586562305\n",
      "val_loss: 0.1508927477851761\n",
      "Progress: 68.4% ... Training loss: 0.087 ... Validation loss: 0.207iteration: 6837\n",
      "train_loss: 0.08747475483984075\n",
      "val_loss: 0.20798398792308667\n",
      "Progress: 68.4% ... Training loss: 0.100 ... Validation loss: 0.160iteration: 6838\n",
      "train_loss: 0.10033041297417336\n",
      "val_loss: 0.16066716537137166\n",
      "Progress: 68.4% ... Training loss: 0.115 ... Validation loss: 0.230iteration: 6839\n",
      "train_loss: 0.11557573901449252\n",
      "val_loss: 0.23086717927268346\n",
      "Progress: 68.4% ... Training loss: 0.109 ... Validation loss: 0.165iteration: 6840\n",
      "train_loss: 0.10969504606703384\n",
      "val_loss: 0.16520665280651683\n",
      "Progress: 68.4% ... Training loss: 0.098 ... Validation loss: 0.218iteration: 6841\n",
      "train_loss: 0.0982912379782595\n",
      "val_loss: 0.2184465365710075\n",
      "Progress: 68.4% ... Training loss: 0.076 ... Validation loss: 0.153iteration: 6842\n",
      "train_loss: 0.07659622402002007\n",
      "val_loss: 0.1534790177922147\n",
      "Progress: 68.4% ... Training loss: 0.091 ... Validation loss: 0.232iteration: 6843\n",
      "train_loss: 0.09197255204614904\n",
      "val_loss: 0.23206276285991084\n",
      "Progress: 68.4% ... Training loss: 0.096 ... Validation loss: 0.159iteration: 6844\n",
      "train_loss: 0.09608022035806259\n",
      "val_loss: 0.15989198153237555\n",
      "Progress: 68.5% ... Training loss: 0.086 ... Validation loss: 0.231iteration: 6845\n",
      "train_loss: 0.08629724930628913\n",
      "val_loss: 0.23160267999227993\n",
      "Progress: 68.5% ... Training loss: 0.096 ... Validation loss: 0.158iteration: 6846\n",
      "train_loss: 0.09622689842603598\n",
      "val_loss: 0.15878661947993183\n",
      "Progress: 68.5% ... Training loss: 0.098 ... Validation loss: 0.257iteration: 6847\n",
      "train_loss: 0.09867204955511581\n",
      "val_loss: 0.2572179602733027\n",
      "Progress: 68.5% ... Training loss: 0.102 ... Validation loss: 0.163iteration: 6848\n",
      "train_loss: 0.10263896799385908\n",
      "val_loss: 0.16337373767651348\n",
      "Progress: 68.5% ... Training loss: 0.092 ... Validation loss: 0.235iteration: 6849\n",
      "train_loss: 0.09258237256905032\n",
      "val_loss: 0.235810072427436\n",
      "Progress: 68.5% ... Training loss: 0.093 ... Validation loss: 0.157iteration: 6850\n",
      "train_loss: 0.09380101661899458\n",
      "val_loss: 0.1574199593671426\n",
      "Progress: 68.5% ... Training loss: 0.079 ... Validation loss: 0.200iteration: 6851\n",
      "train_loss: 0.07990183339828943\n",
      "val_loss: 0.200767526558893\n",
      "Progress: 68.5% ... Training loss: 0.099 ... Validation loss: 0.159iteration: 6852\n",
      "train_loss: 0.09989121750819924\n",
      "val_loss: 0.15976713540787071\n",
      "Progress: 68.5% ... Training loss: 0.066 ... Validation loss: 0.181iteration: 6853\n",
      "train_loss: 0.06679043837862768\n",
      "val_loss: 0.18167804276632993\n",
      "Progress: 68.5% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 6854\n",
      "train_loss: 0.061903239522007464\n",
      "val_loss: 0.15700487264892796\n",
      "Progress: 68.5% ... Training loss: 0.069 ... Validation loss: 0.171iteration: 6855\n",
      "train_loss: 0.06988307596337309\n",
      "val_loss: 0.17104571425689932\n",
      "Progress: 68.6% ... Training loss: 0.063 ... Validation loss: 0.158iteration: 6856\n",
      "train_loss: 0.06312018688603042\n",
      "val_loss: 0.15818310279562856\n",
      "Progress: 68.6% ... Training loss: 0.061 ... Validation loss: 0.169iteration: 6857\n",
      "train_loss: 0.061714350522106616\n",
      "val_loss: 0.1691911779862669\n",
      "Progress: 68.6% ... Training loss: 0.063 ... Validation loss: 0.177iteration: 6858\n",
      "train_loss: 0.06368540603545\n",
      "val_loss: 0.17732738399827735\n",
      "Progress: 68.6% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 6859\n",
      "train_loss: 0.061809180389286074\n",
      "val_loss: 0.1576970704432425\n",
      "Progress: 68.6% ... Training loss: 0.063 ... Validation loss: 0.155iteration: 6860\n",
      "train_loss: 0.06333189028702958\n",
      "val_loss: 0.1555669628591704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 68.6% ... Training loss: 0.061 ... Validation loss: 0.161iteration: 6861\n",
      "train_loss: 0.06101825344293295\n",
      "val_loss: 0.1618472312098883\n",
      "Progress: 68.6% ... Training loss: 0.063 ... Validation loss: 0.152iteration: 6862\n",
      "train_loss: 0.06310998880452826\n",
      "val_loss: 0.15254866607174647\n",
      "Progress: 68.6% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 6863\n",
      "train_loss: 0.06201750106859345\n",
      "val_loss: 0.16513578303803053\n",
      "Progress: 68.6% ... Training loss: 0.066 ... Validation loss: 0.151iteration: 6864\n",
      "train_loss: 0.06646623031538083\n",
      "val_loss: 0.15152872398867018\n",
      "Progress: 68.7% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 6865\n",
      "train_loss: 0.061994587574882916\n",
      "val_loss: 0.17032305616382717\n",
      "Progress: 68.7% ... Training loss: 0.061 ... Validation loss: 0.175iteration: 6866\n",
      "train_loss: 0.061918806603661884\n",
      "val_loss: 0.17544149937902853\n",
      "Progress: 68.7% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 6867\n",
      "train_loss: 0.06579393266883304\n",
      "val_loss: 0.15815961970081874\n",
      "Progress: 68.7% ... Training loss: 0.077 ... Validation loss: 0.204iteration: 6868\n",
      "train_loss: 0.07759278086672936\n",
      "val_loss: 0.20438307738816575\n",
      "Progress: 68.7% ... Training loss: 0.099 ... Validation loss: 0.158iteration: 6869\n",
      "train_loss: 0.09977237358074945\n",
      "val_loss: 0.15855776532154967\n",
      "Progress: 68.7% ... Training loss: 0.084 ... Validation loss: 0.218iteration: 6870\n",
      "train_loss: 0.08473634623986412\n",
      "val_loss: 0.2180647150017879\n",
      "Progress: 68.7% ... Training loss: 0.087 ... Validation loss: 0.152iteration: 6871\n",
      "train_loss: 0.08717039373102341\n",
      "val_loss: 0.1528090147458174\n",
      "Progress: 68.7% ... Training loss: 0.067 ... Validation loss: 0.177iteration: 6872\n",
      "train_loss: 0.06733104560874463\n",
      "val_loss: 0.1777306319636707\n",
      "Progress: 68.7% ... Training loss: 0.073 ... Validation loss: 0.150iteration: 6873\n",
      "train_loss: 0.07327396896430373\n",
      "val_loss: 0.1504394051638507\n",
      "Progress: 68.7% ... Training loss: 0.079 ... Validation loss: 0.203iteration: 6874\n",
      "train_loss: 0.07903713165560866\n",
      "val_loss: 0.2030577442641789\n",
      "Progress: 68.8% ... Training loss: 0.065 ... Validation loss: 0.154iteration: 6875\n",
      "train_loss: 0.065503799296431\n",
      "val_loss: 0.1542595311104442\n",
      "Progress: 68.8% ... Training loss: 0.065 ... Validation loss: 0.191iteration: 6876\n",
      "train_loss: 0.06557545958644742\n",
      "val_loss: 0.19102271342359375\n",
      "Progress: 68.8% ... Training loss: 0.063 ... Validation loss: 0.154iteration: 6877\n",
      "train_loss: 0.06393806720856651\n",
      "val_loss: 0.1540903199600237\n",
      "Progress: 68.8% ... Training loss: 0.062 ... Validation loss: 0.174iteration: 6878\n",
      "train_loss: 0.06268831180527372\n",
      "val_loss: 0.17434420702352815\n",
      "Progress: 68.8% ... Training loss: 0.065 ... Validation loss: 0.157iteration: 6879\n",
      "train_loss: 0.06537120181016948\n",
      "val_loss: 0.15707118830447303\n",
      "Progress: 68.8% ... Training loss: 0.072 ... Validation loss: 0.193iteration: 6880\n",
      "train_loss: 0.07204411532000957\n",
      "val_loss: 0.19312520649904827\n",
      "Progress: 68.8% ... Training loss: 0.074 ... Validation loss: 0.146iteration: 6881\n",
      "train_loss: 0.07434829575568765\n",
      "val_loss: 0.14615564840369644\n",
      "Progress: 68.8% ... Training loss: 0.079 ... Validation loss: 0.193iteration: 6882\n",
      "train_loss: 0.07970032421041215\n",
      "val_loss: 0.1933429262048017\n",
      "Progress: 68.8% ... Training loss: 0.072 ... Validation loss: 0.147iteration: 6883\n",
      "train_loss: 0.07271890766586556\n",
      "val_loss: 0.1473768826590962\n",
      "Progress: 68.8% ... Training loss: 0.063 ... Validation loss: 0.171iteration: 6884\n",
      "train_loss: 0.06336316218106446\n",
      "val_loss: 0.1718395004499061\n",
      "Progress: 68.8% ... Training loss: 0.065 ... Validation loss: 0.154iteration: 6885\n",
      "train_loss: 0.06518654956185567\n",
      "val_loss: 0.1543507203117888\n",
      "Progress: 68.9% ... Training loss: 0.064 ... Validation loss: 0.189iteration: 6886\n",
      "train_loss: 0.0641936439370058\n",
      "val_loss: 0.189831603109491\n",
      "Progress: 68.9% ... Training loss: 0.071 ... Validation loss: 0.160iteration: 6887\n",
      "train_loss: 0.07118449550743348\n",
      "val_loss: 0.16040570048984473\n",
      "Progress: 68.9% ... Training loss: 0.068 ... Validation loss: 0.200iteration: 6888\n",
      "train_loss: 0.06871400995554651\n",
      "val_loss: 0.200453199786909\n",
      "Progress: 68.9% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 6889\n",
      "train_loss: 0.061123354007115646\n",
      "val_loss: 0.1592800039959261\n",
      "Progress: 68.9% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 6890\n",
      "train_loss: 0.06119374685104611\n",
      "val_loss: 0.1687222474504882\n",
      "Progress: 68.9% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 6891\n",
      "train_loss: 0.06125110036391045\n",
      "val_loss: 0.1667574220726585\n",
      "Progress: 68.9% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 6892\n",
      "train_loss: 0.06367535355873284\n",
      "val_loss: 0.15675091902346294\n",
      "Progress: 68.9% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 6893\n",
      "train_loss: 0.06149890192579316\n",
      "val_loss: 0.1669765641987693\n",
      "Progress: 68.9% ... Training loss: 0.068 ... Validation loss: 0.151iteration: 6894\n",
      "train_loss: 0.06878099256837568\n",
      "val_loss: 0.1519262467328634\n",
      "Progress: 69.0% ... Training loss: 0.072 ... Validation loss: 0.198iteration: 6895\n",
      "train_loss: 0.07276219035946128\n",
      "val_loss: 0.19867668999180066\n",
      "Progress: 69.0% ... Training loss: 0.087 ... Validation loss: 0.151iteration: 6896\n",
      "train_loss: 0.08769512151737127\n",
      "val_loss: 0.1513445483669231\n",
      "Progress: 69.0% ... Training loss: 0.091 ... Validation loss: 0.215iteration: 6897\n",
      "train_loss: 0.09185687270225094\n",
      "val_loss: 0.21528116969810004\n",
      "Progress: 69.0% ... Training loss: 0.068 ... Validation loss: 0.151iteration: 6898\n",
      "train_loss: 0.06875565190778796\n",
      "val_loss: 0.15103977254114384\n",
      "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.164iteration: 6899\n",
      "train_loss: 0.061697501874679254\n",
      "val_loss: 0.16411302570743716\n",
      "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.177iteration: 6900\n",
      "train_loss: 0.06130889101934123\n",
      "val_loss: 0.17706475529427318\n",
      "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 6901\n",
      "train_loss: 0.06173835130745624\n",
      "val_loss: 0.16586899170417815\n",
      "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.179iteration: 6902\n",
      "train_loss: 0.06187652184974189\n",
      "val_loss: 0.17977248395046352\n",
      "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 6903\n",
      "train_loss: 0.061461005926354374\n",
      "val_loss: 0.17322025853116008\n",
      "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.180iteration: 6904\n",
      "train_loss: 0.061672925307809316\n",
      "val_loss: 0.18014850371570212\n",
      "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.169iteration: 6905\n",
      "train_loss: 0.06188918467600049\n",
      "val_loss: 0.16941403359552065\n",
      "Progress: 69.1% ... Training loss: 0.061 ... Validation loss: 0.174iteration: 6906\n",
      "train_loss: 0.06169516778482703\n",
      "val_loss: 0.1741077284273927\n",
      "Progress: 69.1% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 6907\n",
      "train_loss: 0.06324776829930899\n",
      "val_loss: 0.1745791131067334\n",
      "Progress: 69.1% ... Training loss: 0.061 ... Validation loss: 0.181iteration: 6908\n",
      "train_loss: 0.061245719345182636\n",
      "val_loss: 0.1817302612881983\n",
      "Progress: 69.1% ... Training loss: 0.062 ... Validation loss: 0.175iteration: 6909\n",
      "train_loss: 0.06261000437636971\n",
      "val_loss: 0.17504996507795864\n",
      "Progress: 69.1% ... Training loss: 0.066 ... Validation loss: 0.180iteration: 6910\n",
      "train_loss: 0.06689249569904071\n",
      "val_loss: 0.18005697032108461\n",
      "Progress: 69.1% ... Training loss: 0.064 ... Validation loss: 0.162iteration: 6911\n",
      "train_loss: 0.06447145825201153\n",
      "val_loss: 0.16254494519284762\n",
      "Progress: 69.1% ... Training loss: 0.063 ... Validation loss: 0.186iteration: 6912\n",
      "train_loss: 0.06316720986439409\n",
      "val_loss: 0.18681908920038326\n",
      "Progress: 69.1% ... Training loss: 0.061 ... Validation loss: 0.186iteration: 6913\n",
      "train_loss: 0.06186516531567267\n",
      "val_loss: 0.18648479756389508\n",
      "Progress: 69.1% ... Training loss: 0.067 ... Validation loss: 0.208iteration: 6914\n",
      "train_loss: 0.06716473901212965\n",
      "val_loss: 0.20848052216637647\n",
      "Progress: 69.2% ... Training loss: 0.067 ... Validation loss: 0.159iteration: 6915\n",
      "train_loss: 0.0670323409166342\n",
      "val_loss: 0.15961208321922282\n",
      "Progress: 69.2% ... Training loss: 0.069 ... Validation loss: 0.198iteration: 6916\n",
      "train_loss: 0.06998820988253915\n",
      "val_loss: 0.1984132657169349\n",
      "Progress: 69.2% ... Training loss: 0.077 ... Validation loss: 0.155iteration: 6917\n",
      "train_loss: 0.07793592513541825\n",
      "val_loss: 0.15577262419035584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 69.2% ... Training loss: 0.069 ... Validation loss: 0.213iteration: 6918\n",
      "train_loss: 0.069174989314374\n",
      "val_loss: 0.21326584380692684\n",
      "Progress: 69.2% ... Training loss: 0.074 ... Validation loss: 0.150iteration: 6919\n",
      "train_loss: 0.07447250325249427\n",
      "val_loss: 0.15052085843717683\n",
      "Progress: 69.2% ... Training loss: 0.070 ... Validation loss: 0.205iteration: 6920\n",
      "train_loss: 0.07054299230330878\n",
      "val_loss: 0.20561982408694884\n",
      "Progress: 69.2% ... Training loss: 0.069 ... Validation loss: 0.161iteration: 6921\n",
      "train_loss: 0.06973678941925035\n",
      "val_loss: 0.16161338721252425\n",
      "Progress: 69.2% ... Training loss: 0.075 ... Validation loss: 0.232iteration: 6922\n",
      "train_loss: 0.07547098709918491\n",
      "val_loss: 0.23268082416550295\n",
      "Progress: 69.2% ... Training loss: 0.074 ... Validation loss: 0.154iteration: 6923\n",
      "train_loss: 0.07467224443066375\n",
      "val_loss: 0.15423937879831365\n",
      "Progress: 69.2% ... Training loss: 0.076 ... Validation loss: 0.232iteration: 6924\n",
      "train_loss: 0.07644279687209206\n",
      "val_loss: 0.23256293746011242\n",
      "Progress: 69.2% ... Training loss: 0.081 ... Validation loss: 0.161iteration: 6925\n",
      "train_loss: 0.0812049983732399\n",
      "val_loss: 0.16116301852776813\n",
      "Progress: 69.3% ... Training loss: 0.074 ... Validation loss: 0.222iteration: 6926\n",
      "train_loss: 0.0748840049894077\n",
      "val_loss: 0.22201019445705086\n",
      "Progress: 69.3% ... Training loss: 0.077 ... Validation loss: 0.158iteration: 6927\n",
      "train_loss: 0.07723809243476308\n",
      "val_loss: 0.15839122344878717\n",
      "Progress: 69.3% ... Training loss: 0.074 ... Validation loss: 0.226iteration: 6928\n",
      "train_loss: 0.07454825838893644\n",
      "val_loss: 0.22681421776599714\n",
      "Progress: 69.3% ... Training loss: 0.068 ... Validation loss: 0.160iteration: 6929\n",
      "train_loss: 0.06805321717937111\n",
      "val_loss: 0.160688926732515\n",
      "Progress: 69.3% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 6930\n",
      "train_loss: 0.06491640827974658\n",
      "val_loss: 0.17817378161319897\n",
      "Progress: 69.3% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 6931\n",
      "train_loss: 0.06306939456889078\n",
      "val_loss: 0.1619343898445031\n",
      "Progress: 69.3% ... Training loss: 0.065 ... Validation loss: 0.185iteration: 6932\n",
      "train_loss: 0.06544237090448383\n",
      "val_loss: 0.18500198804643178\n",
      "Progress: 69.3% ... Training loss: 0.063 ... Validation loss: 0.170iteration: 6933\n",
      "train_loss: 0.0636882328316018\n",
      "val_loss: 0.17070487895521858\n",
      "Progress: 69.3% ... Training loss: 0.069 ... Validation loss: 0.197iteration: 6934\n",
      "train_loss: 0.0692360126282203\n",
      "val_loss: 0.19732894150597752\n",
      "Progress: 69.3% ... Training loss: 0.080 ... Validation loss: 0.151iteration: 6935\n",
      "train_loss: 0.08007524159822953\n",
      "val_loss: 0.15195896748826193\n",
      "Progress: 69.4% ... Training loss: 0.071 ... Validation loss: 0.208iteration: 6936\n",
      "train_loss: 0.0714631551579489\n",
      "val_loss: 0.20822553083425224\n",
      "Progress: 69.4% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 6937\n",
      "train_loss: 0.06207449932726529\n",
      "val_loss: 0.16573821218283546\n",
      "Progress: 69.4% ... Training loss: 0.061 ... Validation loss: 0.180iteration: 6938\n",
      "train_loss: 0.061619257219422185\n",
      "val_loss: 0.1802042683445055\n",
      "Progress: 69.4% ... Training loss: 0.062 ... Validation loss: 0.163iteration: 6939\n",
      "train_loss: 0.06220536023642125\n",
      "val_loss: 0.16381166111167134\n",
      "Progress: 69.4% ... Training loss: 0.070 ... Validation loss: 0.192iteration: 6940\n",
      "train_loss: 0.07051860855020427\n",
      "val_loss: 0.19225135111170152\n",
      "Progress: 69.4% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 6941\n",
      "train_loss: 0.0657824084026641\n",
      "val_loss: 0.15251027093034672\n",
      "Progress: 69.4% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 6942\n",
      "train_loss: 0.06333971490747338\n",
      "val_loss: 0.1747342268130781\n",
      "Progress: 69.4% ... Training loss: 0.061 ... Validation loss: 0.178iteration: 6943\n",
      "train_loss: 0.06197160398918772\n",
      "val_loss: 0.1784541536673146\n",
      "Progress: 69.4% ... Training loss: 0.061 ... Validation loss: 0.164iteration: 6944\n",
      "train_loss: 0.06141580370518806\n",
      "val_loss: 0.16459052229602703\n",
      "Progress: 69.5% ... Training loss: 0.062 ... Validation loss: 0.171iteration: 6945\n",
      "train_loss: 0.06239083469541109\n",
      "val_loss: 0.17147627902003898\n",
      "Progress: 69.5% ... Training loss: 0.062 ... Validation loss: 0.175iteration: 6946\n",
      "train_loss: 0.062100006978461374\n",
      "val_loss: 0.17562525266265078\n",
      "Progress: 69.5% ... Training loss: 0.062 ... Validation loss: 0.162iteration: 6947\n",
      "train_loss: 0.06234547967402559\n",
      "val_loss: 0.16240444937452464\n",
      "Progress: 69.5% ... Training loss: 0.071 ... Validation loss: 0.191iteration: 6948\n",
      "train_loss: 0.0713348454540161\n",
      "val_loss: 0.19180326314078672\n",
      "Progress: 69.5% ... Training loss: 0.078 ... Validation loss: 0.151iteration: 6949\n",
      "train_loss: 0.07869928762222551\n",
      "val_loss: 0.15188355360756806\n",
      "Progress: 69.5% ... Training loss: 0.093 ... Validation loss: 0.248iteration: 6950\n",
      "train_loss: 0.09347446142819624\n",
      "val_loss: 0.2486502237641961\n",
      "Progress: 69.5% ... Training loss: 0.069 ... Validation loss: 0.152iteration: 6951\n",
      "train_loss: 0.06958816664848469\n",
      "val_loss: 0.15227585011339267\n",
      "Progress: 69.5% ... Training loss: 0.062 ... Validation loss: 0.166iteration: 6952\n",
      "train_loss: 0.062141617066318655\n",
      "val_loss: 0.16672701904313428\n",
      "Progress: 69.5% ... Training loss: 0.065 ... Validation loss: 0.193iteration: 6953\n",
      "train_loss: 0.06555783657567389\n",
      "val_loss: 0.19348912569004614\n",
      "Progress: 69.5% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 6954\n",
      "train_loss: 0.06278617631835377\n",
      "val_loss: 0.1654720169543197\n",
      "Progress: 69.5% ... Training loss: 0.062 ... Validation loss: 0.181iteration: 6955\n",
      "train_loss: 0.06247777216469331\n",
      "val_loss: 0.18165697539554126\n",
      "Progress: 69.6% ... Training loss: 0.064 ... Validation loss: 0.155iteration: 6956\n",
      "train_loss: 0.06460444107570201\n",
      "val_loss: 0.1550936002599536\n",
      "Progress: 69.6% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 6957\n",
      "train_loss: 0.0618435950564166\n",
      "val_loss: 0.17184872001648605\n",
      "Progress: 69.6% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 6958\n",
      "train_loss: 0.0613906450589346\n",
      "val_loss: 0.16344403593753598\n",
      "Progress: 69.6% ... Training loss: 0.062 ... Validation loss: 0.181iteration: 6959\n",
      "train_loss: 0.06234594756824911\n",
      "val_loss: 0.1812668877697929\n",
      "Progress: 69.6% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 6960\n",
      "train_loss: 0.06148881122141242\n",
      "val_loss: 0.17020353548218775\n",
      "Progress: 69.6% ... Training loss: 0.067 ... Validation loss: 0.207iteration: 6961\n",
      "train_loss: 0.06700856969954869\n",
      "val_loss: 0.20743437796420156\n",
      "Progress: 69.6% ... Training loss: 0.083 ... Validation loss: 0.152iteration: 6962\n",
      "train_loss: 0.08385115234498698\n",
      "val_loss: 0.15251356597256896\n",
      "Progress: 69.6% ... Training loss: 0.080 ... Validation loss: 0.226iteration: 6963\n",
      "train_loss: 0.08034703177035976\n",
      "val_loss: 0.22621340764725953\n",
      "Progress: 69.6% ... Training loss: 0.072 ... Validation loss: 0.148iteration: 6964\n",
      "train_loss: 0.07227335047986676\n",
      "val_loss: 0.14839902670478375\n",
      "Progress: 69.7% ... Training loss: 0.063 ... Validation loss: 0.185iteration: 6965\n",
      "train_loss: 0.06396096184397353\n",
      "val_loss: 0.18556552354933714\n",
      "Progress: 69.7% ... Training loss: 0.066 ... Validation loss: 0.154iteration: 6966\n",
      "train_loss: 0.06607208403962807\n",
      "val_loss: 0.1549646332620591\n",
      "Progress: 69.7% ... Training loss: 0.067 ... Validation loss: 0.201iteration: 6967\n",
      "train_loss: 0.06750831437400634\n",
      "val_loss: 0.20168975728753433\n",
      "Progress: 69.7% ... Training loss: 0.063 ... Validation loss: 0.153iteration: 6968\n",
      "train_loss: 0.06393235258865004\n",
      "val_loss: 0.15363557802740713\n",
      "Progress: 69.7% ... Training loss: 0.069 ... Validation loss: 0.216iteration: 6969\n",
      "train_loss: 0.06993814570293914\n",
      "val_loss: 0.216058625978614\n",
      "Progress: 69.7% ... Training loss: 0.071 ... Validation loss: 0.152iteration: 6970\n",
      "train_loss: 0.0713743472045858\n",
      "val_loss: 0.15252611770030958\n",
      "Progress: 69.7% ... Training loss: 0.069 ... Validation loss: 0.208iteration: 6971\n",
      "train_loss: 0.06948258751748279\n",
      "val_loss: 0.20800559977206406\n",
      "Progress: 69.7% ... Training loss: 0.066 ... Validation loss: 0.149iteration: 6972\n",
      "train_loss: 0.06681582176112266\n",
      "val_loss: 0.1496358095053453\n",
      "Progress: 69.7% ... Training loss: 0.075 ... Validation loss: 0.206iteration: 6973\n",
      "train_loss: 0.07590459996184264\n",
      "val_loss: 0.2067865614362438\n",
      "Progress: 69.7% ... Training loss: 0.067 ... Validation loss: 0.152iteration: 6974\n",
      "train_loss: 0.06720433850131478\n",
      "val_loss: 0.15261489748469953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 69.8% ... Training loss: 0.067 ... Validation loss: 0.192iteration: 6975\n",
      "train_loss: 0.06700098470761114\n",
      "val_loss: 0.1920386300778184\n",
      "Progress: 69.8% ... Training loss: 0.069 ... Validation loss: 0.146iteration: 6976\n",
      "train_loss: 0.06978233309147032\n",
      "val_loss: 0.1461392196405507\n",
      "Progress: 69.8% ... Training loss: 0.079 ... Validation loss: 0.221iteration: 6977\n",
      "train_loss: 0.07990863342335253\n",
      "val_loss: 0.2210043467148047\n",
      "Progress: 69.8% ... Training loss: 0.075 ... Validation loss: 0.149iteration: 6978\n",
      "train_loss: 0.07575624259252066\n",
      "val_loss: 0.14978225569708045\n",
      "Progress: 69.8% ... Training loss: 0.069 ... Validation loss: 0.205iteration: 6979\n",
      "train_loss: 0.06914001691424515\n",
      "val_loss: 0.2059008311147932\n",
      "Progress: 69.8% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 6980\n",
      "train_loss: 0.06572416962182416\n",
      "val_loss: 0.15232183424729098\n",
      "Progress: 69.8% ... Training loss: 0.062 ... Validation loss: 0.179iteration: 6981\n",
      "train_loss: 0.06229992418490368\n",
      "val_loss: 0.179940363953387\n",
      "Progress: 69.8% ... Training loss: 0.061 ... Validation loss: 0.155iteration: 6982\n",
      "train_loss: 0.0610388601223466\n",
      "val_loss: 0.1556047504049838\n",
      "Progress: 69.8% ... Training loss: 0.061 ... Validation loss: 0.164iteration: 6983\n",
      "train_loss: 0.0616401603936468\n",
      "val_loss: 0.16497704241384017\n",
      "Progress: 69.8% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 6984\n",
      "train_loss: 0.06123560432024585\n",
      "val_loss: 0.17153955185142558\n",
      "Progress: 69.8% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 6985\n",
      "train_loss: 0.06061645525521729\n",
      "val_loss: 0.1672421433493012\n",
      "Progress: 69.9% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 6986\n",
      "train_loss: 0.06195193636788333\n",
      "val_loss: 0.16362145055156466\n",
      "Progress: 69.9% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 6987\n",
      "train_loss: 0.06162800721498864\n",
      "val_loss: 0.16299656260487258\n",
      "Progress: 69.9% ... Training loss: 0.062 ... Validation loss: 0.177iteration: 6988\n",
      "train_loss: 0.06251688365227066\n",
      "val_loss: 0.17707845866033434\n",
      "Progress: 69.9% ... Training loss: 0.061 ... Validation loss: 0.154iteration: 6989\n",
      "train_loss: 0.06138451970913872\n",
      "val_loss: 0.1544378072706627\n",
      "Progress: 69.9% ... Training loss: 0.062 ... Validation loss: 0.169iteration: 6990\n",
      "train_loss: 0.06253667110007542\n",
      "val_loss: 0.16919200172393167\n",
      "Progress: 69.9% ... Training loss: 0.068 ... Validation loss: 0.152iteration: 6991\n",
      "train_loss: 0.06846126690345633\n",
      "val_loss: 0.1523721004046896\n",
      "Progress: 69.9% ... Training loss: 0.070 ... Validation loss: 0.196iteration: 6992\n",
      "train_loss: 0.07075974140224194\n",
      "val_loss: 0.1961318929690387\n",
      "Progress: 69.9% ... Training loss: 0.063 ... Validation loss: 0.158iteration: 6993\n",
      "train_loss: 0.06306743811162216\n",
      "val_loss: 0.15817364990423258\n",
      "Progress: 69.9% ... Training loss: 0.078 ... Validation loss: 0.191iteration: 6994\n",
      "train_loss: 0.07873045706723575\n",
      "val_loss: 0.19164254957825302\n",
      "Progress: 70.0% ... Training loss: 0.074 ... Validation loss: 0.146iteration: 6995\n",
      "train_loss: 0.07472778248125285\n",
      "val_loss: 0.14683120157780957\n",
      "Progress: 70.0% ... Training loss: 0.076 ... Validation loss: 0.211iteration: 6996\n",
      "train_loss: 0.0768036255455062\n",
      "val_loss: 0.21158913366062504\n",
      "Progress: 70.0% ... Training loss: 0.063 ... Validation loss: 0.152iteration: 6997\n",
      "train_loss: 0.06330000207170697\n",
      "val_loss: 0.15250520818382426\n",
      "Progress: 70.0% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 6998\n",
      "train_loss: 0.06100252804097336\n",
      "val_loss: 0.15933390139526643\n",
      "Progress: 70.0% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 6999\n",
      "train_loss: 0.06150873839405084\n",
      "val_loss: 0.16831197892930966\n",
      "Progress: 70.0% ... Training loss: 0.061 ... Validation loss: 0.161iteration: 7000\n",
      "train_loss: 0.06144274550580698\n",
      "val_loss: 0.1616375933751217\n",
      "Progress: 70.0% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7001\n",
      "train_loss: 0.06051225615167391\n",
      "val_loss: 0.16677403633872592\n",
      "Progress: 70.0% ... Training loss: 0.065 ... Validation loss: 0.180iteration: 7002\n",
      "train_loss: 0.06520227156932687\n",
      "val_loss: 0.180702037478603\n",
      "Progress: 70.0% ... Training loss: 0.066 ... Validation loss: 0.152iteration: 7003\n",
      "train_loss: 0.06642164046348051\n",
      "val_loss: 0.15226642052996597\n",
      "Progress: 70.0% ... Training loss: 0.067 ... Validation loss: 0.189iteration: 7004\n",
      "train_loss: 0.06724989343204764\n",
      "val_loss: 0.18988673508990703\n",
      "Progress: 70.0% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7005\n",
      "train_loss: 0.06120089655770075\n",
      "val_loss: 0.16076767540014508\n",
      "Progress: 70.1% ... Training loss: 0.061 ... Validation loss: 0.178iteration: 7006\n",
      "train_loss: 0.061192019582755365\n",
      "val_loss: 0.1786411178638573\n",
      "Progress: 70.1% ... Training loss: 0.063 ... Validation loss: 0.179iteration: 7007\n",
      "train_loss: 0.06391381613749177\n",
      "val_loss: 0.17982389581275973\n",
      "Progress: 70.1% ... Training loss: 0.062 ... Validation loss: 0.188iteration: 7008\n",
      "train_loss: 0.0620619250742087\n",
      "val_loss: 0.1888830226387703\n",
      "Progress: 70.1% ... Training loss: 0.070 ... Validation loss: 0.152iteration: 7009\n",
      "train_loss: 0.0705885456221894\n",
      "val_loss: 0.15215967535019087\n",
      "Progress: 70.1% ... Training loss: 0.093 ... Validation loss: 0.241iteration: 7010\n",
      "train_loss: 0.09398202594798967\n",
      "val_loss: 0.24131260119664696\n",
      "Progress: 70.1% ... Training loss: 0.078 ... Validation loss: 0.146iteration: 7011\n",
      "train_loss: 0.07876749008010762\n",
      "val_loss: 0.14633263826962645\n",
      "Progress: 70.1% ... Training loss: 0.070 ... Validation loss: 0.210iteration: 7012\n",
      "train_loss: 0.07056853631376492\n",
      "val_loss: 0.21083478143962003\n",
      "Progress: 70.1% ... Training loss: 0.069 ... Validation loss: 0.153iteration: 7013\n",
      "train_loss: 0.06922804089505198\n",
      "val_loss: 0.15327643712217948\n",
      "Progress: 70.1% ... Training loss: 0.067 ... Validation loss: 0.204iteration: 7014\n",
      "train_loss: 0.06792546810241228\n",
      "val_loss: 0.2046037156368684\n",
      "Progress: 70.2% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7015\n",
      "train_loss: 0.06149842139252079\n",
      "val_loss: 0.16078715546690733\n",
      "Progress: 70.2% ... Training loss: 0.064 ... Validation loss: 0.158iteration: 7016\n",
      "train_loss: 0.06498631670618844\n",
      "val_loss: 0.1589028643568253\n",
      "Progress: 70.2% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7017\n",
      "train_loss: 0.06109700397679099\n",
      "val_loss: 0.16093903684007047\n",
      "Progress: 70.2% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 7018\n",
      "train_loss: 0.06120045836076035\n",
      "val_loss: 0.1575239585153785\n",
      "Progress: 70.2% ... Training loss: 0.062 ... Validation loss: 0.179iteration: 7019\n",
      "train_loss: 0.062751444006203\n",
      "val_loss: 0.1791390514641041\n",
      "Progress: 70.2% ... Training loss: 0.066 ... Validation loss: 0.148iteration: 7020\n",
      "train_loss: 0.06619128419263934\n",
      "val_loss: 0.14864988583847544\n",
      "Progress: 70.2% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 7021\n",
      "train_loss: 0.06331474352594224\n",
      "val_loss: 0.17835345655738785\n",
      "Progress: 70.2% ... Training loss: 0.061 ... Validation loss: 0.155iteration: 7022\n",
      "train_loss: 0.061461460332786384\n",
      "val_loss: 0.15588006701133642\n",
      "Progress: 70.2% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 7023\n",
      "train_loss: 0.06163284442228368\n",
      "val_loss: 0.16835430484037234\n",
      "Progress: 70.2% ... Training loss: 0.062 ... Validation loss: 0.153iteration: 7024\n",
      "train_loss: 0.0626081016250375\n",
      "val_loss: 0.15316641406363468\n",
      "Progress: 70.2% ... Training loss: 0.062 ... Validation loss: 0.169iteration: 7025\n",
      "train_loss: 0.0623642614251941\n",
      "val_loss: 0.16916493734523727\n",
      "Progress: 70.3% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 7026\n",
      "train_loss: 0.06425217741929694\n",
      "val_loss: 0.15603490144013257\n",
      "Progress: 70.3% ... Training loss: 0.064 ... Validation loss: 0.154iteration: 7027\n",
      "train_loss: 0.06472688881998347\n",
      "val_loss: 0.1540438208442643\n",
      "Progress: 70.3% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 7028\n",
      "train_loss: 0.06419545894521951\n",
      "val_loss: 0.17345108790787112\n",
      "Progress: 70.3% ... Training loss: 0.063 ... Validation loss: 0.149iteration: 7029\n",
      "train_loss: 0.06316793978995658\n",
      "val_loss: 0.14944705385030504\n",
      "Progress: 70.3% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 7030\n",
      "train_loss: 0.061601151446460244\n",
      "val_loss: 0.16321687912281124\n",
      "Progress: 70.3% ... Training loss: 0.080 ... Validation loss: 0.183iteration: 7031\n",
      "train_loss: 0.08099919770179516\n",
      "val_loss: 0.1835161431360551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 70.3% ... Training loss: 0.065 ... Validation loss: 0.151iteration: 7032\n",
      "train_loss: 0.06526006663000689\n",
      "val_loss: 0.15191980950392592\n",
      "Progress: 70.3% ... Training loss: 0.068 ... Validation loss: 0.186iteration: 7033\n",
      "train_loss: 0.06897683906689797\n",
      "val_loss: 0.18699308503690465\n",
      "Progress: 70.3% ... Training loss: 0.068 ... Validation loss: 0.148iteration: 7034\n",
      "train_loss: 0.06840193408516133\n",
      "val_loss: 0.14882327209454418\n",
      "Progress: 70.3% ... Training loss: 0.080 ... Validation loss: 0.213iteration: 7035\n",
      "train_loss: 0.08023520549235362\n",
      "val_loss: 0.21351930276637157\n",
      "Progress: 70.4% ... Training loss: 0.105 ... Validation loss: 0.154iteration: 7036\n",
      "train_loss: 0.10540676135450026\n",
      "val_loss: 0.15448936430971483\n",
      "Progress: 70.4% ... Training loss: 0.117 ... Validation loss: 0.287iteration: 7037\n",
      "train_loss: 0.11755154899549976\n",
      "val_loss: 0.2877554984751025\n",
      "Progress: 70.4% ... Training loss: 0.122 ... Validation loss: 0.166iteration: 7038\n",
      "train_loss: 0.1223729289468002\n",
      "val_loss: 0.16640204742037998\n",
      "Progress: 70.4% ... Training loss: 0.141 ... Validation loss: 0.309iteration: 7039\n",
      "train_loss: 0.14146407851682444\n",
      "val_loss: 0.3090599712033318\n",
      "Progress: 70.4% ... Training loss: 0.180 ... Validation loss: 0.184iteration: 7040\n",
      "train_loss: 0.18043330393154713\n",
      "val_loss: 0.18472092935126894\n",
      "Progress: 70.4% ... Training loss: 0.151 ... Validation loss: 0.341iteration: 7041\n",
      "train_loss: 0.15175763758635138\n",
      "val_loss: 0.341383047720725\n",
      "Progress: 70.4% ... Training loss: 0.134 ... Validation loss: 0.167iteration: 7042\n",
      "train_loss: 0.1340708775309786\n",
      "val_loss: 0.167461993176099\n",
      "Progress: 70.4% ... Training loss: 0.118 ... Validation loss: 0.283iteration: 7043\n",
      "train_loss: 0.11816443886714156\n",
      "val_loss: 0.28341563507496315\n",
      "Progress: 70.4% ... Training loss: 0.093 ... Validation loss: 0.156iteration: 7044\n",
      "train_loss: 0.09311591894854741\n",
      "val_loss: 0.15615455231477893\n",
      "Progress: 70.5% ... Training loss: 0.087 ... Validation loss: 0.209iteration: 7045\n",
      "train_loss: 0.0878105657073873\n",
      "val_loss: 0.20958162515136164\n",
      "Progress: 70.5% ... Training loss: 0.067 ... Validation loss: 0.151iteration: 7046\n",
      "train_loss: 0.06788573449845287\n",
      "val_loss: 0.15176914487278761\n",
      "Progress: 70.5% ... Training loss: 0.064 ... Validation loss: 0.189iteration: 7047\n",
      "train_loss: 0.06401980666397358\n",
      "val_loss: 0.18936494192874362\n",
      "Progress: 70.5% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 7048\n",
      "train_loss: 0.06348330399383255\n",
      "val_loss: 0.1780961109623795\n",
      "Progress: 70.5% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 7049\n",
      "train_loss: 0.061102757982618834\n",
      "val_loss: 0.1623015219091316\n",
      "Progress: 70.5% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 7050\n",
      "train_loss: 0.06393178478341732\n",
      "val_loss: 0.17883194490617432\n",
      "Progress: 70.5% ... Training loss: 0.064 ... Validation loss: 0.154iteration: 7051\n",
      "train_loss: 0.06412646475223083\n",
      "val_loss: 0.15425566837251237\n",
      "Progress: 70.5% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 7052\n",
      "train_loss: 0.06425994847426944\n",
      "val_loss: 0.17009069620277548\n",
      "Progress: 70.5% ... Training loss: 0.061 ... Validation loss: 0.158iteration: 7053\n",
      "train_loss: 0.0612466492135985\n",
      "val_loss: 0.15820820233728616\n",
      "Progress: 70.5% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7054\n",
      "train_loss: 0.06195188164841925\n",
      "val_loss: 0.16033814063931026\n",
      "Progress: 70.5% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 7055\n",
      "train_loss: 0.06129777969348538\n",
      "val_loss: 0.16320639113346858\n",
      "Progress: 70.6% ... Training loss: 0.064 ... Validation loss: 0.189iteration: 7056\n",
      "train_loss: 0.06400889290566213\n",
      "val_loss: 0.18953178065612994\n",
      "Progress: 70.6% ... Training loss: 0.066 ... Validation loss: 0.151iteration: 7057\n",
      "train_loss: 0.0667547398839223\n",
      "val_loss: 0.1518718710034605\n",
      "Progress: 70.6% ... Training loss: 0.063 ... Validation loss: 0.172iteration: 7058\n",
      "train_loss: 0.06379177352384285\n",
      "val_loss: 0.17218165870185484\n",
      "Progress: 70.6% ... Training loss: 0.063 ... Validation loss: 0.149iteration: 7059\n",
      "train_loss: 0.0637383112484903\n",
      "val_loss: 0.14925967956741076\n",
      "Progress: 70.6% ... Training loss: 0.067 ... Validation loss: 0.179iteration: 7060\n",
      "train_loss: 0.06798600320691296\n",
      "val_loss: 0.17942020173507991\n",
      "Progress: 70.6% ... Training loss: 0.067 ... Validation loss: 0.145iteration: 7061\n",
      "train_loss: 0.06748114829849174\n",
      "val_loss: 0.1455690054949682\n",
      "Progress: 70.6% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 7062\n",
      "train_loss: 0.06304724084075186\n",
      "val_loss: 0.16692211309069196\n",
      "Progress: 70.6% ... Training loss: 0.077 ... Validation loss: 0.147iteration: 7063\n",
      "train_loss: 0.07749069361398543\n",
      "val_loss: 0.14748856523569825\n",
      "Progress: 70.6% ... Training loss: 0.070 ... Validation loss: 0.188iteration: 7064\n",
      "train_loss: 0.07045211148968639\n",
      "val_loss: 0.1886618780135991\n",
      "Progress: 70.7% ... Training loss: 0.066 ... Validation loss: 0.153iteration: 7065\n",
      "train_loss: 0.0664340382463392\n",
      "val_loss: 0.15301218201843889\n",
      "Progress: 70.7% ... Training loss: 0.061 ... Validation loss: 0.172iteration: 7066\n",
      "train_loss: 0.06182223425921986\n",
      "val_loss: 0.1726426559369526\n",
      "Progress: 70.7% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 7067\n",
      "train_loss: 0.06131880166622017\n",
      "val_loss: 0.16819882374504133\n",
      "Progress: 70.7% ... Training loss: 0.066 ... Validation loss: 0.187iteration: 7068\n",
      "train_loss: 0.06666496607179026\n",
      "val_loss: 0.18734740948207582\n",
      "Progress: 70.7% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 7069\n",
      "train_loss: 0.07060814154415994\n",
      "val_loss: 0.15015314865051282\n",
      "Progress: 70.7% ... Training loss: 0.070 ... Validation loss: 0.179iteration: 7070\n",
      "train_loss: 0.07080838648539271\n",
      "val_loss: 0.17955115932792873\n",
      "Progress: 70.7% ... Training loss: 0.065 ... Validation loss: 0.151iteration: 7071\n",
      "train_loss: 0.06513211946473538\n",
      "val_loss: 0.15117332166809705\n",
      "Progress: 70.7% ... Training loss: 0.065 ... Validation loss: 0.183iteration: 7072\n",
      "train_loss: 0.06571032555291169\n",
      "val_loss: 0.18306101217009726\n",
      "Progress: 70.7% ... Training loss: 0.068 ... Validation loss: 0.152iteration: 7073\n",
      "train_loss: 0.0687958698075213\n",
      "val_loss: 0.15226240350908476\n",
      "Progress: 70.7% ... Training loss: 0.067 ... Validation loss: 0.201iteration: 7074\n",
      "train_loss: 0.06776366491484133\n",
      "val_loss: 0.20191315785263106\n",
      "Progress: 70.8% ... Training loss: 0.079 ... Validation loss: 0.146iteration: 7075\n",
      "train_loss: 0.07942866080791736\n",
      "val_loss: 0.14632858393961537\n",
      "Progress: 70.8% ... Training loss: 0.067 ... Validation loss: 0.199iteration: 7076\n",
      "train_loss: 0.0671799371569466\n",
      "val_loss: 0.19987564881063102\n",
      "Progress: 70.8% ... Training loss: 0.063 ... Validation loss: 0.167iteration: 7077\n",
      "train_loss: 0.06347908463140267\n",
      "val_loss: 0.16720894408041304\n",
      "Progress: 70.8% ... Training loss: 0.064 ... Validation loss: 0.184iteration: 7078\n",
      "train_loss: 0.06409565159236968\n",
      "val_loss: 0.1847565043544709\n",
      "Progress: 70.8% ... Training loss: 0.066 ... Validation loss: 0.150iteration: 7079\n",
      "train_loss: 0.06621747725235258\n",
      "val_loss: 0.15039757743012305\n",
      "Progress: 70.8% ... Training loss: 0.066 ... Validation loss: 0.201iteration: 7080\n",
      "train_loss: 0.06691883618740972\n",
      "val_loss: 0.20105376774806322\n",
      "Progress: 70.8% ... Training loss: 0.070 ... Validation loss: 0.162iteration: 7081\n",
      "train_loss: 0.0703536202772248\n",
      "val_loss: 0.1623833697267921\n",
      "Progress: 70.8% ... Training loss: 0.078 ... Validation loss: 0.231iteration: 7082\n",
      "train_loss: 0.07846574255805108\n",
      "val_loss: 0.23169849610492108\n",
      "Progress: 70.8% ... Training loss: 0.078 ... Validation loss: 0.148iteration: 7083\n",
      "train_loss: 0.07828164632099723\n",
      "val_loss: 0.14838735066874303\n",
      "Progress: 70.8% ... Training loss: 0.085 ... Validation loss: 0.231iteration: 7084\n",
      "train_loss: 0.08561175762715127\n",
      "val_loss: 0.231858253085216\n",
      "Progress: 70.8% ... Training loss: 0.089 ... Validation loss: 0.153iteration: 7085\n",
      "train_loss: 0.08994921719827048\n",
      "val_loss: 0.15352560198236034\n",
      "Progress: 70.9% ... Training loss: 0.079 ... Validation loss: 0.238iteration: 7086\n",
      "train_loss: 0.07923251499164423\n",
      "val_loss: 0.23867398661363556\n",
      "Progress: 70.9% ... Training loss: 0.075 ... Validation loss: 0.151iteration: 7087\n",
      "train_loss: 0.07570627494244798\n",
      "val_loss: 0.1513524902018525\n",
      "Progress: 70.9% ... Training loss: 0.065 ... Validation loss: 0.183iteration: 7088\n",
      "train_loss: 0.06510251368962508\n",
      "val_loss: 0.18378048338288358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 70.9% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 7089\n",
      "train_loss: 0.06103580900681723\n",
      "val_loss: 0.1713493039979884\n",
      "Progress: 70.9% ... Training loss: 0.060 ... Validation loss: 0.176iteration: 7090\n",
      "train_loss: 0.06079301309277607\n",
      "val_loss: 0.1764851835564171\n",
      "Progress: 70.9% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 7091\n",
      "train_loss: 0.06342073693087978\n",
      "val_loss: 0.15743394572323477\n",
      "Progress: 70.9% ... Training loss: 0.062 ... Validation loss: 0.187iteration: 7092\n",
      "train_loss: 0.06270038882299434\n",
      "val_loss: 0.18705051346544999\n",
      "Progress: 70.9% ... Training loss: 0.061 ... Validation loss: 0.182iteration: 7093\n",
      "train_loss: 0.06184988146683018\n",
      "val_loss: 0.18232633557412153\n",
      "Progress: 70.9% ... Training loss: 0.064 ... Validation loss: 0.168iteration: 7094\n",
      "train_loss: 0.0641685591803268\n",
      "val_loss: 0.1688803245649383\n",
      "Progress: 71.0% ... Training loss: 0.062 ... Validation loss: 0.199iteration: 7095\n",
      "train_loss: 0.06268207179553936\n",
      "val_loss: 0.19905841418993017\n",
      "Progress: 71.0% ... Training loss: 0.067 ... Validation loss: 0.193iteration: 7096\n",
      "train_loss: 0.06718679603995263\n",
      "val_loss: 0.19320547819905742\n",
      "Progress: 71.0% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 7097\n",
      "train_loss: 0.06392647971378607\n",
      "val_loss: 0.15781602121465707\n",
      "Progress: 71.0% ... Training loss: 0.060 ... Validation loss: 0.170iteration: 7098\n",
      "train_loss: 0.060834170589230005\n",
      "val_loss: 0.1709395539769421\n",
      "Progress: 71.0% ... Training loss: 0.061 ... Validation loss: 0.186iteration: 7099\n",
      "train_loss: 0.061123216287919564\n",
      "val_loss: 0.18675155847411046\n",
      "Progress: 71.0% ... Training loss: 0.063 ... Validation loss: 0.185iteration: 7100\n",
      "train_loss: 0.06355902720998677\n",
      "val_loss: 0.18569173656441648\n",
      "Progress: 71.0% ... Training loss: 0.064 ... Validation loss: 0.161iteration: 7101\n",
      "train_loss: 0.06409392469977314\n",
      "val_loss: 0.16105624924444284\n",
      "Progress: 71.0% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 7102\n",
      "train_loss: 0.06152300099952906\n",
      "val_loss: 0.16261959276334342\n",
      "Progress: 71.0% ... Training loss: 0.063 ... Validation loss: 0.186iteration: 7103\n",
      "train_loss: 0.06305072965031105\n",
      "val_loss: 0.18619052812357123\n",
      "Progress: 71.0% ... Training loss: 0.060 ... Validation loss: 0.168iteration: 7104\n",
      "train_loss: 0.0609191149410717\n",
      "val_loss: 0.1681210656995953\n",
      "Progress: 71.0% ... Training loss: 0.063 ... Validation loss: 0.155iteration: 7105\n",
      "train_loss: 0.06315250262295702\n",
      "val_loss: 0.15555260728841214\n",
      "Progress: 71.1% ... Training loss: 0.067 ... Validation loss: 0.183iteration: 7106\n",
      "train_loss: 0.06712644443969056\n",
      "val_loss: 0.18365571500663572\n",
      "Progress: 71.1% ... Training loss: 0.063 ... Validation loss: 0.165iteration: 7107\n",
      "train_loss: 0.06317608354289259\n",
      "val_loss: 0.16520342850367672\n",
      "Progress: 71.1% ... Training loss: 0.076 ... Validation loss: 0.221iteration: 7108\n",
      "train_loss: 0.07600560288549421\n",
      "val_loss: 0.2214501602478278\n",
      "Progress: 71.1% ... Training loss: 0.063 ... Validation loss: 0.162iteration: 7109\n",
      "train_loss: 0.06315448295480644\n",
      "val_loss: 0.16265639932681297\n",
      "Progress: 71.1% ... Training loss: 0.062 ... Validation loss: 0.168iteration: 7110\n",
      "train_loss: 0.06257526203610755\n",
      "val_loss: 0.16879528068936958\n",
      "Progress: 71.1% ... Training loss: 0.061 ... Validation loss: 0.189iteration: 7111\n",
      "train_loss: 0.06111822839968808\n",
      "val_loss: 0.18978680564129177\n",
      "Progress: 71.1% ... Training loss: 0.062 ... Validation loss: 0.161iteration: 7112\n",
      "train_loss: 0.06264246260833628\n",
      "val_loss: 0.16194876900567637\n",
      "Progress: 71.1% ... Training loss: 0.063 ... Validation loss: 0.185iteration: 7113\n",
      "train_loss: 0.06365609865626704\n",
      "val_loss: 0.18581134951732278\n",
      "Progress: 71.1% ... Training loss: 0.062 ... Validation loss: 0.170iteration: 7114\n",
      "train_loss: 0.062122117758927835\n",
      "val_loss: 0.17010490695211763\n",
      "Progress: 71.2% ... Training loss: 0.061 ... Validation loss: 0.184iteration: 7115\n",
      "train_loss: 0.06156171700241684\n",
      "val_loss: 0.184288371429052\n",
      "Progress: 71.2% ... Training loss: 0.060 ... Validation loss: 0.183iteration: 7116\n",
      "train_loss: 0.06058531156177034\n",
      "val_loss: 0.1836617179527523\n",
      "Progress: 71.2% ... Training loss: 0.063 ... Validation loss: 0.169iteration: 7117\n",
      "train_loss: 0.06335643197607167\n",
      "val_loss: 0.1692352993439002\n",
      "Progress: 71.2% ... Training loss: 0.062 ... Validation loss: 0.201iteration: 7118\n",
      "train_loss: 0.06293221102706735\n",
      "val_loss: 0.20143502217499104\n",
      "Progress: 71.2% ... Training loss: 0.068 ... Validation loss: 0.161iteration: 7119\n",
      "train_loss: 0.0686901153979266\n",
      "val_loss: 0.16156237708219584\n",
      "Progress: 71.2% ... Training loss: 0.063 ... Validation loss: 0.209iteration: 7120\n",
      "train_loss: 0.06336644994514218\n",
      "val_loss: 0.20922339829678988\n",
      "Progress: 71.2% ... Training loss: 0.060 ... Validation loss: 0.178iteration: 7121\n",
      "train_loss: 0.060205427883467985\n",
      "val_loss: 0.17833287297621894\n",
      "Progress: 71.2% ... Training loss: 0.060 ... Validation loss: 0.178iteration: 7122\n",
      "train_loss: 0.06010065290249565\n",
      "val_loss: 0.17885358835475576\n",
      "Progress: 71.2% ... Training loss: 0.070 ... Validation loss: 0.208iteration: 7123\n",
      "train_loss: 0.07023435691425661\n",
      "val_loss: 0.20843709164055596\n",
      "Progress: 71.2% ... Training loss: 0.078 ... Validation loss: 0.149iteration: 7124\n",
      "train_loss: 0.07810864978774813\n",
      "val_loss: 0.1494062084443269\n",
      "Progress: 71.2% ... Training loss: 0.062 ... Validation loss: 0.208iteration: 7125\n",
      "train_loss: 0.06296536957763177\n",
      "val_loss: 0.20883867704303302\n",
      "Progress: 71.3% ... Training loss: 0.061 ... Validation loss: 0.167iteration: 7126\n",
      "train_loss: 0.06106088337844874\n",
      "val_loss: 0.1673688278570113\n",
      "Progress: 71.3% ... Training loss: 0.060 ... Validation loss: 0.178iteration: 7127\n",
      "train_loss: 0.060847792737185026\n",
      "val_loss: 0.17835526121925258\n",
      "Progress: 71.3% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 7128\n",
      "train_loss: 0.06110986274086814\n",
      "val_loss: 0.17183052650669584\n",
      "Progress: 71.3% ... Training loss: 0.060 ... Validation loss: 0.179iteration: 7129\n",
      "train_loss: 0.06056874269067845\n",
      "val_loss: 0.1793787582234483\n",
      "Progress: 71.3% ... Training loss: 0.063 ... Validation loss: 0.164iteration: 7130\n",
      "train_loss: 0.06303587025993573\n",
      "val_loss: 0.16436084712209\n",
      "Progress: 71.3% ... Training loss: 0.061 ... Validation loss: 0.187iteration: 7131\n",
      "train_loss: 0.06121903581413316\n",
      "val_loss: 0.18727612502688776\n",
      "Progress: 71.3% ... Training loss: 0.071 ... Validation loss: 0.148iteration: 7132\n",
      "train_loss: 0.07187528819168595\n",
      "val_loss: 0.14883786028052512\n",
      "Progress: 71.3% ... Training loss: 0.067 ... Validation loss: 0.196iteration: 7133\n",
      "train_loss: 0.06783708405776825\n",
      "val_loss: 0.19607961812606006\n",
      "Progress: 71.3% ... Training loss: 0.066 ... Validation loss: 0.148iteration: 7134\n",
      "train_loss: 0.06628306495652497\n",
      "val_loss: 0.1482367244687297\n",
      "Progress: 71.3% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7135\n",
      "train_loss: 0.06064849419826541\n",
      "val_loss: 0.16660438555295773\n",
      "Progress: 71.4% ... Training loss: 0.062 ... Validation loss: 0.189iteration: 7136\n",
      "train_loss: 0.06274726430076122\n",
      "val_loss: 0.1895956509573213\n",
      "Progress: 71.4% ... Training loss: 0.064 ... Validation loss: 0.157iteration: 7137\n",
      "train_loss: 0.0649644738866031\n",
      "val_loss: 0.15775354179844722\n",
      "Progress: 71.4% ... Training loss: 0.060 ... Validation loss: 0.185iteration: 7138\n",
      "train_loss: 0.06090041662741187\n",
      "val_loss: 0.18526292557262936\n",
      "Progress: 71.4% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7139\n",
      "train_loss: 0.06035232734497268\n",
      "val_loss: 0.16698531061431965\n",
      "Progress: 71.4% ... Training loss: 0.062 ... Validation loss: 0.155iteration: 7140\n",
      "train_loss: 0.06274910620407376\n",
      "val_loss: 0.15542382566155308\n",
      "Progress: 71.4% ... Training loss: 0.071 ... Validation loss: 0.199iteration: 7141\n",
      "train_loss: 0.07176056677785281\n",
      "val_loss: 0.1995553651173077\n",
      "Progress: 71.4% ... Training loss: 0.065 ... Validation loss: 0.153iteration: 7142\n",
      "train_loss: 0.06543568064979208\n",
      "val_loss: 0.1530981230636088\n",
      "Progress: 71.4% ... Training loss: 0.071 ... Validation loss: 0.215iteration: 7143\n",
      "train_loss: 0.07150172257406645\n",
      "val_loss: 0.2152154014249144\n",
      "Progress: 71.4% ... Training loss: 0.081 ... Validation loss: 0.152iteration: 7144\n",
      "train_loss: 0.081581857586213\n",
      "val_loss: 0.15248772947437023\n",
      "Progress: 71.5% ... Training loss: 0.070 ... Validation loss: 0.214iteration: 7145\n",
      "train_loss: 0.07073610270003414\n",
      "val_loss: 0.21469970735935506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 71.5% ... Training loss: 0.080 ... Validation loss: 0.154iteration: 7146\n",
      "train_loss: 0.08018561879695506\n",
      "val_loss: 0.15422799116679925\n",
      "Progress: 71.5% ... Training loss: 0.085 ... Validation loss: 0.220iteration: 7147\n",
      "train_loss: 0.08560583194820073\n",
      "val_loss: 0.2208806416481293\n",
      "Progress: 71.5% ... Training loss: 0.101 ... Validation loss: 0.156iteration: 7148\n",
      "train_loss: 0.1016641391942209\n",
      "val_loss: 0.1564640626425007\n",
      "Progress: 71.5% ... Training loss: 0.099 ... Validation loss: 0.259iteration: 7149\n",
      "train_loss: 0.09947694598124603\n",
      "val_loss: 0.259714745259428\n",
      "Progress: 71.5% ... Training loss: 0.093 ... Validation loss: 0.154iteration: 7150\n",
      "train_loss: 0.09375409308952182\n",
      "val_loss: 0.15441193717816887\n",
      "Progress: 71.5% ... Training loss: 0.110 ... Validation loss: 0.268iteration: 7151\n",
      "train_loss: 0.11046507642796571\n",
      "val_loss: 0.2680862714375323\n",
      "Progress: 71.5% ... Training loss: 0.109 ... Validation loss: 0.149iteration: 7152\n",
      "train_loss: 0.10968494596562559\n",
      "val_loss: 0.1499631230029914\n",
      "Progress: 71.5% ... Training loss: 0.140 ... Validation loss: 0.284iteration: 7153\n",
      "train_loss: 0.14099846609508201\n",
      "val_loss: 0.2847140085386681\n",
      "Progress: 71.5% ... Training loss: 0.109 ... Validation loss: 0.150iteration: 7154\n",
      "train_loss: 0.10938329654703875\n",
      "val_loss: 0.15083802910832622\n",
      "Progress: 71.5% ... Training loss: 0.098 ... Validation loss: 0.214iteration: 7155\n",
      "train_loss: 0.09802941647314\n",
      "val_loss: 0.2145975137211215\n",
      "Progress: 71.6% ... Training loss: 0.067 ... Validation loss: 0.148iteration: 7156\n",
      "train_loss: 0.06746280816488491\n",
      "val_loss: 0.14831012485469522\n",
      "Progress: 71.6% ... Training loss: 0.068 ... Validation loss: 0.174iteration: 7157\n",
      "train_loss: 0.06837040286334425\n",
      "val_loss: 0.17449679621785727\n",
      "Progress: 71.6% ... Training loss: 0.081 ... Validation loss: 0.147iteration: 7158\n",
      "train_loss: 0.08101995214742695\n",
      "val_loss: 0.14749769538275687\n",
      "Progress: 71.6% ... Training loss: 0.064 ... Validation loss: 0.192iteration: 7159\n",
      "train_loss: 0.06454299808693316\n",
      "val_loss: 0.19252181098019683\n",
      "Progress: 71.6% ... Training loss: 0.064 ... Validation loss: 0.155iteration: 7160\n",
      "train_loss: 0.064693868603705\n",
      "val_loss: 0.15514017652675957\n",
      "Progress: 71.6% ... Training loss: 0.061 ... Validation loss: 0.175iteration: 7161\n",
      "train_loss: 0.06184209576811062\n",
      "val_loss: 0.17519102033805176\n",
      "Progress: 71.6% ... Training loss: 0.062 ... Validation loss: 0.158iteration: 7162\n",
      "train_loss: 0.06235952200566675\n",
      "val_loss: 0.15830537121932522\n",
      "Progress: 71.6% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 7163\n",
      "train_loss: 0.061526765260806096\n",
      "val_loss: 0.17068282127281525\n",
      "Progress: 71.6% ... Training loss: 0.062 ... Validation loss: 0.162iteration: 7164\n",
      "train_loss: 0.06206277158902767\n",
      "val_loss: 0.16287976869730011\n",
      "Progress: 71.7% ... Training loss: 0.061 ... Validation loss: 0.167iteration: 7165\n",
      "train_loss: 0.0611801223422291\n",
      "val_loss: 0.16764030969007365\n",
      "Progress: 71.7% ... Training loss: 0.062 ... Validation loss: 0.180iteration: 7166\n",
      "train_loss: 0.062106263938409595\n",
      "val_loss: 0.18092546568797166\n",
      "Progress: 71.7% ... Training loss: 0.062 ... Validation loss: 0.158iteration: 7167\n",
      "train_loss: 0.06260369848758209\n",
      "val_loss: 0.15856434996018468\n",
      "Progress: 71.7% ... Training loss: 0.065 ... Validation loss: 0.183iteration: 7168\n",
      "train_loss: 0.06546574150213166\n",
      "val_loss: 0.18324510387291812\n",
      "Progress: 71.7% ... Training loss: 0.068 ... Validation loss: 0.144iteration: 7169\n",
      "train_loss: 0.06887176230593788\n",
      "val_loss: 0.14473392482934921\n",
      "Progress: 71.7% ... Training loss: 0.087 ... Validation loss: 0.223iteration: 7170\n",
      "train_loss: 0.08785509748969547\n",
      "val_loss: 0.22356030375759392\n",
      "Progress: 71.7% ... Training loss: 0.095 ... Validation loss: 0.148iteration: 7171\n",
      "train_loss: 0.09590839790388352\n",
      "val_loss: 0.14808192819409965\n",
      "Progress: 71.7% ... Training loss: 0.089 ... Validation loss: 0.221iteration: 7172\n",
      "train_loss: 0.08996326217456674\n",
      "val_loss: 0.2213604865256877\n",
      "Progress: 71.7% ... Training loss: 0.110 ... Validation loss: 0.154iteration: 7173\n",
      "train_loss: 0.11006503522591142\n",
      "val_loss: 0.1541362747392695\n",
      "Progress: 71.7% ... Training loss: 0.109 ... Validation loss: 0.258iteration: 7174\n",
      "train_loss: 0.1099153432249141\n",
      "val_loss: 0.2586631993728974\n",
      "Progress: 71.8% ... Training loss: 0.081 ... Validation loss: 0.144iteration: 7175\n",
      "train_loss: 0.08137904007983612\n",
      "val_loss: 0.14491438778605983\n",
      "Progress: 71.8% ... Training loss: 0.082 ... Validation loss: 0.215iteration: 7176\n",
      "train_loss: 0.08295417250240358\n",
      "val_loss: 0.2159526614889133\n",
      "Progress: 71.8% ... Training loss: 0.095 ... Validation loss: 0.150iteration: 7177\n",
      "train_loss: 0.09568069083871422\n",
      "val_loss: 0.15062462047883085\n",
      "Progress: 71.8% ... Training loss: 0.077 ... Validation loss: 0.212iteration: 7178\n",
      "train_loss: 0.07781283596015393\n",
      "val_loss: 0.2126071509374527\n",
      "Progress: 71.8% ... Training loss: 0.060 ... Validation loss: 0.160iteration: 7179\n",
      "train_loss: 0.06061984516230792\n",
      "val_loss: 0.1607981547041022\n",
      "Progress: 71.8% ... Training loss: 0.063 ... Validation loss: 0.180iteration: 7180\n",
      "train_loss: 0.06397909635927783\n",
      "val_loss: 0.18031430277137767\n",
      "Progress: 71.8% ... Training loss: 0.065 ... Validation loss: 0.157iteration: 7181\n",
      "train_loss: 0.06584790321484714\n",
      "val_loss: 0.1571725560206499\n",
      "Progress: 71.8% ... Training loss: 0.065 ... Validation loss: 0.191iteration: 7182\n",
      "train_loss: 0.06514330734680518\n",
      "val_loss: 0.19193502910366825\n",
      "Progress: 71.8% ... Training loss: 0.070 ... Validation loss: 0.153iteration: 7183\n",
      "train_loss: 0.07013064450132339\n",
      "val_loss: 0.15343825426519342\n",
      "Progress: 71.8% ... Training loss: 0.063 ... Validation loss: 0.196iteration: 7184\n",
      "train_loss: 0.06391680113806252\n",
      "val_loss: 0.19626140425644667\n",
      "Progress: 71.8% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 7185\n",
      "train_loss: 0.06326682046954901\n",
      "val_loss: 0.1568241453015518\n",
      "Progress: 71.9% ... Training loss: 0.061 ... Validation loss: 0.179iteration: 7186\n",
      "train_loss: 0.061666883176924654\n",
      "val_loss: 0.17935350702789302\n",
      "Progress: 71.9% ... Training loss: 0.062 ... Validation loss: 0.154iteration: 7187\n",
      "train_loss: 0.06239805512030584\n",
      "val_loss: 0.15480409095094613\n",
      "Progress: 71.9% ... Training loss: 0.062 ... Validation loss: 0.175iteration: 7188\n",
      "train_loss: 0.062445057172932056\n",
      "val_loss: 0.17582110747432347\n",
      "Progress: 71.9% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 7189\n",
      "train_loss: 0.06314649132509768\n",
      "val_loss: 0.16633054213858395\n",
      "Progress: 71.9% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7190\n",
      "train_loss: 0.060439276065638925\n",
      "val_loss: 0.16480977448418105\n",
      "Progress: 71.9% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7191\n",
      "train_loss: 0.060815850026284396\n",
      "val_loss: 0.16238480047515094\n",
      "Progress: 71.9% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7192\n",
      "train_loss: 0.060305114238164335\n",
      "val_loss: 0.16654042066972458\n",
      "Progress: 71.9% ... Training loss: 0.062 ... Validation loss: 0.181iteration: 7193\n",
      "train_loss: 0.06263043537056123\n",
      "val_loss: 0.18116057058364765\n",
      "Progress: 71.9% ... Training loss: 0.066 ... Validation loss: 0.160iteration: 7194\n",
      "train_loss: 0.06616780410230305\n",
      "val_loss: 0.160143915729408\n",
      "Progress: 72.0% ... Training loss: 0.060 ... Validation loss: 0.175iteration: 7195\n",
      "train_loss: 0.060289249209609666\n",
      "val_loss: 0.1750358962626587\n",
      "Progress: 72.0% ... Training loss: 0.062 ... Validation loss: 0.160iteration: 7196\n",
      "train_loss: 0.06267549957030297\n",
      "val_loss: 0.1606489508214142\n",
      "Progress: 72.0% ... Training loss: 0.064 ... Validation loss: 0.194iteration: 7197\n",
      "train_loss: 0.06422876661613823\n",
      "val_loss: 0.19486919898017005\n",
      "Progress: 72.0% ... Training loss: 0.078 ... Validation loss: 0.161iteration: 7198\n",
      "train_loss: 0.07810332030802682\n",
      "val_loss: 0.1615116112108424\n",
      "Progress: 72.0% ... Training loss: 0.078 ... Validation loss: 0.228iteration: 7199\n",
      "train_loss: 0.07872246986748456\n",
      "val_loss: 0.2289372562405053\n",
      "Progress: 72.0% ... Training loss: 0.099 ... Validation loss: 0.156iteration: 7200\n",
      "train_loss: 0.09924213269868544\n",
      "val_loss: 0.15684369451705413\n",
      "Progress: 72.0% ... Training loss: 0.070 ... Validation loss: 0.188iteration: 7201\n",
      "train_loss: 0.07010761530655708\n",
      "val_loss: 0.18843825769353412\n",
      "Progress: 72.0% ... Training loss: 0.084 ... Validation loss: 0.156iteration: 7202\n",
      "train_loss: 0.08436821855667324\n",
      "val_loss: 0.15684035175482597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 72.0% ... Training loss: 0.085 ... Validation loss: 0.217iteration: 7203\n",
      "train_loss: 0.08530664179962628\n",
      "val_loss: 0.21779166587037588\n",
      "Progress: 72.0% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 7204\n",
      "train_loss: 0.06477020704011849\n",
      "val_loss: 0.15690070903139983\n",
      "Progress: 72.0% ... Training loss: 0.062 ... Validation loss: 0.191iteration: 7205\n",
      "train_loss: 0.06285096170423675\n",
      "val_loss: 0.19104630909502154\n",
      "Progress: 72.1% ... Training loss: 0.064 ... Validation loss: 0.167iteration: 7206\n",
      "train_loss: 0.06452830359153425\n",
      "val_loss: 0.16710002075381786\n",
      "Progress: 72.1% ... Training loss: 0.061 ... Validation loss: 0.191iteration: 7207\n",
      "train_loss: 0.06165602137149208\n",
      "val_loss: 0.19169654395868221\n",
      "Progress: 72.1% ... Training loss: 0.060 ... Validation loss: 0.178iteration: 7208\n",
      "train_loss: 0.06067889730457747\n",
      "val_loss: 0.1787450294930989\n",
      "Progress: 72.1% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 7209\n",
      "train_loss: 0.06016102574182149\n",
      "val_loss: 0.17348495061943994\n",
      "Progress: 72.1% ... Training loss: 0.060 ... Validation loss: 0.176iteration: 7210\n",
      "train_loss: 0.06095834865604512\n",
      "val_loss: 0.17669968215209822\n",
      "Progress: 72.1% ... Training loss: 0.064 ... Validation loss: 0.187iteration: 7211\n",
      "train_loss: 0.06482373807848273\n",
      "val_loss: 0.18742553043262175\n",
      "Progress: 72.1% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 7212\n",
      "train_loss: 0.061872403935394796\n",
      "val_loss: 0.16290199707663677\n",
      "Progress: 72.1% ... Training loss: 0.060 ... Validation loss: 0.175iteration: 7213\n",
      "train_loss: 0.06051902019653139\n",
      "val_loss: 0.17599591636702586\n",
      "Progress: 72.1% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7214\n",
      "train_loss: 0.06065002251423243\n",
      "val_loss: 0.1641832547162725\n",
      "Progress: 72.2% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 7215\n",
      "train_loss: 0.0611792092051667\n",
      "val_loss: 0.16368140136034035\n",
      "Progress: 72.2% ... Training loss: 0.060 ... Validation loss: 0.188iteration: 7216\n",
      "train_loss: 0.060951207608183615\n",
      "val_loss: 0.18873999607709474\n",
      "Progress: 72.2% ... Training loss: 0.067 ... Validation loss: 0.222iteration: 7217\n",
      "train_loss: 0.06746120897298326\n",
      "val_loss: 0.22234136420175024\n",
      "Progress: 72.2% ... Training loss: 0.060 ... Validation loss: 0.181iteration: 7218\n",
      "train_loss: 0.06094541153707158\n",
      "val_loss: 0.18157566491547958\n",
      "Progress: 72.2% ... Training loss: 0.066 ... Validation loss: 0.207iteration: 7219\n",
      "train_loss: 0.06629822608448557\n",
      "val_loss: 0.20783344643588716\n",
      "Progress: 72.2% ... Training loss: 0.067 ... Validation loss: 0.158iteration: 7220\n",
      "train_loss: 0.06729247336221948\n",
      "val_loss: 0.1589934594953286\n",
      "Progress: 72.2% ... Training loss: 0.065 ... Validation loss: 0.193iteration: 7221\n",
      "train_loss: 0.0655110448162934\n",
      "val_loss: 0.1938366955092968\n",
      "Progress: 72.2% ... Training loss: 0.098 ... Validation loss: 0.161iteration: 7222\n",
      "train_loss: 0.098927636233709\n",
      "val_loss: 0.16133875493702998\n",
      "Progress: 72.2% ... Training loss: 0.068 ... Validation loss: 0.229iteration: 7223\n",
      "train_loss: 0.06899338587932342\n",
      "val_loss: 0.2293750399308465\n",
      "Progress: 72.2% ... Training loss: 0.083 ... Validation loss: 0.155iteration: 7224\n",
      "train_loss: 0.08345156563658394\n",
      "val_loss: 0.15521121067774088\n",
      "Progress: 72.2% ... Training loss: 0.126 ... Validation loss: 0.301iteration: 7225\n",
      "train_loss: 0.12675882095583219\n",
      "val_loss: 0.3018446679346661\n",
      "Progress: 72.3% ... Training loss: 0.098 ... Validation loss: 0.159iteration: 7226\n",
      "train_loss: 0.09854158359285696\n",
      "val_loss: 0.1596911891009043\n",
      "Progress: 72.3% ... Training loss: 0.087 ... Validation loss: 0.234iteration: 7227\n",
      "train_loss: 0.08727887482358986\n",
      "val_loss: 0.2347417835709961\n",
      "Progress: 72.3% ... Training loss: 0.106 ... Validation loss: 0.160iteration: 7228\n",
      "train_loss: 0.10687892434127017\n",
      "val_loss: 0.16074868850672627\n",
      "Progress: 72.3% ... Training loss: 0.082 ... Validation loss: 0.218iteration: 7229\n",
      "train_loss: 0.08267154905846631\n",
      "val_loss: 0.21875048276161735\n",
      "Progress: 72.3% ... Training loss: 0.065 ... Validation loss: 0.153iteration: 7230\n",
      "train_loss: 0.06599907156080848\n",
      "val_loss: 0.15340481432471786\n",
      "Progress: 72.3% ... Training loss: 0.061 ... Validation loss: 0.174iteration: 7231\n",
      "train_loss: 0.06181829995154978\n",
      "val_loss: 0.17423039502878915\n",
      "Progress: 72.3% ... Training loss: 0.068 ... Validation loss: 0.154iteration: 7232\n",
      "train_loss: 0.06847895594714558\n",
      "val_loss: 0.15443781688986066\n",
      "Progress: 72.3% ... Training loss: 0.064 ... Validation loss: 0.191iteration: 7233\n",
      "train_loss: 0.06404512128123352\n",
      "val_loss: 0.19167909847841347\n",
      "Progress: 72.3% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 7234\n",
      "train_loss: 0.06167807710661275\n",
      "val_loss: 0.1658986008567088\n",
      "Progress: 72.3% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 7235\n",
      "train_loss: 0.06052160591977814\n",
      "val_loss: 0.16599194037780693\n",
      "Progress: 72.4% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 7236\n",
      "train_loss: 0.06080935130476595\n",
      "val_loss: 0.16741153415780427\n",
      "Progress: 72.4% ... Training loss: 0.071 ... Validation loss: 0.150iteration: 7237\n",
      "train_loss: 0.07187559196023437\n",
      "val_loss: 0.1509847489212148\n",
      "Progress: 72.4% ... Training loss: 0.071 ... Validation loss: 0.203iteration: 7238\n",
      "train_loss: 0.07103295237973785\n",
      "val_loss: 0.20331547944138897\n",
      "Progress: 72.4% ... Training loss: 0.078 ... Validation loss: 0.151iteration: 7239\n",
      "train_loss: 0.07894802868580786\n",
      "val_loss: 0.15141120492886706\n",
      "Progress: 72.4% ... Training loss: 0.067 ... Validation loss: 0.187iteration: 7240\n",
      "train_loss: 0.06762786389210794\n",
      "val_loss: 0.18727504020839136\n",
      "Progress: 72.4% ... Training loss: 0.066 ... Validation loss: 0.151iteration: 7241\n",
      "train_loss: 0.0661648483021602\n",
      "val_loss: 0.1519855128872927\n",
      "Progress: 72.4% ... Training loss: 0.078 ... Validation loss: 0.194iteration: 7242\n",
      "train_loss: 0.07870338844408199\n",
      "val_loss: 0.19418815132046446\n",
      "Progress: 72.4% ... Training loss: 0.068 ... Validation loss: 0.156iteration: 7243\n",
      "train_loss: 0.06827076989633322\n",
      "val_loss: 0.15647260827133763\n",
      "Progress: 72.4% ... Training loss: 0.078 ... Validation loss: 0.203iteration: 7244\n",
      "train_loss: 0.07865499039583565\n",
      "val_loss: 0.20365734003327501\n",
      "Progress: 72.5% ... Training loss: 0.079 ... Validation loss: 0.155iteration: 7245\n",
      "train_loss: 0.07939660536147088\n",
      "val_loss: 0.15592765589056232\n",
      "Progress: 72.5% ... Training loss: 0.083 ... Validation loss: 0.219iteration: 7246\n",
      "train_loss: 0.08393399172300864\n",
      "val_loss: 0.2190576267233932\n",
      "Progress: 72.5% ... Training loss: 0.082 ... Validation loss: 0.153iteration: 7247\n",
      "train_loss: 0.08242972331430273\n",
      "val_loss: 0.15390084604411464\n",
      "Progress: 72.5% ... Training loss: 0.093 ... Validation loss: 0.251iteration: 7248\n",
      "train_loss: 0.09338092762981862\n",
      "val_loss: 0.25155901489303095\n",
      "Progress: 72.5% ... Training loss: 0.094 ... Validation loss: 0.154iteration: 7249\n",
      "train_loss: 0.09453685681513377\n",
      "val_loss: 0.15455635772748133\n",
      "Progress: 72.5% ... Training loss: 0.113 ... Validation loss: 0.255iteration: 7250\n",
      "train_loss: 0.11356686734342862\n",
      "val_loss: 0.2550264711423661\n",
      "Progress: 72.5% ... Training loss: 0.115 ... Validation loss: 0.158iteration: 7251\n",
      "train_loss: 0.1158327974185746\n",
      "val_loss: 0.15808273638576772\n",
      "Progress: 72.5% ... Training loss: 0.103 ... Validation loss: 0.258iteration: 7252\n",
      "train_loss: 0.10331226791188766\n",
      "val_loss: 0.25828507755149555\n",
      "Progress: 72.5% ... Training loss: 0.093 ... Validation loss: 0.152iteration: 7253\n",
      "train_loss: 0.09364788608173161\n",
      "val_loss: 0.15206055384209557\n",
      "Progress: 72.5% ... Training loss: 0.082 ... Validation loss: 0.240iteration: 7254\n",
      "train_loss: 0.08268074386297289\n",
      "val_loss: 0.2404335650896205\n",
      "Progress: 72.5% ... Training loss: 0.071 ... Validation loss: 0.153iteration: 7255\n",
      "train_loss: 0.07119857197283158\n",
      "val_loss: 0.1536254002225972\n",
      "Progress: 72.6% ... Training loss: 0.071 ... Validation loss: 0.210iteration: 7256\n",
      "train_loss: 0.07169138971839449\n",
      "val_loss: 0.21047694636613426\n",
      "Progress: 72.6% ... Training loss: 0.069 ... Validation loss: 0.158iteration: 7257\n",
      "train_loss: 0.06970398135677228\n",
      "val_loss: 0.1588293861277341\n",
      "Progress: 72.6% ... Training loss: 0.062 ... Validation loss: 0.185iteration: 7258\n",
      "train_loss: 0.06212292869952863\n",
      "val_loss: 0.18597257487679672\n",
      "Progress: 72.6% ... Training loss: 0.063 ... Validation loss: 0.165iteration: 7259\n",
      "train_loss: 0.06367232332001957\n",
      "val_loss: 0.16550931362908253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 72.6% ... Training loss: 0.074 ... Validation loss: 0.195iteration: 7260\n",
      "train_loss: 0.0742638455374088\n",
      "val_loss: 0.19579918605731442\n",
      "Progress: 72.6% ... Training loss: 0.065 ... Validation loss: 0.154iteration: 7261\n",
      "train_loss: 0.06558409882058377\n",
      "val_loss: 0.1547526468171512\n",
      "Progress: 72.6% ... Training loss: 0.063 ... Validation loss: 0.184iteration: 7262\n",
      "train_loss: 0.06336415957520004\n",
      "val_loss: 0.18447875592508614\n",
      "Progress: 72.6% ... Training loss: 0.066 ... Validation loss: 0.158iteration: 7263\n",
      "train_loss: 0.06678609507299486\n",
      "val_loss: 0.15881038066578784\n",
      "Progress: 72.6% ... Training loss: 0.060 ... Validation loss: 0.170iteration: 7264\n",
      "train_loss: 0.06033037916227808\n",
      "val_loss: 0.17001351062252779\n",
      "Progress: 72.7% ... Training loss: 0.068 ... Validation loss: 0.149iteration: 7265\n",
      "train_loss: 0.06861316154083778\n",
      "val_loss: 0.14954674989431066\n",
      "Progress: 72.7% ... Training loss: 0.091 ... Validation loss: 0.193iteration: 7266\n",
      "train_loss: 0.09118941077452455\n",
      "val_loss: 0.19397633080646856\n",
      "Progress: 72.7% ... Training loss: 0.075 ... Validation loss: 0.153iteration: 7267\n",
      "train_loss: 0.07512169885277327\n",
      "val_loss: 0.15320388027837462\n",
      "Progress: 72.7% ... Training loss: 0.087 ... Validation loss: 0.189iteration: 7268\n",
      "train_loss: 0.0879774050021565\n",
      "val_loss: 0.1896258546772039\n",
      "Progress: 72.7% ... Training loss: 0.073 ... Validation loss: 0.148iteration: 7269\n",
      "train_loss: 0.073288728656965\n",
      "val_loss: 0.14897958981674908\n",
      "Progress: 72.7% ... Training loss: 0.069 ... Validation loss: 0.187iteration: 7270\n",
      "train_loss: 0.06905868640902182\n",
      "val_loss: 0.18704855562359984\n",
      "Progress: 72.7% ... Training loss: 0.061 ... Validation loss: 0.148iteration: 7271\n",
      "train_loss: 0.06194483028772992\n",
      "val_loss: 0.14854384240029744\n",
      "Progress: 72.7% ... Training loss: 0.061 ... Validation loss: 0.172iteration: 7272\n",
      "train_loss: 0.061589156275643625\n",
      "val_loss: 0.17252053097096326\n",
      "Progress: 72.7% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 7273\n",
      "train_loss: 0.06360091971979542\n",
      "val_loss: 0.15745846224731888\n",
      "Progress: 72.7% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 7274\n",
      "train_loss: 0.06055661713123466\n",
      "val_loss: 0.16761507868947917\n",
      "Progress: 72.8% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 7275\n",
      "train_loss: 0.06333512810710727\n",
      "val_loss: 0.1569660172284217\n",
      "Progress: 72.8% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7276\n",
      "train_loss: 0.06085564490548658\n",
      "val_loss: 0.16259790527500248\n",
      "Progress: 72.8% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7277\n",
      "train_loss: 0.06054142573034948\n",
      "val_loss: 0.16497910876390293\n",
      "Progress: 72.8% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7278\n",
      "train_loss: 0.060443496496926166\n",
      "val_loss: 0.16492094471897858\n",
      "Progress: 72.8% ... Training loss: 0.061 ... Validation loss: 0.156iteration: 7279\n",
      "train_loss: 0.06125994813246383\n",
      "val_loss: 0.15609343540024817\n",
      "Progress: 72.8% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7280\n",
      "train_loss: 0.06054168607822917\n",
      "val_loss: 0.1663126852233273\n",
      "Progress: 72.8% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 7281\n",
      "train_loss: 0.06140113953172267\n",
      "val_loss: 0.1524068844459429\n",
      "Progress: 72.8% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 7282\n",
      "train_loss: 0.0605638537085064\n",
      "val_loss: 0.1742521373493429\n",
      "Progress: 72.8% ... Training loss: 0.063 ... Validation loss: 0.188iteration: 7283\n",
      "train_loss: 0.0637324942267784\n",
      "val_loss: 0.18833995472058768\n",
      "Progress: 72.8% ... Training loss: 0.062 ... Validation loss: 0.152iteration: 7284\n",
      "train_loss: 0.06244358539856632\n",
      "val_loss: 0.1520410205381579\n",
      "Progress: 72.8% ... Training loss: 0.070 ... Validation loss: 0.193iteration: 7285\n",
      "train_loss: 0.07026285548819655\n",
      "val_loss: 0.19312679927927737\n",
      "Progress: 72.9% ... Training loss: 0.067 ... Validation loss: 0.150iteration: 7286\n",
      "train_loss: 0.06703170361971798\n",
      "val_loss: 0.15098560426523053\n",
      "Progress: 72.9% ... Training loss: 0.070 ... Validation loss: 0.189iteration: 7287\n",
      "train_loss: 0.07079578754185571\n",
      "val_loss: 0.18967635054718596\n",
      "Progress: 72.9% ... Training loss: 0.061 ... Validation loss: 0.161iteration: 7288\n",
      "train_loss: 0.06102257781380326\n",
      "val_loss: 0.16180822591535096\n",
      "Progress: 72.9% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7289\n",
      "train_loss: 0.06040883293105545\n",
      "val_loss: 0.16259061983199338\n",
      "Progress: 72.9% ... Training loss: 0.061 ... Validation loss: 0.167iteration: 7290\n",
      "train_loss: 0.06139466510286738\n",
      "val_loss: 0.16754566144553643\n",
      "Progress: 72.9% ... Training loss: 0.060 ... Validation loss: 0.170iteration: 7291\n",
      "train_loss: 0.060862682410427624\n",
      "val_loss: 0.17083018664031197\n",
      "Progress: 72.9% ... Training loss: 0.061 ... Validation loss: 0.164iteration: 7292\n",
      "train_loss: 0.06100535125442109\n",
      "val_loss: 0.1643039356744458\n",
      "Progress: 72.9% ... Training loss: 0.067 ... Validation loss: 0.174iteration: 7293\n",
      "train_loss: 0.06717116943473778\n",
      "val_loss: 0.17454441239115043\n",
      "Progress: 72.9% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 7294\n",
      "train_loss: 0.0606565126519378\n",
      "val_loss: 0.16373290842537155\n",
      "Progress: 73.0% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 7295\n",
      "train_loss: 0.06024571295876815\n",
      "val_loss: 0.16968167209733068\n",
      "Progress: 73.0% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 7296\n",
      "train_loss: 0.06032262880978851\n",
      "val_loss: 0.16794966538574788\n",
      "Progress: 73.0% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 7297\n",
      "train_loss: 0.06102755738913166\n",
      "val_loss: 0.17380871348029803\n",
      "Progress: 73.0% ... Training loss: 0.060 ... Validation loss: 0.171iteration: 7298\n",
      "train_loss: 0.06080007076864968\n",
      "val_loss: 0.17127841996100035\n",
      "Progress: 73.0% ... Training loss: 0.064 ... Validation loss: 0.155iteration: 7299\n",
      "train_loss: 0.06409131045965284\n",
      "val_loss: 0.15570552626484507\n",
      "Progress: 73.0% ... Training loss: 0.062 ... Validation loss: 0.182iteration: 7300\n",
      "train_loss: 0.062483634936017246\n",
      "val_loss: 0.18238698219595745\n",
      "Progress: 73.0% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7301\n",
      "train_loss: 0.06103081868101932\n",
      "val_loss: 0.160815327185214\n",
      "Progress: 73.0% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 7302\n",
      "train_loss: 0.06057537084663928\n",
      "val_loss: 0.1740234920377365\n",
      "Progress: 73.0% ... Training loss: 0.075 ... Validation loss: 0.210iteration: 7303\n",
      "train_loss: 0.0750285152304836\n",
      "val_loss: 0.21015251443859384\n",
      "Progress: 73.0% ... Training loss: 0.063 ... Validation loss: 0.155iteration: 7304\n",
      "train_loss: 0.06327678552924594\n",
      "val_loss: 0.15583859386254253\n",
      "Progress: 73.0% ... Training loss: 0.071 ... Validation loss: 0.187iteration: 7305\n",
      "train_loss: 0.07120893330766069\n",
      "val_loss: 0.18731427064797448\n",
      "Progress: 73.1% ... Training loss: 0.069 ... Validation loss: 0.147iteration: 7306\n",
      "train_loss: 0.06938195879768991\n",
      "val_loss: 0.1478426809553483\n",
      "Progress: 73.1% ... Training loss: 0.066 ... Validation loss: 0.188iteration: 7307\n",
      "train_loss: 0.06681186727268089\n",
      "val_loss: 0.18810401192890117\n",
      "Progress: 73.1% ... Training loss: 0.067 ... Validation loss: 0.164iteration: 7308\n",
      "train_loss: 0.06798884307860567\n",
      "val_loss: 0.16408840619561257\n",
      "Progress: 73.1% ... Training loss: 0.062 ... Validation loss: 0.180iteration: 7309\n",
      "train_loss: 0.06281754959632992\n",
      "val_loss: 0.18028478301910816\n",
      "Progress: 73.1% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7310\n",
      "train_loss: 0.061707379142745725\n",
      "val_loss: 0.16026240391294866\n",
      "Progress: 73.1% ... Training loss: 0.061 ... Validation loss: 0.180iteration: 7311\n",
      "train_loss: 0.06181700666998805\n",
      "val_loss: 0.18096266040225784\n",
      "Progress: 73.1% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 7312\n",
      "train_loss: 0.06127518922411295\n",
      "val_loss: 0.1689538730318151\n",
      "Progress: 73.1% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7313\n",
      "train_loss: 0.06085616592929715\n",
      "val_loss: 0.16610475607636804\n",
      "Progress: 73.1% ... Training loss: 0.064 ... Validation loss: 0.186iteration: 7314\n",
      "train_loss: 0.0643954225987868\n",
      "val_loss: 0.18601517406937343\n",
      "Progress: 73.2% ... Training loss: 0.061 ... Validation loss: 0.156iteration: 7315\n",
      "train_loss: 0.06102595834646267\n",
      "val_loss: 0.15643925090625768\n",
      "Progress: 73.2% ... Training loss: 0.067 ... Validation loss: 0.180iteration: 7316\n",
      "train_loss: 0.06737441163229489\n",
      "val_loss: 0.18095131684414578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 73.2% ... Training loss: 0.062 ... Validation loss: 0.152iteration: 7317\n",
      "train_loss: 0.06246937305408719\n",
      "val_loss: 0.15284407221508176\n",
      "Progress: 73.2% ... Training loss: 0.061 ... Validation loss: 0.172iteration: 7318\n",
      "train_loss: 0.06144951344516284\n",
      "val_loss: 0.17242862647672855\n",
      "Progress: 73.2% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 7319\n",
      "train_loss: 0.06044591289757315\n",
      "val_loss: 0.16762760117053696\n",
      "Progress: 73.2% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 7320\n",
      "train_loss: 0.06032892415816597\n",
      "val_loss: 0.15744023573163585\n",
      "Progress: 73.2% ... Training loss: 0.065 ... Validation loss: 0.180iteration: 7321\n",
      "train_loss: 0.06556456330121022\n",
      "val_loss: 0.18034657345369756\n",
      "Progress: 73.2% ... Training loss: 0.061 ... Validation loss: 0.150iteration: 7322\n",
      "train_loss: 0.061964048897743526\n",
      "val_loss: 0.15020299370683263\n",
      "Progress: 73.2% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 7323\n",
      "train_loss: 0.06098227696517951\n",
      "val_loss: 0.17303862062347414\n",
      "Progress: 73.2% ... Training loss: 0.060 ... Validation loss: 0.158iteration: 7324\n",
      "train_loss: 0.06083599035566196\n",
      "val_loss: 0.15876575303222265\n",
      "Progress: 73.2% ... Training loss: 0.062 ... Validation loss: 0.177iteration: 7325\n",
      "train_loss: 0.06257140758781153\n",
      "val_loss: 0.17744854236508478\n",
      "Progress: 73.3% ... Training loss: 0.061 ... Validation loss: 0.158iteration: 7326\n",
      "train_loss: 0.06102156590013882\n",
      "val_loss: 0.1580297328572841\n",
      "Progress: 73.3% ... Training loss: 0.060 ... Validation loss: 0.171iteration: 7327\n",
      "train_loss: 0.06010569193533656\n",
      "val_loss: 0.17181333370593316\n",
      "Progress: 73.3% ... Training loss: 0.064 ... Validation loss: 0.166iteration: 7328\n",
      "train_loss: 0.06470108578630185\n",
      "val_loss: 0.166610901725869\n",
      "Progress: 73.3% ... Training loss: 0.061 ... Validation loss: 0.179iteration: 7329\n",
      "train_loss: 0.061224020526332634\n",
      "val_loss: 0.17963974069735028\n",
      "Progress: 73.3% ... Training loss: 0.071 ... Validation loss: 0.155iteration: 7330\n",
      "train_loss: 0.07146501925544967\n",
      "val_loss: 0.15510379403661723\n",
      "Progress: 73.3% ... Training loss: 0.064 ... Validation loss: 0.183iteration: 7331\n",
      "train_loss: 0.06484507201008687\n",
      "val_loss: 0.1835239884634885\n",
      "Progress: 73.3% ... Training loss: 0.064 ... Validation loss: 0.155iteration: 7332\n",
      "train_loss: 0.06470572615039608\n",
      "val_loss: 0.15558934161965476\n",
      "Progress: 73.3% ... Training loss: 0.065 ... Validation loss: 0.192iteration: 7333\n",
      "train_loss: 0.06534715638576691\n",
      "val_loss: 0.19270297036284248\n",
      "Progress: 73.3% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 7334\n",
      "train_loss: 0.060265295041206014\n",
      "val_loss: 0.16562860080030864\n",
      "Progress: 73.3% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7335\n",
      "train_loss: 0.06023133214979019\n",
      "val_loss: 0.16696875349625756\n",
      "Progress: 73.4% ... Training loss: 0.062 ... Validation loss: 0.179iteration: 7336\n",
      "train_loss: 0.062271892140603156\n",
      "val_loss: 0.17979916613959057\n",
      "Progress: 73.4% ... Training loss: 0.064 ... Validation loss: 0.163iteration: 7337\n",
      "train_loss: 0.06468358078796055\n",
      "val_loss: 0.16359188147690934\n",
      "Progress: 73.4% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 7338\n",
      "train_loss: 0.06032298770563505\n",
      "val_loss: 0.1734423763252144\n",
      "Progress: 73.4% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 7339\n",
      "train_loss: 0.06026720944707585\n",
      "val_loss: 0.15935946434311712\n",
      "Progress: 73.4% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 7340\n",
      "train_loss: 0.05987055544316346\n",
      "val_loss: 0.17085439559483057\n",
      "Progress: 73.4% ... Training loss: 0.065 ... Validation loss: 0.193iteration: 7341\n",
      "train_loss: 0.06556730175081424\n",
      "val_loss: 0.19364723057028646\n",
      "Progress: 73.4% ... Training loss: 0.064 ... Validation loss: 0.157iteration: 7342\n",
      "train_loss: 0.06449704722062008\n",
      "val_loss: 0.15740798296053524\n",
      "Progress: 73.4% ... Training loss: 0.076 ... Validation loss: 0.202iteration: 7343\n",
      "train_loss: 0.07618123043532544\n",
      "val_loss: 0.20261212266996761\n",
      "Progress: 73.4% ... Training loss: 0.074 ... Validation loss: 0.154iteration: 7344\n",
      "train_loss: 0.07435506985470583\n",
      "val_loss: 0.15462251439035232\n",
      "Progress: 73.5% ... Training loss: 0.099 ... Validation loss: 0.253iteration: 7345\n",
      "train_loss: 0.0992402810280097\n",
      "val_loss: 0.2530610606206479\n",
      "Progress: 73.5% ... Training loss: 0.098 ... Validation loss: 0.157iteration: 7346\n",
      "train_loss: 0.09849011589418795\n",
      "val_loss: 0.1573258749479537\n",
      "Progress: 73.5% ... Training loss: 0.074 ... Validation loss: 0.209iteration: 7347\n",
      "train_loss: 0.07471579077307035\n",
      "val_loss: 0.20995574893132735\n",
      "Progress: 73.5% ... Training loss: 0.070 ... Validation loss: 0.157iteration: 7348\n",
      "train_loss: 0.07040585565063756\n",
      "val_loss: 0.15762379438866822\n",
      "Progress: 73.5% ... Training loss: 0.066 ... Validation loss: 0.198iteration: 7349\n",
      "train_loss: 0.06690532553190738\n",
      "val_loss: 0.19863545641106045\n",
      "Progress: 73.5% ... Training loss: 0.061 ... Validation loss: 0.169iteration: 7350\n",
      "train_loss: 0.061745813149416906\n",
      "val_loss: 0.16966824366257738\n",
      "Progress: 73.5% ... Training loss: 0.062 ... Validation loss: 0.160iteration: 7351\n",
      "train_loss: 0.062253414497766384\n",
      "val_loss: 0.16052457221049782\n",
      "Progress: 73.5% ... Training loss: 0.061 ... Validation loss: 0.167iteration: 7352\n",
      "train_loss: 0.06111948987085075\n",
      "val_loss: 0.16763826434243054\n",
      "Progress: 73.5% ... Training loss: 0.060 ... Validation loss: 0.179iteration: 7353\n",
      "train_loss: 0.060987772921512924\n",
      "val_loss: 0.17955382985399707\n",
      "Progress: 73.5% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 7354\n",
      "train_loss: 0.06047445368743785\n",
      "val_loss: 0.16900733818130395\n",
      "Progress: 73.5% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7355\n",
      "train_loss: 0.060590680883476356\n",
      "val_loss: 0.16463788333785473\n",
      "Progress: 73.6% ... Training loss: 0.063 ... Validation loss: 0.180iteration: 7356\n",
      "train_loss: 0.0636974077096117\n",
      "val_loss: 0.18021618247198634\n",
      "Progress: 73.6% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 7357\n",
      "train_loss: 0.06434092191046263\n",
      "val_loss: 0.15661071266286342\n",
      "Progress: 73.6% ... Training loss: 0.069 ... Validation loss: 0.203iteration: 7358\n",
      "train_loss: 0.06907878227925332\n",
      "val_loss: 0.2036515400166992\n",
      "Progress: 73.6% ... Training loss: 0.062 ... Validation loss: 0.162iteration: 7359\n",
      "train_loss: 0.06226857973100006\n",
      "val_loss: 0.16240325206950434\n",
      "Progress: 73.6% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 7360\n",
      "train_loss: 0.059939131890491895\n",
      "val_loss: 0.16918863044855026\n",
      "Progress: 73.6% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 7361\n",
      "train_loss: 0.06016298562521082\n",
      "val_loss: 0.1656875879563839\n",
      "Progress: 73.6% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 7362\n",
      "train_loss: 0.06045712264533758\n",
      "val_loss: 0.16360965249192827\n",
      "Progress: 73.6% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 7363\n",
      "train_loss: 0.060083279612780566\n",
      "val_loss: 0.1637567904233785\n",
      "Progress: 73.6% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 7364\n",
      "train_loss: 0.06331881895285703\n",
      "val_loss: 0.18206332889156396\n",
      "Progress: 73.7% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7365\n",
      "train_loss: 0.05986369382451729\n",
      "val_loss: 0.16428175383664134\n",
      "Progress: 73.7% ... Training loss: 0.067 ... Validation loss: 0.184iteration: 7366\n",
      "train_loss: 0.06795249227195332\n",
      "val_loss: 0.18400679143158452\n",
      "Progress: 73.7% ... Training loss: 0.072 ... Validation loss: 0.153iteration: 7367\n",
      "train_loss: 0.07241275527907068\n",
      "val_loss: 0.1535338222904705\n",
      "Progress: 73.7% ... Training loss: 0.065 ... Validation loss: 0.199iteration: 7368\n",
      "train_loss: 0.06558588969051538\n",
      "val_loss: 0.1992827353563259\n",
      "Progress: 73.7% ... Training loss: 0.069 ... Validation loss: 0.151iteration: 7369\n",
      "train_loss: 0.06936673051852378\n",
      "val_loss: 0.15167371449187306\n",
      "Progress: 73.7% ... Training loss: 0.070 ... Validation loss: 0.190iteration: 7370\n",
      "train_loss: 0.07044333268397697\n",
      "val_loss: 0.19058686419716883\n",
      "Progress: 73.7% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 7371\n",
      "train_loss: 0.05969471878709094\n",
      "val_loss: 0.15761356944106442\n",
      "Progress: 73.7% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 7372\n",
      "train_loss: 0.060738998823519984\n",
      "val_loss: 0.16918785667619105\n",
      "Progress: 73.7% ... Training loss: 0.064 ... Validation loss: 0.147iteration: 7373\n",
      "train_loss: 0.06454324324290417\n",
      "val_loss: 0.14757120608050178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 73.7% ... Training loss: 0.072 ... Validation loss: 0.206iteration: 7374\n",
      "train_loss: 0.07237324532876423\n",
      "val_loss: 0.20641463816235242\n",
      "Progress: 73.8% ... Training loss: 0.073 ... Validation loss: 0.145iteration: 7375\n",
      "train_loss: 0.07323883701089\n",
      "val_loss: 0.14530757180644194\n",
      "Progress: 73.8% ... Training loss: 0.064 ... Validation loss: 0.177iteration: 7376\n",
      "train_loss: 0.06407747910558509\n",
      "val_loss: 0.17712630225446524\n",
      "Progress: 73.8% ... Training loss: 0.060 ... Validation loss: 0.153iteration: 7377\n",
      "train_loss: 0.06092958217995452\n",
      "val_loss: 0.15347217939225782\n",
      "Progress: 73.8% ... Training loss: 0.072 ... Validation loss: 0.150iteration: 7378\n",
      "train_loss: 0.07224552309305932\n",
      "val_loss: 0.1507117361033713\n",
      "Progress: 73.8% ... Training loss: 0.069 ... Validation loss: 0.191iteration: 7379\n",
      "train_loss: 0.06917770200996348\n",
      "val_loss: 0.1919247460060237\n",
      "Progress: 73.8% ... Training loss: 0.060 ... Validation loss: 0.148iteration: 7380\n",
      "train_loss: 0.06090833010093194\n",
      "val_loss: 0.14807978083945558\n",
      "Progress: 73.8% ... Training loss: 0.065 ... Validation loss: 0.151iteration: 7381\n",
      "train_loss: 0.06585548228193888\n",
      "val_loss: 0.1510452972782255\n",
      "Progress: 73.8% ... Training loss: 0.068 ... Validation loss: 0.201iteration: 7382\n",
      "train_loss: 0.06865383120593836\n",
      "val_loss: 0.2019553950755415\n",
      "Progress: 73.8% ... Training loss: 0.060 ... Validation loss: 0.171iteration: 7383\n",
      "train_loss: 0.06074818628363798\n",
      "val_loss: 0.1715368338599378\n",
      "Progress: 73.8% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 7384\n",
      "train_loss: 0.06161505770288656\n",
      "val_loss: 0.17132901599497594\n",
      "Progress: 73.8% ... Training loss: 0.066 ... Validation loss: 0.144iteration: 7385\n",
      "train_loss: 0.06600995272083494\n",
      "val_loss: 0.144260036547782\n",
      "Progress: 73.9% ... Training loss: 0.061 ... Validation loss: 0.174iteration: 7386\n",
      "train_loss: 0.06137951716354881\n",
      "val_loss: 0.17444573382729558\n",
      "Progress: 73.9% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 7387\n",
      "train_loss: 0.06387570945342681\n",
      "val_loss: 0.16170553458058026\n",
      "Progress: 73.9% ... Training loss: 0.061 ... Validation loss: 0.176iteration: 7388\n",
      "train_loss: 0.061096937122559\n",
      "val_loss: 0.17624593115803167\n",
      "Progress: 73.9% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 7389\n",
      "train_loss: 0.05977046493657328\n",
      "val_loss: 0.15991949508722877\n",
      "Progress: 73.9% ... Training loss: 0.060 ... Validation loss: 0.150iteration: 7390\n",
      "train_loss: 0.060723750945456706\n",
      "val_loss: 0.15033091047687527\n",
      "Progress: 73.9% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 7391\n",
      "train_loss: 0.060515281398154146\n",
      "val_loss: 0.15561095795319616\n",
      "Progress: 73.9% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 7392\n",
      "train_loss: 0.06161799344980515\n",
      "val_loss: 0.1490550439529181\n",
      "Progress: 73.9% ... Training loss: 0.065 ... Validation loss: 0.179iteration: 7393\n",
      "train_loss: 0.06536300421139878\n",
      "val_loss: 0.1795453549660112\n",
      "Progress: 73.9% ... Training loss: 0.060 ... Validation loss: 0.158iteration: 7394\n",
      "train_loss: 0.06022704432361338\n",
      "val_loss: 0.15898259778315377\n",
      "Progress: 74.0% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 7395\n",
      "train_loss: 0.06012913618513971\n",
      "val_loss: 0.15619515698835654\n",
      "Progress: 74.0% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7396\n",
      "train_loss: 0.06024196272894192\n",
      "val_loss: 0.16443780649743703\n",
      "Progress: 74.0% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7397\n",
      "train_loss: 0.060249310486529026\n",
      "val_loss: 0.1628896212978537\n",
      "Progress: 74.0% ... Training loss: 0.063 ... Validation loss: 0.171iteration: 7398\n",
      "train_loss: 0.0632774468024954\n",
      "val_loss: 0.17130927559095777\n",
      "Progress: 74.0% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 7399\n",
      "train_loss: 0.06131139475777442\n",
      "val_loss: 0.1713658704625893\n",
      "Progress: 74.0% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 7400\n",
      "train_loss: 0.06356325029282082\n",
      "val_loss: 0.17464428425111198\n",
      "Progress: 74.0% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 7401\n",
      "train_loss: 0.060597788350107096\n",
      "val_loss: 0.15950702630396255\n",
      "Progress: 74.0% ... Training loss: 0.062 ... Validation loss: 0.146iteration: 7402\n",
      "train_loss: 0.06273872964447175\n",
      "val_loss: 0.146617450793344\n",
      "Progress: 74.0% ... Training loss: 0.059 ... Validation loss: 0.173iteration: 7403\n",
      "train_loss: 0.059809011292892206\n",
      "val_loss: 0.17300499948952835\n",
      "Progress: 74.0% ... Training loss: 0.059 ... Validation loss: 0.165iteration: 7404\n",
      "train_loss: 0.05991686770295669\n",
      "val_loss: 0.16539199588831208\n",
      "Progress: 74.0% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 7405\n",
      "train_loss: 0.060244780650834974\n",
      "val_loss: 0.16328403818902587\n",
      "Progress: 74.1% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 7406\n",
      "train_loss: 0.060009268440630696\n",
      "val_loss: 0.16774807578132847\n",
      "Progress: 74.1% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 7407\n",
      "train_loss: 0.060013759943940925\n",
      "val_loss: 0.165714767475673\n",
      "Progress: 74.1% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 7408\n",
      "train_loss: 0.06008122546102708\n",
      "val_loss: 0.17246433157284197\n",
      "Progress: 74.1% ... Training loss: 0.059 ... Validation loss: 0.165iteration: 7409\n",
      "train_loss: 0.05951407456110745\n",
      "val_loss: 0.16535644481865922\n",
      "Progress: 74.1% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 7410\n",
      "train_loss: 0.060077250355063214\n",
      "val_loss: 0.16961490451946423\n",
      "Progress: 74.1% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 7411\n",
      "train_loss: 0.06029351428323896\n",
      "val_loss: 0.15721016626240927\n",
      "Progress: 74.1% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 7412\n",
      "train_loss: 0.059778132040237575\n",
      "val_loss: 0.17171671287805498\n",
      "Progress: 74.1% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 7413\n",
      "train_loss: 0.05979403692221275\n",
      "val_loss: 0.1701597860269837\n",
      "Progress: 74.1% ... Training loss: 0.064 ... Validation loss: 0.182iteration: 7414\n",
      "train_loss: 0.06473523317114226\n",
      "val_loss: 0.18211176723292108\n",
      "Progress: 74.2% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 7415\n",
      "train_loss: 0.06072430533276761\n",
      "val_loss: 0.1612109570990024\n",
      "Progress: 74.2% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 7416\n",
      "train_loss: 0.06057509114619413\n",
      "val_loss: 0.1561793065587248\n",
      "Progress: 74.2% ... Training loss: 0.068 ... Validation loss: 0.156iteration: 7417\n",
      "train_loss: 0.06837105624696678\n",
      "val_loss: 0.15670063680173704\n",
      "Progress: 74.2% ... Training loss: 0.061 ... Validation loss: 0.184iteration: 7418\n",
      "train_loss: 0.06175458890676709\n",
      "val_loss: 0.1848357990363673\n",
      "Progress: 74.2% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7419\n",
      "train_loss: 0.060332164305380465\n",
      "val_loss: 0.16681784369497774\n",
      "Progress: 74.2% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 7420\n",
      "train_loss: 0.059720145130943\n",
      "val_loss: 0.1619404156032029\n",
      "Progress: 74.2% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 7421\n",
      "train_loss: 0.060415949659950056\n",
      "val_loss: 0.16549726923211386\n",
      "Progress: 74.2% ... Training loss: 0.059 ... Validation loss: 0.165iteration: 7422\n",
      "train_loss: 0.05951460967500257\n",
      "val_loss: 0.16510986336325947\n",
      "Progress: 74.2% ... Training loss: 0.064 ... Validation loss: 0.145iteration: 7423\n",
      "train_loss: 0.06431102127384429\n",
      "val_loss: 0.14566897380886373\n",
      "Progress: 74.2% ... Training loss: 0.065 ... Validation loss: 0.170iteration: 7424\n",
      "train_loss: 0.06587281393382266\n",
      "val_loss: 0.17006405877819417\n",
      "Progress: 74.2% ... Training loss: 0.067 ... Validation loss: 0.146iteration: 7425\n",
      "train_loss: 0.06793156302509099\n",
      "val_loss: 0.14672485324197174\n",
      "Progress: 74.3% ... Training loss: 0.067 ... Validation loss: 0.187iteration: 7426\n",
      "train_loss: 0.06795918290778137\n",
      "val_loss: 0.1873012212602435\n",
      "Progress: 74.3% ... Training loss: 0.066 ... Validation loss: 0.151iteration: 7427\n",
      "train_loss: 0.06667008810356648\n",
      "val_loss: 0.15119100211504996\n",
      "Progress: 74.3% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 7428\n",
      "train_loss: 0.061337926518534175\n",
      "val_loss: 0.1730120622688179\n",
      "Progress: 74.3% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 7429\n",
      "train_loss: 0.06115620433098494\n",
      "val_loss: 0.1662805846587506\n",
      "Progress: 74.3% ... Training loss: 0.060 ... Validation loss: 0.184iteration: 7430\n",
      "train_loss: 0.06063166676534994\n",
      "val_loss: 0.1841223344781149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 74.3% ... Training loss: 0.060 ... Validation loss: 0.153iteration: 7431\n",
      "train_loss: 0.060785460949405214\n",
      "val_loss: 0.1538487820400374\n",
      "Progress: 74.3% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 7432\n",
      "train_loss: 0.061620381612727565\n",
      "val_loss: 0.16886511113326816\n",
      "Progress: 74.3% ... Training loss: 0.062 ... Validation loss: 0.147iteration: 7433\n",
      "train_loss: 0.062487145213539354\n",
      "val_loss: 0.1476467784604558\n",
      "Progress: 74.3% ... Training loss: 0.067 ... Validation loss: 0.186iteration: 7434\n",
      "train_loss: 0.06711681726089391\n",
      "val_loss: 0.18642895114188268\n",
      "Progress: 74.3% ... Training loss: 0.063 ... Validation loss: 0.148iteration: 7435\n",
      "train_loss: 0.06306322684267245\n",
      "val_loss: 0.14822041274805675\n",
      "Progress: 74.4% ... Training loss: 0.070 ... Validation loss: 0.170iteration: 7436\n",
      "train_loss: 0.07091176538769366\n",
      "val_loss: 0.1709833849085777\n",
      "Progress: 74.4% ... Training loss: 0.066 ... Validation loss: 0.147iteration: 7437\n",
      "train_loss: 0.0667199420343918\n",
      "val_loss: 0.14720790619802815\n",
      "Progress: 74.4% ... Training loss: 0.067 ... Validation loss: 0.166iteration: 7438\n",
      "train_loss: 0.06780746251737858\n",
      "val_loss: 0.1660109740324538\n",
      "Progress: 74.4% ... Training loss: 0.061 ... Validation loss: 0.150iteration: 7439\n",
      "train_loss: 0.061628937934434976\n",
      "val_loss: 0.1501744724416446\n",
      "Progress: 74.4% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 7440\n",
      "train_loss: 0.061091304541351886\n",
      "val_loss: 0.1539787506647019\n",
      "Progress: 74.4% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 7441\n",
      "train_loss: 0.0632850592586481\n",
      "val_loss: 0.16684695360732818\n",
      "Progress: 74.4% ... Training loss: 0.062 ... Validation loss: 0.148iteration: 7442\n",
      "train_loss: 0.0627090619203546\n",
      "val_loss: 0.14888225516170767\n",
      "Progress: 74.4% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 7443\n",
      "train_loss: 0.06255730784558425\n",
      "val_loss: 0.1728736308091753\n",
      "Progress: 74.4% ... Training loss: 0.076 ... Validation loss: 0.149iteration: 7444\n",
      "train_loss: 0.07697289774448318\n",
      "val_loss: 0.14972545646868768\n",
      "Progress: 74.5% ... Training loss: 0.070 ... Validation loss: 0.193iteration: 7445\n",
      "train_loss: 0.07094234796838382\n",
      "val_loss: 0.1934648285249305\n",
      "Progress: 74.5% ... Training loss: 0.068 ... Validation loss: 0.149iteration: 7446\n",
      "train_loss: 0.06872135486415626\n",
      "val_loss: 0.14993903390768\n",
      "Progress: 74.5% ... Training loss: 0.071 ... Validation loss: 0.204iteration: 7447\n",
      "train_loss: 0.07114365781229856\n",
      "val_loss: 0.20486939798155743\n",
      "Progress: 74.5% ... Training loss: 0.076 ... Validation loss: 0.146iteration: 7448\n",
      "train_loss: 0.07619879723559526\n",
      "val_loss: 0.14693969388405007\n",
      "Progress: 74.5% ... Training loss: 0.061 ... Validation loss: 0.169iteration: 7449\n",
      "train_loss: 0.06163942475728305\n",
      "val_loss: 0.16980546013941533\n",
      "Progress: 74.5% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 7450\n",
      "train_loss: 0.06193998085924983\n",
      "val_loss: 0.15714845863050805\n",
      "Progress: 74.5% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 7451\n",
      "train_loss: 0.06103515364345805\n",
      "val_loss: 0.15964433760687682\n",
      "Progress: 74.5% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 7452\n",
      "train_loss: 0.06289437202345506\n",
      "val_loss: 0.18362346521465228\n",
      "Progress: 74.5% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 7453\n",
      "train_loss: 0.06557984025133635\n",
      "val_loss: 0.15284142028778003\n",
      "Progress: 74.5% ... Training loss: 0.060 ... Validation loss: 0.160iteration: 7454\n",
      "train_loss: 0.06084413212705231\n",
      "val_loss: 0.16062523565461234\n",
      "Progress: 74.5% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7455\n",
      "train_loss: 0.06049684592194113\n",
      "val_loss: 0.16200115863037362\n",
      "Progress: 74.6% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 7456\n",
      "train_loss: 0.05967675224358806\n",
      "val_loss: 0.1603580718380333\n",
      "Progress: 74.6% ... Training loss: 0.059 ... Validation loss: 0.163iteration: 7457\n",
      "train_loss: 0.059530908331030245\n",
      "val_loss: 0.16391056059631717\n",
      "Progress: 74.6% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 7458\n",
      "train_loss: 0.060467066595252666\n",
      "val_loss: 0.16760570148940956\n",
      "Progress: 74.6% ... Training loss: 0.063 ... Validation loss: 0.155iteration: 7459\n",
      "train_loss: 0.06312797019790013\n",
      "val_loss: 0.1552712447107891\n",
      "Progress: 74.6% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 7460\n",
      "train_loss: 0.06099328347241052\n",
      "val_loss: 0.17316418014841245\n",
      "Progress: 74.6% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 7461\n",
      "train_loss: 0.05970604663613212\n",
      "val_loss: 0.151070285110077\n",
      "Progress: 74.6% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 7462\n",
      "train_loss: 0.06069173051525844\n",
      "val_loss: 0.15163983073438722\n",
      "Progress: 74.6% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 7463\n",
      "train_loss: 0.0595479816546146\n",
      "val_loss: 0.15915621345012446\n",
      "Progress: 74.6% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 7464\n",
      "train_loss: 0.059736197038968035\n",
      "val_loss: 0.1557335508102365\n",
      "Progress: 74.7% ... Training loss: 0.062 ... Validation loss: 0.147iteration: 7465\n",
      "train_loss: 0.06265475735459447\n",
      "val_loss: 0.14703050683268162\n",
      "Progress: 74.7% ... Training loss: 0.063 ... Validation loss: 0.185iteration: 7466\n",
      "train_loss: 0.06317250195102873\n",
      "val_loss: 0.18564214512682156\n",
      "Progress: 74.7% ... Training loss: 0.063 ... Validation loss: 0.152iteration: 7467\n",
      "train_loss: 0.0633064143347135\n",
      "val_loss: 0.1527253010783274\n",
      "Progress: 74.7% ... Training loss: 0.060 ... Validation loss: 0.153iteration: 7468\n",
      "train_loss: 0.06064806821903708\n",
      "val_loss: 0.15397034798756737\n",
      "Progress: 74.7% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 7469\n",
      "train_loss: 0.060076244443502194\n",
      "val_loss: 0.15622130442398743\n",
      "Progress: 74.7% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 7470\n",
      "train_loss: 0.06005481021487772\n",
      "val_loss: 0.15577704383391003\n",
      "Progress: 74.7% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 7471\n",
      "train_loss: 0.06045033725212749\n",
      "val_loss: 0.14971606904824436\n",
      "Progress: 74.7% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 7472\n",
      "train_loss: 0.061759252836587245\n",
      "val_loss: 0.1662844578760932\n",
      "Progress: 74.7% ... Training loss: 0.067 ... Validation loss: 0.146iteration: 7473\n",
      "train_loss: 0.06768254767736905\n",
      "val_loss: 0.1462234267090407\n",
      "Progress: 74.7% ... Training loss: 0.088 ... Validation loss: 0.214iteration: 7474\n",
      "train_loss: 0.08866641362930196\n",
      "val_loss: 0.21476675425912767\n",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.150iteration: 7475\n",
      "train_loss: 0.06085394512503585\n",
      "val_loss: 0.15087996358243344\n",
      "Progress: 74.8% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7476\n",
      "train_loss: 0.06107134580187072\n",
      "val_loss: 0.16034832889704737\n",
      "Progress: 74.8% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 7477\n",
      "train_loss: 0.05987807746100222\n",
      "val_loss: 0.1611076812087257\n",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 7478\n",
      "train_loss: 0.06070385587340895\n",
      "val_loss: 0.15196490267025475\n",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7479\n",
      "train_loss: 0.060211085108075435\n",
      "val_loss: 0.1641745429565083\n",
      "Progress: 74.8% ... Training loss: 0.061 ... Validation loss: 0.161iteration: 7480\n",
      "train_loss: 0.061147291166798526\n",
      "val_loss: 0.16192892249963664\n",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 7481\n",
      "train_loss: 0.06061022799555973\n",
      "val_loss: 0.1525796150213408\n",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.148iteration: 7482\n",
      "train_loss: 0.06099668825352121\n",
      "val_loss: 0.14883365505829405\n",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 7483\n",
      "train_loss: 0.060174972190588004\n",
      "val_loss: 0.15552068373694936\n",
      "Progress: 74.8% ... Training loss: 0.061 ... Validation loss: 0.150iteration: 7484\n",
      "train_loss: 0.061691645090107446\n",
      "val_loss: 0.1507795655975969\n",
      "Progress: 74.8% ... Training loss: 0.065 ... Validation loss: 0.170iteration: 7485\n",
      "train_loss: 0.06516554419832493\n",
      "val_loss: 0.17022492143899387\n",
      "Progress: 74.9% ... Training loss: 0.073 ... Validation loss: 0.150iteration: 7486\n",
      "train_loss: 0.07311226330874583\n",
      "val_loss: 0.1505344641909466\n",
      "Progress: 74.9% ... Training loss: 0.076 ... Validation loss: 0.189iteration: 7487\n",
      "train_loss: 0.07634163528889024\n",
      "val_loss: 0.18985281975966606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 74.9% ... Training loss: 0.069 ... Validation loss: 0.144iteration: 7488\n",
      "train_loss: 0.06956112151734885\n",
      "val_loss: 0.14461413175969168\n",
      "Progress: 74.9% ... Training loss: 0.076 ... Validation loss: 0.196iteration: 7489\n",
      "train_loss: 0.07642810884860708\n",
      "val_loss: 0.1968903335742938\n",
      "Progress: 74.9% ... Training loss: 0.064 ... Validation loss: 0.148iteration: 7490\n",
      "train_loss: 0.0647248150962792\n",
      "val_loss: 0.14896277700456442\n",
      "Progress: 74.9% ... Training loss: 0.073 ... Validation loss: 0.178iteration: 7491\n",
      "train_loss: 0.073648474965003\n",
      "val_loss: 0.17837360928415313\n",
      "Progress: 74.9% ... Training loss: 0.102 ... Validation loss: 0.159iteration: 7492\n",
      "train_loss: 0.10262053641417766\n",
      "val_loss: 0.15937640577913767\n",
      "Progress: 74.9% ... Training loss: 0.081 ... Validation loss: 0.231iteration: 7493\n",
      "train_loss: 0.08164643324297721\n",
      "val_loss: 0.23136642414494848\n",
      "Progress: 74.9% ... Training loss: 0.077 ... Validation loss: 0.151iteration: 7494\n",
      "train_loss: 0.07789894732255272\n",
      "val_loss: 0.1516908590354412\n",
      "Progress: 75.0% ... Training loss: 0.077 ... Validation loss: 0.217iteration: 7495\n",
      "train_loss: 0.07754849149077596\n",
      "val_loss: 0.21705518843988542\n",
      "Progress: 75.0% ... Training loss: 0.063 ... Validation loss: 0.158iteration: 7496\n",
      "train_loss: 0.0639434480976804\n",
      "val_loss: 0.15880855170628974\n",
      "Progress: 75.0% ... Training loss: 0.066 ... Validation loss: 0.176iteration: 7497\n",
      "train_loss: 0.06652472674422574\n",
      "val_loss: 0.17608109491072874\n",
      "Progress: 75.0% ... Training loss: 0.062 ... Validation loss: 0.155iteration: 7498\n",
      "train_loss: 0.06216224166568397\n",
      "val_loss: 0.15524999319555666\n",
      "Progress: 75.0% ... Training loss: 0.069 ... Validation loss: 0.193iteration: 7499\n",
      "train_loss: 0.06980443135053888\n",
      "val_loss: 0.19341953676217827\n",
      "Progress: 75.0% ... Training loss: 0.085 ... Validation loss: 0.150iteration: 7500\n",
      "train_loss: 0.08515815833381035\n",
      "val_loss: 0.15006708122058854\n",
      "Progress: 75.0% ... Training loss: 0.073 ... Validation loss: 0.205iteration: 7501\n",
      "train_loss: 0.07397252140553166\n",
      "val_loss: 0.20574990826664588\n",
      "Progress: 75.0% ... Training loss: 0.062 ... Validation loss: 0.156iteration: 7502\n",
      "train_loss: 0.06291857698124279\n",
      "val_loss: 0.15671505916644587\n",
      "Progress: 75.0% ... Training loss: 0.068 ... Validation loss: 0.199iteration: 7503\n",
      "train_loss: 0.06850613503332831\n",
      "val_loss: 0.19917765697952794\n",
      "Progress: 75.0% ... Training loss: 0.063 ... Validation loss: 0.151iteration: 7504\n",
      "train_loss: 0.06397902044494547\n",
      "val_loss: 0.15192518457238746\n",
      "Progress: 75.0% ... Training loss: 0.070 ... Validation loss: 0.199iteration: 7505\n",
      "train_loss: 0.07093859585571065\n",
      "val_loss: 0.19961091924836233\n",
      "Progress: 75.1% ... Training loss: 0.093 ... Validation loss: 0.147iteration: 7506\n",
      "train_loss: 0.09392111632228257\n",
      "val_loss: 0.14702391790396835\n",
      "Progress: 75.1% ... Training loss: 0.109 ... Validation loss: 0.259iteration: 7507\n",
      "train_loss: 0.10916434933555685\n",
      "val_loss: 0.2593809090766272\n",
      "Progress: 75.1% ... Training loss: 0.102 ... Validation loss: 0.149iteration: 7508\n",
      "train_loss: 0.10225511344938679\n",
      "val_loss: 0.14908403705844625\n",
      "Progress: 75.1% ... Training loss: 0.076 ... Validation loss: 0.202iteration: 7509\n",
      "train_loss: 0.07638255585263767\n",
      "val_loss: 0.2023040269175817\n",
      "Progress: 75.1% ... Training loss: 0.077 ... Validation loss: 0.144iteration: 7510\n",
      "train_loss: 0.07712126954260715\n",
      "val_loss: 0.14428555114246294\n",
      "Progress: 75.1% ... Training loss: 0.062 ... Validation loss: 0.186iteration: 7511\n",
      "train_loss: 0.06219552415140054\n",
      "val_loss: 0.18603804736196336\n",
      "Progress: 75.1% ... Training loss: 0.060 ... Validation loss: 0.170iteration: 7512\n",
      "train_loss: 0.06030973099456296\n",
      "val_loss: 0.17004706495296606\n",
      "Progress: 75.1% ... Training loss: 0.060 ... Validation loss: 0.168iteration: 7513\n",
      "train_loss: 0.060429422803705946\n",
      "val_loss: 0.16854292885891592\n",
      "Progress: 75.1% ... Training loss: 0.061 ... Validation loss: 0.156iteration: 7514\n",
      "train_loss: 0.061836125863100326\n",
      "val_loss: 0.1567764687991602\n",
      "Progress: 75.2% ... Training loss: 0.061 ... Validation loss: 0.189iteration: 7515\n",
      "train_loss: 0.06181171174549453\n",
      "val_loss: 0.18982604717815776\n",
      "Progress: 75.2% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 7516\n",
      "train_loss: 0.06118879855011305\n",
      "val_loss: 0.1651537149428287\n",
      "Progress: 75.2% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 7517\n",
      "train_loss: 0.06012726839524593\n",
      "val_loss: 0.1690983977948687\n",
      "Progress: 75.2% ... Training loss: 0.062 ... Validation loss: 0.182iteration: 7518\n",
      "train_loss: 0.06241546009438449\n",
      "val_loss: 0.18202214401650538\n",
      "Progress: 75.2% ... Training loss: 0.072 ... Validation loss: 0.147iteration: 7519\n",
      "train_loss: 0.07275204381460809\n",
      "val_loss: 0.14771994063389338\n",
      "Progress: 75.2% ... Training loss: 0.081 ... Validation loss: 0.201iteration: 7520\n",
      "train_loss: 0.08197786032033891\n",
      "val_loss: 0.20112927603791964\n",
      "Progress: 75.2% ... Training loss: 0.089 ... Validation loss: 0.146iteration: 7521\n",
      "train_loss: 0.08924069320307211\n",
      "val_loss: 0.14699120669299665\n",
      "Progress: 75.2% ... Training loss: 0.092 ... Validation loss: 0.242iteration: 7522\n",
      "train_loss: 0.09283629623627222\n",
      "val_loss: 0.24200876548236425\n",
      "Progress: 75.2% ... Training loss: 0.089 ... Validation loss: 0.155iteration: 7523\n",
      "train_loss: 0.08964982133158826\n",
      "val_loss: 0.15584319109591754\n",
      "Progress: 75.2% ... Training loss: 0.090 ... Validation loss: 0.226iteration: 7524\n",
      "train_loss: 0.09086359561252161\n",
      "val_loss: 0.22646225040800666\n",
      "Progress: 75.2% ... Training loss: 0.094 ... Validation loss: 0.152iteration: 7525\n",
      "train_loss: 0.09472335618165982\n",
      "val_loss: 0.15230059736266227\n",
      "Progress: 75.3% ... Training loss: 0.075 ... Validation loss: 0.197iteration: 7526\n",
      "train_loss: 0.07538270028654678\n",
      "val_loss: 0.19785320630702377\n",
      "Progress: 75.3% ... Training loss: 0.070 ... Validation loss: 0.148iteration: 7527\n",
      "train_loss: 0.07019961019725565\n",
      "val_loss: 0.14862554499772604\n",
      "Progress: 75.3% ... Training loss: 0.072 ... Validation loss: 0.196iteration: 7528\n",
      "train_loss: 0.07262920172984942\n",
      "val_loss: 0.19607671025751536\n",
      "Progress: 75.3% ... Training loss: 0.068 ... Validation loss: 0.146iteration: 7529\n",
      "train_loss: 0.06827448939377971\n",
      "val_loss: 0.14652209237203578\n",
      "Progress: 75.3% ... Training loss: 0.065 ... Validation loss: 0.167iteration: 7530\n",
      "train_loss: 0.06559676528847373\n",
      "val_loss: 0.16741415031226675\n",
      "Progress: 75.3% ... Training loss: 0.067 ... Validation loss: 0.148iteration: 7531\n",
      "train_loss: 0.06784595775632138\n",
      "val_loss: 0.14815744409710194\n",
      "Progress: 75.3% ... Training loss: 0.068 ... Validation loss: 0.193iteration: 7532\n",
      "train_loss: 0.06873276013694816\n",
      "val_loss: 0.19303787001021394\n",
      "Progress: 75.3% ... Training loss: 0.066 ... Validation loss: 0.147iteration: 7533\n",
      "train_loss: 0.06644512091985882\n",
      "val_loss: 0.14792844645014958\n",
      "Progress: 75.3% ... Training loss: 0.069 ... Validation loss: 0.171iteration: 7534\n",
      "train_loss: 0.06943120343463713\n",
      "val_loss: 0.17123004619345344\n",
      "Progress: 75.3% ... Training loss: 0.062 ... Validation loss: 0.161iteration: 7535\n",
      "train_loss: 0.06270623523892346\n",
      "val_loss: 0.1617760309450593\n",
      "Progress: 75.4% ... Training loss: 0.073 ... Validation loss: 0.213iteration: 7536\n",
      "train_loss: 0.07363140298613051\n",
      "val_loss: 0.21367343365568117\n",
      "Progress: 75.4% ... Training loss: 0.071 ... Validation loss: 0.157iteration: 7537\n",
      "train_loss: 0.07175409578310175\n",
      "val_loss: 0.15778733270262227\n",
      "Progress: 75.4% ... Training loss: 0.091 ... Validation loss: 0.237iteration: 7538\n",
      "train_loss: 0.09176010621717179\n",
      "val_loss: 0.23797035158363056\n",
      "Progress: 75.4% ... Training loss: 0.099 ... Validation loss: 0.155iteration: 7539\n",
      "train_loss: 0.09944394236739207\n",
      "val_loss: 0.15561166036979954\n",
      "Progress: 75.4% ... Training loss: 0.092 ... Validation loss: 0.253iteration: 7540\n",
      "train_loss: 0.09276460337205611\n",
      "val_loss: 0.2537642996562521\n",
      "Progress: 75.4% ... Training loss: 0.069 ... Validation loss: 0.151iteration: 7541\n",
      "train_loss: 0.06940287441748759\n",
      "val_loss: 0.1510395181868906\n",
      "Progress: 75.4% ... Training loss: 0.070 ... Validation loss: 0.198iteration: 7542\n",
      "train_loss: 0.07081047353561154\n",
      "val_loss: 0.19847394705861704\n",
      "Progress: 75.4% ... Training loss: 0.070 ... Validation loss: 0.158iteration: 7543\n",
      "train_loss: 0.0700501576177833\n",
      "val_loss: 0.1583158135810087\n",
      "Progress: 75.4% ... Training loss: 0.078 ... Validation loss: 0.207iteration: 7544\n",
      "train_loss: 0.07837053353675595\n",
      "val_loss: 0.20753480009368858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 75.5% ... Training loss: 0.092 ... Validation loss: 0.151iteration: 7545\n",
      "train_loss: 0.09213304847074401\n",
      "val_loss: 0.1516130704600316\n",
      "Progress: 75.5% ... Training loss: 0.110 ... Validation loss: 0.255iteration: 7546\n",
      "train_loss: 0.11099654475979523\n",
      "val_loss: 0.25501151022211865\n",
      "Progress: 75.5% ... Training loss: 0.120 ... Validation loss: 0.161iteration: 7547\n",
      "train_loss: 0.12007524806831164\n",
      "val_loss: 0.16123080348096192\n",
      "Progress: 75.5% ... Training loss: 0.084 ... Validation loss: 0.213iteration: 7548\n",
      "train_loss: 0.0845682534967406\n",
      "val_loss: 0.21395975208335954\n",
      "Progress: 75.5% ... Training loss: 0.092 ... Validation loss: 0.151iteration: 7549\n",
      "train_loss: 0.09204896328542832\n",
      "val_loss: 0.15109608492976725\n",
      "Progress: 75.5% ... Training loss: 0.089 ... Validation loss: 0.212iteration: 7550\n",
      "train_loss: 0.08973445221133011\n",
      "val_loss: 0.21275586447362826\n",
      "Progress: 75.5% ... Training loss: 0.096 ... Validation loss: 0.152iteration: 7551\n",
      "train_loss: 0.09694077268375753\n",
      "val_loss: 0.15238406989433928\n",
      "Progress: 75.5% ... Training loss: 0.113 ... Validation loss: 0.238iteration: 7552\n",
      "train_loss: 0.11385422523268708\n",
      "val_loss: 0.23860888779483524\n",
      "Progress: 75.5% ... Training loss: 0.098 ... Validation loss: 0.156iteration: 7553\n",
      "train_loss: 0.0988803052258679\n",
      "val_loss: 0.15699655095448783\n",
      "Progress: 75.5% ... Training loss: 0.094 ... Validation loss: 0.216iteration: 7554\n",
      "train_loss: 0.09416626683624205\n",
      "val_loss: 0.21654506448529492\n",
      "Progress: 75.5% ... Training loss: 0.093 ... Validation loss: 0.155iteration: 7555\n",
      "train_loss: 0.09330284649132613\n",
      "val_loss: 0.15587802570881576\n",
      "Progress: 75.6% ... Training loss: 0.070 ... Validation loss: 0.191iteration: 7556\n",
      "train_loss: 0.07052233010512682\n",
      "val_loss: 0.1917839478301573\n",
      "Progress: 75.6% ... Training loss: 0.069 ... Validation loss: 0.144iteration: 7557\n",
      "train_loss: 0.06984983809727041\n",
      "val_loss: 0.14475719107744162\n",
      "Progress: 75.6% ... Training loss: 0.061 ... Validation loss: 0.172iteration: 7558\n",
      "train_loss: 0.06107191501183799\n",
      "val_loss: 0.17240899364347523\n",
      "Progress: 75.6% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7559\n",
      "train_loss: 0.0608566257491389\n",
      "val_loss: 0.16411813383294493\n",
      "Progress: 75.6% ... Training loss: 0.070 ... Validation loss: 0.144iteration: 7560\n",
      "train_loss: 0.07033434221253879\n",
      "val_loss: 0.14465981381830062\n",
      "Progress: 75.6% ... Training loss: 0.071 ... Validation loss: 0.198iteration: 7561\n",
      "train_loss: 0.07119513749952991\n",
      "val_loss: 0.19857448447583462\n",
      "Progress: 75.6% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 7562\n",
      "train_loss: 0.07086966028291249\n",
      "val_loss: 0.15025963673660933\n",
      "Progress: 75.6% ... Training loss: 0.072 ... Validation loss: 0.197iteration: 7563\n",
      "train_loss: 0.07286352374547825\n",
      "val_loss: 0.1976768160569383\n",
      "Progress: 75.6% ... Training loss: 0.062 ... Validation loss: 0.145iteration: 7564\n",
      "train_loss: 0.06207261517639351\n",
      "val_loss: 0.1457906562426841\n",
      "Progress: 75.7% ... Training loss: 0.059 ... Validation loss: 0.166iteration: 7565\n",
      "train_loss: 0.05994580341410366\n",
      "val_loss: 0.16616571398300042\n",
      "Progress: 75.7% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 7566\n",
      "train_loss: 0.06144979198525089\n",
      "val_loss: 0.17362640834258708\n",
      "Progress: 75.7% ... Training loss: 0.059 ... Validation loss: 0.150iteration: 7567\n",
      "train_loss: 0.05998968079640109\n",
      "val_loss: 0.15061255542963084\n",
      "Progress: 75.7% ... Training loss: 0.063 ... Validation loss: 0.172iteration: 7568\n",
      "train_loss: 0.06322282375104422\n",
      "val_loss: 0.1725149046683785\n",
      "Progress: 75.7% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 7569\n",
      "train_loss: 0.06040191793500985\n",
      "val_loss: 0.1497704833290647\n",
      "Progress: 75.7% ... Training loss: 0.060 ... Validation loss: 0.153iteration: 7570\n",
      "train_loss: 0.06027623359113954\n",
      "val_loss: 0.15317136485789803\n",
      "Progress: 75.7% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 7571\n",
      "train_loss: 0.06087111995579391\n",
      "val_loss: 0.1527336386195839\n",
      "Progress: 75.7% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 7572\n",
      "train_loss: 0.06382683398780385\n",
      "val_loss: 0.1575803434532555\n",
      "Progress: 75.7% ... Training loss: 0.062 ... Validation loss: 0.165iteration: 7573\n",
      "train_loss: 0.06224141896177724\n",
      "val_loss: 0.16530839622004856\n",
      "Progress: 75.7% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 7574\n",
      "train_loss: 0.059523386493423015\n",
      "val_loss: 0.15751435798022204\n",
      "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.151iteration: 7575\n",
      "train_loss: 0.06209619983748317\n",
      "val_loss: 0.15157872503800646\n",
      "Progress: 75.8% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7576\n",
      "train_loss: 0.05946866109132294\n",
      "val_loss: 0.16474222340405614\n",
      "Progress: 75.8% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 7577\n",
      "train_loss: 0.059358127523713985\n",
      "val_loss: 0.15676619947938053\n",
      "Progress: 75.8% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7578\n",
      "train_loss: 0.06197144862628713\n",
      "val_loss: 0.16042541939582808\n",
      "Progress: 75.8% ... Training loss: 0.074 ... Validation loss: 0.147iteration: 7579\n",
      "train_loss: 0.07455167205880525\n",
      "val_loss: 0.14703523808983102\n",
      "Progress: 75.8% ... Training loss: 0.071 ... Validation loss: 0.196iteration: 7580\n",
      "train_loss: 0.07167043018169292\n",
      "val_loss: 0.19621254351713943\n",
      "Progress: 75.8% ... Training loss: 0.065 ... Validation loss: 0.148iteration: 7581\n",
      "train_loss: 0.06566195166840552\n",
      "val_loss: 0.1489001323529173\n",
      "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.158iteration: 7582\n",
      "train_loss: 0.06256936398876566\n",
      "val_loss: 0.15885238264774534\n",
      "Progress: 75.8% ... Training loss: 0.059 ... Validation loss: 0.158iteration: 7583\n",
      "train_loss: 0.05930877431601523\n",
      "val_loss: 0.1586101318929132\n",
      "Progress: 75.8% ... Training loss: 0.060 ... Validation loss: 0.154iteration: 7584\n",
      "train_loss: 0.06012678501391694\n",
      "val_loss: 0.1543002188139396\n",
      "Progress: 75.8% ... Training loss: 0.061 ... Validation loss: 0.145iteration: 7585\n",
      "train_loss: 0.06110110120692934\n",
      "val_loss: 0.14538969031521418\n",
      "Progress: 75.9% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 7586\n",
      "train_loss: 0.05917012886307009\n",
      "val_loss: 0.1572245284856873\n",
      "Progress: 75.9% ... Training loss: 0.061 ... Validation loss: 0.150iteration: 7587\n",
      "train_loss: 0.061653274121384966\n",
      "val_loss: 0.15035537478774638\n",
      "Progress: 75.9% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 7588\n",
      "train_loss: 0.06001566663136884\n",
      "val_loss: 0.1578763055960585\n",
      "Progress: 75.9% ... Training loss: 0.061 ... Validation loss: 0.176iteration: 7589\n",
      "train_loss: 0.061592263733975174\n",
      "val_loss: 0.1768366932137753\n",
      "Progress: 75.9% ... Training loss: 0.061 ... Validation loss: 0.167iteration: 7590\n",
      "train_loss: 0.061118466482947977\n",
      "val_loss: 0.1670374723952392\n",
      "Progress: 75.9% ... Training loss: 0.060 ... Validation loss: 0.158iteration: 7591\n",
      "train_loss: 0.060051762494031415\n",
      "val_loss: 0.1581413686117615\n",
      "Progress: 75.9% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 7592\n",
      "train_loss: 0.060569870610893\n",
      "val_loss: 0.15944138904109725\n",
      "Progress: 75.9% ... Training loss: 0.062 ... Validation loss: 0.145iteration: 7593\n",
      "train_loss: 0.06278550021936781\n",
      "val_loss: 0.14575599347515067\n",
      "Progress: 75.9% ... Training loss: 0.061 ... Validation loss: 0.158iteration: 7594\n",
      "train_loss: 0.061684746778369534\n",
      "val_loss: 0.1589509414725379\n",
      "Progress: 76.0% ... Training loss: 0.068 ... Validation loss: 0.179iteration: 7595\n",
      "train_loss: 0.06829473185051908\n",
      "val_loss: 0.17972074949631983\n",
      "Progress: 76.0% ... Training loss: 0.070 ... Validation loss: 0.145iteration: 7596\n",
      "train_loss: 0.07014088452022935\n",
      "val_loss: 0.1451037409321807\n",
      "Progress: 76.0% ... Training loss: 0.093 ... Validation loss: 0.216iteration: 7597\n",
      "train_loss: 0.09348024518513877\n",
      "val_loss: 0.21611021088863028\n",
      "Progress: 76.0% ... Training loss: 0.097 ... Validation loss: 0.147iteration: 7598\n",
      "train_loss: 0.0977889038677057\n",
      "val_loss: 0.14777178045711115\n",
      "Progress: 76.0% ... Training loss: 0.077 ... Validation loss: 0.188iteration: 7599\n",
      "train_loss: 0.07702480557074667\n",
      "val_loss: 0.18896458339080568\n",
      "Progress: 76.0% ... Training loss: 0.069 ... Validation loss: 0.143iteration: 7600\n",
      "train_loss: 0.06912124788397846\n",
      "val_loss: 0.14358577831459482\n",
      "Progress: 76.0% ... Training loss: 0.071 ... Validation loss: 0.189iteration: 7601\n",
      "train_loss: 0.0712228318589088\n",
      "val_loss: 0.1897090881662972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 76.0% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 7602\n",
      "train_loss: 0.06179024026202816\n",
      "val_loss: 0.15309269814418486\n",
      "Progress: 76.0% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 7603\n",
      "train_loss: 0.06154702242614458\n",
      "val_loss: 0.15951167962680005\n",
      "Progress: 76.0% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 7604\n",
      "train_loss: 0.06054783717781061\n",
      "val_loss: 0.15637344755624136\n",
      "Progress: 76.0% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 7605\n",
      "train_loss: 0.06133558819310913\n",
      "val_loss: 0.1739111928108349\n",
      "Progress: 76.1% ... Training loss: 0.073 ... Validation loss: 0.149iteration: 7606\n",
      "train_loss: 0.0734722950999712\n",
      "val_loss: 0.1496458852307738\n",
      "Progress: 76.1% ... Training loss: 0.074 ... Validation loss: 0.213iteration: 7607\n",
      "train_loss: 0.07430177717010455\n",
      "val_loss: 0.2136529912878438\n",
      "Progress: 76.1% ... Training loss: 0.073 ... Validation loss: 0.150iteration: 7608\n",
      "train_loss: 0.0738622683543426\n",
      "val_loss: 0.15088970717717498\n",
      "Progress: 76.1% ... Training loss: 0.062 ... Validation loss: 0.186iteration: 7609\n",
      "train_loss: 0.06258693116566741\n",
      "val_loss: 0.18617341354480377\n",
      "Progress: 76.1% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 7610\n",
      "train_loss: 0.05973609927351236\n",
      "val_loss: 0.16148428058669928\n",
      "Progress: 76.1% ... Training loss: 0.063 ... Validation loss: 0.181iteration: 7611\n",
      "train_loss: 0.0638329009627641\n",
      "val_loss: 0.18123556071813846\n",
      "Progress: 76.1% ... Training loss: 0.061 ... Validation loss: 0.164iteration: 7612\n",
      "train_loss: 0.06151518760659241\n",
      "val_loss: 0.16407622014850679\n",
      "Progress: 76.1% ... Training loss: 0.062 ... Validation loss: 0.173iteration: 7613\n",
      "train_loss: 0.06269251055728796\n",
      "val_loss: 0.1738610559466087\n",
      "Progress: 76.1% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7614\n",
      "train_loss: 0.06012804312406823\n",
      "val_loss: 0.16232154311104252\n",
      "Progress: 76.2% ... Training loss: 0.060 ... Validation loss: 0.160iteration: 7615\n",
      "train_loss: 0.06049948713948293\n",
      "val_loss: 0.16041144504137614\n",
      "Progress: 76.2% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 7616\n",
      "train_loss: 0.061195202453685184\n",
      "val_loss: 0.17055671482195933\n",
      "Progress: 76.2% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7617\n",
      "train_loss: 0.06056149999182375\n",
      "val_loss: 0.16226659573293062\n",
      "Progress: 76.2% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 7618\n",
      "train_loss: 0.06082208880132474\n",
      "val_loss: 0.15605746855625383\n",
      "Progress: 76.2% ... Training loss: 0.062 ... Validation loss: 0.180iteration: 7619\n",
      "train_loss: 0.06251233764829671\n",
      "val_loss: 0.18015349521623883\n",
      "Progress: 76.2% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 7620\n",
      "train_loss: 0.06077628990445617\n",
      "val_loss: 0.14909485696201957\n",
      "Progress: 76.2% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 7621\n",
      "train_loss: 0.06232818621380678\n",
      "val_loss: 0.183605670647397\n",
      "Progress: 76.2% ... Training loss: 0.065 ... Validation loss: 0.151iteration: 7622\n",
      "train_loss: 0.06549884689525032\n",
      "val_loss: 0.15157117242878343\n",
      "Progress: 76.2% ... Training loss: 0.062 ... Validation loss: 0.176iteration: 7623\n",
      "train_loss: 0.06251812139203307\n",
      "val_loss: 0.17685547323607642\n",
      "Progress: 76.2% ... Training loss: 0.065 ... Validation loss: 0.145iteration: 7624\n",
      "train_loss: 0.06598876830358333\n",
      "val_loss: 0.14536965139989766\n",
      "Progress: 76.2% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 7625\n",
      "train_loss: 0.06688566602421459\n",
      "val_loss: 0.1716631880431694\n",
      "Progress: 76.3% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 7626\n",
      "train_loss: 0.06043172047547842\n",
      "val_loss: 0.15607404074783765\n",
      "Progress: 76.3% ... Training loss: 0.059 ... Validation loss: 0.158iteration: 7627\n",
      "train_loss: 0.05974143465487566\n",
      "val_loss: 0.15898470766245007\n",
      "Progress: 76.3% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 7628\n",
      "train_loss: 0.05953924250142472\n",
      "val_loss: 0.1680241819467999\n",
      "Progress: 76.3% ... Training loss: 0.059 ... Validation loss: 0.172iteration: 7629\n",
      "train_loss: 0.059446724299751404\n",
      "val_loss: 0.17203475695533246\n",
      "Progress: 76.3% ... Training loss: 0.061 ... Validation loss: 0.156iteration: 7630\n",
      "train_loss: 0.06172333949089202\n",
      "val_loss: 0.1568589399294898\n",
      "Progress: 76.3% ... Training loss: 0.062 ... Validation loss: 0.184iteration: 7631\n",
      "train_loss: 0.0629178758019682\n",
      "val_loss: 0.18470683332049928\n",
      "Progress: 76.3% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 7632\n",
      "train_loss: 0.06123420807277442\n",
      "val_loss: 0.15292973710086516\n",
      "Progress: 76.3% ... Training loss: 0.061 ... Validation loss: 0.191iteration: 7633\n",
      "train_loss: 0.06179850447056082\n",
      "val_loss: 0.1913051745408451\n",
      "Progress: 76.3% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 7634\n",
      "train_loss: 0.058983558580923814\n",
      "val_loss: 0.1647928671162545\n",
      "Progress: 76.3% ... Training loss: 0.064 ... Validation loss: 0.150iteration: 7635\n",
      "train_loss: 0.0642367390600286\n",
      "val_loss: 0.15080299069292075\n",
      "Progress: 76.4% ... Training loss: 0.077 ... Validation loss: 0.213iteration: 7636\n",
      "train_loss: 0.07798294439865021\n",
      "val_loss: 0.21373624993551646\n",
      "Progress: 76.4% ... Training loss: 0.075 ... Validation loss: 0.142iteration: 7637\n",
      "train_loss: 0.07594492836182866\n",
      "val_loss: 0.1429618417639296\n",
      "Progress: 76.4% ... Training loss: 0.073 ... Validation loss: 0.216iteration: 7638\n",
      "train_loss: 0.07389880057542274\n",
      "val_loss: 0.21698753002542945\n",
      "Progress: 76.4% ... Training loss: 0.081 ... Validation loss: 0.146iteration: 7639\n",
      "train_loss: 0.08108766443883664\n",
      "val_loss: 0.14687518346120693\n",
      "Progress: 76.4% ... Training loss: 0.097 ... Validation loss: 0.242iteration: 7640\n",
      "train_loss: 0.09759771663118529\n",
      "val_loss: 0.24292881696635787\n",
      "Progress: 76.4% ... Training loss: 0.095 ... Validation loss: 0.153iteration: 7641\n",
      "train_loss: 0.09534170150137745\n",
      "val_loss: 0.15318450182113555\n",
      "Progress: 76.4% ... Training loss: 0.072 ... Validation loss: 0.204iteration: 7642\n",
      "train_loss: 0.07264983168727325\n",
      "val_loss: 0.20470162507406028\n",
      "Progress: 76.4% ... Training loss: 0.083 ... Validation loss: 0.150iteration: 7643\n",
      "train_loss: 0.0831620220016023\n",
      "val_loss: 0.15099989618601828\n",
      "Progress: 76.4% ... Training loss: 0.064 ... Validation loss: 0.181iteration: 7644\n",
      "train_loss: 0.06429614741952973\n",
      "val_loss: 0.1811279118794203\n",
      "Progress: 76.5% ... Training loss: 0.063 ... Validation loss: 0.153iteration: 7645\n",
      "train_loss: 0.06318235577691678\n",
      "val_loss: 0.15312028387987395\n",
      "Progress: 76.5% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 7646\n",
      "train_loss: 0.06286581009267614\n",
      "val_loss: 0.1720854767658708\n",
      "Progress: 76.5% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 7647\n",
      "train_loss: 0.059524905546953055\n",
      "val_loss: 0.15581588488329573\n",
      "Progress: 76.5% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 7648\n",
      "train_loss: 0.05970509818655496\n",
      "val_loss: 0.151323810687267\n",
      "Progress: 76.5% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 7649\n",
      "train_loss: 0.059984641790899636\n",
      "val_loss: 0.15743532312667105\n",
      "Progress: 76.5% ... Training loss: 0.064 ... Validation loss: 0.146iteration: 7650\n",
      "train_loss: 0.06457330185708149\n",
      "val_loss: 0.14628874524589933\n",
      "Progress: 76.5% ... Training loss: 0.077 ... Validation loss: 0.184iteration: 7651\n",
      "train_loss: 0.07725231207519584\n",
      "val_loss: 0.18423383422388365\n",
      "Progress: 76.5% ... Training loss: 0.083 ... Validation loss: 0.148iteration: 7652\n",
      "train_loss: 0.08380609611181343\n",
      "val_loss: 0.148475945595429\n",
      "Progress: 76.5% ... Training loss: 0.090 ... Validation loss: 0.212iteration: 7653\n",
      "train_loss: 0.09051112990168941\n",
      "val_loss: 0.21215704491481494\n",
      "Progress: 76.5% ... Training loss: 0.078 ... Validation loss: 0.146iteration: 7654\n",
      "train_loss: 0.07887292587738698\n",
      "val_loss: 0.14606540192781023\n",
      "Progress: 76.5% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 7655\n",
      "train_loss: 0.06014378945883705\n",
      "val_loss: 0.16310964681972184\n",
      "Progress: 76.6% ... Training loss: 0.060 ... Validation loss: 0.148iteration: 7656\n",
      "train_loss: 0.0607037240006709\n",
      "val_loss: 0.14836670142883165\n",
      "Progress: 76.6% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 7657\n",
      "train_loss: 0.060011824826958764\n",
      "val_loss: 0.15996706681161632\n",
      "Progress: 76.6% ... Training loss: 0.060 ... Validation loss: 0.153iteration: 7658\n",
      "train_loss: 0.060349807543323426\n",
      "val_loss: 0.1531328240935852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 76.6% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 7659\n",
      "train_loss: 0.05986779273850952\n",
      "val_loss: 0.16112767433236858\n",
      "Progress: 76.6% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 7660\n",
      "train_loss: 0.06073076457033148\n",
      "val_loss: 0.14920623696632362\n",
      "Progress: 76.6% ... Training loss: 0.063 ... Validation loss: 0.167iteration: 7661\n",
      "train_loss: 0.0630976748999672\n",
      "val_loss: 0.1672504743829395\n",
      "Progress: 76.6% ... Training loss: 0.061 ... Validation loss: 0.148iteration: 7662\n",
      "train_loss: 0.06126353820452325\n",
      "val_loss: 0.1481965333463581\n",
      "Progress: 76.6% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 7663\n",
      "train_loss: 0.05925802160660316\n",
      "val_loss: 0.16077212357802384\n",
      "Progress: 76.6% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 7664\n",
      "train_loss: 0.05933840346341097\n",
      "val_loss: 0.1590362846134645\n",
      "Progress: 76.7% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 7665\n",
      "train_loss: 0.061172160735128094\n",
      "val_loss: 0.16515406847236697\n",
      "Progress: 76.7% ... Training loss: 0.067 ... Validation loss: 0.145iteration: 7666\n",
      "train_loss: 0.06746460865005229\n",
      "val_loss: 0.14500078357096416\n",
      "Progress: 76.7% ... Training loss: 0.061 ... Validation loss: 0.156iteration: 7667\n",
      "train_loss: 0.06163928677600558\n",
      "val_loss: 0.15686073500995168\n",
      "Progress: 76.7% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 7668\n",
      "train_loss: 0.061633472262126336\n",
      "val_loss: 0.14985718028303935\n",
      "Progress: 76.7% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 7669\n",
      "train_loss: 0.060252539314379655\n",
      "val_loss: 0.1652627729626273\n",
      "Progress: 76.7% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 7670\n",
      "train_loss: 0.059066608595163946\n",
      "val_loss: 0.1556795348794114\n",
      "Progress: 76.7% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 7671\n",
      "train_loss: 0.05913161827525232\n",
      "val_loss: 0.1560644606667259\n",
      "Progress: 76.7% ... Training loss: 0.061 ... Validation loss: 0.158iteration: 7672\n",
      "train_loss: 0.0619083373218376\n",
      "val_loss: 0.15878331482359298\n",
      "Progress: 76.7% ... Training loss: 0.064 ... Validation loss: 0.151iteration: 7673\n",
      "train_loss: 0.06457740597357886\n",
      "val_loss: 0.1513102342263035\n",
      "Progress: 76.7% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 7674\n",
      "train_loss: 0.05981092525123442\n",
      "val_loss: 0.1560850005474754\n",
      "Progress: 76.8% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 7675\n",
      "train_loss: 0.061112081281170315\n",
      "val_loss: 0.15969428129068577\n",
      "Progress: 76.8% ... Training loss: 0.060 ... Validation loss: 0.148iteration: 7676\n",
      "train_loss: 0.06076006930840089\n",
      "val_loss: 0.1489592512037009\n",
      "Progress: 76.8% ... Training loss: 0.063 ... Validation loss: 0.153iteration: 7677\n",
      "train_loss: 0.06343260213866514\n",
      "val_loss: 0.1530624222712898\n",
      "Progress: 76.8% ... Training loss: 0.062 ... Validation loss: 0.144iteration: 7678\n",
      "train_loss: 0.06238022576617\n",
      "val_loss: 0.1445974185535306\n",
      "Progress: 76.8% ... Training loss: 0.077 ... Validation loss: 0.206iteration: 7679\n",
      "train_loss: 0.07743022152397093\n",
      "val_loss: 0.20622574002476446\n",
      "Progress: 76.8% ... Training loss: 0.106 ... Validation loss: 0.158iteration: 7680\n",
      "train_loss: 0.10601726830469278\n",
      "val_loss: 0.15849007450200361\n",
      "Progress: 76.8% ... Training loss: 0.092 ... Validation loss: 0.257iteration: 7681\n",
      "train_loss: 0.09262484231967312\n",
      "val_loss: 0.2570627267445933\n",
      "Progress: 76.8% ... Training loss: 0.093 ... Validation loss: 0.150iteration: 7682\n",
      "train_loss: 0.09399234289927545\n",
      "val_loss: 0.1509176217505214\n",
      "Progress: 76.8% ... Training loss: 0.111 ... Validation loss: 0.240iteration: 7683\n",
      "train_loss: 0.11162629338864091\n",
      "val_loss: 0.24064111434133154\n",
      "Progress: 76.8% ... Training loss: 0.131 ... Validation loss: 0.168iteration: 7684\n",
      "train_loss: 0.1315644872332788\n",
      "val_loss: 0.16877666104753877\n",
      "Progress: 76.8% ... Training loss: 0.134 ... Validation loss: 0.270iteration: 7685\n",
      "train_loss: 0.13419295728298206\n",
      "val_loss: 0.27003487210166244\n",
      "Progress: 76.9% ... Training loss: 0.135 ... Validation loss: 0.181iteration: 7686\n",
      "train_loss: 0.13555254953359785\n",
      "val_loss: 0.18188265354760738\n",
      "Progress: 76.9% ... Training loss: 0.118 ... Validation loss: 0.265iteration: 7687\n",
      "train_loss: 0.11832362090514588\n",
      "val_loss: 0.26568584755490915\n",
      "Progress: 76.9% ... Training loss: 0.117 ... Validation loss: 0.171iteration: 7688\n",
      "train_loss: 0.11735373087952658\n",
      "val_loss: 0.17167200962663107\n",
      "Progress: 76.9% ... Training loss: 0.106 ... Validation loss: 0.256iteration: 7689\n",
      "train_loss: 0.10685579507753619\n",
      "val_loss: 0.25635464316207124\n",
      "Progress: 76.9% ... Training loss: 0.092 ... Validation loss: 0.159iteration: 7690\n",
      "train_loss: 0.0921933251127802\n",
      "val_loss: 0.1596505552107536\n",
      "Progress: 76.9% ... Training loss: 0.097 ... Validation loss: 0.231iteration: 7691\n",
      "train_loss: 0.097326989847586\n",
      "val_loss: 0.23145546206426634\n",
      "Progress: 76.9% ... Training loss: 0.082 ... Validation loss: 0.160iteration: 7692\n",
      "train_loss: 0.08286131396115208\n",
      "val_loss: 0.16082703478871377\n",
      "Progress: 76.9% ... Training loss: 0.091 ... Validation loss: 0.261iteration: 7693\n",
      "train_loss: 0.09101704105993003\n",
      "val_loss: 0.2612698092782619\n",
      "Progress: 76.9% ... Training loss: 0.105 ... Validation loss: 0.160iteration: 7694\n",
      "train_loss: 0.10572828974688019\n",
      "val_loss: 0.1603198329485445\n",
      "Progress: 77.0% ... Training loss: 0.094 ... Validation loss: 0.241iteration: 7695\n",
      "train_loss: 0.09433774639047374\n",
      "val_loss: 0.24183632536621502\n",
      "Progress: 77.0% ... Training loss: 0.085 ... Validation loss: 0.153iteration: 7696\n",
      "train_loss: 0.08574898676087303\n",
      "val_loss: 0.15322739533802146\n",
      "Progress: 77.0% ... Training loss: 0.086 ... Validation loss: 0.232iteration: 7697\n",
      "train_loss: 0.08615669969435581\n",
      "val_loss: 0.23239984345843254\n",
      "Progress: 77.0% ... Training loss: 0.073 ... Validation loss: 0.151iteration: 7698\n",
      "train_loss: 0.0738918736833666\n",
      "val_loss: 0.15167690889013402\n",
      "Progress: 77.0% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 7699\n",
      "train_loss: 0.06362817488457372\n",
      "val_loss: 0.18273296565979608\n",
      "Progress: 77.0% ... Training loss: 0.074 ... Validation loss: 0.150iteration: 7700\n",
      "train_loss: 0.0749115658167516\n",
      "val_loss: 0.15096628911205187\n",
      "Progress: 77.0% ... Training loss: 0.062 ... Validation loss: 0.186iteration: 7701\n",
      "train_loss: 0.06221505907618165\n",
      "val_loss: 0.1866014014836634\n",
      "Progress: 77.0% ... Training loss: 0.068 ... Validation loss: 0.149iteration: 7702\n",
      "train_loss: 0.06821896394979202\n",
      "val_loss: 0.14926459230644223\n",
      "Progress: 77.0% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 7703\n",
      "train_loss: 0.06285668174621634\n",
      "val_loss: 0.18385259092810868\n",
      "Progress: 77.0% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7704\n",
      "train_loss: 0.061561559116554705\n",
      "val_loss: 0.16091610038835305\n",
      "Progress: 77.0% ... Training loss: 0.070 ... Validation loss: 0.190iteration: 7705\n",
      "train_loss: 0.07010918841728948\n",
      "val_loss: 0.1907728270846126\n",
      "Progress: 77.1% ... Training loss: 0.071 ... Validation loss: 0.159iteration: 7706\n",
      "train_loss: 0.07183560902113997\n",
      "val_loss: 0.1591601064601715\n",
      "Progress: 77.1% ... Training loss: 0.073 ... Validation loss: 0.214iteration: 7707\n",
      "train_loss: 0.07376647584176921\n",
      "val_loss: 0.21480973394189734\n",
      "Progress: 77.1% ... Training loss: 0.077 ... Validation loss: 0.150iteration: 7708\n",
      "train_loss: 0.07705049123238561\n",
      "val_loss: 0.15017736355525113\n",
      "Progress: 77.1% ... Training loss: 0.065 ... Validation loss: 0.191iteration: 7709\n",
      "train_loss: 0.06595876808588223\n",
      "val_loss: 0.19101476145788635\n",
      "Progress: 77.1% ... Training loss: 0.063 ... Validation loss: 0.151iteration: 7710\n",
      "train_loss: 0.06303937648196004\n",
      "val_loss: 0.15157244686634108\n",
      "Progress: 77.1% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7711\n",
      "train_loss: 0.06001648655360723\n",
      "val_loss: 0.16651198733567396\n",
      "Progress: 77.1% ... Training loss: 0.060 ... Validation loss: 0.158iteration: 7712\n",
      "train_loss: 0.06032783959369256\n",
      "val_loss: 0.15836069464237415\n",
      "Progress: 77.1% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 7713\n",
      "train_loss: 0.06064134718640165\n",
      "val_loss: 0.17218195468338088\n",
      "Progress: 77.1% ... Training loss: 0.062 ... Validation loss: 0.153iteration: 7714\n",
      "train_loss: 0.0627298724195819\n",
      "val_loss: 0.15332835384331867\n",
      "Progress: 77.2% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7715\n",
      "train_loss: 0.060612726397090144\n",
      "val_loss: 0.16695408262493958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 77.2% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 7716\n",
      "train_loss: 0.06584390538112816\n",
      "val_loss: 0.1524710309888597\n",
      "Progress: 77.2% ... Training loss: 0.063 ... Validation loss: 0.175iteration: 7717\n",
      "train_loss: 0.0631497172806758\n",
      "val_loss: 0.17597003452326287\n",
      "Progress: 77.2% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 7718\n",
      "train_loss: 0.059624704356223655\n",
      "val_loss: 0.16027880921063303\n",
      "Progress: 77.2% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 7719\n",
      "train_loss: 0.060329714511906565\n",
      "val_loss: 0.16153106498152967\n",
      "Progress: 77.2% ... Training loss: 0.060 ... Validation loss: 0.175iteration: 7720\n",
      "train_loss: 0.06004316394581075\n",
      "val_loss: 0.17564715218692442\n",
      "Progress: 77.2% ... Training loss: 0.059 ... Validation loss: 0.163iteration: 7721\n",
      "train_loss: 0.05973422763702499\n",
      "val_loss: 0.1632029410272086\n",
      "Progress: 77.2% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 7722\n",
      "train_loss: 0.06066641400474878\n",
      "val_loss: 0.1692096279184246\n",
      "Progress: 77.2% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 7723\n",
      "train_loss: 0.05941825865881576\n",
      "val_loss: 0.16884471858203737\n",
      "Progress: 77.2% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7724\n",
      "train_loss: 0.060487257326910064\n",
      "val_loss: 0.1622261786638459\n",
      "Progress: 77.2% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 7725\n",
      "train_loss: 0.059589101356794294\n",
      "val_loss: 0.17157551386711486\n",
      "Progress: 77.3% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 7726\n",
      "train_loss: 0.05983593388447997\n",
      "val_loss: 0.15967801850527\n",
      "Progress: 77.3% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 7727\n",
      "train_loss: 0.05913055954022326\n",
      "val_loss: 0.15626085141886498\n",
      "Progress: 77.3% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 7728\n",
      "train_loss: 0.05984203922348549\n",
      "val_loss: 0.15150632279156218\n",
      "Progress: 77.3% ... Training loss: 0.059 ... Validation loss: 0.153iteration: 7729\n",
      "train_loss: 0.05999835936763484\n",
      "val_loss: 0.15320868889919315\n",
      "Progress: 77.3% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 7730\n",
      "train_loss: 0.06102622822950116\n",
      "val_loss: 0.16528412557976999\n",
      "Progress: 77.3% ... Training loss: 0.061 ... Validation loss: 0.147iteration: 7731\n",
      "train_loss: 0.06172982277587387\n",
      "val_loss: 0.14732720584331413\n",
      "Progress: 77.3% ... Training loss: 0.062 ... Validation loss: 0.168iteration: 7732\n",
      "train_loss: 0.06254723519515924\n",
      "val_loss: 0.1689775591933106\n",
      "Progress: 77.3% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 7733\n",
      "train_loss: 0.06147271206720788\n",
      "val_loss: 0.14911680921560796\n",
      "Progress: 77.3% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 7734\n",
      "train_loss: 0.060279756706354715\n",
      "val_loss: 0.1577657833320309\n",
      "Progress: 77.3% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 7735\n",
      "train_loss: 0.059166522076128734\n",
      "val_loss: 0.15633099242044837\n",
      "Progress: 77.4% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 7736\n",
      "train_loss: 0.06062005621044027\n",
      "val_loss: 0.15668449939731954\n",
      "Progress: 77.4% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7737\n",
      "train_loss: 0.05936759357823419\n",
      "val_loss: 0.1648829822624907\n",
      "Progress: 77.4% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 7738\n",
      "train_loss: 0.06091810767083227\n",
      "val_loss: 0.15265964206184834\n",
      "Progress: 77.4% ... Training loss: 0.066 ... Validation loss: 0.187iteration: 7739\n",
      "train_loss: 0.06614336043999365\n",
      "val_loss: 0.18773101559600802\n",
      "Progress: 77.4% ... Training loss: 0.064 ... Validation loss: 0.150iteration: 7740\n",
      "train_loss: 0.06499836968184695\n",
      "val_loss: 0.1502965362304205\n",
      "Progress: 77.4% ... Training loss: 0.068 ... Validation loss: 0.193iteration: 7741\n",
      "train_loss: 0.06820706458639886\n",
      "val_loss: 0.19324988551571903\n",
      "Progress: 77.4% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 7742\n",
      "train_loss: 0.07096609883933087\n",
      "val_loss: 0.15065462625240406\n",
      "Progress: 77.4% ... Training loss: 0.068 ... Validation loss: 0.185iteration: 7743\n",
      "train_loss: 0.06882800215212934\n",
      "val_loss: 0.1853131136100471\n",
      "Progress: 77.4% ... Training loss: 0.067 ... Validation loss: 0.157iteration: 7744\n",
      "train_loss: 0.06792178818585672\n",
      "val_loss: 0.15726136274193092\n",
      "Progress: 77.5% ... Training loss: 0.064 ... Validation loss: 0.201iteration: 7745\n",
      "train_loss: 0.06488716642171442\n",
      "val_loss: 0.2014496306808235\n",
      "Progress: 77.5% ... Training loss: 0.067 ... Validation loss: 0.149iteration: 7746\n",
      "train_loss: 0.0670548001384254\n",
      "val_loss: 0.1497158887638759\n",
      "Progress: 77.5% ... Training loss: 0.063 ... Validation loss: 0.181iteration: 7747\n",
      "train_loss: 0.06376112159371376\n",
      "val_loss: 0.1817498154975262\n",
      "Progress: 77.5% ... Training loss: 0.075 ... Validation loss: 0.154iteration: 7748\n",
      "train_loss: 0.07545321335782527\n",
      "val_loss: 0.1542902580678647\n",
      "Progress: 77.5% ... Training loss: 0.098 ... Validation loss: 0.218iteration: 7749\n",
      "train_loss: 0.09812325017306166\n",
      "val_loss: 0.21859474801068549\n",
      "Progress: 77.5% ... Training loss: 0.077 ... Validation loss: 0.148iteration: 7750\n",
      "train_loss: 0.07705294146864702\n",
      "val_loss: 0.14880755645241195\n",
      "Progress: 77.5% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 7751\n",
      "train_loss: 0.06318878461543366\n",
      "val_loss: 0.17876171175651598\n",
      "Progress: 77.5% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 7752\n",
      "train_loss: 0.058820911362270675\n",
      "val_loss: 0.16584343918236147\n",
      "Progress: 77.5% ... Training loss: 0.061 ... Validation loss: 0.185iteration: 7753\n",
      "train_loss: 0.06157625086431311\n",
      "val_loss: 0.18566861246709437\n",
      "Progress: 77.5% ... Training loss: 0.062 ... Validation loss: 0.159iteration: 7754\n",
      "train_loss: 0.06285932866706097\n",
      "val_loss: 0.15997339615504644\n",
      "Progress: 77.5% ... Training loss: 0.064 ... Validation loss: 0.184iteration: 7755\n",
      "train_loss: 0.06479004021980163\n",
      "val_loss: 0.1840440297517399\n",
      "Progress: 77.6% ... Training loss: 0.065 ... Validation loss: 0.154iteration: 7756\n",
      "train_loss: 0.06503664791483786\n",
      "val_loss: 0.1546821855589273\n",
      "Progress: 77.6% ... Training loss: 0.062 ... Validation loss: 0.180iteration: 7757\n",
      "train_loss: 0.06251109320993824\n",
      "val_loss: 0.18034885629582503\n",
      "Progress: 77.6% ... Training loss: 0.061 ... Validation loss: 0.155iteration: 7758\n",
      "train_loss: 0.0612120587655505\n",
      "val_loss: 0.1557795247825776\n",
      "Progress: 77.6% ... Training loss: 0.061 ... Validation loss: 0.178iteration: 7759\n",
      "train_loss: 0.06188231416866509\n",
      "val_loss: 0.17814968230298278\n",
      "Progress: 77.6% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 7760\n",
      "train_loss: 0.059524949784167606\n",
      "val_loss: 0.1626041112315482\n",
      "Progress: 77.6% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7761\n",
      "train_loss: 0.05963844538623686\n",
      "val_loss: 0.16461006149170762\n",
      "Progress: 77.6% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 7762\n",
      "train_loss: 0.05914475660871592\n",
      "val_loss: 0.16192993575902645\n",
      "Progress: 77.6% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 7763\n",
      "train_loss: 0.06022604067969169\n",
      "val_loss: 0.15998625275418477\n",
      "Progress: 77.6% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 7764\n",
      "train_loss: 0.060700460735088856\n",
      "val_loss: 0.15921824343888122\n",
      "Progress: 77.7% ... Training loss: 0.062 ... Validation loss: 0.184iteration: 7765\n",
      "train_loss: 0.06250198267844853\n",
      "val_loss: 0.18472206167774743\n",
      "Progress: 77.7% ... Training loss: 0.064 ... Validation loss: 0.153iteration: 7766\n",
      "train_loss: 0.06401354940444835\n",
      "val_loss: 0.15376652500754145\n",
      "Progress: 77.7% ... Training loss: 0.068 ... Validation loss: 0.184iteration: 7767\n",
      "train_loss: 0.06827536964861532\n",
      "val_loss: 0.18412141146295574\n",
      "Progress: 77.7% ... Training loss: 0.075 ... Validation loss: 0.149iteration: 7768\n",
      "train_loss: 0.0755557863694485\n",
      "val_loss: 0.1491878089374305\n",
      "Progress: 77.7% ... Training loss: 0.087 ... Validation loss: 0.217iteration: 7769\n",
      "train_loss: 0.08709500495433366\n",
      "val_loss: 0.21723050481022202\n",
      "Progress: 77.7% ... Training loss: 0.068 ... Validation loss: 0.158iteration: 7770\n",
      "train_loss: 0.06841595963981902\n",
      "val_loss: 0.1586436057064708\n",
      "Progress: 77.7% ... Training loss: 0.082 ... Validation loss: 0.236iteration: 7771\n",
      "train_loss: 0.08226573788876772\n",
      "val_loss: 0.2360229550947408\n",
      "Progress: 77.7% ... Training loss: 0.069 ... Validation loss: 0.156iteration: 7772\n",
      "train_loss: 0.06955929363544444\n",
      "val_loss: 0.15611221747441723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 77.7% ... Training loss: 0.077 ... Validation loss: 0.195iteration: 7773\n",
      "train_loss: 0.07789695646431263\n",
      "val_loss: 0.19511294313230765\n",
      "Progress: 77.7% ... Training loss: 0.066 ... Validation loss: 0.148iteration: 7774\n",
      "train_loss: 0.06624727686946356\n",
      "val_loss: 0.14833862132025066\n",
      "Progress: 77.8% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 7775\n",
      "train_loss: 0.05912855758761149\n",
      "val_loss: 0.1692637719684346\n",
      "Progress: 77.8% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 7776\n",
      "train_loss: 0.060226593070970884\n",
      "val_loss: 0.17262906442060036\n",
      "Progress: 77.8% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 7777\n",
      "train_loss: 0.05967389427687943\n",
      "val_loss: 0.1604154601067263\n",
      "Progress: 77.8% ... Training loss: 0.061 ... Validation loss: 0.187iteration: 7778\n",
      "train_loss: 0.06150814447094503\n",
      "val_loss: 0.187579957168765\n",
      "Progress: 77.8% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 7779\n",
      "train_loss: 0.06005598191327337\n",
      "val_loss: 0.16389968699169105\n",
      "Progress: 77.8% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 7780\n",
      "train_loss: 0.05889229859898426\n",
      "val_loss: 0.16534173388239487\n",
      "Progress: 77.8% ... Training loss: 0.063 ... Validation loss: 0.150iteration: 7781\n",
      "train_loss: 0.06307417408840128\n",
      "val_loss: 0.15021614998503602\n",
      "Progress: 77.8% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 7782\n",
      "train_loss: 0.05984830831561601\n",
      "val_loss: 0.16230139134522278\n",
      "Progress: 77.8% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 7783\n",
      "train_loss: 0.059991903431865196\n",
      "val_loss: 0.1557236450429286\n",
      "Progress: 77.8% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 7784\n",
      "train_loss: 0.06151401312115224\n",
      "val_loss: 0.16810295086318075\n",
      "Progress: 77.8% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 7785\n",
      "train_loss: 0.06013250118808157\n",
      "val_loss: 0.15255511346276382\n",
      "Progress: 77.9% ... Training loss: 0.059 ... Validation loss: 0.172iteration: 7786\n",
      "train_loss: 0.05931088025577808\n",
      "val_loss: 0.1722764608848017\n",
      "Progress: 77.9% ... Training loss: 0.061 ... Validation loss: 0.150iteration: 7787\n",
      "train_loss: 0.061266689434865074\n",
      "val_loss: 0.1506613452588892\n",
      "Progress: 77.9% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7788\n",
      "train_loss: 0.05943890443508385\n",
      "val_loss: 0.164008584444881\n",
      "Progress: 77.9% ... Training loss: 0.065 ... Validation loss: 0.186iteration: 7789\n",
      "train_loss: 0.06521244710089058\n",
      "val_loss: 0.18610047990952575\n",
      "Progress: 77.9% ... Training loss: 0.066 ... Validation loss: 0.156iteration: 7790\n",
      "train_loss: 0.06639171097160382\n",
      "val_loss: 0.1565338698033979\n",
      "Progress: 77.9% ... Training loss: 0.068 ... Validation loss: 0.185iteration: 7791\n",
      "train_loss: 0.06842504142088526\n",
      "val_loss: 0.18565728790295372\n",
      "Progress: 77.9% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 7792\n",
      "train_loss: 0.06031741138597\n",
      "val_loss: 0.16119743868490047\n",
      "Progress: 77.9% ... Training loss: 0.062 ... Validation loss: 0.174iteration: 7793\n",
      "train_loss: 0.06278084856208159\n",
      "val_loss: 0.17431153804390215\n",
      "Progress: 77.9% ... Training loss: 0.086 ... Validation loss: 0.153iteration: 7794\n",
      "train_loss: 0.08660450536414761\n",
      "val_loss: 0.1533383500631714\n",
      "Progress: 78.0% ... Training loss: 0.082 ... Validation loss: 0.210iteration: 7795\n",
      "train_loss: 0.08260884381398563\n",
      "val_loss: 0.21008358792290435\n",
      "Progress: 78.0% ... Training loss: 0.072 ... Validation loss: 0.149iteration: 7796\n",
      "train_loss: 0.07204380493250422\n",
      "val_loss: 0.14900216826998766\n",
      "Progress: 78.0% ... Training loss: 0.076 ... Validation loss: 0.208iteration: 7797\n",
      "train_loss: 0.0768771494806528\n",
      "val_loss: 0.20839117224144843\n",
      "Progress: 78.0% ... Training loss: 0.099 ... Validation loss: 0.161iteration: 7798\n",
      "train_loss: 0.09902148603068321\n",
      "val_loss: 0.16128226859941613\n",
      "Progress: 78.0% ... Training loss: 0.093 ... Validation loss: 0.223iteration: 7799\n",
      "train_loss: 0.09351563491657057\n",
      "val_loss: 0.2238339439129345\n",
      "Progress: 78.0% ... Training loss: 0.083 ... Validation loss: 0.154iteration: 7800\n",
      "train_loss: 0.08384635993483486\n",
      "val_loss: 0.15451095608202617\n",
      "Progress: 78.0% ... Training loss: 0.077 ... Validation loss: 0.200iteration: 7801\n",
      "train_loss: 0.07735292219389195\n",
      "val_loss: 0.2008898693632245\n",
      "Progress: 78.0% ... Training loss: 0.075 ... Validation loss: 0.150iteration: 7802\n",
      "train_loss: 0.0757709396631969\n",
      "val_loss: 0.15031251749092867\n",
      "Progress: 78.0% ... Training loss: 0.074 ... Validation loss: 0.205iteration: 7803\n",
      "train_loss: 0.0742041689720273\n",
      "val_loss: 0.2050351351133741\n",
      "Progress: 78.0% ... Training loss: 0.072 ... Validation loss: 0.150iteration: 7804\n",
      "train_loss: 0.07259971369509716\n",
      "val_loss: 0.15039728203439043\n",
      "Progress: 78.0% ... Training loss: 0.083 ... Validation loss: 0.234iteration: 7805\n",
      "train_loss: 0.08301513671009933\n",
      "val_loss: 0.23424008672333713\n",
      "Progress: 78.1% ... Training loss: 0.085 ... Validation loss: 0.147iteration: 7806\n",
      "train_loss: 0.08500087636740682\n",
      "val_loss: 0.14780948278953782\n",
      "Progress: 78.1% ... Training loss: 0.072 ... Validation loss: 0.200iteration: 7807\n",
      "train_loss: 0.07253355102305425\n",
      "val_loss: 0.20069536598126875\n",
      "Progress: 78.1% ... Training loss: 0.073 ... Validation loss: 0.149iteration: 7808\n",
      "train_loss: 0.07331247179859254\n",
      "val_loss: 0.14972826227887037\n",
      "Progress: 78.1% ... Training loss: 0.090 ... Validation loss: 0.238iteration: 7809\n",
      "train_loss: 0.0907675537502889\n",
      "val_loss: 0.23836564131829777\n",
      "Progress: 78.1% ... Training loss: 0.070 ... Validation loss: 0.144iteration: 7810\n",
      "train_loss: 0.07017563722258979\n",
      "val_loss: 0.14441372631897725\n",
      "Progress: 78.1% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 7811\n",
      "train_loss: 0.0609147066241238\n",
      "val_loss: 0.17203322566823878\n",
      "Progress: 78.1% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 7812\n",
      "train_loss: 0.06118856204797382\n",
      "val_loss: 0.152679542826344\n",
      "Progress: 78.1% ... Training loss: 0.059 ... Validation loss: 0.173iteration: 7813\n",
      "train_loss: 0.059540072314749004\n",
      "val_loss: 0.173291655773356\n",
      "Progress: 78.1% ... Training loss: 0.062 ... Validation loss: 0.168iteration: 7814\n",
      "train_loss: 0.0623700027816274\n",
      "val_loss: 0.1680146628801796\n",
      "Progress: 78.2% ... Training loss: 0.068 ... Validation loss: 0.204iteration: 7815\n",
      "train_loss: 0.06839110812673171\n",
      "val_loss: 0.20426525408089316\n",
      "Progress: 78.2% ... Training loss: 0.062 ... Validation loss: 0.152iteration: 7816\n",
      "train_loss: 0.0624968940518627\n",
      "val_loss: 0.15210622219302244\n",
      "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.163iteration: 7817\n",
      "train_loss: 0.059067553655399355\n",
      "val_loss: 0.16320970552341948\n",
      "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 7818\n",
      "train_loss: 0.059136607885879565\n",
      "val_loss: 0.16254018792908637\n",
      "Progress: 78.2% ... Training loss: 0.060 ... Validation loss: 0.153iteration: 7819\n",
      "train_loss: 0.06055858066166782\n",
      "val_loss: 0.1532005760487252\n",
      "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 7820\n",
      "train_loss: 0.059451528539644916\n",
      "val_loss: 0.15934189699497267\n",
      "Progress: 78.2% ... Training loss: 0.064 ... Validation loss: 0.159iteration: 7821\n",
      "train_loss: 0.06435128842233219\n",
      "val_loss: 0.15902877585434105\n",
      "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.172iteration: 7822\n",
      "train_loss: 0.05991374864391108\n",
      "val_loss: 0.17286345962361757\n",
      "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7823\n",
      "train_loss: 0.05993484785898967\n",
      "val_loss: 0.16488958846041418\n",
      "Progress: 78.2% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 7824\n",
      "train_loss: 0.06004354593657368\n",
      "val_loss: 0.15687613418943258\n",
      "Progress: 78.2% ... Training loss: 0.063 ... Validation loss: 0.171iteration: 7825\n",
      "train_loss: 0.06359523798711976\n",
      "val_loss: 0.17117702445819652\n",
      "Progress: 78.3% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 7826\n",
      "train_loss: 0.05890735763125707\n",
      "val_loss: 0.1591351636670973\n",
      "Progress: 78.3% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 7827\n",
      "train_loss: 0.05878874778285225\n",
      "val_loss: 0.1645085714176399\n",
      "Progress: 78.3% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 7828\n",
      "train_loss: 0.06036593148821923\n",
      "val_loss: 0.1726603954164922\n",
      "Progress: 78.3% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 7829\n",
      "train_loss: 0.061163478095530244\n",
      "val_loss: 0.15965132191599213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 78.3% ... Training loss: 0.066 ... Validation loss: 0.189iteration: 7830\n",
      "train_loss: 0.06601036909223752\n",
      "val_loss: 0.18913050946230062\n",
      "Progress: 78.3% ... Training loss: 0.061 ... Validation loss: 0.150iteration: 7831\n",
      "train_loss: 0.0613460665178044\n",
      "val_loss: 0.15046377676757558\n",
      "Progress: 78.3% ... Training loss: 0.063 ... Validation loss: 0.176iteration: 7832\n",
      "train_loss: 0.0632116305569322\n",
      "val_loss: 0.17609035177782698\n",
      "Progress: 78.3% ... Training loss: 0.059 ... Validation loss: 0.158iteration: 7833\n",
      "train_loss: 0.059715247462827435\n",
      "val_loss: 0.1582858882768207\n",
      "Progress: 78.3% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7834\n",
      "train_loss: 0.05915430668813437\n",
      "val_loss: 0.1642790499381859\n",
      "Progress: 78.3% ... Training loss: 0.060 ... Validation loss: 0.187iteration: 7835\n",
      "train_loss: 0.06042446208928812\n",
      "val_loss: 0.18704760282962093\n",
      "Progress: 78.4% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 7836\n",
      "train_loss: 0.05883464886443514\n",
      "val_loss: 0.1710774037041368\n",
      "Progress: 78.4% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 7837\n",
      "train_loss: 0.05937167593325934\n",
      "val_loss: 0.1715089623779358\n",
      "Progress: 78.4% ... Training loss: 0.060 ... Validation loss: 0.177iteration: 7838\n",
      "train_loss: 0.060365546637859924\n",
      "val_loss: 0.1778251366668895\n",
      "Progress: 78.4% ... Training loss: 0.062 ... Validation loss: 0.162iteration: 7839\n",
      "train_loss: 0.062221018499219495\n",
      "val_loss: 0.1621291001654273\n",
      "Progress: 78.4% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 7840\n",
      "train_loss: 0.0600424889054495\n",
      "val_loss: 0.17381942449085505\n",
      "Progress: 78.4% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 7841\n",
      "train_loss: 0.05869752764393905\n",
      "val_loss: 0.1693831952821146\n",
      "Progress: 78.4% ... Training loss: 0.059 ... Validation loss: 0.178iteration: 7842\n",
      "train_loss: 0.059591846998078506\n",
      "val_loss: 0.1783697441624096\n",
      "Progress: 78.4% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 7843\n",
      "train_loss: 0.061599129206214014\n",
      "val_loss: 0.1604109349501166\n",
      "Progress: 78.4% ... Training loss: 0.059 ... Validation loss: 0.174iteration: 7844\n",
      "train_loss: 0.05915071041581713\n",
      "val_loss: 0.17457602882985104\n",
      "Progress: 78.5% ... Training loss: 0.059 ... Validation loss: 0.167iteration: 7845\n",
      "train_loss: 0.059139975095960154\n",
      "val_loss: 0.16756071659683652\n",
      "Progress: 78.5% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 7846\n",
      "train_loss: 0.059004579166694125\n",
      "val_loss: 0.16240930481669694\n",
      "Progress: 78.5% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 7847\n",
      "train_loss: 0.06009704241588952\n",
      "val_loss: 0.164146161394429\n",
      "Progress: 78.5% ... Training loss: 0.059 ... Validation loss: 0.153iteration: 7848\n",
      "train_loss: 0.05979966054617728\n",
      "val_loss: 0.15386583710930155\n",
      "Progress: 78.5% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 7849\n",
      "train_loss: 0.06168864828797454\n",
      "val_loss: 0.17135308358357987\n",
      "Progress: 78.5% ... Training loss: 0.063 ... Validation loss: 0.151iteration: 7850\n",
      "train_loss: 0.06390833003307417\n",
      "val_loss: 0.15178366341119476\n",
      "Progress: 78.5% ... Training loss: 0.063 ... Validation loss: 0.183iteration: 7851\n",
      "train_loss: 0.06335231877755712\n",
      "val_loss: 0.1830061705299317\n",
      "Progress: 78.5% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 7852\n",
      "train_loss: 0.05875748313086914\n",
      "val_loss: 0.16412604646784018\n",
      "Progress: 78.5% ... Training loss: 0.059 ... Validation loss: 0.175iteration: 7853\n",
      "train_loss: 0.05988399881247328\n",
      "val_loss: 0.17522855901308473\n",
      "Progress: 78.5% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 7854\n",
      "train_loss: 0.06001820017766074\n",
      "val_loss: 0.15558497278452937\n",
      "Progress: 78.5% ... Training loss: 0.064 ... Validation loss: 0.187iteration: 7855\n",
      "train_loss: 0.06416662303223468\n",
      "val_loss: 0.18780820071065862\n",
      "Progress: 78.6% ... Training loss: 0.064 ... Validation loss: 0.150iteration: 7856\n",
      "train_loss: 0.06481573410579292\n",
      "val_loss: 0.1506681655086755\n",
      "Progress: 78.6% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7857\n",
      "train_loss: 0.05961685038615811\n",
      "val_loss: 0.16415532426655693\n",
      "Progress: 78.6% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7858\n",
      "train_loss: 0.0598843598625301\n",
      "val_loss: 0.164405638438154\n",
      "Progress: 78.6% ... Training loss: 0.059 ... Validation loss: 0.154iteration: 7859\n",
      "train_loss: 0.05960209405184677\n",
      "val_loss: 0.1543923569160231\n",
      "Progress: 78.6% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 7860\n",
      "train_loss: 0.05997317998771887\n",
      "val_loss: 0.1615777059807114\n",
      "Progress: 78.6% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 7861\n",
      "train_loss: 0.06368756027974827\n",
      "val_loss: 0.156865382919389\n",
      "Progress: 78.6% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 7862\n",
      "train_loss: 0.06214402048623111\n",
      "val_loss: 0.18360514767590266\n",
      "Progress: 78.6% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 7863\n",
      "train_loss: 0.059552670624866196\n",
      "val_loss: 0.16037843703604643\n",
      "Progress: 78.6% ... Training loss: 0.061 ... Validation loss: 0.172iteration: 7864\n",
      "train_loss: 0.06199944790525426\n",
      "val_loss: 0.17211723090764722\n",
      "Progress: 78.7% ... Training loss: 0.059 ... Validation loss: 0.173iteration: 7865\n",
      "train_loss: 0.05956654753711768\n",
      "val_loss: 0.17364363335052235\n",
      "Progress: 78.7% ... Training loss: 0.059 ... Validation loss: 0.163iteration: 7866\n",
      "train_loss: 0.05958658171059279\n",
      "val_loss: 0.16328018430563276\n",
      "Progress: 78.7% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7867\n",
      "train_loss: 0.059526911904900705\n",
      "val_loss: 0.16482370538618352\n",
      "Progress: 78.7% ... Training loss: 0.059 ... Validation loss: 0.163iteration: 7868\n",
      "train_loss: 0.05952218461360759\n",
      "val_loss: 0.1631472165204266\n",
      "Progress: 78.7% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 7869\n",
      "train_loss: 0.05987141803527368\n",
      "val_loss: 0.16109547998381152\n",
      "Progress: 78.7% ... Training loss: 0.060 ... Validation loss: 0.160iteration: 7870\n",
      "train_loss: 0.06011291060156268\n",
      "val_loss: 0.16064688597555324\n",
      "Progress: 78.7% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 7871\n",
      "train_loss: 0.06106573948794602\n",
      "val_loss: 0.15267441137944404\n",
      "Progress: 78.7% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 7872\n",
      "train_loss: 0.059856057124154906\n",
      "val_loss: 0.16265113989582888\n",
      "Progress: 78.7% ... Training loss: 0.060 ... Validation loss: 0.168iteration: 7873\n",
      "train_loss: 0.060309224564804115\n",
      "val_loss: 0.16861952172572894\n",
      "Progress: 78.7% ... Training loss: 0.068 ... Validation loss: 0.149iteration: 7874\n",
      "train_loss: 0.0683927417555125\n",
      "val_loss: 0.1493190576972239\n",
      "Progress: 78.8% ... Training loss: 0.061 ... Validation loss: 0.184iteration: 7875\n",
      "train_loss: 0.06190272621983237\n",
      "val_loss: 0.18466451440084775\n",
      "Progress: 78.8% ... Training loss: 0.068 ... Validation loss: 0.155iteration: 7876\n",
      "train_loss: 0.06884790375951298\n",
      "val_loss: 0.1559329548028614\n",
      "Progress: 78.8% ... Training loss: 0.068 ... Validation loss: 0.197iteration: 7877\n",
      "train_loss: 0.06828672754309462\n",
      "val_loss: 0.19756799230777947\n",
      "Progress: 78.8% ... Training loss: 0.065 ... Validation loss: 0.153iteration: 7878\n",
      "train_loss: 0.06541043373516509\n",
      "val_loss: 0.15384577862139284\n",
      "Progress: 78.8% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 7879\n",
      "train_loss: 0.062133753610063576\n",
      "val_loss: 0.17233726901559668\n",
      "Progress: 78.8% ... Training loss: 0.065 ... Validation loss: 0.192iteration: 7880\n",
      "train_loss: 0.06512981120647722\n",
      "val_loss: 0.1926913973948026\n",
      "Progress: 78.8% ... Training loss: 0.072 ... Validation loss: 0.157iteration: 7881\n",
      "train_loss: 0.07250365005829176\n",
      "val_loss: 0.15714192561480902\n",
      "Progress: 78.8% ... Training loss: 0.066 ... Validation loss: 0.191iteration: 7882\n",
      "train_loss: 0.06664181103580288\n",
      "val_loss: 0.1911715104490606\n",
      "Progress: 78.8% ... Training loss: 0.069 ... Validation loss: 0.157iteration: 7883\n",
      "train_loss: 0.06957244935351156\n",
      "val_loss: 0.15768288287214147\n",
      "Progress: 78.8% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7884\n",
      "train_loss: 0.06021176093261817\n",
      "val_loss: 0.16683950617597987\n",
      "Progress: 78.8% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 7885\n",
      "train_loss: 0.05933565239335369\n",
      "val_loss: 0.16845016328872756\n",
      "Progress: 78.9% ... Training loss: 0.059 ... Validation loss: 0.177iteration: 7886\n",
      "train_loss: 0.059687163596928536\n",
      "val_loss: 0.1773178984204536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 78.9% ... Training loss: 0.062 ... Validation loss: 0.155iteration: 7887\n",
      "train_loss: 0.06249320662906183\n",
      "val_loss: 0.15562031111094887\n",
      "Progress: 78.9% ... Training loss: 0.069 ... Validation loss: 0.192iteration: 7888\n",
      "train_loss: 0.0690634457910435\n",
      "val_loss: 0.19206419812287076\n",
      "Progress: 78.9% ... Training loss: 0.068 ... Validation loss: 0.156iteration: 7889\n",
      "train_loss: 0.06885423852901204\n",
      "val_loss: 0.15601700993062834\n",
      "Progress: 78.9% ... Training loss: 0.060 ... Validation loss: 0.176iteration: 7890\n",
      "train_loss: 0.06007616487432245\n",
      "val_loss: 0.17668595332344125\n",
      "Progress: 78.9% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 7891\n",
      "train_loss: 0.059273814057112996\n",
      "val_loss: 0.16913266078345124\n",
      "Progress: 78.9% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 7892\n",
      "train_loss: 0.05890690663429898\n",
      "val_loss: 0.16701439236054477\n",
      "Progress: 78.9% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 7893\n",
      "train_loss: 0.06201322259120439\n",
      "val_loss: 0.18364929765522575\n",
      "Progress: 78.9% ... Training loss: 0.060 ... Validation loss: 0.176iteration: 7894\n",
      "train_loss: 0.060149750064646346\n",
      "val_loss: 0.1765717929210689\n",
      "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.158iteration: 7895\n",
      "train_loss: 0.06199254621378042\n",
      "val_loss: 0.1586953915507584\n",
      "Progress: 79.0% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7896\n",
      "train_loss: 0.060472504221640855\n",
      "val_loss: 0.16642804084635301\n",
      "Progress: 79.0% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 7897\n",
      "train_loss: 0.0608616265060467\n",
      "val_loss: 0.16729430129318182\n",
      "Progress: 79.0% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 7898\n",
      "train_loss: 0.06342877965837919\n",
      "val_loss: 0.15727625203183845\n",
      "Progress: 79.0% ... Training loss: 0.068 ... Validation loss: 0.189iteration: 7899\n",
      "train_loss: 0.06824001920373765\n",
      "val_loss: 0.1894117296929604\n",
      "Progress: 79.0% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 7900\n",
      "train_loss: 0.070967863756033\n",
      "val_loss: 0.1504152631793433\n",
      "Progress: 79.0% ... Training loss: 0.065 ... Validation loss: 0.190iteration: 7901\n",
      "train_loss: 0.0656573085543202\n",
      "val_loss: 0.19048861145617582\n",
      "Progress: 79.0% ... Training loss: 0.069 ... Validation loss: 0.152iteration: 7902\n",
      "train_loss: 0.06916291340226599\n",
      "val_loss: 0.1528365272152183\n",
      "Progress: 79.0% ... Training loss: 0.059 ... Validation loss: 0.176iteration: 7903\n",
      "train_loss: 0.05993355647353964\n",
      "val_loss: 0.17685818871246667\n",
      "Progress: 79.0% ... Training loss: 0.059 ... Validation loss: 0.180iteration: 7904\n",
      "train_loss: 0.05977843851351228\n",
      "val_loss: 0.18095396501754474\n",
      "Progress: 79.0% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 7905\n",
      "train_loss: 0.06129574130590684\n",
      "val_loss: 0.162417906998598\n",
      "Progress: 79.1% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 7906\n",
      "train_loss: 0.059681360339899986\n",
      "val_loss: 0.15954509598671074\n",
      "Progress: 79.1% ... Training loss: 0.059 ... Validation loss: 0.174iteration: 7907\n",
      "train_loss: 0.0599240905606854\n",
      "val_loss: 0.17440254071328667\n",
      "Progress: 79.1% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 7908\n",
      "train_loss: 0.06071639424463367\n",
      "val_loss: 0.1749717699161971\n",
      "Progress: 79.1% ... Training loss: 0.059 ... Validation loss: 0.179iteration: 7909\n",
      "train_loss: 0.059474363297872854\n",
      "val_loss: 0.17928276766348444\n",
      "Progress: 79.1% ... Training loss: 0.059 ... Validation loss: 0.172iteration: 7910\n",
      "train_loss: 0.059656035579887794\n",
      "val_loss: 0.1724996341202686\n",
      "Progress: 79.1% ... Training loss: 0.062 ... Validation loss: 0.190iteration: 7911\n",
      "train_loss: 0.06235479829506631\n",
      "val_loss: 0.19029712464091603\n",
      "Progress: 79.1% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 7912\n",
      "train_loss: 0.05901541106623523\n",
      "val_loss: 0.16854076964791073\n",
      "Progress: 79.1% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 7913\n",
      "train_loss: 0.060118621133289696\n",
      "val_loss: 0.16601970915537007\n",
      "Progress: 79.1% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 7914\n",
      "train_loss: 0.05869839753729908\n",
      "val_loss: 0.15928069429127795\n",
      "Progress: 79.2% ... Training loss: 0.060 ... Validation loss: 0.168iteration: 7915\n",
      "train_loss: 0.06003308123588491\n",
      "val_loss: 0.16824690727474764\n",
      "Progress: 79.2% ... Training loss: 0.058 ... Validation loss: 0.168iteration: 7916\n",
      "train_loss: 0.058920772652864765\n",
      "val_loss: 0.16807453402183256\n",
      "Progress: 79.2% ... Training loss: 0.061 ... Validation loss: 0.186iteration: 7917\n",
      "train_loss: 0.06160170085125531\n",
      "val_loss: 0.186258747259613\n",
      "Progress: 79.2% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 7918\n",
      "train_loss: 0.05901739277078999\n",
      "val_loss: 0.1682508527946175\n",
      "Progress: 79.2% ... Training loss: 0.060 ... Validation loss: 0.153iteration: 7919\n",
      "train_loss: 0.06011097847171612\n",
      "val_loss: 0.15303214594144623\n",
      "Progress: 79.2% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 7920\n",
      "train_loss: 0.05936838969576411\n",
      "val_loss: 0.15673729446838333\n",
      "Progress: 79.2% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 7921\n",
      "train_loss: 0.059692855327103986\n",
      "val_loss: 0.1642813126296848\n",
      "Progress: 79.2% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 7922\n",
      "train_loss: 0.05897395734373927\n",
      "val_loss: 0.16743087162213383\n",
      "Progress: 79.2% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 7923\n",
      "train_loss: 0.05944008071646078\n",
      "val_loss: 0.15646182764566857\n",
      "Progress: 79.2% ... Training loss: 0.070 ... Validation loss: 0.154iteration: 7924\n",
      "train_loss: 0.07059735409446433\n",
      "val_loss: 0.1547532684914623\n",
      "Progress: 79.2% ... Training loss: 0.067 ... Validation loss: 0.190iteration: 7925\n",
      "train_loss: 0.06752592067889004\n",
      "val_loss: 0.19073094654528988\n",
      "Progress: 79.3% ... Training loss: 0.067 ... Validation loss: 0.149iteration: 7926\n",
      "train_loss: 0.06782240040620623\n",
      "val_loss: 0.14937561154836967\n",
      "Progress: 79.3% ... Training loss: 0.067 ... Validation loss: 0.184iteration: 7927\n",
      "train_loss: 0.06771563468323737\n",
      "val_loss: 0.1848988307905142\n",
      "Progress: 79.3% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 7928\n",
      "train_loss: 0.06125174997026465\n",
      "val_loss: 0.15738392173626373\n",
      "Progress: 79.3% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 7929\n",
      "train_loss: 0.06486920073614076\n",
      "val_loss: 0.1734263023478558\n",
      "Progress: 79.3% ... Training loss: 0.066 ... Validation loss: 0.149iteration: 7930\n",
      "train_loss: 0.06649554653421426\n",
      "val_loss: 0.14981203004029248\n",
      "Progress: 79.3% ... Training loss: 0.073 ... Validation loss: 0.194iteration: 7931\n",
      "train_loss: 0.07323098283866625\n",
      "val_loss: 0.19416247231868305\n",
      "Progress: 79.3% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 7932\n",
      "train_loss: 0.06065096316963523\n",
      "val_loss: 0.15532771545835103\n",
      "Progress: 79.3% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 7933\n",
      "train_loss: 0.06008653269563805\n",
      "val_loss: 0.1738121114964871\n",
      "Progress: 79.3% ... Training loss: 0.059 ... Validation loss: 0.153iteration: 7934\n",
      "train_loss: 0.059881409240082444\n",
      "val_loss: 0.1532130442238041\n",
      "Progress: 79.3% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 7935\n",
      "train_loss: 0.05940858909262033\n",
      "val_loss: 0.15506117362857186\n",
      "Progress: 79.4% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 7936\n",
      "train_loss: 0.06033288102418414\n",
      "val_loss: 0.14988553803506394\n",
      "Progress: 79.4% ... Training loss: 0.059 ... Validation loss: 0.152iteration: 7937\n",
      "train_loss: 0.05983414871833035\n",
      "val_loss: 0.15298709904210617\n",
      "Progress: 79.4% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 7938\n",
      "train_loss: 0.0601423735168598\n",
      "val_loss: 0.15786863657778127\n",
      "Progress: 79.4% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 7939\n",
      "train_loss: 0.058898607832610514\n",
      "val_loss: 0.16150559665418754\n",
      "Progress: 79.4% ... Training loss: 0.067 ... Validation loss: 0.182iteration: 7940\n",
      "train_loss: 0.06797787473129231\n",
      "val_loss: 0.18208359873241373\n",
      "Progress: 79.4% ... Training loss: 0.064 ... Validation loss: 0.148iteration: 7941\n",
      "train_loss: 0.06454294406661445\n",
      "val_loss: 0.14828550154683134\n",
      "Progress: 79.4% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 7942\n",
      "train_loss: 0.06452327865129834\n",
      "val_loss: 0.17092784304909878\n",
      "Progress: 79.4% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 7943\n",
      "train_loss: 0.07070318969505024\n",
      "val_loss: 0.15075114863127284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 79.4% ... Training loss: 0.077 ... Validation loss: 0.202iteration: 7944\n",
      "train_loss: 0.07775640634727607\n",
      "val_loss: 0.20248188830501251\n",
      "Progress: 79.5% ... Training loss: 0.085 ... Validation loss: 0.154iteration: 7945\n",
      "train_loss: 0.08579578726447425\n",
      "val_loss: 0.1545726778024286\n",
      "Progress: 79.5% ... Training loss: 0.070 ... Validation loss: 0.208iteration: 7946\n",
      "train_loss: 0.07080308704532223\n",
      "val_loss: 0.20892507275204367\n",
      "Progress: 79.5% ... Training loss: 0.062 ... Validation loss: 0.153iteration: 7947\n",
      "train_loss: 0.06250291266063757\n",
      "val_loss: 0.15360540696944353\n",
      "Progress: 79.5% ... Training loss: 0.059 ... Validation loss: 0.175iteration: 7948\n",
      "train_loss: 0.059051746362538994\n",
      "val_loss: 0.17587914921095224\n",
      "Progress: 79.5% ... Training loss: 0.064 ... Validation loss: 0.153iteration: 7949\n",
      "train_loss: 0.06498528655614691\n",
      "val_loss: 0.1531973353799656\n",
      "Progress: 79.5% ... Training loss: 0.063 ... Validation loss: 0.187iteration: 7950\n",
      "train_loss: 0.06356744676981513\n",
      "val_loss: 0.1876366187278571\n",
      "Progress: 79.5% ... Training loss: 0.067 ... Validation loss: 0.154iteration: 7951\n",
      "train_loss: 0.06701262467828419\n",
      "val_loss: 0.15492846839395172\n",
      "Progress: 79.5% ... Training loss: 0.062 ... Validation loss: 0.196iteration: 7952\n",
      "train_loss: 0.062066248347696196\n",
      "val_loss: 0.1962298356758757\n",
      "Progress: 79.5% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 7953\n",
      "train_loss: 0.060355638978126236\n",
      "val_loss: 0.1674188426374048\n",
      "Progress: 79.5% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 7954\n",
      "train_loss: 0.06032449770570426\n",
      "val_loss: 0.1695967490978654\n",
      "Progress: 79.5% ... Training loss: 0.062 ... Validation loss: 0.156iteration: 7955\n",
      "train_loss: 0.0624667358244563\n",
      "val_loss: 0.15617326869094764\n",
      "Progress: 79.6% ... Training loss: 0.068 ... Validation loss: 0.197iteration: 7956\n",
      "train_loss: 0.06834894132361259\n",
      "val_loss: 0.19778905008664185\n",
      "Progress: 79.6% ... Training loss: 0.070 ... Validation loss: 0.152iteration: 7957\n",
      "train_loss: 0.0701659453296988\n",
      "val_loss: 0.15259226289659575\n",
      "Progress: 79.6% ... Training loss: 0.081 ... Validation loss: 0.217iteration: 7958\n",
      "train_loss: 0.08119807035749733\n",
      "val_loss: 0.2172635260295532\n",
      "Progress: 79.6% ... Training loss: 0.085 ... Validation loss: 0.161iteration: 7959\n",
      "train_loss: 0.08595535362698922\n",
      "val_loss: 0.1611900751493932\n",
      "Progress: 79.6% ... Training loss: 0.099 ... Validation loss: 0.252iteration: 7960\n",
      "train_loss: 0.09986490883819957\n",
      "val_loss: 0.2523008480256361\n",
      "Progress: 79.6% ... Training loss: 0.075 ... Validation loss: 0.152iteration: 7961\n",
      "train_loss: 0.07514163269248994\n",
      "val_loss: 0.15270527441605034\n",
      "Progress: 79.6% ... Training loss: 0.063 ... Validation loss: 0.191iteration: 7962\n",
      "train_loss: 0.06306794930640588\n",
      "val_loss: 0.1914254376548323\n",
      "Progress: 79.6% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 7963\n",
      "train_loss: 0.0599622442124576\n",
      "val_loss: 0.1629397564500499\n",
      "Progress: 79.6% ... Training loss: 0.060 ... Validation loss: 0.177iteration: 7964\n",
      "train_loss: 0.06050765288647312\n",
      "val_loss: 0.17721578966449963\n",
      "Progress: 79.7% ... Training loss: 0.059 ... Validation loss: 0.167iteration: 7965\n",
      "train_loss: 0.05935150722306214\n",
      "val_loss: 0.16749551594860718\n",
      "Progress: 79.7% ... Training loss: 0.059 ... Validation loss: 0.185iteration: 7966\n",
      "train_loss: 0.05975442832878597\n",
      "val_loss: 0.18521888872530923\n",
      "Progress: 79.7% ... Training loss: 0.061 ... Validation loss: 0.161iteration: 7967\n",
      "train_loss: 0.061585901299451275\n",
      "val_loss: 0.16198454582121619\n",
      "Progress: 79.7% ... Training loss: 0.059 ... Validation loss: 0.167iteration: 7968\n",
      "train_loss: 0.05932474130593568\n",
      "val_loss: 0.16787732695079433\n",
      "Progress: 79.7% ... Training loss: 0.065 ... Validation loss: 0.150iteration: 7969\n",
      "train_loss: 0.0654580260708142\n",
      "val_loss: 0.15067965419312376\n",
      "Progress: 79.7% ... Training loss: 0.066 ... Validation loss: 0.195iteration: 7970\n",
      "train_loss: 0.06635464613270571\n",
      "val_loss: 0.19598389590063361\n",
      "Progress: 79.7% ... Training loss: 0.063 ... Validation loss: 0.154iteration: 7971\n",
      "train_loss: 0.06358830005408636\n",
      "val_loss: 0.15422238675887154\n",
      "Progress: 79.7% ... Training loss: 0.085 ... Validation loss: 0.217iteration: 7972\n",
      "train_loss: 0.08532307956270399\n",
      "val_loss: 0.21709487119768195\n",
      "Progress: 79.7% ... Training loss: 0.070 ... Validation loss: 0.149iteration: 7973\n",
      "train_loss: 0.07038321334412818\n",
      "val_loss: 0.14982034510418443\n",
      "Progress: 79.7% ... Training loss: 0.067 ... Validation loss: 0.205iteration: 7974\n",
      "train_loss: 0.06798333557702985\n",
      "val_loss: 0.2058567769410188\n",
      "Progress: 79.8% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 7975\n",
      "train_loss: 0.061098214842311836\n",
      "val_loss: 0.15910331274225503\n",
      "Progress: 79.8% ... Training loss: 0.061 ... Validation loss: 0.191iteration: 7976\n",
      "train_loss: 0.06126277281036631\n",
      "val_loss: 0.19143754453920622\n",
      "Progress: 79.8% ... Training loss: 0.060 ... Validation loss: 0.182iteration: 7977\n",
      "train_loss: 0.06076403866378033\n",
      "val_loss: 0.18277569376727532\n",
      "Progress: 79.8% ... Training loss: 0.062 ... Validation loss: 0.156iteration: 7978\n",
      "train_loss: 0.06210241286674584\n",
      "val_loss: 0.1564365249693139\n",
      "Progress: 79.8% ... Training loss: 0.059 ... Validation loss: 0.179iteration: 7979\n",
      "train_loss: 0.059018122131154094\n",
      "val_loss: 0.1796433218486304\n",
      "Progress: 79.8% ... Training loss: 0.062 ... Validation loss: 0.167iteration: 7980\n",
      "train_loss: 0.06212755866170476\n",
      "val_loss: 0.16763716319031655\n",
      "Progress: 79.8% ... Training loss: 0.067 ... Validation loss: 0.205iteration: 7981\n",
      "train_loss: 0.06734513222449008\n",
      "val_loss: 0.2050685375575943\n",
      "Progress: 79.8% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 7982\n",
      "train_loss: 0.06181658024249001\n",
      "val_loss: 0.15928426078699437\n",
      "Progress: 79.8% ... Training loss: 0.068 ... Validation loss: 0.193iteration: 7983\n",
      "train_loss: 0.06840234113906699\n",
      "val_loss: 0.19363945776493097\n",
      "Progress: 79.8% ... Training loss: 0.070 ... Validation loss: 0.147iteration: 7984\n",
      "train_loss: 0.07010059107806405\n",
      "val_loss: 0.1476206038890571\n",
      "Progress: 79.8% ... Training loss: 0.065 ... Validation loss: 0.184iteration: 7985\n",
      "train_loss: 0.06578649746090368\n",
      "val_loss: 0.18450212194192342\n",
      "Progress: 79.9% ... Training loss: 0.062 ... Validation loss: 0.157iteration: 7986\n",
      "train_loss: 0.06252237080677388\n",
      "val_loss: 0.15782612586029104\n",
      "Progress: 79.9% ... Training loss: 0.059 ... Validation loss: 0.174iteration: 7987\n",
      "train_loss: 0.05984061586233223\n",
      "val_loss: 0.17430481735112285\n",
      "Progress: 79.9% ... Training loss: 0.067 ... Validation loss: 0.149iteration: 7988\n",
      "train_loss: 0.06780170898552615\n",
      "val_loss: 0.1498292357514899\n",
      "Progress: 79.9% ... Training loss: 0.064 ... Validation loss: 0.186iteration: 7989\n",
      "train_loss: 0.06409937634809161\n",
      "val_loss: 0.18686805606503457\n",
      "Progress: 79.9% ... Training loss: 0.064 ... Validation loss: 0.155iteration: 7990\n",
      "train_loss: 0.06423962467489608\n",
      "val_loss: 0.15576734627921673\n",
      "Progress: 79.9% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 7991\n",
      "train_loss: 0.060094761794302794\n",
      "val_loss: 0.16239062799374515\n",
      "Progress: 79.9% ... Training loss: 0.064 ... Validation loss: 0.158iteration: 7992\n",
      "train_loss: 0.06461019983117841\n",
      "val_loss: 0.15844412851019765\n",
      "Progress: 79.9% ... Training loss: 0.065 ... Validation loss: 0.181iteration: 7993\n",
      "train_loss: 0.06570568998669755\n",
      "val_loss: 0.18166768403918346\n",
      "Progress: 79.9% ... Training loss: 0.070 ... Validation loss: 0.148iteration: 7994\n",
      "train_loss: 0.07023696245955476\n",
      "val_loss: 0.1486171555410381\n",
      "Progress: 80.0% ... Training loss: 0.069 ... Validation loss: 0.200iteration: 7995\n",
      "train_loss: 0.06935835073162529\n",
      "val_loss: 0.20082909075749458\n",
      "Progress: 80.0% ... Training loss: 0.061 ... Validation loss: 0.158iteration: 7996\n",
      "train_loss: 0.06137283867796263\n",
      "val_loss: 0.15878387819309883\n",
      "Progress: 80.0% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 7997\n",
      "train_loss: 0.06055783075634651\n",
      "val_loss: 0.1720685986681679\n",
      "Progress: 80.0% ... Training loss: 0.067 ... Validation loss: 0.149iteration: 7998\n",
      "train_loss: 0.06780900524455005\n",
      "val_loss: 0.14960861478230772\n",
      "Progress: 80.0% ... Training loss: 0.090 ... Validation loss: 0.210iteration: 7999\n",
      "train_loss: 0.09090856209146281\n",
      "val_loss: 0.21025217509220914\n",
      "Progress: 80.0% ... Training loss: 0.078 ... Validation loss: 0.154iteration: 8000\n",
      "train_loss: 0.07843393414718636\n",
      "val_loss: 0.1547149429171881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 80.0% ... Training loss: 0.072 ... Validation loss: 0.198iteration: 8001\n",
      "train_loss: 0.07291525366800027\n",
      "val_loss: 0.19895216056575385\n",
      "Progress: 80.0% ... Training loss: 0.088 ... Validation loss: 0.156iteration: 8002\n",
      "train_loss: 0.0880787527962581\n",
      "val_loss: 0.15695193721576833\n",
      "Progress: 80.0% ... Training loss: 0.077 ... Validation loss: 0.214iteration: 8003\n",
      "train_loss: 0.07791589880803151\n",
      "val_loss: 0.21413716976137817\n",
      "Progress: 80.0% ... Training loss: 0.065 ... Validation loss: 0.149iteration: 8004\n",
      "train_loss: 0.06573089320192992\n",
      "val_loss: 0.14910065638884268\n",
      "Progress: 80.0% ... Training loss: 0.066 ... Validation loss: 0.169iteration: 8005\n",
      "train_loss: 0.06618112272991687\n",
      "val_loss: 0.16942196660441478\n",
      "Progress: 80.1% ... Training loss: 0.059 ... Validation loss: 0.165iteration: 8006\n",
      "train_loss: 0.0599527562295799\n",
      "val_loss: 0.16504461969601145\n",
      "Progress: 80.1% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 8007\n",
      "train_loss: 0.06090340457220734\n",
      "val_loss: 0.1745739568525108\n",
      "Progress: 80.1% ... Training loss: 0.077 ... Validation loss: 0.147iteration: 8008\n",
      "train_loss: 0.07742948181211272\n",
      "val_loss: 0.14738348363347017\n",
      "Progress: 80.1% ... Training loss: 0.075 ... Validation loss: 0.203iteration: 8009\n",
      "train_loss: 0.0753149394705138\n",
      "val_loss: 0.20393448035787604\n",
      "Progress: 80.1% ... Training loss: 0.083 ... Validation loss: 0.150iteration: 8010\n",
      "train_loss: 0.08390124004545783\n",
      "val_loss: 0.1500778589771125\n",
      "Progress: 80.1% ... Training loss: 0.087 ... Validation loss: 0.213iteration: 8011\n",
      "train_loss: 0.08769089858748964\n",
      "val_loss: 0.21359128416322934\n",
      "Progress: 80.1% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 8012\n",
      "train_loss: 0.06125525277183214\n",
      "val_loss: 0.1491390085600124\n",
      "Progress: 80.1% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8013\n",
      "train_loss: 0.05854726458211837\n",
      "val_loss: 0.16536361668568766\n",
      "Progress: 80.1% ... Training loss: 0.058 ... Validation loss: 0.154iteration: 8014\n",
      "train_loss: 0.058661240233024874\n",
      "val_loss: 0.15465462780957986\n",
      "Progress: 80.2% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 8015\n",
      "train_loss: 0.06078436585141003\n",
      "val_loss: 0.16943056518388475\n",
      "Progress: 80.2% ... Training loss: 0.068 ... Validation loss: 0.147iteration: 8016\n",
      "train_loss: 0.06824470659778852\n",
      "val_loss: 0.1478731700462448\n",
      "Progress: 80.2% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 8017\n",
      "train_loss: 0.06343145176419089\n",
      "val_loss: 0.18239305553763324\n",
      "Progress: 80.2% ... Training loss: 0.066 ... Validation loss: 0.154iteration: 8018\n",
      "train_loss: 0.06692935708010989\n",
      "val_loss: 0.15423477454847712\n",
      "Progress: 80.2% ... Training loss: 0.061 ... Validation loss: 0.182iteration: 8019\n",
      "train_loss: 0.06198483475248285\n",
      "val_loss: 0.18221620408042127\n",
      "Progress: 80.2% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 8020\n",
      "train_loss: 0.061198564969398156\n",
      "val_loss: 0.15735994033684747\n",
      "Progress: 80.2% ... Training loss: 0.063 ... Validation loss: 0.186iteration: 8021\n",
      "train_loss: 0.0631751993179735\n",
      "val_loss: 0.18676924029480865\n",
      "Progress: 80.2% ... Training loss: 0.060 ... Validation loss: 0.154iteration: 8022\n",
      "train_loss: 0.06063259760291234\n",
      "val_loss: 0.1545410293985696\n",
      "Progress: 80.2% ... Training loss: 0.058 ... Validation loss: 0.174iteration: 8023\n",
      "train_loss: 0.05898184636795549\n",
      "val_loss: 0.174654862030014\n",
      "Progress: 80.2% ... Training loss: 0.059 ... Validation loss: 0.173iteration: 8024\n",
      "train_loss: 0.0594692724521977\n",
      "val_loss: 0.17316802512799198\n",
      "Progress: 80.2% ... Training loss: 0.058 ... Validation loss: 0.168iteration: 8025\n",
      "train_loss: 0.05861698922402233\n",
      "val_loss: 0.1684774419782417\n",
      "Progress: 80.3% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8026\n",
      "train_loss: 0.05842218289182584\n",
      "val_loss: 0.16461070084576837\n",
      "Progress: 80.3% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8027\n",
      "train_loss: 0.05835678925337396\n",
      "val_loss: 0.16530950375316336\n",
      "Progress: 80.3% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 8028\n",
      "train_loss: 0.06083632068510637\n",
      "val_loss: 0.1611491311681059\n",
      "Progress: 80.3% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 8029\n",
      "train_loss: 0.0621729954122452\n",
      "val_loss: 0.18357104328551657\n",
      "Progress: 80.3% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 8030\n",
      "train_loss: 0.06366494819032745\n",
      "val_loss: 0.15691636938375592\n",
      "Progress: 80.3% ... Training loss: 0.061 ... Validation loss: 0.195iteration: 8031\n",
      "train_loss: 0.061338067728871355\n",
      "val_loss: 0.195123747890653\n",
      "Progress: 80.3% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 8032\n",
      "train_loss: 0.061855605910514595\n",
      "val_loss: 0.16650177457696339\n",
      "Progress: 80.3% ... Training loss: 0.058 ... Validation loss: 0.180iteration: 8033\n",
      "train_loss: 0.05876265972866953\n",
      "val_loss: 0.18011686779546116\n",
      "Progress: 80.3% ... Training loss: 0.063 ... Validation loss: 0.155iteration: 8034\n",
      "train_loss: 0.06385829101931448\n",
      "val_loss: 0.15517635914878572\n",
      "Progress: 80.3% ... Training loss: 0.061 ... Validation loss: 0.181iteration: 8035\n",
      "train_loss: 0.061962383044311606\n",
      "val_loss: 0.18154952223648185\n",
      "Progress: 80.4% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 8036\n",
      "train_loss: 0.059488275691895985\n",
      "val_loss: 0.16440557287147\n",
      "Progress: 80.4% ... Training loss: 0.068 ... Validation loss: 0.197iteration: 8037\n",
      "train_loss: 0.0681697211331649\n",
      "val_loss: 0.19783378982330035\n",
      "Progress: 80.4% ... Training loss: 0.068 ... Validation loss: 0.149iteration: 8038\n",
      "train_loss: 0.0689146510073454\n",
      "val_loss: 0.1495754639620619\n",
      "Progress: 80.4% ... Training loss: 0.074 ... Validation loss: 0.198iteration: 8039\n",
      "train_loss: 0.07495174261141206\n",
      "val_loss: 0.19841798469201133\n",
      "Progress: 80.4% ... Training loss: 0.065 ... Validation loss: 0.147iteration: 8040\n",
      "train_loss: 0.06534203611979011\n",
      "val_loss: 0.1476340837756688\n",
      "Progress: 80.4% ... Training loss: 0.061 ... Validation loss: 0.182iteration: 8041\n",
      "train_loss: 0.06148076934091427\n",
      "val_loss: 0.18295272882232466\n",
      "Progress: 80.4% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8042\n",
      "train_loss: 0.05898148529191817\n",
      "val_loss: 0.1707312442452421\n",
      "Progress: 80.4% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 8043\n",
      "train_loss: 0.05861106828475996\n",
      "val_loss: 0.1662041961674541\n",
      "Progress: 80.4% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8044\n",
      "train_loss: 0.05859696109815631\n",
      "val_loss: 0.16475014707230107\n",
      "Progress: 80.5% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 8045\n",
      "train_loss: 0.058368474162452236\n",
      "val_loss: 0.16194550279616557\n",
      "Progress: 80.5% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 8046\n",
      "train_loss: 0.06003165163668363\n",
      "val_loss: 0.15791992321369636\n",
      "Progress: 80.5% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 8047\n",
      "train_loss: 0.061039769060268645\n",
      "val_loss: 0.17113590423329805\n",
      "Progress: 80.5% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 8048\n",
      "train_loss: 0.05904437399387455\n",
      "val_loss: 0.16435715596330308\n",
      "Progress: 80.5% ... Training loss: 0.065 ... Validation loss: 0.153iteration: 8049\n",
      "train_loss: 0.06518220911346816\n",
      "val_loss: 0.15374630771646405\n",
      "Progress: 80.5% ... Training loss: 0.070 ... Validation loss: 0.194iteration: 8050\n",
      "train_loss: 0.07048366659068417\n",
      "val_loss: 0.1949302581454073\n",
      "Progress: 80.5% ... Training loss: 0.075 ... Validation loss: 0.152iteration: 8051\n",
      "train_loss: 0.07517086155859323\n",
      "val_loss: 0.15297167981449036\n",
      "Progress: 80.5% ... Training loss: 0.065 ... Validation loss: 0.182iteration: 8052\n",
      "train_loss: 0.06521489722501701\n",
      "val_loss: 0.18263110127579474\n",
      "Progress: 80.5% ... Training loss: 0.059 ... Validation loss: 0.149iteration: 8053\n",
      "train_loss: 0.0597736458381025\n",
      "val_loss: 0.149903703560069\n",
      "Progress: 80.5% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 8054\n",
      "train_loss: 0.05960813468209816\n",
      "val_loss: 0.15790251604664945\n",
      "Progress: 80.5% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8055\n",
      "train_loss: 0.05840605642267891\n",
      "val_loss: 0.16776536230361772\n",
      "Progress: 80.6% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 8056\n",
      "train_loss: 0.05880591037271715\n",
      "val_loss: 0.15944066922295563\n",
      "Progress: 80.6% ... Training loss: 0.060 ... Validation loss: 0.179iteration: 8057\n",
      "train_loss: 0.06081868686148203\n",
      "val_loss: 0.1797800331412676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 80.6% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8058\n",
      "train_loss: 0.05846395719557644\n",
      "val_loss: 0.16441564255723246\n",
      "Progress: 80.6% ... Training loss: 0.060 ... Validation loss: 0.186iteration: 8059\n",
      "train_loss: 0.06031738282528372\n",
      "val_loss: 0.18621941758060956\n",
      "Progress: 80.6% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8060\n",
      "train_loss: 0.05868989034086455\n",
      "val_loss: 0.17077525431918822\n",
      "Progress: 80.6% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8061\n",
      "train_loss: 0.058796375240287774\n",
      "val_loss: 0.16344230828323836\n",
      "Progress: 80.6% ... Training loss: 0.066 ... Validation loss: 0.189iteration: 8062\n",
      "train_loss: 0.06603598395331935\n",
      "val_loss: 0.1899763789309606\n",
      "Progress: 80.6% ... Training loss: 0.065 ... Validation loss: 0.155iteration: 8063\n",
      "train_loss: 0.0653622549767526\n",
      "val_loss: 0.15539064808400277\n",
      "Progress: 80.6% ... Training loss: 0.071 ... Validation loss: 0.199iteration: 8064\n",
      "train_loss: 0.07149247523955078\n",
      "val_loss: 0.19903941542962425\n",
      "Progress: 80.7% ... Training loss: 0.066 ... Validation loss: 0.148iteration: 8065\n",
      "train_loss: 0.06649713615343583\n",
      "val_loss: 0.14880113369466239\n",
      "Progress: 80.7% ... Training loss: 0.060 ... Validation loss: 0.169iteration: 8066\n",
      "train_loss: 0.06017412126905154\n",
      "val_loss: 0.1698327274111206\n",
      "Progress: 80.7% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 8067\n",
      "train_loss: 0.05904357287353332\n",
      "val_loss: 0.15911613318211404\n",
      "Progress: 80.7% ... Training loss: 0.064 ... Validation loss: 0.185iteration: 8068\n",
      "train_loss: 0.06408300674699624\n",
      "val_loss: 0.1856148193676432\n",
      "Progress: 80.7% ... Training loss: 0.070 ... Validation loss: 0.151iteration: 8069\n",
      "train_loss: 0.07072988692007912\n",
      "val_loss: 0.15133244276635635\n",
      "Progress: 80.7% ... Training loss: 0.090 ... Validation loss: 0.235iteration: 8070\n",
      "train_loss: 0.09059299891818823\n",
      "val_loss: 0.23506477366224\n",
      "Progress: 80.7% ... Training loss: 0.098 ... Validation loss: 0.150iteration: 8071\n",
      "train_loss: 0.09849138155990532\n",
      "val_loss: 0.15052680062082488\n",
      "Progress: 80.7% ... Training loss: 0.105 ... Validation loss: 0.241iteration: 8072\n",
      "train_loss: 0.10579154151596142\n",
      "val_loss: 0.24133444950798746\n",
      "Progress: 80.7% ... Training loss: 0.086 ... Validation loss: 0.148iteration: 8073\n",
      "train_loss: 0.08642438386409726\n",
      "val_loss: 0.14813352539667762\n",
      "Progress: 80.7% ... Training loss: 0.085 ... Validation loss: 0.229iteration: 8074\n",
      "train_loss: 0.08562059453106247\n",
      "val_loss: 0.22973881382787004\n",
      "Progress: 80.8% ... Training loss: 0.069 ... Validation loss: 0.144iteration: 8075\n",
      "train_loss: 0.06997236259714115\n",
      "val_loss: 0.14452388299125934\n",
      "Progress: 80.8% ... Training loss: 0.074 ... Validation loss: 0.210iteration: 8076\n",
      "train_loss: 0.07402692367527687\n",
      "val_loss: 0.21053287342330027\n",
      "Progress: 80.8% ... Training loss: 0.089 ... Validation loss: 0.156iteration: 8077\n",
      "train_loss: 0.08939120963076347\n",
      "val_loss: 0.15607517615912267\n",
      "Progress: 80.8% ... Training loss: 0.081 ... Validation loss: 0.223iteration: 8078\n",
      "train_loss: 0.08168272520896132\n",
      "val_loss: 0.22364881233736725\n",
      "Progress: 80.8% ... Training loss: 0.068 ... Validation loss: 0.151iteration: 8079\n",
      "train_loss: 0.06894177155051476\n",
      "val_loss: 0.1512376524948865\n",
      "Progress: 80.8% ... Training loss: 0.065 ... Validation loss: 0.199iteration: 8080\n",
      "train_loss: 0.06598725474084091\n",
      "val_loss: 0.1992944373622187\n",
      "Progress: 80.8% ... Training loss: 0.062 ... Validation loss: 0.161iteration: 8081\n",
      "train_loss: 0.062328221750160076\n",
      "val_loss: 0.16190456321442492\n",
      "Progress: 80.8% ... Training loss: 0.061 ... Validation loss: 0.187iteration: 8082\n",
      "train_loss: 0.06162598368227852\n",
      "val_loss: 0.18738547628245103\n",
      "Progress: 80.8% ... Training loss: 0.063 ... Validation loss: 0.154iteration: 8083\n",
      "train_loss: 0.06355946174848884\n",
      "val_loss: 0.15445791143794094\n",
      "Progress: 80.8% ... Training loss: 0.068 ... Validation loss: 0.203iteration: 8084\n",
      "train_loss: 0.06834693605955\n",
      "val_loss: 0.2038482688329447\n",
      "Progress: 80.8% ... Training loss: 0.067 ... Validation loss: 0.154iteration: 8085\n",
      "train_loss: 0.06714340514978104\n",
      "val_loss: 0.15489024086320013\n",
      "Progress: 80.9% ... Training loss: 0.060 ... Validation loss: 0.179iteration: 8086\n",
      "train_loss: 0.06093880408569322\n",
      "val_loss: 0.1796855103586322\n",
      "Progress: 80.9% ... Training loss: 0.059 ... Validation loss: 0.174iteration: 8087\n",
      "train_loss: 0.05987045465302746\n",
      "val_loss: 0.17441489211187103\n",
      "Progress: 80.9% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 8088\n",
      "train_loss: 0.059927537925665214\n",
      "val_loss: 0.15739874133806314\n",
      "Progress: 80.9% ... Training loss: 0.063 ... Validation loss: 0.159iteration: 8089\n",
      "train_loss: 0.06305204714622215\n",
      "val_loss: 0.15943757815355966\n",
      "Progress: 80.9% ... Training loss: 0.063 ... Validation loss: 0.188iteration: 8090\n",
      "train_loss: 0.06334339850051633\n",
      "val_loss: 0.18881674439080756\n",
      "Progress: 80.9% ... Training loss: 0.071 ... Validation loss: 0.148iteration: 8091\n",
      "train_loss: 0.07120824168146085\n",
      "val_loss: 0.14812257918040345\n",
      "Progress: 80.9% ... Training loss: 0.066 ... Validation loss: 0.200iteration: 8092\n",
      "train_loss: 0.06655862316649147\n",
      "val_loss: 0.20009414220780605\n",
      "Progress: 80.9% ... Training loss: 0.071 ... Validation loss: 0.152iteration: 8093\n",
      "train_loss: 0.07113408762085269\n",
      "val_loss: 0.15209382638130942\n",
      "Progress: 80.9% ... Training loss: 0.065 ... Validation loss: 0.197iteration: 8094\n",
      "train_loss: 0.06502842617860084\n",
      "val_loss: 0.19728072486446457\n",
      "Progress: 81.0% ... Training loss: 0.062 ... Validation loss: 0.151iteration: 8095\n",
      "train_loss: 0.06267969709633663\n",
      "val_loss: 0.15148169600856215\n",
      "Progress: 81.0% ... Training loss: 0.059 ... Validation loss: 0.166iteration: 8096\n",
      "train_loss: 0.05928850185616532\n",
      "val_loss: 0.16687535045918023\n",
      "Progress: 81.0% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 8097\n",
      "train_loss: 0.060854957928310305\n",
      "val_loss: 0.1562252550491427\n",
      "Progress: 81.0% ... Training loss: 0.059 ... Validation loss: 0.185iteration: 8098\n",
      "train_loss: 0.05999795672473225\n",
      "val_loss: 0.18511842314191698\n",
      "Progress: 81.0% ... Training loss: 0.059 ... Validation loss: 0.176iteration: 8099\n",
      "train_loss: 0.05975709485472178\n",
      "val_loss: 0.17620143184857257\n",
      "Progress: 81.0% ... Training loss: 0.061 ... Validation loss: 0.185iteration: 8100\n",
      "train_loss: 0.06171339014947954\n",
      "val_loss: 0.18547233335518606\n",
      "Progress: 81.0% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 8101\n",
      "train_loss: 0.060810434142616276\n",
      "val_loss: 0.15543060379008505\n",
      "Progress: 81.0% ... Training loss: 0.061 ... Validation loss: 0.190iteration: 8102\n",
      "train_loss: 0.06161991571213398\n",
      "val_loss: 0.19086319542402178\n",
      "Progress: 81.0% ... Training loss: 0.062 ... Validation loss: 0.152iteration: 8103\n",
      "train_loss: 0.0629701775974183\n",
      "val_loss: 0.1521479304128641\n",
      "Progress: 81.0% ... Training loss: 0.061 ... Validation loss: 0.195iteration: 8104\n",
      "train_loss: 0.06139961216379084\n",
      "val_loss: 0.1954638662199105\n",
      "Progress: 81.0% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 8105\n",
      "train_loss: 0.06378313482742258\n",
      "val_loss: 0.15693731221618068\n",
      "Progress: 81.1% ... Training loss: 0.063 ... Validation loss: 0.195iteration: 8106\n",
      "train_loss: 0.06324645176699842\n",
      "val_loss: 0.19522291263933822\n",
      "Progress: 81.1% ... Training loss: 0.066 ... Validation loss: 0.151iteration: 8107\n",
      "train_loss: 0.06681931131274235\n",
      "val_loss: 0.151950099542696\n",
      "Progress: 81.1% ... Training loss: 0.061 ... Validation loss: 0.188iteration: 8108\n",
      "train_loss: 0.061500485570832604\n",
      "val_loss: 0.18884195517989177\n",
      "Progress: 81.1% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 8109\n",
      "train_loss: 0.05949178739106017\n",
      "val_loss: 0.1568364364132077\n",
      "Progress: 81.1% ... Training loss: 0.060 ... Validation loss: 0.190iteration: 8110\n",
      "train_loss: 0.06014932978657754\n",
      "val_loss: 0.19044476714258288\n",
      "Progress: 81.1% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 8111\n",
      "train_loss: 0.05986876894646153\n",
      "val_loss: 0.16817242732568502\n",
      "Progress: 81.1% ... Training loss: 0.058 ... Validation loss: 0.178iteration: 8112\n",
      "train_loss: 0.05893502906199333\n",
      "val_loss: 0.17824176189092147\n",
      "Progress: 81.1% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 8113\n",
      "train_loss: 0.06075596079888096\n",
      "val_loss: 0.16149778197568135\n",
      "Progress: 81.1% ... Training loss: 0.058 ... Validation loss: 0.168iteration: 8114\n",
      "train_loss: 0.058577436199651424\n",
      "val_loss: 0.16899053773211825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 81.2% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 8115\n",
      "train_loss: 0.06105597845767382\n",
      "val_loss: 0.16205992103395342\n",
      "Progress: 81.2% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 8116\n",
      "train_loss: 0.05986972798706914\n",
      "val_loss: 0.1695575890245715\n",
      "Progress: 81.2% ... Training loss: 0.061 ... Validation loss: 0.189iteration: 8117\n",
      "train_loss: 0.061862206485586935\n",
      "val_loss: 0.18986586585896764\n",
      "Progress: 81.2% ... Training loss: 0.071 ... Validation loss: 0.152iteration: 8118\n",
      "train_loss: 0.07186287379197594\n",
      "val_loss: 0.15208044110697475\n",
      "Progress: 81.2% ... Training loss: 0.087 ... Validation loss: 0.219iteration: 8119\n",
      "train_loss: 0.08782125022698346\n",
      "val_loss: 0.21945015482861172\n",
      "Progress: 81.2% ... Training loss: 0.071 ... Validation loss: 0.151iteration: 8120\n",
      "train_loss: 0.07166814273898504\n",
      "val_loss: 0.1519854340943468\n",
      "Progress: 81.2% ... Training loss: 0.063 ... Validation loss: 0.185iteration: 8121\n",
      "train_loss: 0.06327421816417099\n",
      "val_loss: 0.18571032522913455\n",
      "Progress: 81.2% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 8122\n",
      "train_loss: 0.058630428442936006\n",
      "val_loss: 0.16144856496633955\n",
      "Progress: 81.2% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8123\n",
      "train_loss: 0.058499038660024746\n",
      "val_loss: 0.1708056893858685\n",
      "Progress: 81.2% ... Training loss: 0.061 ... Validation loss: 0.147iteration: 8124\n",
      "train_loss: 0.06189978837057211\n",
      "val_loss: 0.1474477482757137\n",
      "Progress: 81.2% ... Training loss: 0.058 ... Validation loss: 0.177iteration: 8125\n",
      "train_loss: 0.05870152193699485\n",
      "val_loss: 0.17743853209625177\n",
      "Progress: 81.3% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8126\n",
      "train_loss: 0.05860169085568093\n",
      "val_loss: 0.16342901085802553\n",
      "Progress: 81.3% ... Training loss: 0.058 ... Validation loss: 0.175iteration: 8127\n",
      "train_loss: 0.058551101584677065\n",
      "val_loss: 0.17548717696256658\n",
      "Progress: 81.3% ... Training loss: 0.061 ... Validation loss: 0.182iteration: 8128\n",
      "train_loss: 0.06144996899356059\n",
      "val_loss: 0.182355086540934\n",
      "Progress: 81.3% ... Training loss: 0.063 ... Validation loss: 0.158iteration: 8129\n",
      "train_loss: 0.06304043763538958\n",
      "val_loss: 0.15804441966705277\n",
      "Progress: 81.3% ... Training loss: 0.068 ... Validation loss: 0.214iteration: 8130\n",
      "train_loss: 0.06807840881540202\n",
      "val_loss: 0.2147406103328354\n",
      "Progress: 81.3% ... Training loss: 0.072 ... Validation loss: 0.156iteration: 8131\n",
      "train_loss: 0.07275987275001417\n",
      "val_loss: 0.1565030054571965\n",
      "Progress: 81.3% ... Training loss: 0.072 ... Validation loss: 0.229iteration: 8132\n",
      "train_loss: 0.07253396073887693\n",
      "val_loss: 0.2290279799215885\n",
      "Progress: 81.3% ... Training loss: 0.070 ... Validation loss: 0.159iteration: 8133\n",
      "train_loss: 0.07048698679619954\n",
      "val_loss: 0.15913523544828342\n",
      "Progress: 81.3% ... Training loss: 0.064 ... Validation loss: 0.207iteration: 8134\n",
      "train_loss: 0.06496241132556242\n",
      "val_loss: 0.2073965807760428\n",
      "Progress: 81.3% ... Training loss: 0.063 ... Validation loss: 0.163iteration: 8135\n",
      "train_loss: 0.06363796300733032\n",
      "val_loss: 0.16389868814492242\n",
      "Progress: 81.4% ... Training loss: 0.065 ... Validation loss: 0.213iteration: 8136\n",
      "train_loss: 0.06508640675255506\n",
      "val_loss: 0.21301117441408784\n",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 8137\n",
      "train_loss: 0.05869100373634086\n",
      "val_loss: 0.1667265228913283\n",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.173iteration: 8138\n",
      "train_loss: 0.05851092665122808\n",
      "val_loss: 0.17386701964700696\n",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.182iteration: 8139\n",
      "train_loss: 0.05890800327507068\n",
      "val_loss: 0.18243677273754078\n",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.174iteration: 8140\n",
      "train_loss: 0.058637177679943815\n",
      "val_loss: 0.1741443662623584\n",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.175iteration: 8141\n",
      "train_loss: 0.05879696811487041\n",
      "val_loss: 0.17521884597361806\n",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8142\n",
      "train_loss: 0.058442692987512444\n",
      "val_loss: 0.17068295593987712\n",
      "Progress: 81.4% ... Training loss: 0.059 ... Validation loss: 0.186iteration: 8143\n",
      "train_loss: 0.05982239924882749\n",
      "val_loss: 0.18695334965263813\n",
      "Progress: 81.4% ... Training loss: 0.063 ... Validation loss: 0.168iteration: 8144\n",
      "train_loss: 0.06384798780037051\n",
      "val_loss: 0.1683428694467803\n",
      "Progress: 81.5% ... Training loss: 0.060 ... Validation loss: 0.184iteration: 8145\n",
      "train_loss: 0.06025729966628491\n",
      "val_loss: 0.18419834625774584\n",
      "Progress: 81.5% ... Training loss: 0.085 ... Validation loss: 0.155iteration: 8146\n",
      "train_loss: 0.08508704811059614\n",
      "val_loss: 0.15575838274728065\n",
      "Progress: 81.5% ... Training loss: 0.083 ... Validation loss: 0.225iteration: 8147\n",
      "train_loss: 0.08372360957011094\n",
      "val_loss: 0.22593081821258415\n",
      "Progress: 81.5% ... Training loss: 0.093 ... Validation loss: 0.155iteration: 8148\n",
      "train_loss: 0.09343294004307814\n",
      "val_loss: 0.1551631837893776\n",
      "Progress: 81.5% ... Training loss: 0.073 ... Validation loss: 0.219iteration: 8149\n",
      "train_loss: 0.07346778870833633\n",
      "val_loss: 0.21969253140716702\n",
      "Progress: 81.5% ... Training loss: 0.070 ... Validation loss: 0.156iteration: 8150\n",
      "train_loss: 0.07081512932781554\n",
      "val_loss: 0.1563492794639806\n",
      "Progress: 81.5% ... Training loss: 0.064 ... Validation loss: 0.192iteration: 8151\n",
      "train_loss: 0.06479287490132495\n",
      "val_loss: 0.19207504793499774\n",
      "Progress: 81.5% ... Training loss: 0.062 ... Validation loss: 0.159iteration: 8152\n",
      "train_loss: 0.06201858628752381\n",
      "val_loss: 0.15906825421108267\n",
      "Progress: 81.5% ... Training loss: 0.063 ... Validation loss: 0.192iteration: 8153\n",
      "train_loss: 0.06328828526670015\n",
      "val_loss: 0.1921380188348082\n",
      "Progress: 81.5% ... Training loss: 0.063 ... Validation loss: 0.160iteration: 8154\n",
      "train_loss: 0.06398646324016113\n",
      "val_loss: 0.16054982654910563\n",
      "Progress: 81.5% ... Training loss: 0.058 ... Validation loss: 0.177iteration: 8155\n",
      "train_loss: 0.05877491832207964\n",
      "val_loss: 0.1777741769775522\n",
      "Progress: 81.6% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8156\n",
      "train_loss: 0.0587138994610194\n",
      "val_loss: 0.1676473135906253\n",
      "Progress: 81.6% ... Training loss: 0.059 ... Validation loss: 0.178iteration: 8157\n",
      "train_loss: 0.05936401294291075\n",
      "val_loss: 0.17852642900153096\n",
      "Progress: 81.6% ... Training loss: 0.071 ... Validation loss: 0.152iteration: 8158\n",
      "train_loss: 0.07163602173771759\n",
      "val_loss: 0.15275372650301716\n",
      "Progress: 81.6% ... Training loss: 0.065 ... Validation loss: 0.197iteration: 8159\n",
      "train_loss: 0.06527967019989948\n",
      "val_loss: 0.19764089712909108\n",
      "Progress: 81.6% ... Training loss: 0.072 ... Validation loss: 0.161iteration: 8160\n",
      "train_loss: 0.07290626865585369\n",
      "val_loss: 0.16130820218765948\n",
      "Progress: 81.6% ... Training loss: 0.063 ... Validation loss: 0.196iteration: 8161\n",
      "train_loss: 0.06322528420654004\n",
      "val_loss: 0.19636604868628682\n",
      "Progress: 81.6% ... Training loss: 0.080 ... Validation loss: 0.166iteration: 8162\n",
      "train_loss: 0.08001984481234421\n",
      "val_loss: 0.16619753142040894\n",
      "Progress: 81.6% ... Training loss: 0.080 ... Validation loss: 0.225iteration: 8163\n",
      "train_loss: 0.08061235574979633\n",
      "val_loss: 0.22549370896674945\n",
      "Progress: 81.6% ... Training loss: 0.069 ... Validation loss: 0.165iteration: 8164\n",
      "train_loss: 0.06917780677431269\n",
      "val_loss: 0.16546990418875096\n",
      "Progress: 81.7% ... Training loss: 0.069 ... Validation loss: 0.212iteration: 8165\n",
      "train_loss: 0.06967026367094063\n",
      "val_loss: 0.21279022153730864\n",
      "Progress: 81.7% ... Training loss: 0.058 ... Validation loss: 0.173iteration: 8166\n",
      "train_loss: 0.058772274381556425\n",
      "val_loss: 0.17378809494818268\n",
      "Progress: 81.7% ... Training loss: 0.058 ... Validation loss: 0.175iteration: 8167\n",
      "train_loss: 0.058520362000570834\n",
      "val_loss: 0.17509725980858476\n",
      "Progress: 81.7% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 8168\n",
      "train_loss: 0.0590098824365249\n",
      "val_loss: 0.17116818895453303\n",
      "Progress: 81.7% ... Training loss: 0.065 ... Validation loss: 0.152iteration: 8169\n",
      "train_loss: 0.06514425089225877\n",
      "val_loss: 0.15225684092501243\n",
      "Progress: 81.7% ... Training loss: 0.063 ... Validation loss: 0.181iteration: 8170\n",
      "train_loss: 0.0637742778484388\n",
      "val_loss: 0.18175108325750627\n",
      "Progress: 81.7% ... Training loss: 0.067 ... Validation loss: 0.149iteration: 8171\n",
      "train_loss: 0.06754220409725352\n",
      "val_loss: 0.1492115860055701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 81.7% ... Training loss: 0.073 ... Validation loss: 0.210iteration: 8172\n",
      "train_loss: 0.07321610447778021\n",
      "val_loss: 0.21095573810427115\n",
      "Progress: 81.7% ... Training loss: 0.074 ... Validation loss: 0.152iteration: 8173\n",
      "train_loss: 0.07437857886984256\n",
      "val_loss: 0.15282749555158573\n",
      "Progress: 81.7% ... Training loss: 0.094 ... Validation loss: 0.257iteration: 8174\n",
      "train_loss: 0.09420422180296795\n",
      "val_loss: 0.25713086924004375\n",
      "Progress: 81.8% ... Training loss: 0.130 ... Validation loss: 0.175iteration: 8175\n",
      "train_loss: 0.13043396431608076\n",
      "val_loss: 0.17575936158832856\n",
      "Progress: 81.8% ... Training loss: 0.095 ... Validation loss: 0.239iteration: 8176\n",
      "train_loss: 0.09549782929115755\n",
      "val_loss: 0.239656379966924\n",
      "Progress: 81.8% ... Training loss: 0.077 ... Validation loss: 0.159iteration: 8177\n",
      "train_loss: 0.07773741310840108\n",
      "val_loss: 0.15938504324479197\n",
      "Progress: 81.8% ... Training loss: 0.061 ... Validation loss: 0.189iteration: 8178\n",
      "train_loss: 0.06189397155255595\n",
      "val_loss: 0.1897479541317428\n",
      "Progress: 81.8% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 8179\n",
      "train_loss: 0.05857725525252845\n",
      "val_loss: 0.17109603458436054\n",
      "Progress: 81.8% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 8180\n",
      "train_loss: 0.06180399836508326\n",
      "val_loss: 0.15987335339704273\n",
      "Progress: 81.8% ... Training loss: 0.059 ... Validation loss: 0.180iteration: 8181\n",
      "train_loss: 0.05914948816680349\n",
      "val_loss: 0.1806890672817428\n",
      "Progress: 81.8% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8182\n",
      "train_loss: 0.05862666487664397\n",
      "val_loss: 0.17014726806483094\n",
      "Progress: 81.8% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8183\n",
      "train_loss: 0.05891803625893027\n",
      "val_loss: 0.16777668549446004\n",
      "Progress: 81.8% ... Training loss: 0.071 ... Validation loss: 0.153iteration: 8184\n",
      "train_loss: 0.07151735120774062\n",
      "val_loss: 0.15370932862954964\n",
      "Progress: 81.8% ... Training loss: 0.070 ... Validation loss: 0.193iteration: 8185\n",
      "train_loss: 0.07018520756840937\n",
      "val_loss: 0.1935648179932175\n",
      "Progress: 81.9% ... Training loss: 0.074 ... Validation loss: 0.154iteration: 8186\n",
      "train_loss: 0.07497948208782049\n",
      "val_loss: 0.15449764936884314\n",
      "Progress: 81.9% ... Training loss: 0.069 ... Validation loss: 0.190iteration: 8187\n",
      "train_loss: 0.06959147631275303\n",
      "val_loss: 0.19012417168602566\n",
      "Progress: 81.9% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 8188\n",
      "train_loss: 0.06140942042544876\n",
      "val_loss: 0.1525293872849856\n",
      "Progress: 81.9% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8189\n",
      "train_loss: 0.058708792211873306\n",
      "val_loss: 0.16779479365405997\n",
      "Progress: 81.9% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 8190\n",
      "train_loss: 0.05872178534870108\n",
      "val_loss: 0.16628853312964392\n",
      "Progress: 81.9% ... Training loss: 0.063 ... Validation loss: 0.153iteration: 8191\n",
      "train_loss: 0.06329749957299777\n",
      "val_loss: 0.15364338008948736\n",
      "Progress: 81.9% ... Training loss: 0.070 ... Validation loss: 0.191iteration: 8192\n",
      "train_loss: 0.07005840947669029\n",
      "val_loss: 0.19156439399316583\n",
      "Progress: 81.9% ... Training loss: 0.094 ... Validation loss: 0.157iteration: 8193\n",
      "train_loss: 0.09402234205453844\n",
      "val_loss: 0.1572895367636715\n",
      "Progress: 81.9% ... Training loss: 0.097 ... Validation loss: 0.224iteration: 8194\n",
      "train_loss: 0.09702440094111793\n",
      "val_loss: 0.22414021980209112\n",
      "Progress: 82.0% ... Training loss: 0.074 ... Validation loss: 0.150iteration: 8195\n",
      "train_loss: 0.07491099007079523\n",
      "val_loss: 0.1502188489765045\n",
      "Progress: 82.0% ... Training loss: 0.065 ... Validation loss: 0.189iteration: 8196\n",
      "train_loss: 0.06500823167629995\n",
      "val_loss: 0.1893283000935371\n",
      "Progress: 82.0% ... Training loss: 0.078 ... Validation loss: 0.153iteration: 8197\n",
      "train_loss: 0.07887748029244779\n",
      "val_loss: 0.15370896558036334\n",
      "Progress: 82.0% ... Training loss: 0.079 ... Validation loss: 0.232iteration: 8198\n",
      "train_loss: 0.07980664401489969\n",
      "val_loss: 0.23211928118684094\n",
      "Progress: 82.0% ... Training loss: 0.082 ... Validation loss: 0.155iteration: 8199\n",
      "train_loss: 0.08259518268572798\n",
      "val_loss: 0.1554610275828944\n",
      "Progress: 82.0% ... Training loss: 0.103 ... Validation loss: 0.256iteration: 8200\n",
      "train_loss: 0.10350459533802964\n",
      "val_loss: 0.25674016749088274\n",
      "Progress: 82.0% ... Training loss: 0.143 ... Validation loss: 0.172iteration: 8201\n",
      "train_loss: 0.14348627415735044\n",
      "val_loss: 0.17259698773755094\n",
      "Progress: 82.0% ... Training loss: 0.087 ... Validation loss: 0.236iteration: 8202\n",
      "train_loss: 0.08704919187029378\n",
      "val_loss: 0.2366664035972919\n",
      "Progress: 82.0% ... Training loss: 0.134 ... Validation loss: 0.167iteration: 8203\n",
      "train_loss: 0.13446145985447233\n",
      "val_loss: 0.16730409579976502\n",
      "Progress: 82.0% ... Training loss: 0.121 ... Validation loss: 0.290iteration: 8204\n",
      "train_loss: 0.12121351051443169\n",
      "val_loss: 0.2905118599131095\n",
      "Progress: 82.0% ... Training loss: 0.092 ... Validation loss: 0.159iteration: 8205\n",
      "train_loss: 0.09231886639245042\n",
      "val_loss: 0.1592408054381371\n",
      "Progress: 82.1% ... Training loss: 0.090 ... Validation loss: 0.229iteration: 8206\n",
      "train_loss: 0.09019225551563102\n",
      "val_loss: 0.22961821812779878\n",
      "Progress: 82.1% ... Training loss: 0.070 ... Validation loss: 0.153iteration: 8207\n",
      "train_loss: 0.0701682645735315\n",
      "val_loss: 0.153790405625547\n",
      "Progress: 82.1% ... Training loss: 0.065 ... Validation loss: 0.194iteration: 8208\n",
      "train_loss: 0.06597116980847502\n",
      "val_loss: 0.19440248519457393\n",
      "Progress: 82.1% ... Training loss: 0.058 ... Validation loss: 0.158iteration: 8209\n",
      "train_loss: 0.05886838787751855\n",
      "val_loss: 0.15881150084548654\n",
      "Progress: 82.1% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 8210\n",
      "train_loss: 0.05971120833727317\n",
      "val_loss: 0.156917688538107\n",
      "Progress: 82.1% ... Training loss: 0.067 ... Validation loss: 0.195iteration: 8211\n",
      "train_loss: 0.06724342321795161\n",
      "val_loss: 0.195597468399587\n",
      "Progress: 82.1% ... Training loss: 0.074 ... Validation loss: 0.152iteration: 8212\n",
      "train_loss: 0.07454199640811479\n",
      "val_loss: 0.15229811529169962\n",
      "Progress: 82.1% ... Training loss: 0.077 ... Validation loss: 0.211iteration: 8213\n",
      "train_loss: 0.07711495987568827\n",
      "val_loss: 0.2117822439910053\n",
      "Progress: 82.1% ... Training loss: 0.093 ... Validation loss: 0.161iteration: 8214\n",
      "train_loss: 0.09328796649668879\n",
      "val_loss: 0.16118907830669194\n",
      "Progress: 82.2% ... Training loss: 0.073 ... Validation loss: 0.216iteration: 8215\n",
      "train_loss: 0.07397887739958747\n",
      "val_loss: 0.21619439762308476\n",
      "Progress: 82.2% ... Training loss: 0.065 ... Validation loss: 0.158iteration: 8216\n",
      "train_loss: 0.06592498836963387\n",
      "val_loss: 0.15845797332005318\n",
      "Progress: 82.2% ... Training loss: 0.063 ... Validation loss: 0.181iteration: 8217\n",
      "train_loss: 0.06335841241349471\n",
      "val_loss: 0.18115471684495607\n",
      "Progress: 82.2% ... Training loss: 0.064 ... Validation loss: 0.163iteration: 8218\n",
      "train_loss: 0.06462361032503317\n",
      "val_loss: 0.16353803154594385\n",
      "Progress: 82.2% ... Training loss: 0.065 ... Validation loss: 0.186iteration: 8219\n",
      "train_loss: 0.06586799503293765\n",
      "val_loss: 0.186417663989504\n",
      "Progress: 82.2% ... Training loss: 0.063 ... Validation loss: 0.161iteration: 8220\n",
      "train_loss: 0.06338483227741376\n",
      "val_loss: 0.16165335790697088\n",
      "Progress: 82.2% ... Training loss: 0.059 ... Validation loss: 0.173iteration: 8221\n",
      "train_loss: 0.059784362518595466\n",
      "val_loss: 0.1733362119502627\n",
      "Progress: 82.2% ... Training loss: 0.060 ... Validation loss: 0.177iteration: 8222\n",
      "train_loss: 0.06081879111971596\n",
      "val_loss: 0.1772399080837771\n",
      "Progress: 82.2% ... Training loss: 0.073 ... Validation loss: 0.149iteration: 8223\n",
      "train_loss: 0.07387694672961174\n",
      "val_loss: 0.14962022271831232\n",
      "Progress: 82.2% ... Training loss: 0.065 ... Validation loss: 0.181iteration: 8224\n",
      "train_loss: 0.06582438349389286\n",
      "val_loss: 0.18134096471859745\n",
      "Progress: 82.2% ... Training loss: 0.066 ... Validation loss: 0.152iteration: 8225\n",
      "train_loss: 0.06601376964244662\n",
      "val_loss: 0.15208516137950562\n",
      "Progress: 82.3% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 8226\n",
      "train_loss: 0.06041433815564452\n",
      "val_loss: 0.17484996998364458\n",
      "Progress: 82.3% ... Training loss: 0.068 ... Validation loss: 0.151iteration: 8227\n",
      "train_loss: 0.06818698858979352\n",
      "val_loss: 0.15138361448269955\n",
      "Progress: 82.3% ... Training loss: 0.069 ... Validation loss: 0.196iteration: 8228\n",
      "train_loss: 0.06964750467867813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.19694743751962823\n",
      "Progress: 82.3% ... Training loss: 0.061 ... Validation loss: 0.155iteration: 8229\n",
      "train_loss: 0.061472975667999814\n",
      "val_loss: 0.1553786373362739\n",
      "Progress: 82.3% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 8230\n",
      "train_loss: 0.0645803259311237\n",
      "val_loss: 0.1765161837001689\n",
      "Progress: 82.3% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 8231\n",
      "train_loss: 0.06051981708834607\n",
      "val_loss: 0.15119871278474248\n",
      "Progress: 82.3% ... Training loss: 0.074 ... Validation loss: 0.176iteration: 8232\n",
      "train_loss: 0.07409037359966289\n",
      "val_loss: 0.17600960273361851\n",
      "Progress: 82.3% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 8233\n",
      "train_loss: 0.05932371806966466\n",
      "val_loss: 0.15540336761150997\n",
      "Progress: 82.3% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 8234\n",
      "train_loss: 0.05898744173350771\n",
      "val_loss: 0.15701625175738157\n",
      "Progress: 82.3% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 8235\n",
      "train_loss: 0.05907913435842326\n",
      "val_loss: 0.17126140039804755\n",
      "Progress: 82.4% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 8236\n",
      "train_loss: 0.05907397883935789\n",
      "val_loss: 0.15996408155046088\n",
      "Progress: 82.4% ... Training loss: 0.058 ... Validation loss: 0.158iteration: 8237\n",
      "train_loss: 0.0587309600915868\n",
      "val_loss: 0.1589929876724838\n",
      "Progress: 82.4% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 8238\n",
      "train_loss: 0.05992020437107161\n",
      "val_loss: 0.17102572343441685\n",
      "Progress: 82.4% ... Training loss: 0.071 ... Validation loss: 0.164iteration: 8239\n",
      "train_loss: 0.07192075552447737\n",
      "val_loss: 0.1640347416714665\n",
      "Progress: 82.4% ... Training loss: 0.064 ... Validation loss: 0.192iteration: 8240\n",
      "train_loss: 0.06429760926118108\n",
      "val_loss: 0.19254844485636735\n",
      "Progress: 82.4% ... Training loss: 0.062 ... Validation loss: 0.163iteration: 8241\n",
      "train_loss: 0.06277244005866535\n",
      "val_loss: 0.16360327100970037\n",
      "Progress: 82.4% ... Training loss: 0.065 ... Validation loss: 0.191iteration: 8242\n",
      "train_loss: 0.06555152859387015\n",
      "val_loss: 0.1918338605708072\n",
      "Progress: 82.4% ... Training loss: 0.073 ... Validation loss: 0.153iteration: 8243\n",
      "train_loss: 0.07375875081172109\n",
      "val_loss: 0.15305889248651813\n",
      "Progress: 82.4% ... Training loss: 0.066 ... Validation loss: 0.176iteration: 8244\n",
      "train_loss: 0.06623258220078969\n",
      "val_loss: 0.17673431670557596\n",
      "Progress: 82.5% ... Training loss: 0.071 ... Validation loss: 0.154iteration: 8245\n",
      "train_loss: 0.07199557642172225\n",
      "val_loss: 0.1540673465612475\n",
      "Progress: 82.5% ... Training loss: 0.070 ... Validation loss: 0.191iteration: 8246\n",
      "train_loss: 0.07075156917715071\n",
      "val_loss: 0.19131865001965437\n",
      "Progress: 82.5% ... Training loss: 0.081 ... Validation loss: 0.155iteration: 8247\n",
      "train_loss: 0.08158065589866152\n",
      "val_loss: 0.15502115420639173\n",
      "Progress: 82.5% ... Training loss: 0.103 ... Validation loss: 0.231iteration: 8248\n",
      "train_loss: 0.10314692815572926\n",
      "val_loss: 0.23126182377965657\n",
      "Progress: 82.5% ... Training loss: 0.101 ... Validation loss: 0.163iteration: 8249\n",
      "train_loss: 0.10178714436942718\n",
      "val_loss: 0.16385232367614455\n",
      "Progress: 82.5% ... Training loss: 0.097 ... Validation loss: 0.207iteration: 8250\n",
      "train_loss: 0.09779339389796124\n",
      "val_loss: 0.20782945289325633\n",
      "Progress: 82.5% ... Training loss: 0.088 ... Validation loss: 0.159iteration: 8251\n",
      "train_loss: 0.08839298263683396\n",
      "val_loss: 0.15976911339879687\n",
      "Progress: 82.5% ... Training loss: 0.073 ... Validation loss: 0.199iteration: 8252\n",
      "train_loss: 0.07377354167607925\n",
      "val_loss: 0.19917980755113665\n",
      "Progress: 82.5% ... Training loss: 0.062 ... Validation loss: 0.156iteration: 8253\n",
      "train_loss: 0.06296769997278504\n",
      "val_loss: 0.15622713227650276\n",
      "Progress: 82.5% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 8254\n",
      "train_loss: 0.060764684711638636\n",
      "val_loss: 0.1747605155532401\n",
      "Progress: 82.5% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 8255\n",
      "train_loss: 0.06328112556151946\n",
      "val_loss: 0.15797574783274537\n",
      "Progress: 82.6% ... Training loss: 0.065 ... Validation loss: 0.197iteration: 8256\n",
      "train_loss: 0.06536282088712753\n",
      "val_loss: 0.1970119927287719\n",
      "Progress: 82.6% ... Training loss: 0.067 ... Validation loss: 0.154iteration: 8257\n",
      "train_loss: 0.06711890704565023\n",
      "val_loss: 0.15459737902926635\n",
      "Progress: 82.6% ... Training loss: 0.060 ... Validation loss: 0.185iteration: 8258\n",
      "train_loss: 0.06056295152730219\n",
      "val_loss: 0.1853901482631517\n",
      "Progress: 82.6% ... Training loss: 0.059 ... Validation loss: 0.173iteration: 8259\n",
      "train_loss: 0.059658799967453414\n",
      "val_loss: 0.17313317437099163\n",
      "Progress: 82.6% ... Training loss: 0.072 ... Validation loss: 0.153iteration: 8260\n",
      "train_loss: 0.0729935498990405\n",
      "val_loss: 0.15384100432665587\n",
      "Progress: 82.6% ... Training loss: 0.070 ... Validation loss: 0.210iteration: 8261\n",
      "train_loss: 0.07085111843728484\n",
      "val_loss: 0.21056398685268857\n",
      "Progress: 82.6% ... Training loss: 0.062 ... Validation loss: 0.152iteration: 8262\n",
      "train_loss: 0.06274985471835227\n",
      "val_loss: 0.1523147402693873\n",
      "Progress: 82.6% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 8263\n",
      "train_loss: 0.06017970160284584\n",
      "val_loss: 0.16103993811602152\n",
      "Progress: 82.6% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 8264\n",
      "train_loss: 0.06060594820296644\n",
      "val_loss: 0.17455560985392576\n",
      "Progress: 82.7% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 8265\n",
      "train_loss: 0.061510061144305676\n",
      "val_loss: 0.15988884615376692\n",
      "Progress: 82.7% ... Training loss: 0.072 ... Validation loss: 0.195iteration: 8266\n",
      "train_loss: 0.0721351810413383\n",
      "val_loss: 0.19503656306348338\n",
      "Progress: 82.7% ... Training loss: 0.079 ... Validation loss: 0.151iteration: 8267\n",
      "train_loss: 0.07962140627763509\n",
      "val_loss: 0.15154650438579984\n",
      "Progress: 82.7% ... Training loss: 0.078 ... Validation loss: 0.213iteration: 8268\n",
      "train_loss: 0.07885740907502937\n",
      "val_loss: 0.21334157895436556\n",
      "Progress: 82.7% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 8269\n",
      "train_loss: 0.061498062788054746\n",
      "val_loss: 0.1499994164001152\n",
      "Progress: 82.7% ... Training loss: 0.060 ... Validation loss: 0.177iteration: 8270\n",
      "train_loss: 0.06006138253141289\n",
      "val_loss: 0.17768417747258358\n",
      "Progress: 82.7% ... Training loss: 0.059 ... Validation loss: 0.183iteration: 8271\n",
      "train_loss: 0.059674983933279614\n",
      "val_loss: 0.18375583375866197\n",
      "Progress: 82.7% ... Training loss: 0.059 ... Validation loss: 0.166iteration: 8272\n",
      "train_loss: 0.05921656102965983\n",
      "val_loss: 0.16664647332054633\n",
      "Progress: 82.7% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 8273\n",
      "train_loss: 0.05947053186604586\n",
      "val_loss: 0.1572963432329866\n",
      "Progress: 82.7% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 8274\n",
      "train_loss: 0.05805073610549712\n",
      "val_loss: 0.16951083944253978\n",
      "Progress: 82.8% ... Training loss: 0.059 ... Validation loss: 0.163iteration: 8275\n",
      "train_loss: 0.05901585956092708\n",
      "val_loss: 0.1637456052115523\n",
      "Progress: 82.8% ... Training loss: 0.066 ... Validation loss: 0.196iteration: 8276\n",
      "train_loss: 0.06673276619260611\n",
      "val_loss: 0.19606213879089399\n",
      "Progress: 82.8% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 8277\n",
      "train_loss: 0.06304472739796259\n",
      "val_loss: 0.15633143317096462\n",
      "Progress: 82.8% ... Training loss: 0.067 ... Validation loss: 0.189iteration: 8278\n",
      "train_loss: 0.06742679787577664\n",
      "val_loss: 0.18918228899174408\n",
      "Progress: 82.8% ... Training loss: 0.064 ... Validation loss: 0.151iteration: 8279\n",
      "train_loss: 0.06486655255246045\n",
      "val_loss: 0.15186490927054475\n",
      "Progress: 82.8% ... Training loss: 0.062 ... Validation loss: 0.168iteration: 8280\n",
      "train_loss: 0.062191423073520345\n",
      "val_loss: 0.16860587257721435\n",
      "Progress: 82.8% ... Training loss: 0.064 ... Validation loss: 0.181iteration: 8281\n",
      "train_loss: 0.06417686459324737\n",
      "val_loss: 0.18146335967452107\n",
      "Progress: 82.8% ... Training loss: 0.060 ... Validation loss: 0.154iteration: 8282\n",
      "train_loss: 0.06025788591835727\n",
      "val_loss: 0.15497113561708473\n",
      "Progress: 82.8% ... Training loss: 0.069 ... Validation loss: 0.200iteration: 8283\n",
      "train_loss: 0.0693875031085095\n",
      "val_loss: 0.2000156873812021\n",
      "Progress: 82.8% ... Training loss: 0.066 ... Validation loss: 0.153iteration: 8284\n",
      "train_loss: 0.06694982442590633\n",
      "val_loss: 0.15374327837647372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 82.8% ... Training loss: 0.065 ... Validation loss: 0.187iteration: 8285\n",
      "train_loss: 0.06517347503754838\n",
      "val_loss: 0.18792374552803165\n",
      "Progress: 82.9% ... Training loss: 0.062 ... Validation loss: 0.157iteration: 8286\n",
      "train_loss: 0.06248019119814604\n",
      "val_loss: 0.15794881601230576\n",
      "Progress: 82.9% ... Training loss: 0.058 ... Validation loss: 0.174iteration: 8287\n",
      "train_loss: 0.058953703813031146\n",
      "val_loss: 0.17430580080970334\n",
      "Progress: 82.9% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8288\n",
      "train_loss: 0.05828459813019723\n",
      "val_loss: 0.1670081543759553\n",
      "Progress: 82.9% ... Training loss: 0.061 ... Validation loss: 0.156iteration: 8289\n",
      "train_loss: 0.061712878180939\n",
      "val_loss: 0.1567031695724611\n",
      "Progress: 82.9% ... Training loss: 0.061 ... Validation loss: 0.176iteration: 8290\n",
      "train_loss: 0.06157126970248014\n",
      "val_loss: 0.17698764438810785\n",
      "Progress: 82.9% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8291\n",
      "train_loss: 0.05870246324251159\n",
      "val_loss: 0.16368864982327136\n",
      "Progress: 82.9% ... Training loss: 0.060 ... Validation loss: 0.177iteration: 8292\n",
      "train_loss: 0.06022557218968097\n",
      "val_loss: 0.17750819427158707\n",
      "Progress: 82.9% ... Training loss: 0.074 ... Validation loss: 0.151iteration: 8293\n",
      "train_loss: 0.07441299733864473\n",
      "val_loss: 0.15177472746923582\n",
      "Progress: 82.9% ... Training loss: 0.071 ... Validation loss: 0.215iteration: 8294\n",
      "train_loss: 0.07182051468354327\n",
      "val_loss: 0.2156998720258948\n",
      "Progress: 83.0% ... Training loss: 0.083 ... Validation loss: 0.153iteration: 8295\n",
      "train_loss: 0.08313684401750435\n",
      "val_loss: 0.15325381923017775\n",
      "Progress: 83.0% ... Training loss: 0.077 ... Validation loss: 0.224iteration: 8296\n",
      "train_loss: 0.07767755252969083\n",
      "val_loss: 0.22489534996020347\n",
      "Progress: 83.0% ... Training loss: 0.110 ... Validation loss: 0.162iteration: 8297\n",
      "train_loss: 0.11009132440803117\n",
      "val_loss: 0.16240022900500087\n",
      "Progress: 83.0% ... Training loss: 0.112 ... Validation loss: 0.265iteration: 8298\n",
      "train_loss: 0.11249344557422274\n",
      "val_loss: 0.26586376146011537\n",
      "Progress: 83.0% ... Training loss: 0.085 ... Validation loss: 0.152iteration: 8299\n",
      "train_loss: 0.08549384594966648\n",
      "val_loss: 0.1523983095280745\n",
      "Progress: 83.0% ... Training loss: 0.118 ... Validation loss: 0.238iteration: 8300\n",
      "train_loss: 0.11820201020920021\n",
      "val_loss: 0.23879458665142736\n",
      "Progress: 83.0% ... Training loss: 0.095 ... Validation loss: 0.161iteration: 8301\n",
      "train_loss: 0.09513944119089428\n",
      "val_loss: 0.1613071732191555\n",
      "Progress: 83.0% ... Training loss: 0.090 ... Validation loss: 0.219iteration: 8302\n",
      "train_loss: 0.09089694075426805\n",
      "val_loss: 0.21924466182676727\n",
      "Progress: 83.0% ... Training loss: 0.073 ... Validation loss: 0.153iteration: 8303\n",
      "train_loss: 0.07364378866785282\n",
      "val_loss: 0.1535091620917815\n",
      "Progress: 83.0% ... Training loss: 0.070 ... Validation loss: 0.206iteration: 8304\n",
      "train_loss: 0.07022201592448159\n",
      "val_loss: 0.20664352740191488\n",
      "Progress: 83.0% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 8305\n",
      "train_loss: 0.060087172497211606\n",
      "val_loss: 0.15742599728777995\n",
      "Progress: 83.1% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8306\n",
      "train_loss: 0.058944238609037566\n",
      "val_loss: 0.1653740469815803\n",
      "Progress: 83.1% ... Training loss: 0.061 ... Validation loss: 0.154iteration: 8307\n",
      "train_loss: 0.06178456619757979\n",
      "val_loss: 0.15472066171048032\n",
      "Progress: 83.1% ... Training loss: 0.060 ... Validation loss: 0.184iteration: 8308\n",
      "train_loss: 0.060322557571451677\n",
      "val_loss: 0.18434960349436882\n",
      "Progress: 83.1% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 8309\n",
      "train_loss: 0.05844451217114954\n",
      "val_loss: 0.16182463880213224\n",
      "Progress: 83.1% ... Training loss: 0.060 ... Validation loss: 0.184iteration: 8310\n",
      "train_loss: 0.06053421995825486\n",
      "val_loss: 0.18411120709736994\n",
      "Progress: 83.1% ... Training loss: 0.062 ... Validation loss: 0.154iteration: 8311\n",
      "train_loss: 0.0621274398501713\n",
      "val_loss: 0.15466542398437869\n",
      "Progress: 83.1% ... Training loss: 0.058 ... Validation loss: 0.162iteration: 8312\n",
      "train_loss: 0.05875141228060543\n",
      "val_loss: 0.16249161673086082\n",
      "Progress: 83.1% ... Training loss: 0.058 ... Validation loss: 0.179iteration: 8313\n",
      "train_loss: 0.058661177382081264\n",
      "val_loss: 0.179699924189703\n",
      "Progress: 83.1% ... Training loss: 0.064 ... Validation loss: 0.155iteration: 8314\n",
      "train_loss: 0.06489671974070764\n",
      "val_loss: 0.1551250977843009\n",
      "Progress: 83.2% ... Training loss: 0.061 ... Validation loss: 0.189iteration: 8315\n",
      "train_loss: 0.06131800743436092\n",
      "val_loss: 0.1896141060676175\n",
      "Progress: 83.2% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 8316\n",
      "train_loss: 0.06011224320344631\n",
      "val_loss: 0.16427068607718437\n",
      "Progress: 83.2% ... Training loss: 0.060 ... Validation loss: 0.185iteration: 8317\n",
      "train_loss: 0.06028118227671605\n",
      "val_loss: 0.18588467488374175\n",
      "Progress: 83.2% ... Training loss: 0.057 ... Validation loss: 0.162iteration: 8318\n",
      "train_loss: 0.05793905502544227\n",
      "val_loss: 0.16290735712670787\n",
      "Progress: 83.2% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 8319\n",
      "train_loss: 0.05850225656895274\n",
      "val_loss: 0.17148098307500387\n",
      "Progress: 83.2% ... Training loss: 0.058 ... Validation loss: 0.176iteration: 8320\n",
      "train_loss: 0.058286443351356056\n",
      "val_loss: 0.1769329306681358\n",
      "Progress: 83.2% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8321\n",
      "train_loss: 0.058083908500375155\n",
      "val_loss: 0.16450106301375833\n",
      "Progress: 83.2% ... Training loss: 0.059 ... Validation loss: 0.178iteration: 8322\n",
      "train_loss: 0.059059937500704957\n",
      "val_loss: 0.17829132712204634\n",
      "Progress: 83.2% ... Training loss: 0.062 ... Validation loss: 0.152iteration: 8323\n",
      "train_loss: 0.062347040430821996\n",
      "val_loss: 0.15292231477558893\n",
      "Progress: 83.2% ... Training loss: 0.071 ... Validation loss: 0.188iteration: 8324\n",
      "train_loss: 0.07138235140842354\n",
      "val_loss: 0.18853217519951404\n",
      "Progress: 83.2% ... Training loss: 0.068 ... Validation loss: 0.152iteration: 8325\n",
      "train_loss: 0.06804183766907627\n",
      "val_loss: 0.15251819871963856\n",
      "Progress: 83.3% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 8326\n",
      "train_loss: 0.059821264843920675\n",
      "val_loss: 0.16857615728561579\n",
      "Progress: 83.3% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8327\n",
      "train_loss: 0.05824707892736242\n",
      "val_loss: 0.16749117608037667\n",
      "Progress: 83.3% ... Training loss: 0.062 ... Validation loss: 0.158iteration: 8328\n",
      "train_loss: 0.06213018303991577\n",
      "val_loss: 0.15895946791895632\n",
      "Progress: 83.3% ... Training loss: 0.061 ... Validation loss: 0.179iteration: 8329\n",
      "train_loss: 0.061309777056370314\n",
      "val_loss: 0.1799251640779552\n",
      "Progress: 83.3% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8330\n",
      "train_loss: 0.058954892563326464\n",
      "val_loss: 0.1673258453003353\n",
      "Progress: 83.3% ... Training loss: 0.059 ... Validation loss: 0.178iteration: 8331\n",
      "train_loss: 0.059180913793666086\n",
      "val_loss: 0.1782896077727895\n",
      "Progress: 83.3% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 8332\n",
      "train_loss: 0.059150967037083224\n",
      "val_loss: 0.1702739114694718\n",
      "Progress: 83.3% ... Training loss: 0.058 ... Validation loss: 0.172iteration: 8333\n",
      "train_loss: 0.058741416271463655\n",
      "val_loss: 0.17274174142357954\n",
      "Progress: 83.3% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 8334\n",
      "train_loss: 0.06037615512338607\n",
      "val_loss: 0.1679961907114134\n",
      "Progress: 83.3% ... Training loss: 0.060 ... Validation loss: 0.187iteration: 8335\n",
      "train_loss: 0.06093220260898697\n",
      "val_loss: 0.18752036260773303\n",
      "Progress: 83.4% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 8336\n",
      "train_loss: 0.06047294831187685\n",
      "val_loss: 0.16504756539983312\n",
      "Progress: 83.4% ... Training loss: 0.061 ... Validation loss: 0.184iteration: 8337\n",
      "train_loss: 0.06101306275674232\n",
      "val_loss: 0.18462683202925104\n",
      "Progress: 83.4% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8338\n",
      "train_loss: 0.058652033936631126\n",
      "val_loss: 0.17055037776813073\n",
      "Progress: 83.4% ... Training loss: 0.059 ... Validation loss: 0.175iteration: 8339\n",
      "train_loss: 0.05946914139539227\n",
      "val_loss: 0.17583300927527906\n",
      "Progress: 83.4% ... Training loss: 0.063 ... Validation loss: 0.149iteration: 8340\n",
      "train_loss: 0.06375815355101183\n",
      "val_loss: 0.14968895460180467\n",
      "Progress: 83.4% ... Training loss: 0.060 ... Validation loss: 0.167iteration: 8341\n",
      "train_loss: 0.06046046865994865\n",
      "val_loss: 0.1677126597460046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 83.4% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 8342\n",
      "train_loss: 0.05862889628592073\n",
      "val_loss: 0.1614287455243845\n",
      "Progress: 83.4% ... Training loss: 0.062 ... Validation loss: 0.194iteration: 8343\n",
      "train_loss: 0.06207019373815697\n",
      "val_loss: 0.19467115238406557\n",
      "Progress: 83.4% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 8344\n",
      "train_loss: 0.0586378749207488\n",
      "val_loss: 0.16940572651877311\n",
      "Progress: 83.5% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 8345\n",
      "train_loss: 0.060423253735516935\n",
      "val_loss: 0.16629539249524794\n",
      "Progress: 83.5% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 8346\n",
      "train_loss: 0.05820442353143066\n",
      "val_loss: 0.16100072221428083\n",
      "Progress: 83.5% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 8347\n",
      "train_loss: 0.060729190189474425\n",
      "val_loss: 0.15549389190817478\n",
      "Progress: 83.5% ... Training loss: 0.071 ... Validation loss: 0.209iteration: 8348\n",
      "train_loss: 0.07115258383915915\n",
      "val_loss: 0.20955457617014572\n",
      "Progress: 83.5% ... Training loss: 0.067 ... Validation loss: 0.153iteration: 8349\n",
      "train_loss: 0.06702244632975511\n",
      "val_loss: 0.1539238570097209\n",
      "Progress: 83.5% ... Training loss: 0.058 ... Validation loss: 0.172iteration: 8350\n",
      "train_loss: 0.05825365633635768\n",
      "val_loss: 0.17270214625682104\n",
      "Progress: 83.5% ... Training loss: 0.058 ... Validation loss: 0.160iteration: 8351\n",
      "train_loss: 0.05850986094259177\n",
      "val_loss: 0.1604902634851744\n",
      "Progress: 83.5% ... Training loss: 0.060 ... Validation loss: 0.183iteration: 8352\n",
      "train_loss: 0.06027025469839153\n",
      "val_loss: 0.18319295350443265\n",
      "Progress: 83.5% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 8353\n",
      "train_loss: 0.05988022892533275\n",
      "val_loss: 0.16245977915818033\n",
      "Progress: 83.5% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8354\n",
      "train_loss: 0.05878158565106209\n",
      "val_loss: 0.1655858814103627\n",
      "Progress: 83.5% ... Training loss: 0.058 ... Validation loss: 0.182iteration: 8355\n",
      "train_loss: 0.05891286264472994\n",
      "val_loss: 0.18282706121981437\n",
      "Progress: 83.6% ... Training loss: 0.058 ... Validation loss: 0.173iteration: 8356\n",
      "train_loss: 0.05804949158777966\n",
      "val_loss: 0.17301165186237452\n",
      "Progress: 83.6% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 8357\n",
      "train_loss: 0.06136655518498334\n",
      "val_loss: 0.15763330953585192\n",
      "Progress: 83.6% ... Training loss: 0.060 ... Validation loss: 0.188iteration: 8358\n",
      "train_loss: 0.06009902515548782\n",
      "val_loss: 0.18831904722170228\n",
      "Progress: 83.6% ... Training loss: 0.057 ... Validation loss: 0.169iteration: 8359\n",
      "train_loss: 0.0577545394004176\n",
      "val_loss: 0.16991788477077097\n",
      "Progress: 83.6% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8360\n",
      "train_loss: 0.05823820378090066\n",
      "val_loss: 0.1636681206826893\n",
      "Progress: 83.6% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8361\n",
      "train_loss: 0.058729833427059466\n",
      "val_loss: 0.17003584010321793\n",
      "Progress: 83.6% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8362\n",
      "train_loss: 0.05822092413051182\n",
      "val_loss: 0.16522606122641398\n",
      "Progress: 83.6% ... Training loss: 0.062 ... Validation loss: 0.157iteration: 8363\n",
      "train_loss: 0.062232328776615956\n",
      "val_loss: 0.15710349384051842\n",
      "Progress: 83.6% ... Training loss: 0.069 ... Validation loss: 0.218iteration: 8364\n",
      "train_loss: 0.06995581160045056\n",
      "val_loss: 0.21857553938878843\n",
      "Progress: 83.7% ... Training loss: 0.082 ... Validation loss: 0.151iteration: 8365\n",
      "train_loss: 0.08227140077895109\n",
      "val_loss: 0.1510526360483993\n",
      "Progress: 83.7% ... Training loss: 0.102 ... Validation loss: 0.263iteration: 8366\n",
      "train_loss: 0.10298848503380777\n",
      "val_loss: 0.26348131480076914\n",
      "Progress: 83.7% ... Training loss: 0.097 ... Validation loss: 0.150iteration: 8367\n",
      "train_loss: 0.09711013643997961\n",
      "val_loss: 0.15084086190733528\n",
      "Progress: 83.7% ... Training loss: 0.109 ... Validation loss: 0.278iteration: 8368\n",
      "train_loss: 0.10962481790430034\n",
      "val_loss: 0.2784040078394993\n",
      "Progress: 83.7% ... Training loss: 0.086 ... Validation loss: 0.151iteration: 8369\n",
      "train_loss: 0.08611989114832945\n",
      "val_loss: 0.15141521115216738\n",
      "Progress: 83.7% ... Training loss: 0.064 ... Validation loss: 0.205iteration: 8370\n",
      "train_loss: 0.06440199835569324\n",
      "val_loss: 0.2050681477270079\n",
      "Progress: 83.7% ... Training loss: 0.066 ... Validation loss: 0.159iteration: 8371\n",
      "train_loss: 0.06688208307847565\n",
      "val_loss: 0.1596844851395511\n",
      "Progress: 83.7% ... Training loss: 0.072 ... Validation loss: 0.199iteration: 8372\n",
      "train_loss: 0.07203211162760763\n",
      "val_loss: 0.1994723583482929\n",
      "Progress: 83.7% ... Training loss: 0.077 ... Validation loss: 0.150iteration: 8373\n",
      "train_loss: 0.07748695114287801\n",
      "val_loss: 0.1509157851823045\n",
      "Progress: 83.7% ... Training loss: 0.081 ... Validation loss: 0.211iteration: 8374\n",
      "train_loss: 0.08125986675089805\n",
      "val_loss: 0.2114766025924033\n",
      "Progress: 83.8% ... Training loss: 0.074 ... Validation loss: 0.154iteration: 8375\n",
      "train_loss: 0.0748215082543099\n",
      "val_loss: 0.15491444726424125\n",
      "Progress: 83.8% ... Training loss: 0.101 ... Validation loss: 0.245iteration: 8376\n",
      "train_loss: 0.10167557556941656\n",
      "val_loss: 0.2450442965495614\n",
      "Progress: 83.8% ... Training loss: 0.072 ... Validation loss: 0.150iteration: 8377\n",
      "train_loss: 0.07209834339778072\n",
      "val_loss: 0.15057473417607403\n",
      "Progress: 83.8% ... Training loss: 0.072 ... Validation loss: 0.207iteration: 8378\n",
      "train_loss: 0.07217390320130965\n",
      "val_loss: 0.2071546070467473\n",
      "Progress: 83.8% ... Training loss: 0.071 ... Validation loss: 0.150iteration: 8379\n",
      "train_loss: 0.07113943856566299\n",
      "val_loss: 0.15089440527882658\n",
      "Progress: 83.8% ... Training loss: 0.071 ... Validation loss: 0.203iteration: 8380\n",
      "train_loss: 0.07175658087900641\n",
      "val_loss: 0.20366072012930808\n",
      "Progress: 83.8% ... Training loss: 0.085 ... Validation loss: 0.152iteration: 8381\n",
      "train_loss: 0.08581078953318988\n",
      "val_loss: 0.15222197575470425\n",
      "Progress: 83.8% ... Training loss: 0.076 ... Validation loss: 0.222iteration: 8382\n",
      "train_loss: 0.07640623559702016\n",
      "val_loss: 0.2225331614759405\n",
      "Progress: 83.8% ... Training loss: 0.068 ... Validation loss: 0.146iteration: 8383\n",
      "train_loss: 0.06881548548886585\n",
      "val_loss: 0.14657908548415804\n",
      "Progress: 83.8% ... Training loss: 0.073 ... Validation loss: 0.205iteration: 8384\n",
      "train_loss: 0.0737908488283799\n",
      "val_loss: 0.20558202146666815\n",
      "Progress: 83.8% ... Training loss: 0.067 ... Validation loss: 0.151iteration: 8385\n",
      "train_loss: 0.06763533957772226\n",
      "val_loss: 0.1516526361867694\n",
      "Progress: 83.9% ... Training loss: 0.069 ... Validation loss: 0.203iteration: 8386\n",
      "train_loss: 0.06917402719813327\n",
      "val_loss: 0.20320110071002517\n",
      "Progress: 83.9% ... Training loss: 0.079 ... Validation loss: 0.147iteration: 8387\n",
      "train_loss: 0.07995931842233239\n",
      "val_loss: 0.1473643299538749\n",
      "Progress: 83.9% ... Training loss: 0.059 ... Validation loss: 0.174iteration: 8388\n",
      "train_loss: 0.059828356367554125\n",
      "val_loss: 0.17489781086945452\n",
      "Progress: 83.9% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 8389\n",
      "train_loss: 0.06003697089818865\n",
      "val_loss: 0.1733514929045583\n",
      "Progress: 83.9% ... Training loss: 0.058 ... Validation loss: 0.160iteration: 8390\n",
      "train_loss: 0.058072916138953226\n",
      "val_loss: 0.1601044804491939\n",
      "Progress: 83.9% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8391\n",
      "train_loss: 0.058595005244518335\n",
      "val_loss: 0.163753826862564\n",
      "Progress: 83.9% ... Training loss: 0.057 ... Validation loss: 0.164iteration: 8392\n",
      "train_loss: 0.05786277467353613\n",
      "val_loss: 0.16445361914151824\n",
      "Progress: 83.9% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 8393\n",
      "train_loss: 0.060089678270281985\n",
      "val_loss: 0.1566504062744118\n",
      "Progress: 83.9% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8394\n",
      "train_loss: 0.0580641122091068\n",
      "val_loss: 0.16315704105660314\n",
      "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 8395\n",
      "train_loss: 0.05883968364974364\n",
      "val_loss: 0.15371577074102585\n",
      "Progress: 84.0% ... Training loss: 0.060 ... Validation loss: 0.170iteration: 8396\n",
      "train_loss: 0.06069574010197593\n",
      "val_loss: 0.1703658783207341\n",
      "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 8397\n",
      "train_loss: 0.0585275123562653\n",
      "val_loss: 0.15746722220291598\n",
      "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8398\n",
      "train_loss: 0.0580729336978041\n",
      "val_loss: 0.1656536123359548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.156iteration: 8399\n",
      "train_loss: 0.05885968467187825\n",
      "val_loss: 0.1567105402214561\n",
      "Progress: 84.0% ... Training loss: 0.059 ... Validation loss: 0.165iteration: 8400\n",
      "train_loss: 0.05967304856643029\n",
      "val_loss: 0.1654793320205017\n",
      "Progress: 84.0% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 8401\n",
      "train_loss: 0.060598759934858575\n",
      "val_loss: 0.17244733586396171\n",
      "Progress: 84.0% ... Training loss: 0.060 ... Validation loss: 0.154iteration: 8402\n",
      "train_loss: 0.06038506830880808\n",
      "val_loss: 0.15445259411790285\n",
      "Progress: 84.0% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 8403\n",
      "train_loss: 0.057690347100300014\n",
      "val_loss: 0.1601427641564289\n",
      "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8404\n",
      "train_loss: 0.058753562118255755\n",
      "val_loss: 0.1670770153280094\n",
      "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 8405\n",
      "train_loss: 0.05867482556300663\n",
      "val_loss: 0.17192246906232517\n",
      "Progress: 84.1% ... Training loss: 0.061 ... Validation loss: 0.158iteration: 8406\n",
      "train_loss: 0.06183981863939751\n",
      "val_loss: 0.15889959704393503\n",
      "Progress: 84.1% ... Training loss: 0.062 ... Validation loss: 0.170iteration: 8407\n",
      "train_loss: 0.06205409731196961\n",
      "val_loss: 0.170240768717851\n",
      "Progress: 84.1% ... Training loss: 0.059 ... Validation loss: 0.153iteration: 8408\n",
      "train_loss: 0.05983836538940541\n",
      "val_loss: 0.1535341294884532\n",
      "Progress: 84.1% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 8409\n",
      "train_loss: 0.05951737551906384\n",
      "val_loss: 0.1703709458717367\n",
      "Progress: 84.1% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8410\n",
      "train_loss: 0.05815890784450615\n",
      "val_loss: 0.16498666056143527\n",
      "Progress: 84.1% ... Training loss: 0.069 ... Validation loss: 0.156iteration: 8411\n",
      "train_loss: 0.06962311473163744\n",
      "val_loss: 0.15653473744093482\n",
      "Progress: 84.1% ... Training loss: 0.069 ... Validation loss: 0.188iteration: 8412\n",
      "train_loss: 0.0699553520944088\n",
      "val_loss: 0.18813015424446872\n",
      "Progress: 84.1% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 8413\n",
      "train_loss: 0.07072109556382278\n",
      "val_loss: 0.15060067585232761\n",
      "Progress: 84.1% ... Training loss: 0.060 ... Validation loss: 0.177iteration: 8414\n",
      "train_loss: 0.06053734609154767\n",
      "val_loss: 0.1770934341635566\n",
      "Progress: 84.2% ... Training loss: 0.062 ... Validation loss: 0.148iteration: 8415\n",
      "train_loss: 0.0627847745770114\n",
      "val_loss: 0.1483715662191738\n",
      "Progress: 84.2% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 8416\n",
      "train_loss: 0.0584296552657268\n",
      "val_loss: 0.16966268205076082\n",
      "Progress: 84.2% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 8417\n",
      "train_loss: 0.05790994166529289\n",
      "val_loss: 0.15911140141472752\n",
      "Progress: 84.2% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8418\n",
      "train_loss: 0.058621285925659565\n",
      "val_loss: 0.16700546200345487\n",
      "Progress: 84.2% ... Training loss: 0.057 ... Validation loss: 0.156iteration: 8419\n",
      "train_loss: 0.057589803315703215\n",
      "val_loss: 0.1561758340308642\n",
      "Progress: 84.2% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 8420\n",
      "train_loss: 0.059079242957529796\n",
      "val_loss: 0.17196735530261623\n",
      "Progress: 84.2% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8421\n",
      "train_loss: 0.058573720660354184\n",
      "val_loss: 0.16501100452576434\n",
      "Progress: 84.2% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 8422\n",
      "train_loss: 0.058522367969658755\n",
      "val_loss: 0.1669639687845421\n",
      "Progress: 84.2% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 8423\n",
      "train_loss: 0.0577396129120559\n",
      "val_loss: 0.15984676175716608\n",
      "Progress: 84.2% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8424\n",
      "train_loss: 0.05880896755624807\n",
      "val_loss: 0.16599632374968426\n",
      "Progress: 84.2% ... Training loss: 0.063 ... Validation loss: 0.150iteration: 8425\n",
      "train_loss: 0.0637851690477094\n",
      "val_loss: 0.15091929801341472\n",
      "Progress: 84.3% ... Training loss: 0.064 ... Validation loss: 0.174iteration: 8426\n",
      "train_loss: 0.06471809252219812\n",
      "val_loss: 0.17479939008441459\n",
      "Progress: 84.3% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 8427\n",
      "train_loss: 0.05987263769234078\n",
      "val_loss: 0.16823724817013463\n",
      "Progress: 84.3% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 8428\n",
      "train_loss: 0.0597000695087582\n",
      "val_loss: 0.15115379847549695\n",
      "Progress: 84.3% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 8429\n",
      "train_loss: 0.05921336814961854\n",
      "val_loss: 0.16994564630760756\n",
      "Progress: 84.3% ... Training loss: 0.060 ... Validation loss: 0.146iteration: 8430\n",
      "train_loss: 0.06096472258107851\n",
      "val_loss: 0.1469926044913768\n",
      "Progress: 84.3% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 8431\n",
      "train_loss: 0.06084410744710199\n",
      "val_loss: 0.16528405793397002\n",
      "Progress: 84.3% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 8432\n",
      "train_loss: 0.06037453834839056\n",
      "val_loss: 0.15523465484208365\n",
      "Progress: 84.3% ... Training loss: 0.061 ... Validation loss: 0.175iteration: 8433\n",
      "train_loss: 0.06175177797046849\n",
      "val_loss: 0.17507900707291127\n",
      "Progress: 84.3% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 8434\n",
      "train_loss: 0.061217340101265914\n",
      "val_loss: 0.1536148867476181\n",
      "Progress: 84.3% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 8435\n",
      "train_loss: 0.05910524847249738\n",
      "val_loss: 0.1649834405915273\n",
      "Progress: 84.4% ... Training loss: 0.063 ... Validation loss: 0.156iteration: 8436\n",
      "train_loss: 0.06348883796204906\n",
      "val_loss: 0.1568624120460941\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.186iteration: 8437\n",
      "train_loss: 0.060288865490912356\n",
      "val_loss: 0.18675238602201846\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 8438\n",
      "train_loss: 0.06002409371011834\n",
      "val_loss: 0.15753137655768343\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 8439\n",
      "train_loss: 0.060551378507626784\n",
      "val_loss: 0.15210909690645139\n",
      "Progress: 84.4% ... Training loss: 0.062 ... Validation loss: 0.177iteration: 8440\n",
      "train_loss: 0.06203431426768356\n",
      "val_loss: 0.17746374784251767\n",
      "Progress: 84.4% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 8441\n",
      "train_loss: 0.058703861897836065\n",
      "val_loss: 0.15992034337553657\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.181iteration: 8442\n",
      "train_loss: 0.0606158791338706\n",
      "val_loss: 0.1813998481379053\n",
      "Progress: 84.4% ... Training loss: 0.067 ... Validation loss: 0.150iteration: 8443\n",
      "train_loss: 0.06735929559065992\n",
      "val_loss: 0.1507553140914205\n",
      "Progress: 84.4% ... Training loss: 0.060 ... Validation loss: 0.180iteration: 8444\n",
      "train_loss: 0.0609108237182127\n",
      "val_loss: 0.18017167649161733\n",
      "Progress: 84.5% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 8445\n",
      "train_loss: 0.061688643069307186\n",
      "val_loss: 0.17062624307437677\n",
      "Progress: 84.5% ... Training loss: 0.060 ... Validation loss: 0.147iteration: 8446\n",
      "train_loss: 0.06055583879009283\n",
      "val_loss: 0.1476406290476764\n",
      "Progress: 84.5% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 8447\n",
      "train_loss: 0.059429700461659754\n",
      "val_loss: 0.17054771545609196\n",
      "Progress: 84.5% ... Training loss: 0.068 ... Validation loss: 0.147iteration: 8448\n",
      "train_loss: 0.06853024959118077\n",
      "val_loss: 0.14739125406693146\n",
      "Progress: 84.5% ... Training loss: 0.080 ... Validation loss: 0.197iteration: 8449\n",
      "train_loss: 0.08053635477335679\n",
      "val_loss: 0.19740813787310457\n",
      "Progress: 84.5% ... Training loss: 0.094 ... Validation loss: 0.151iteration: 8450\n",
      "train_loss: 0.09476789184622046\n",
      "val_loss: 0.15132901910091043\n",
      "Progress: 84.5% ... Training loss: 0.107 ... Validation loss: 0.241iteration: 8451\n",
      "train_loss: 0.10770426042599907\n",
      "val_loss: 0.24179548951298263\n",
      "Progress: 84.5% ... Training loss: 0.090 ... Validation loss: 0.147iteration: 8452\n",
      "train_loss: 0.09027230953611672\n",
      "val_loss: 0.14777864696688406\n",
      "Progress: 84.5% ... Training loss: 0.126 ... Validation loss: 0.227iteration: 8453\n",
      "train_loss: 0.12622330001975937\n",
      "val_loss: 0.22795638139814436\n",
      "Progress: 84.5% ... Training loss: 0.125 ... Validation loss: 0.160iteration: 8454\n",
      "train_loss: 0.1254960978236105\n",
      "val_loss: 0.16062736035301886\n",
      "Progress: 84.5% ... Training loss: 0.107 ... Validation loss: 0.244iteration: 8455\n",
      "train_loss: 0.10717299916628416\n",
      "val_loss: 0.24436947495548236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 84.6% ... Training loss: 0.126 ... Validation loss: 0.157iteration: 8456\n",
      "train_loss: 0.12687406563738826\n",
      "val_loss: 0.15758130786880492\n",
      "Progress: 84.6% ... Training loss: 0.133 ... Validation loss: 0.253iteration: 8457\n",
      "train_loss: 0.13367884348710382\n",
      "val_loss: 0.25365938738695426\n",
      "Progress: 84.6% ... Training loss: 0.174 ... Validation loss: 0.189iteration: 8458\n",
      "train_loss: 0.1748504185653237\n",
      "val_loss: 0.18991072341810333\n",
      "Progress: 84.6% ... Training loss: 0.191 ... Validation loss: 0.339iteration: 8459\n",
      "train_loss: 0.19158354486441956\n",
      "val_loss: 0.3395452170079376\n",
      "Progress: 84.6% ... Training loss: 0.168 ... Validation loss: 0.180iteration: 8460\n",
      "train_loss: 0.16803096259257222\n",
      "val_loss: 0.1800360596868755\n",
      "Progress: 84.6% ... Training loss: 0.136 ... Validation loss: 0.273iteration: 8461\n",
      "train_loss: 0.13693064880146338\n",
      "val_loss: 0.27356158291276933\n",
      "Progress: 84.6% ... Training loss: 0.109 ... Validation loss: 0.157iteration: 8462\n",
      "train_loss: 0.10907421576152009\n",
      "val_loss: 0.1575126671593233\n",
      "Progress: 84.6% ... Training loss: 0.113 ... Validation loss: 0.251iteration: 8463\n",
      "train_loss: 0.11333774135498438\n",
      "val_loss: 0.2511279793630039\n",
      "Progress: 84.6% ... Training loss: 0.105 ... Validation loss: 0.161iteration: 8464\n",
      "train_loss: 0.10564878642323947\n",
      "val_loss: 0.16155219111858704\n",
      "Progress: 84.7% ... Training loss: 0.095 ... Validation loss: 0.233iteration: 8465\n",
      "train_loss: 0.09519809944623135\n",
      "val_loss: 0.23375889543018177\n",
      "Progress: 84.7% ... Training loss: 0.080 ... Validation loss: 0.150iteration: 8466\n",
      "train_loss: 0.08095537202983537\n",
      "val_loss: 0.15034553739957923\n",
      "Progress: 84.7% ... Training loss: 0.068 ... Validation loss: 0.191iteration: 8467\n",
      "train_loss: 0.06873230587485045\n",
      "val_loss: 0.19133168627263294\n",
      "Progress: 84.7% ... Training loss: 0.081 ... Validation loss: 0.152iteration: 8468\n",
      "train_loss: 0.08100505172293927\n",
      "val_loss: 0.1525723127057234\n",
      "Progress: 84.7% ... Training loss: 0.061 ... Validation loss: 0.179iteration: 8469\n",
      "train_loss: 0.06120897954365563\n",
      "val_loss: 0.17996862749706477\n",
      "Progress: 84.7% ... Training loss: 0.072 ... Validation loss: 0.154iteration: 8470\n",
      "train_loss: 0.07250677040213506\n",
      "val_loss: 0.15431148145917425\n",
      "Progress: 84.7% ... Training loss: 0.068 ... Validation loss: 0.190iteration: 8471\n",
      "train_loss: 0.06861365583630316\n",
      "val_loss: 0.19091400348237428\n",
      "Progress: 84.7% ... Training loss: 0.068 ... Validation loss: 0.149iteration: 8472\n",
      "train_loss: 0.06835733967495217\n",
      "val_loss: 0.14946626097766405\n",
      "Progress: 84.7% ... Training loss: 0.059 ... Validation loss: 0.166iteration: 8473\n",
      "train_loss: 0.05978491871846277\n",
      "val_loss: 0.1664098285734961\n",
      "Progress: 84.7% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 8474\n",
      "train_loss: 0.05881758088759023\n",
      "val_loss: 0.15793652788159354\n",
      "Progress: 84.8% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 8475\n",
      "train_loss: 0.0592355274068943\n",
      "val_loss: 0.1626093632664374\n",
      "Progress: 84.8% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 8476\n",
      "train_loss: 0.06173206414850477\n",
      "val_loss: 0.15376886678188112\n",
      "Progress: 84.8% ... Training loss: 0.067 ... Validation loss: 0.191iteration: 8477\n",
      "train_loss: 0.06744525810254957\n",
      "val_loss: 0.19105031124218277\n",
      "Progress: 84.8% ... Training loss: 0.066 ... Validation loss: 0.150iteration: 8478\n",
      "train_loss: 0.06689906655097266\n",
      "val_loss: 0.15047425731102979\n",
      "Progress: 84.8% ... Training loss: 0.059 ... Validation loss: 0.176iteration: 8479\n",
      "train_loss: 0.05961944051479204\n",
      "val_loss: 0.17625477374751894\n",
      "Progress: 84.8% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 8480\n",
      "train_loss: 0.058138538068261726\n",
      "val_loss: 0.15990852290876975\n",
      "Progress: 84.8% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 8481\n",
      "train_loss: 0.06395250823570961\n",
      "val_loss: 0.1820861849685303\n",
      "Progress: 84.8% ... Training loss: 0.061 ... Validation loss: 0.151iteration: 8482\n",
      "train_loss: 0.06154397796564586\n",
      "val_loss: 0.1510629081300789\n",
      "Progress: 84.8% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 8483\n",
      "train_loss: 0.05767041739118842\n",
      "val_loss: 0.15990796647279007\n",
      "Progress: 84.8% ... Training loss: 0.062 ... Validation loss: 0.171iteration: 8484\n",
      "train_loss: 0.06295756574924978\n",
      "val_loss: 0.17150060178239043\n",
      "Progress: 84.8% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 8485\n",
      "train_loss: 0.05964376798887084\n",
      "val_loss: 0.1695023614916585\n",
      "Progress: 84.9% ... Training loss: 0.065 ... Validation loss: 0.153iteration: 8486\n",
      "train_loss: 0.0654006720889353\n",
      "val_loss: 0.15360712519232694\n",
      "Progress: 84.9% ... Training loss: 0.073 ... Validation loss: 0.215iteration: 8487\n",
      "train_loss: 0.07370244564750464\n",
      "val_loss: 0.21532502873363984\n",
      "Progress: 84.9% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 8488\n",
      "train_loss: 0.05959057460546742\n",
      "val_loss: 0.1601840269330258\n",
      "Progress: 84.9% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8489\n",
      "train_loss: 0.05823723746860574\n",
      "val_loss: 0.16702078256654457\n",
      "Progress: 84.9% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 8490\n",
      "train_loss: 0.06145918325483922\n",
      "val_loss: 0.1520117798557857\n",
      "Progress: 84.9% ... Training loss: 0.062 ... Validation loss: 0.181iteration: 8491\n",
      "train_loss: 0.06269358477547922\n",
      "val_loss: 0.1812143869936547\n",
      "Progress: 84.9% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 8492\n",
      "train_loss: 0.05939862404438981\n",
      "val_loss: 0.15965808772358434\n",
      "Progress: 84.9% ... Training loss: 0.060 ... Validation loss: 0.176iteration: 8493\n",
      "train_loss: 0.06096530212855343\n",
      "val_loss: 0.17635331644459368\n",
      "Progress: 84.9% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 8494\n",
      "train_loss: 0.05867606463818497\n",
      "val_loss: 0.15782698830929506\n",
      "Progress: 85.0% ... Training loss: 0.071 ... Validation loss: 0.198iteration: 8495\n",
      "train_loss: 0.07151628053873174\n",
      "val_loss: 0.198783669789063\n",
      "Progress: 85.0% ... Training loss: 0.080 ... Validation loss: 0.149iteration: 8496\n",
      "train_loss: 0.08074757141109627\n",
      "val_loss: 0.1496494410907188\n",
      "Progress: 85.0% ... Training loss: 0.084 ... Validation loss: 0.208iteration: 8497\n",
      "train_loss: 0.08450808192426879\n",
      "val_loss: 0.20888795825348572\n",
      "Progress: 85.0% ... Training loss: 0.072 ... Validation loss: 0.148iteration: 8498\n",
      "train_loss: 0.07286592669345271\n",
      "val_loss: 0.14885756161742705\n",
      "Progress: 85.0% ... Training loss: 0.068 ... Validation loss: 0.194iteration: 8499\n",
      "train_loss: 0.06836769287846681\n",
      "val_loss: 0.19429583553737026\n",
      "Progress: 85.0% ... Training loss: 0.079 ... Validation loss: 0.150iteration: 8500\n",
      "train_loss: 0.0799104728137865\n",
      "val_loss: 0.15096728294816025\n",
      "Progress: 85.0% ... Training loss: 0.097 ... Validation loss: 0.214iteration: 8501\n",
      "train_loss: 0.0971124972928221\n",
      "val_loss: 0.21486846093701192\n",
      "Progress: 85.0% ... Training loss: 0.114 ... Validation loss: 0.164iteration: 8502\n",
      "train_loss: 0.11455540342862487\n",
      "val_loss: 0.16460903224255038\n",
      "Progress: 85.0% ... Training loss: 0.084 ... Validation loss: 0.216iteration: 8503\n",
      "train_loss: 0.08472660334827528\n",
      "val_loss: 0.21605061856906857\n",
      "Progress: 85.0% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 8504\n",
      "train_loss: 0.061784562721102866\n",
      "val_loss: 0.15236056558717234\n",
      "Progress: 85.0% ... Training loss: 0.065 ... Validation loss: 0.180iteration: 8505\n",
      "train_loss: 0.06564636823659033\n",
      "val_loss: 0.18025328401267648\n",
      "Progress: 85.1% ... Training loss: 0.063 ... Validation loss: 0.150iteration: 8506\n",
      "train_loss: 0.06355661565626912\n",
      "val_loss: 0.15047166335947706\n",
      "Progress: 85.1% ... Training loss: 0.073 ... Validation loss: 0.201iteration: 8507\n",
      "train_loss: 0.07367201362742178\n",
      "val_loss: 0.20148878636299056\n",
      "Progress: 85.1% ... Training loss: 0.061 ... Validation loss: 0.147iteration: 8508\n",
      "train_loss: 0.061708289342812625\n",
      "val_loss: 0.14711012847517857\n",
      "Progress: 85.1% ... Training loss: 0.058 ... Validation loss: 0.168iteration: 8509\n",
      "train_loss: 0.0588017511945518\n",
      "val_loss: 0.16854634889187536\n",
      "Progress: 85.1% ... Training loss: 0.058 ... Validation loss: 0.154iteration: 8510\n",
      "train_loss: 0.058434443511271555\n",
      "val_loss: 0.15400678443424887\n",
      "Progress: 85.1% ... Training loss: 0.059 ... Validation loss: 0.166iteration: 8511\n",
      "train_loss: 0.05947170986452766\n",
      "val_loss: 0.16656214524493\n",
      "Progress: 85.1% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 8512\n",
      "train_loss: 0.058841024107011274\n",
      "val_loss: 0.1690846063719159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 85.1% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8513\n",
      "train_loss: 0.057856120674335945\n",
      "val_loss: 0.16314665145955431\n",
      "Progress: 85.1% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 8514\n",
      "train_loss: 0.05757673278803839\n",
      "val_loss: 0.16545160329360384\n",
      "Progress: 85.2% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 8515\n",
      "train_loss: 0.05769052959216238\n",
      "val_loss: 0.1660780354725992\n",
      "Progress: 85.2% ... Training loss: 0.059 ... Validation loss: 0.183iteration: 8516\n",
      "train_loss: 0.059055339423765764\n",
      "val_loss: 0.18396420575542619\n",
      "Progress: 85.2% ... Training loss: 0.057 ... Validation loss: 0.171iteration: 8517\n",
      "train_loss: 0.05787944328659457\n",
      "val_loss: 0.1710624538032367\n",
      "Progress: 85.2% ... Training loss: 0.060 ... Validation loss: 0.158iteration: 8518\n",
      "train_loss: 0.060709234494734914\n",
      "val_loss: 0.15820964129488319\n",
      "Progress: 85.2% ... Training loss: 0.060 ... Validation loss: 0.190iteration: 8519\n",
      "train_loss: 0.06004773713424842\n",
      "val_loss: 0.19001487713004575\n",
      "Progress: 85.2% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 8520\n",
      "train_loss: 0.05787142494358423\n",
      "val_loss: 0.17039827571215524\n",
      "Progress: 85.2% ... Training loss: 0.057 ... Validation loss: 0.164iteration: 8521\n",
      "train_loss: 0.057932550341973164\n",
      "val_loss: 0.16448219208695247\n",
      "Progress: 85.2% ... Training loss: 0.058 ... Validation loss: 0.174iteration: 8522\n",
      "train_loss: 0.05856866917959389\n",
      "val_loss: 0.17453331563185376\n",
      "Progress: 85.2% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 8523\n",
      "train_loss: 0.05770695931593493\n",
      "val_loss: 0.17066591662077937\n",
      "Progress: 85.2% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8524\n",
      "train_loss: 0.05875208930086658\n",
      "val_loss: 0.1708890936809895\n",
      "Progress: 85.2% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8525\n",
      "train_loss: 0.05802090152180388\n",
      "val_loss: 0.16445005594065673\n",
      "Progress: 85.3% ... Training loss: 0.062 ... Validation loss: 0.150iteration: 8526\n",
      "train_loss: 0.06270159328913093\n",
      "val_loss: 0.15026645879297834\n",
      "Progress: 85.3% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 8527\n",
      "train_loss: 0.05787062468385406\n",
      "val_loss: 0.17037601722784818\n",
      "Progress: 85.3% ... Training loss: 0.082 ... Validation loss: 0.148iteration: 8528\n",
      "train_loss: 0.08265191470018587\n",
      "val_loss: 0.14885344540309034\n",
      "Progress: 85.3% ... Training loss: 0.069 ... Validation loss: 0.186iteration: 8529\n",
      "train_loss: 0.06957607152822176\n",
      "val_loss: 0.1868158949830835\n",
      "Progress: 85.3% ... Training loss: 0.076 ... Validation loss: 0.149iteration: 8530\n",
      "train_loss: 0.07613390990423728\n",
      "val_loss: 0.14904796118914618\n",
      "Progress: 85.3% ... Training loss: 0.097 ... Validation loss: 0.208iteration: 8531\n",
      "train_loss: 0.09779621940675777\n",
      "val_loss: 0.20881252199180847\n",
      "Progress: 85.3% ... Training loss: 0.072 ... Validation loss: 0.147iteration: 8532\n",
      "train_loss: 0.07207587404932514\n",
      "val_loss: 0.14712828279573514\n",
      "Progress: 85.3% ... Training loss: 0.063 ... Validation loss: 0.182iteration: 8533\n",
      "train_loss: 0.06346416305678705\n",
      "val_loss: 0.18273639202823774\n",
      "Progress: 85.3% ... Training loss: 0.062 ... Validation loss: 0.155iteration: 8534\n",
      "train_loss: 0.0627385985113409\n",
      "val_loss: 0.1554495087882339\n",
      "Progress: 85.3% ... Training loss: 0.072 ... Validation loss: 0.200iteration: 8535\n",
      "train_loss: 0.07287107772950462\n",
      "val_loss: 0.20098284364683702\n",
      "Progress: 85.4% ... Training loss: 0.072 ... Validation loss: 0.151iteration: 8536\n",
      "train_loss: 0.07295120485045654\n",
      "val_loss: 0.15113546302532713\n",
      "Progress: 85.4% ... Training loss: 0.065 ... Validation loss: 0.188iteration: 8537\n",
      "train_loss: 0.06594663799870634\n",
      "val_loss: 0.18845801235189874\n",
      "Progress: 85.4% ... Training loss: 0.077 ... Validation loss: 0.153iteration: 8538\n",
      "train_loss: 0.07752497821462387\n",
      "val_loss: 0.15386493310426477\n",
      "Progress: 85.4% ... Training loss: 0.085 ... Validation loss: 0.211iteration: 8539\n",
      "train_loss: 0.085958995956034\n",
      "val_loss: 0.2114218023886235\n",
      "Progress: 85.4% ... Training loss: 0.074 ... Validation loss: 0.159iteration: 8540\n",
      "train_loss: 0.07415084037248484\n",
      "val_loss: 0.1596660675007036\n",
      "Progress: 85.4% ... Training loss: 0.073 ... Validation loss: 0.184iteration: 8541\n",
      "train_loss: 0.07382432211210165\n",
      "val_loss: 0.18419404236771247\n",
      "Progress: 85.4% ... Training loss: 0.064 ... Validation loss: 0.151iteration: 8542\n",
      "train_loss: 0.06494754973813603\n",
      "val_loss: 0.15142986369379613\n",
      "Progress: 85.4% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 8543\n",
      "train_loss: 0.05829197769698909\n",
      "val_loss: 0.1660625566252003\n",
      "Progress: 85.4% ... Training loss: 0.059 ... Validation loss: 0.172iteration: 8544\n",
      "train_loss: 0.059418806792349135\n",
      "val_loss: 0.17213842275417207\n",
      "Progress: 85.5% ... Training loss: 0.061 ... Validation loss: 0.161iteration: 8545\n",
      "train_loss: 0.061170475394229415\n",
      "val_loss: 0.1617801890705367\n",
      "Progress: 85.5% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 8546\n",
      "train_loss: 0.0593053859936035\n",
      "val_loss: 0.16994934138826556\n",
      "Progress: 85.5% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 8547\n",
      "train_loss: 0.05889904989830333\n",
      "val_loss: 0.161789086097456\n",
      "Progress: 85.5% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 8548\n",
      "train_loss: 0.05900043248510513\n",
      "val_loss: 0.16411287006748695\n",
      "Progress: 85.5% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8549\n",
      "train_loss: 0.05871133918547067\n",
      "val_loss: 0.1646812191004807\n",
      "Progress: 85.5% ... Training loss: 0.058 ... Validation loss: 0.168iteration: 8550\n",
      "train_loss: 0.05867323651683986\n",
      "val_loss: 0.16852012395332164\n",
      "Progress: 85.5% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8551\n",
      "train_loss: 0.05846850081177253\n",
      "val_loss: 0.1656044812115724\n",
      "Progress: 85.5% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 8552\n",
      "train_loss: 0.0606150560667145\n",
      "val_loss: 0.1498020957715054\n",
      "Progress: 85.5% ... Training loss: 0.058 ... Validation loss: 0.155iteration: 8553\n",
      "train_loss: 0.05890688990951775\n",
      "val_loss: 0.1553631172872828\n",
      "Progress: 85.5% ... Training loss: 0.063 ... Validation loss: 0.154iteration: 8554\n",
      "train_loss: 0.06306032861648404\n",
      "val_loss: 0.15475985264076397\n",
      "Progress: 85.5% ... Training loss: 0.058 ... Validation loss: 0.160iteration: 8555\n",
      "train_loss: 0.058217221474784746\n",
      "val_loss: 0.16044518019154896\n",
      "Progress: 85.6% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 8556\n",
      "train_loss: 0.06016966469119511\n",
      "val_loss: 0.15605948276732307\n",
      "Progress: 85.6% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 8557\n",
      "train_loss: 0.05853476024411288\n",
      "val_loss: 0.15105670345136216\n",
      "Progress: 85.6% ... Training loss: 0.066 ... Validation loss: 0.172iteration: 8558\n",
      "train_loss: 0.0666868065191361\n",
      "val_loss: 0.17209793905662543\n",
      "Progress: 85.6% ... Training loss: 0.076 ... Validation loss: 0.149iteration: 8559\n",
      "train_loss: 0.0763149867692443\n",
      "val_loss: 0.14995380563863148\n",
      "Progress: 85.6% ... Training loss: 0.085 ... Validation loss: 0.207iteration: 8560\n",
      "train_loss: 0.08515218913493006\n",
      "val_loss: 0.20796060599093025\n",
      "Progress: 85.6% ... Training loss: 0.065 ... Validation loss: 0.146iteration: 8561\n",
      "train_loss: 0.06515801590178696\n",
      "val_loss: 0.14636635879419257\n",
      "Progress: 85.6% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 8562\n",
      "train_loss: 0.059181751005922274\n",
      "val_loss: 0.15615034442643233\n",
      "Progress: 85.6% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 8563\n",
      "train_loss: 0.06053650413232439\n",
      "val_loss: 0.1528199088496867\n",
      "Progress: 85.6% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8564\n",
      "train_loss: 0.05824193699373346\n",
      "val_loss: 0.17084007161824782\n",
      "Progress: 85.7% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 8565\n",
      "train_loss: 0.06014656225060431\n",
      "val_loss: 0.16273261655439608\n",
      "Progress: 85.7% ... Training loss: 0.069 ... Validation loss: 0.199iteration: 8566\n",
      "train_loss: 0.06920093493374851\n",
      "val_loss: 0.1990181757293898\n",
      "Progress: 85.7% ... Training loss: 0.070 ... Validation loss: 0.150iteration: 8567\n",
      "train_loss: 0.07068206448513722\n",
      "val_loss: 0.15067238969139857\n",
      "Progress: 85.7% ... Training loss: 0.067 ... Validation loss: 0.193iteration: 8568\n",
      "train_loss: 0.06707495747397879\n",
      "val_loss: 0.19383391761048677\n",
      "Progress: 85.7% ... Training loss: 0.062 ... Validation loss: 0.151iteration: 8569\n",
      "train_loss: 0.06260429998420834\n",
      "val_loss: 0.15174605986442338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 85.7% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 8570\n",
      "train_loss: 0.0612633589662055\n",
      "val_loss: 0.17360820004202002\n",
      "Progress: 85.7% ... Training loss: 0.059 ... Validation loss: 0.154iteration: 8571\n",
      "train_loss: 0.059982427060322245\n",
      "val_loss: 0.15492028859708096\n",
      "Progress: 85.7% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 8572\n",
      "train_loss: 0.059098537749216855\n",
      "val_loss: 0.15564695442566295\n",
      "Progress: 85.7% ... Training loss: 0.059 ... Validation loss: 0.180iteration: 8573\n",
      "train_loss: 0.05954263264936323\n",
      "val_loss: 0.18019935494467937\n",
      "Progress: 85.7% ... Training loss: 0.070 ... Validation loss: 0.154iteration: 8574\n",
      "train_loss: 0.07040389444309891\n",
      "val_loss: 0.1542645914517527\n",
      "Progress: 85.8% ... Training loss: 0.080 ... Validation loss: 0.187iteration: 8575\n",
      "train_loss: 0.0801291955068496\n",
      "val_loss: 0.18735711461264168\n",
      "Progress: 85.8% ... Training loss: 0.064 ... Validation loss: 0.149iteration: 8576\n",
      "train_loss: 0.06418422930684864\n",
      "val_loss: 0.14941508708345652\n",
      "Progress: 85.8% ... Training loss: 0.059 ... Validation loss: 0.173iteration: 8577\n",
      "train_loss: 0.059418893415403976\n",
      "val_loss: 0.1736468083915228\n",
      "Progress: 85.8% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8578\n",
      "train_loss: 0.05839263097863456\n",
      "val_loss: 0.16700418527421948\n",
      "Progress: 85.8% ... Training loss: 0.060 ... Validation loss: 0.176iteration: 8579\n",
      "train_loss: 0.06056311880551325\n",
      "val_loss: 0.17602935711644313\n",
      "Progress: 85.8% ... Training loss: 0.068 ... Validation loss: 0.153iteration: 8580\n",
      "train_loss: 0.06804559461056783\n",
      "val_loss: 0.15377397330274165\n",
      "Progress: 85.8% ... Training loss: 0.061 ... Validation loss: 0.181iteration: 8581\n",
      "train_loss: 0.061121422904361386\n",
      "val_loss: 0.18166294569173605\n",
      "Progress: 85.8% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 8582\n",
      "train_loss: 0.060010432078716903\n",
      "val_loss: 0.15630530385054753\n",
      "Progress: 85.8% ... Training loss: 0.071 ... Validation loss: 0.202iteration: 8583\n",
      "train_loss: 0.07108870454731755\n",
      "val_loss: 0.20297439695764388\n",
      "Progress: 85.8% ... Training loss: 0.066 ... Validation loss: 0.152iteration: 8584\n",
      "train_loss: 0.06638607114817348\n",
      "val_loss: 0.15210612079892954\n",
      "Progress: 85.8% ... Training loss: 0.063 ... Validation loss: 0.195iteration: 8585\n",
      "train_loss: 0.06398497163806256\n",
      "val_loss: 0.19597799147939723\n",
      "Progress: 85.9% ... Training loss: 0.068 ... Validation loss: 0.151iteration: 8586\n",
      "train_loss: 0.06870650279114685\n",
      "val_loss: 0.15113439099691\n",
      "Progress: 85.9% ... Training loss: 0.060 ... Validation loss: 0.168iteration: 8587\n",
      "train_loss: 0.06040691864254687\n",
      "val_loss: 0.16804828454557827\n",
      "Progress: 85.9% ... Training loss: 0.060 ... Validation loss: 0.175iteration: 8588\n",
      "train_loss: 0.06095895005422122\n",
      "val_loss: 0.1758713318025826\n",
      "Progress: 85.9% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8589\n",
      "train_loss: 0.057557434627827725\n",
      "val_loss: 0.1630353824503038\n",
      "Progress: 85.9% ... Training loss: 0.059 ... Validation loss: 0.152iteration: 8590\n",
      "train_loss: 0.05952610120276873\n",
      "val_loss: 0.15260575644036198\n",
      "Progress: 85.9% ... Training loss: 0.068 ... Validation loss: 0.191iteration: 8591\n",
      "train_loss: 0.06884720325286796\n",
      "val_loss: 0.19190120249454634\n",
      "Progress: 85.9% ... Training loss: 0.068 ... Validation loss: 0.146iteration: 8592\n",
      "train_loss: 0.06856947456681031\n",
      "val_loss: 0.14616474913691768\n",
      "Progress: 85.9% ... Training loss: 0.069 ... Validation loss: 0.175iteration: 8593\n",
      "train_loss: 0.06985114107693721\n",
      "val_loss: 0.17516931456566529\n",
      "Progress: 85.9% ... Training loss: 0.068 ... Validation loss: 0.144iteration: 8594\n",
      "train_loss: 0.06898734193233393\n",
      "val_loss: 0.14429246392672035\n",
      "Progress: 86.0% ... Training loss: 0.081 ... Validation loss: 0.202iteration: 8595\n",
      "train_loss: 0.08115590168868406\n",
      "val_loss: 0.20295976005086366\n",
      "Progress: 86.0% ... Training loss: 0.099 ... Validation loss: 0.155iteration: 8596\n",
      "train_loss: 0.09990771729632987\n",
      "val_loss: 0.15547151058851902\n",
      "Progress: 86.0% ... Training loss: 0.074 ... Validation loss: 0.191iteration: 8597\n",
      "train_loss: 0.0747713248603817\n",
      "val_loss: 0.1915108261506963\n",
      "Progress: 86.0% ... Training loss: 0.079 ... Validation loss: 0.154iteration: 8598\n",
      "train_loss: 0.07923094285428259\n",
      "val_loss: 0.1544955612408406\n",
      "Progress: 86.0% ... Training loss: 0.073 ... Validation loss: 0.197iteration: 8599\n",
      "train_loss: 0.07310741483235428\n",
      "val_loss: 0.19758375509136228\n",
      "Progress: 86.0% ... Training loss: 0.074 ... Validation loss: 0.151iteration: 8600\n",
      "train_loss: 0.07426343179594871\n",
      "val_loss: 0.15175973047166758\n",
      "Progress: 86.0% ... Training loss: 0.064 ... Validation loss: 0.195iteration: 8601\n",
      "train_loss: 0.06466074504919939\n",
      "val_loss: 0.19593166700521109\n",
      "Progress: 86.0% ... Training loss: 0.072 ... Validation loss: 0.157iteration: 8602\n",
      "train_loss: 0.07248698166169987\n",
      "val_loss: 0.15712720041790298\n",
      "Progress: 86.0% ... Training loss: 0.063 ... Validation loss: 0.188iteration: 8603\n",
      "train_loss: 0.06343521574078438\n",
      "val_loss: 0.188570500663656\n",
      "Progress: 86.0% ... Training loss: 0.062 ... Validation loss: 0.163iteration: 8604\n",
      "train_loss: 0.06278304237512981\n",
      "val_loss: 0.16363332810938447\n",
      "Progress: 86.0% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 8605\n",
      "train_loss: 0.05917288102577008\n",
      "val_loss: 0.16879572045828964\n",
      "Progress: 86.1% ... Training loss: 0.059 ... Validation loss: 0.180iteration: 8606\n",
      "train_loss: 0.05996910868126704\n",
      "val_loss: 0.18059800405785656\n",
      "Progress: 86.1% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 8607\n",
      "train_loss: 0.0640046799655396\n",
      "val_loss: 0.15689621762406278\n",
      "Progress: 86.1% ... Training loss: 0.062 ... Validation loss: 0.183iteration: 8608\n",
      "train_loss: 0.06202352426438702\n",
      "val_loss: 0.1833921960668667\n",
      "Progress: 86.1% ... Training loss: 0.076 ... Validation loss: 0.155iteration: 8609\n",
      "train_loss: 0.07611044472644628\n",
      "val_loss: 0.15501678210843065\n",
      "Progress: 86.1% ... Training loss: 0.072 ... Validation loss: 0.206iteration: 8610\n",
      "train_loss: 0.07210896303337191\n",
      "val_loss: 0.20650507615600502\n",
      "Progress: 86.1% ... Training loss: 0.064 ... Validation loss: 0.158iteration: 8611\n",
      "train_loss: 0.06490297077887383\n",
      "val_loss: 0.15836088223745956\n",
      "Progress: 86.1% ... Training loss: 0.065 ... Validation loss: 0.201iteration: 8612\n",
      "train_loss: 0.06557291084067733\n",
      "val_loss: 0.20178275508463248\n",
      "Progress: 86.1% ... Training loss: 0.061 ... Validation loss: 0.163iteration: 8613\n",
      "train_loss: 0.061467268899358926\n",
      "val_loss: 0.16308585721352076\n",
      "Progress: 86.1% ... Training loss: 0.066 ... Validation loss: 0.170iteration: 8614\n",
      "train_loss: 0.06676742238444179\n",
      "val_loss: 0.1701816224934653\n",
      "Progress: 86.2% ... Training loss: 0.063 ... Validation loss: 0.198iteration: 8615\n",
      "train_loss: 0.06330451641069806\n",
      "val_loss: 0.19867631757095358\n",
      "Progress: 86.2% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 8616\n",
      "train_loss: 0.060371937976844176\n",
      "val_loss: 0.16121871406408003\n",
      "Progress: 86.2% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8617\n",
      "train_loss: 0.0586845944002204\n",
      "val_loss: 0.17064976459637657\n",
      "Progress: 86.2% ... Training loss: 0.066 ... Validation loss: 0.155iteration: 8618\n",
      "train_loss: 0.06694731462520799\n",
      "val_loss: 0.15564482622659487\n",
      "Progress: 86.2% ... Training loss: 0.071 ... Validation loss: 0.206iteration: 8619\n",
      "train_loss: 0.07185602771250002\n",
      "val_loss: 0.20677798533542757\n",
      "Progress: 86.2% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 8620\n",
      "train_loss: 0.05975732412862602\n",
      "val_loss: 0.15666477510430138\n",
      "Progress: 86.2% ... Training loss: 0.066 ... Validation loss: 0.188iteration: 8621\n",
      "train_loss: 0.06698564684001909\n",
      "val_loss: 0.1885672012242453\n",
      "Progress: 86.2% ... Training loss: 0.060 ... Validation loss: 0.154iteration: 8622\n",
      "train_loss: 0.06075064051554844\n",
      "val_loss: 0.15449478957490384\n",
      "Progress: 86.2% ... Training loss: 0.060 ... Validation loss: 0.179iteration: 8623\n",
      "train_loss: 0.06008526887732261\n",
      "val_loss: 0.17954876172242662\n",
      "Progress: 86.2% ... Training loss: 0.068 ... Validation loss: 0.159iteration: 8624\n",
      "train_loss: 0.06874766819509881\n",
      "val_loss: 0.15904128644848525\n",
      "Progress: 86.2% ... Training loss: 0.065 ... Validation loss: 0.207iteration: 8625\n",
      "train_loss: 0.06538470195151731\n",
      "val_loss: 0.20705673961296417\n",
      "Progress: 86.3% ... Training loss: 0.063 ... Validation loss: 0.157iteration: 8626\n",
      "train_loss: 0.06397437518380178\n",
      "val_loss: 0.15761306530032954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 86.3% ... Training loss: 0.068 ... Validation loss: 0.207iteration: 8627\n",
      "train_loss: 0.06807822625482635\n",
      "val_loss: 0.20782125301264734\n",
      "Progress: 86.3% ... Training loss: 0.067 ... Validation loss: 0.156iteration: 8628\n",
      "train_loss: 0.06763577909617335\n",
      "val_loss: 0.15628035377563376\n",
      "Progress: 86.3% ... Training loss: 0.073 ... Validation loss: 0.190iteration: 8629\n",
      "train_loss: 0.07340563441844125\n",
      "val_loss: 0.19010864567502622\n",
      "Progress: 86.3% ... Training loss: 0.064 ... Validation loss: 0.152iteration: 8630\n",
      "train_loss: 0.06466179616112296\n",
      "val_loss: 0.15295177996474332\n",
      "Progress: 86.3% ... Training loss: 0.072 ... Validation loss: 0.186iteration: 8631\n",
      "train_loss: 0.0724830359746175\n",
      "val_loss: 0.18661572753845806\n",
      "Progress: 86.3% ... Training loss: 0.091 ... Validation loss: 0.157iteration: 8632\n",
      "train_loss: 0.09135271058015637\n",
      "val_loss: 0.15747715312540198\n",
      "Progress: 86.3% ... Training loss: 0.083 ... Validation loss: 0.216iteration: 8633\n",
      "train_loss: 0.08378456628061168\n",
      "val_loss: 0.21634415977906896\n",
      "Progress: 86.3% ... Training loss: 0.081 ... Validation loss: 0.154iteration: 8634\n",
      "train_loss: 0.08197041790044021\n",
      "val_loss: 0.1547217679077063\n",
      "Progress: 86.3% ... Training loss: 0.098 ... Validation loss: 0.233iteration: 8635\n",
      "train_loss: 0.09875622163968584\n",
      "val_loss: 0.23319552128371726\n",
      "Progress: 86.4% ... Training loss: 0.086 ... Validation loss: 0.150iteration: 8636\n",
      "train_loss: 0.08655124416730793\n",
      "val_loss: 0.15081065307283673\n",
      "Progress: 86.4% ... Training loss: 0.081 ... Validation loss: 0.215iteration: 8637\n",
      "train_loss: 0.08177437555752652\n",
      "val_loss: 0.21526468718175434\n",
      "Progress: 86.4% ... Training loss: 0.073 ... Validation loss: 0.149iteration: 8638\n",
      "train_loss: 0.07367863395569531\n",
      "val_loss: 0.1497049397830879\n",
      "Progress: 86.4% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 8639\n",
      "train_loss: 0.06056403726284128\n",
      "val_loss: 0.17422037698853515\n",
      "Progress: 86.4% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 8640\n",
      "train_loss: 0.061301806448247426\n",
      "val_loss: 0.15743672232370295\n",
      "Progress: 86.4% ... Training loss: 0.063 ... Validation loss: 0.188iteration: 8641\n",
      "train_loss: 0.06338162202904027\n",
      "val_loss: 0.18827213198789294\n",
      "Progress: 86.4% ... Training loss: 0.069 ... Validation loss: 0.155iteration: 8642\n",
      "train_loss: 0.06910914103937295\n",
      "val_loss: 0.15590975914691874\n",
      "Progress: 86.4% ... Training loss: 0.071 ... Validation loss: 0.222iteration: 8643\n",
      "train_loss: 0.07176348736840078\n",
      "val_loss: 0.2229134537657729\n",
      "Progress: 86.4% ... Training loss: 0.062 ... Validation loss: 0.158iteration: 8644\n",
      "train_loss: 0.06242974097038321\n",
      "val_loss: 0.15883482997529075\n",
      "Progress: 86.5% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8645\n",
      "train_loss: 0.058539811054529975\n",
      "val_loss: 0.16796700200170203\n",
      "Progress: 86.5% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 8646\n",
      "train_loss: 0.06103231389349723\n",
      "val_loss: 0.1528331651814932\n",
      "Progress: 86.5% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8647\n",
      "train_loss: 0.05880150151340819\n",
      "val_loss: 0.16421023067675747\n",
      "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.154iteration: 8648\n",
      "train_loss: 0.059344142648725895\n",
      "val_loss: 0.15426292244728274\n",
      "Progress: 86.5% ... Training loss: 0.065 ... Validation loss: 0.180iteration: 8649\n",
      "train_loss: 0.06546131780816702\n",
      "val_loss: 0.18074628104863946\n",
      "Progress: 86.5% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 8650\n",
      "train_loss: 0.0584376329024379\n",
      "val_loss: 0.16154952909420814\n",
      "Progress: 86.5% ... Training loss: 0.064 ... Validation loss: 0.152iteration: 8651\n",
      "train_loss: 0.06419894974634419\n",
      "val_loss: 0.15200971784885592\n",
      "Progress: 86.5% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 8652\n",
      "train_loss: 0.06485445085641323\n",
      "val_loss: 0.17357989243048683\n",
      "Progress: 86.5% ... Training loss: 0.068 ... Validation loss: 0.147iteration: 8653\n",
      "train_loss: 0.06820768319108284\n",
      "val_loss: 0.14718918763962766\n",
      "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 8654\n",
      "train_loss: 0.05981226785870729\n",
      "val_loss: 0.1591166725054611\n",
      "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.179iteration: 8655\n",
      "train_loss: 0.059375017521038624\n",
      "val_loss: 0.1793059907145433\n",
      "Progress: 86.6% ... Training loss: 0.081 ... Validation loss: 0.153iteration: 8656\n",
      "train_loss: 0.08164585171435633\n",
      "val_loss: 0.1530826222418681\n",
      "Progress: 86.6% ... Training loss: 0.081 ... Validation loss: 0.238iteration: 8657\n",
      "train_loss: 0.08120735043120783\n",
      "val_loss: 0.23815534242855854\n",
      "Progress: 86.6% ... Training loss: 0.090 ... Validation loss: 0.162iteration: 8658\n",
      "train_loss: 0.09006668004770853\n",
      "val_loss: 0.1627640959729847\n",
      "Progress: 86.6% ... Training loss: 0.085 ... Validation loss: 0.248iteration: 8659\n",
      "train_loss: 0.08538037716579522\n",
      "val_loss: 0.24831121315516616\n",
      "Progress: 86.6% ... Training loss: 0.064 ... Validation loss: 0.151iteration: 8660\n",
      "train_loss: 0.06435724632770552\n",
      "val_loss: 0.1515371105612206\n",
      "Progress: 86.6% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8661\n",
      "train_loss: 0.05848803886701747\n",
      "val_loss: 0.17000458317789002\n",
      "Progress: 86.6% ... Training loss: 0.058 ... Validation loss: 0.160iteration: 8662\n",
      "train_loss: 0.05842184282851841\n",
      "val_loss: 0.16092884060957824\n",
      "Progress: 86.6% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 8663\n",
      "train_loss: 0.05860390990907462\n",
      "val_loss: 0.15985173472315525\n",
      "Progress: 86.6% ... Training loss: 0.061 ... Validation loss: 0.161iteration: 8664\n",
      "train_loss: 0.06182902420228788\n",
      "val_loss: 0.16111335015115943\n",
      "Progress: 86.7% ... Training loss: 0.063 ... Validation loss: 0.184iteration: 8665\n",
      "train_loss: 0.06371457299789\n",
      "val_loss: 0.18410923681276117\n",
      "Progress: 86.7% ... Training loss: 0.071 ... Validation loss: 0.150iteration: 8666\n",
      "train_loss: 0.07199203856560066\n",
      "val_loss: 0.15055362566500272\n",
      "Progress: 86.7% ... Training loss: 0.085 ... Validation loss: 0.224iteration: 8667\n",
      "train_loss: 0.08583312961454895\n",
      "val_loss: 0.22426214906690592\n",
      "Progress: 86.7% ... Training loss: 0.072 ... Validation loss: 0.148iteration: 8668\n",
      "train_loss: 0.07254333715681043\n",
      "val_loss: 0.14843930864057456\n",
      "Progress: 86.7% ... Training loss: 0.084 ... Validation loss: 0.207iteration: 8669\n",
      "train_loss: 0.08405381464275649\n",
      "val_loss: 0.20709572925016428\n",
      "Progress: 86.7% ... Training loss: 0.077 ... Validation loss: 0.153iteration: 8670\n",
      "train_loss: 0.07796211333923055\n",
      "val_loss: 0.15315386376308612\n",
      "Progress: 86.7% ... Training loss: 0.067 ... Validation loss: 0.189iteration: 8671\n",
      "train_loss: 0.06755013987798911\n",
      "val_loss: 0.1890209009416736\n",
      "Progress: 86.7% ... Training loss: 0.071 ... Validation loss: 0.154iteration: 8672\n",
      "train_loss: 0.07104958166858237\n",
      "val_loss: 0.1549189192990086\n",
      "Progress: 86.7% ... Training loss: 0.075 ... Validation loss: 0.203iteration: 8673\n",
      "train_loss: 0.07598157288050003\n",
      "val_loss: 0.203276734085149\n",
      "Progress: 86.7% ... Training loss: 0.068 ... Validation loss: 0.153iteration: 8674\n",
      "train_loss: 0.06804173940430705\n",
      "val_loss: 0.15301888828766905\n",
      "Progress: 86.8% ... Training loss: 0.064 ... Validation loss: 0.184iteration: 8675\n",
      "train_loss: 0.06457049410110142\n",
      "val_loss: 0.18486121718617526\n",
      "Progress: 86.8% ... Training loss: 0.069 ... Validation loss: 0.153iteration: 8676\n",
      "train_loss: 0.06961584141842928\n",
      "val_loss: 0.1538856813296233\n",
      "Progress: 86.8% ... Training loss: 0.065 ... Validation loss: 0.194iteration: 8677\n",
      "train_loss: 0.06581778754455796\n",
      "val_loss: 0.1940879184210741\n",
      "Progress: 86.8% ... Training loss: 0.062 ... Validation loss: 0.161iteration: 8678\n",
      "train_loss: 0.062323776450764236\n",
      "val_loss: 0.16165264975598173\n",
      "Progress: 86.8% ... Training loss: 0.061 ... Validation loss: 0.189iteration: 8679\n",
      "train_loss: 0.06180221907199451\n",
      "val_loss: 0.1895237851038801\n",
      "Progress: 86.8% ... Training loss: 0.062 ... Validation loss: 0.154iteration: 8680\n",
      "train_loss: 0.0628948935882238\n",
      "val_loss: 0.15479274067510046\n",
      "Progress: 86.8% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 8681\n",
      "train_loss: 0.05888925077827414\n",
      "val_loss: 0.1690899994238767\n",
      "Progress: 86.8% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 8682\n",
      "train_loss: 0.05942506506887227\n",
      "val_loss: 0.15102424257629793\n",
      "Progress: 86.8% ... Training loss: 0.060 ... Validation loss: 0.180iteration: 8683\n",
      "train_loss: 0.060827987098819786\n",
      "val_loss: 0.1809531197417814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 86.8% ... Training loss: 0.057 ... Validation loss: 0.161iteration: 8684\n",
      "train_loss: 0.05761456139548507\n",
      "val_loss: 0.16131488278184414\n",
      "Progress: 86.8% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 8685\n",
      "train_loss: 0.058627362034756714\n",
      "val_loss: 0.15709185292121058\n",
      "Progress: 86.9% ... Training loss: 0.057 ... Validation loss: 0.164iteration: 8686\n",
      "train_loss: 0.057877554723385245\n",
      "val_loss: 0.16467334077249637\n",
      "Progress: 86.9% ... Training loss: 0.057 ... Validation loss: 0.172iteration: 8687\n",
      "train_loss: 0.0577565410111317\n",
      "val_loss: 0.17280180438540801\n",
      "Progress: 86.9% ... Training loss: 0.064 ... Validation loss: 0.158iteration: 8688\n",
      "train_loss: 0.06445665757322488\n",
      "val_loss: 0.15813000358559337\n",
      "Progress: 86.9% ... Training loss: 0.057 ... Validation loss: 0.161iteration: 8689\n",
      "train_loss: 0.05769056471640994\n",
      "val_loss: 0.16155278157948294\n",
      "Progress: 86.9% ... Training loss: 0.057 ... Validation loss: 0.158iteration: 8690\n",
      "train_loss: 0.05747268657153131\n",
      "val_loss: 0.15859906490860548\n",
      "Progress: 86.9% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8691\n",
      "train_loss: 0.05783609509730427\n",
      "val_loss: 0.16308292171334476\n",
      "Progress: 86.9% ... Training loss: 0.079 ... Validation loss: 0.152iteration: 8692\n",
      "train_loss: 0.0793861082949963\n",
      "val_loss: 0.15280534776740579\n",
      "Progress: 86.9% ... Training loss: 0.074 ... Validation loss: 0.210iteration: 8693\n",
      "train_loss: 0.07408781393349023\n",
      "val_loss: 0.21083095413945332\n",
      "Progress: 86.9% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 8694\n",
      "train_loss: 0.06198520746134722\n",
      "val_loss: 0.15352409965873623\n",
      "Progress: 87.0% ... Training loss: 0.061 ... Validation loss: 0.178iteration: 8695\n",
      "train_loss: 0.061053185691432955\n",
      "val_loss: 0.17804057285568708\n",
      "Progress: 87.0% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 8696\n",
      "train_loss: 0.061661813996368706\n",
      "val_loss: 0.14918510898486734\n",
      "Progress: 87.0% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 8697\n",
      "train_loss: 0.058690183281011124\n",
      "val_loss: 0.16442453517662367\n",
      "Progress: 87.0% ... Training loss: 0.107 ... Validation loss: 0.190iteration: 8698\n",
      "train_loss: 0.10770850500886797\n",
      "val_loss: 0.19023841474692904\n",
      "Progress: 87.0% ... Training loss: 0.093 ... Validation loss: 0.160iteration: 8699\n",
      "train_loss: 0.09305326255187034\n",
      "val_loss: 0.16010187531630898\n",
      "Progress: 87.0% ... Training loss: 0.084 ... Validation loss: 0.225iteration: 8700\n",
      "train_loss: 0.08462688339556988\n",
      "val_loss: 0.22582506477661035\n",
      "Progress: 87.0% ... Training loss: 0.082 ... Validation loss: 0.154iteration: 8701\n",
      "train_loss: 0.08292430834136083\n",
      "val_loss: 0.1545053637006735\n",
      "Progress: 87.0% ... Training loss: 0.074 ... Validation loss: 0.201iteration: 8702\n",
      "train_loss: 0.07409562182033168\n",
      "val_loss: 0.20166282466015728\n",
      "Progress: 87.0% ... Training loss: 0.077 ... Validation loss: 0.153iteration: 8703\n",
      "train_loss: 0.07764765118013252\n",
      "val_loss: 0.15381143308169673\n",
      "Progress: 87.0% ... Training loss: 0.069 ... Validation loss: 0.185iteration: 8704\n",
      "train_loss: 0.06987603214424082\n",
      "val_loss: 0.18594897905892177\n",
      "Progress: 87.0% ... Training loss: 0.070 ... Validation loss: 0.146iteration: 8705\n",
      "train_loss: 0.07054764252622908\n",
      "val_loss: 0.14653330522827138\n",
      "Progress: 87.1% ... Training loss: 0.063 ... Validation loss: 0.166iteration: 8706\n",
      "train_loss: 0.06300766290886908\n",
      "val_loss: 0.1665034659716171\n",
      "Progress: 87.1% ... Training loss: 0.062 ... Validation loss: 0.147iteration: 8707\n",
      "train_loss: 0.06284833806310781\n",
      "val_loss: 0.14788950855050434\n",
      "Progress: 87.1% ... Training loss: 0.064 ... Validation loss: 0.190iteration: 8708\n",
      "train_loss: 0.06450904708585642\n",
      "val_loss: 0.1904697987084943\n",
      "Progress: 87.1% ... Training loss: 0.073 ... Validation loss: 0.154iteration: 8709\n",
      "train_loss: 0.07375181399293104\n",
      "val_loss: 0.15453470101336075\n",
      "Progress: 87.1% ... Training loss: 0.073 ... Validation loss: 0.185iteration: 8710\n",
      "train_loss: 0.07349428136060471\n",
      "val_loss: 0.1857410969184652\n",
      "Progress: 87.1% ... Training loss: 0.073 ... Validation loss: 0.152iteration: 8711\n",
      "train_loss: 0.07393239743821652\n",
      "val_loss: 0.15272294032919276\n",
      "Progress: 87.1% ... Training loss: 0.071 ... Validation loss: 0.188iteration: 8712\n",
      "train_loss: 0.07155332683833276\n",
      "val_loss: 0.18848931630539087\n",
      "Progress: 87.1% ... Training loss: 0.078 ... Validation loss: 0.151iteration: 8713\n",
      "train_loss: 0.07881906570668615\n",
      "val_loss: 0.15109346231157794\n",
      "Progress: 87.1% ... Training loss: 0.066 ... Validation loss: 0.179iteration: 8714\n",
      "train_loss: 0.06625406754592036\n",
      "val_loss: 0.17959802214732\n",
      "Progress: 87.2% ... Training loss: 0.059 ... Validation loss: 0.148iteration: 8715\n",
      "train_loss: 0.05933424932776392\n",
      "val_loss: 0.14816884676147554\n",
      "Progress: 87.2% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8716\n",
      "train_loss: 0.058001495719759995\n",
      "val_loss: 0.16525380252300848\n",
      "Progress: 87.2% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 8717\n",
      "train_loss: 0.060408162104586494\n",
      "val_loss: 0.1747593749059928\n",
      "Progress: 87.2% ... Training loss: 0.060 ... Validation loss: 0.150iteration: 8718\n",
      "train_loss: 0.06070112238445558\n",
      "val_loss: 0.1502421021620094\n",
      "Progress: 87.2% ... Training loss: 0.061 ... Validation loss: 0.179iteration: 8719\n",
      "train_loss: 0.06137780692702397\n",
      "val_loss: 0.17911956672870677\n",
      "Progress: 87.2% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 8720\n",
      "train_loss: 0.05755265326305501\n",
      "val_loss: 0.16009374554182465\n",
      "Progress: 87.2% ... Training loss: 0.060 ... Validation loss: 0.178iteration: 8721\n",
      "train_loss: 0.06036605689990879\n",
      "val_loss: 0.17806634424632964\n",
      "Progress: 87.2% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 8722\n",
      "train_loss: 0.060772963271736445\n",
      "val_loss: 0.15114429975918547\n",
      "Progress: 87.2% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 8723\n",
      "train_loss: 0.06156187423860659\n",
      "val_loss: 0.17030179030395168\n",
      "Progress: 87.2% ... Training loss: 0.066 ... Validation loss: 0.155iteration: 8724\n",
      "train_loss: 0.06670173497668537\n",
      "val_loss: 0.1555389660962447\n",
      "Progress: 87.2% ... Training loss: 0.060 ... Validation loss: 0.183iteration: 8725\n",
      "train_loss: 0.060964511579346965\n",
      "val_loss: 0.183183710368313\n",
      "Progress: 87.3% ... Training loss: 0.064 ... Validation loss: 0.153iteration: 8726\n",
      "train_loss: 0.06427343438029019\n",
      "val_loss: 0.15301323577712542\n",
      "Progress: 87.3% ... Training loss: 0.062 ... Validation loss: 0.182iteration: 8727\n",
      "train_loss: 0.06263744318328042\n",
      "val_loss: 0.18291310461018512\n",
      "Progress: 87.3% ... Training loss: 0.059 ... Validation loss: 0.152iteration: 8728\n",
      "train_loss: 0.059113474972780196\n",
      "val_loss: 0.15205804614084953\n",
      "Progress: 87.3% ... Training loss: 0.059 ... Validation loss: 0.167iteration: 8729\n",
      "train_loss: 0.059995816626866226\n",
      "val_loss: 0.16753061491739893\n",
      "Progress: 87.3% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 8730\n",
      "train_loss: 0.058544540176517744\n",
      "val_loss: 0.1615366505524134\n",
      "Progress: 87.3% ... Training loss: 0.063 ... Validation loss: 0.151iteration: 8731\n",
      "train_loss: 0.06352063274724142\n",
      "val_loss: 0.15126635259842586\n",
      "Progress: 87.3% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 8732\n",
      "train_loss: 0.05925865924932797\n",
      "val_loss: 0.16972355991809437\n",
      "Progress: 87.3% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 8733\n",
      "train_loss: 0.058086966660347364\n",
      "val_loss: 0.1576286297917498\n",
      "Progress: 87.3% ... Training loss: 0.062 ... Validation loss: 0.155iteration: 8734\n",
      "train_loss: 0.06253061278999832\n",
      "val_loss: 0.15548952152440965\n",
      "Progress: 87.3% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 8735\n",
      "train_loss: 0.06105425384436934\n",
      "val_loss: 0.15983559598917937\n",
      "Progress: 87.4% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 8736\n",
      "train_loss: 0.05801788259526032\n",
      "val_loss: 0.15308987970507681\n",
      "Progress: 87.4% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 8737\n",
      "train_loss: 0.06096323908610901\n",
      "val_loss: 0.15684791829679723\n",
      "Progress: 87.4% ... Training loss: 0.058 ... Validation loss: 0.152iteration: 8738\n",
      "train_loss: 0.05815444464383956\n",
      "val_loss: 0.1524478394904746\n",
      "Progress: 87.4% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 8739\n",
      "train_loss: 0.05907792233117487\n",
      "val_loss: 0.16053583541601782\n",
      "Progress: 87.4% ... Training loss: 0.058 ... Validation loss: 0.176iteration: 8740\n",
      "train_loss: 0.058990169454877776\n",
      "val_loss: 0.17639106629973342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 87.4% ... Training loss: 0.059 ... Validation loss: 0.150iteration: 8741\n",
      "train_loss: 0.05986953277676294\n",
      "val_loss: 0.15021818020157543\n",
      "Progress: 87.4% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 8742\n",
      "train_loss: 0.05828122235947456\n",
      "val_loss: 0.15116575764467197\n",
      "Progress: 87.4% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8743\n",
      "train_loss: 0.058107143947780715\n",
      "val_loss: 0.17072622031917842\n",
      "Progress: 87.4% ... Training loss: 0.059 ... Validation loss: 0.154iteration: 8744\n",
      "train_loss: 0.05918227837003614\n",
      "val_loss: 0.15435899900750255\n",
      "Progress: 87.5% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8745\n",
      "train_loss: 0.058802202531786925\n",
      "val_loss: 0.16576230682601623\n",
      "Progress: 87.5% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 8746\n",
      "train_loss: 0.057659768800885004\n",
      "val_loss: 0.1707511714152228\n",
      "Progress: 87.5% ... Training loss: 0.059 ... Validation loss: 0.172iteration: 8747\n",
      "train_loss: 0.05910207527421945\n",
      "val_loss: 0.17243161762910225\n",
      "Progress: 87.5% ... Training loss: 0.075 ... Validation loss: 0.149iteration: 8748\n",
      "train_loss: 0.0753344224264424\n",
      "val_loss: 0.14912670964355457\n",
      "Progress: 87.5% ... Training loss: 0.071 ... Validation loss: 0.190iteration: 8749\n",
      "train_loss: 0.07102858461970685\n",
      "val_loss: 0.1909791564582789\n",
      "Progress: 87.5% ... Training loss: 0.070 ... Validation loss: 0.146iteration: 8750\n",
      "train_loss: 0.07003176396358263\n",
      "val_loss: 0.14635668936006224\n",
      "Progress: 87.5% ... Training loss: 0.068 ... Validation loss: 0.195iteration: 8751\n",
      "train_loss: 0.06889419408923327\n",
      "val_loss: 0.1954311648781928\n",
      "Progress: 87.5% ... Training loss: 0.062 ... Validation loss: 0.149iteration: 8752\n",
      "train_loss: 0.06205947059668939\n",
      "val_loss: 0.14964629452729106\n",
      "Progress: 87.5% ... Training loss: 0.061 ... Validation loss: 0.176iteration: 8753\n",
      "train_loss: 0.06139770338700734\n",
      "val_loss: 0.1764426907865917\n",
      "Progress: 87.5% ... Training loss: 0.059 ... Validation loss: 0.154iteration: 8754\n",
      "train_loss: 0.059141668306187535\n",
      "val_loss: 0.1542162405996569\n",
      "Progress: 87.5% ... Training loss: 0.057 ... Validation loss: 0.156iteration: 8755\n",
      "train_loss: 0.057873081272921234\n",
      "val_loss: 0.156718769958097\n",
      "Progress: 87.6% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 8756\n",
      "train_loss: 0.057591336200314154\n",
      "val_loss: 0.16096980559136842\n",
      "Progress: 87.6% ... Training loss: 0.059 ... Validation loss: 0.164iteration: 8757\n",
      "train_loss: 0.05917598848442544\n",
      "val_loss: 0.16427223268533067\n",
      "Progress: 87.6% ... Training loss: 0.060 ... Validation loss: 0.148iteration: 8758\n",
      "train_loss: 0.0606480980478454\n",
      "val_loss: 0.1487941475369851\n",
      "Progress: 87.6% ... Training loss: 0.062 ... Validation loss: 0.171iteration: 8759\n",
      "train_loss: 0.062409404904812345\n",
      "val_loss: 0.17109157774949044\n",
      "Progress: 87.6% ... Training loss: 0.062 ... Validation loss: 0.151iteration: 8760\n",
      "train_loss: 0.06206013730373236\n",
      "val_loss: 0.15160381107070564\n",
      "Progress: 87.6% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 8761\n",
      "train_loss: 0.061877036248969904\n",
      "val_loss: 0.17358166453853024\n",
      "Progress: 87.6% ... Training loss: 0.058 ... Validation loss: 0.146iteration: 8762\n",
      "train_loss: 0.05895098444433988\n",
      "val_loss: 0.1467416853292637\n",
      "Progress: 87.6% ... Training loss: 0.057 ... Validation loss: 0.158iteration: 8763\n",
      "train_loss: 0.057630861232459175\n",
      "val_loss: 0.15804314597371708\n",
      "Progress: 87.6% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 8764\n",
      "train_loss: 0.05989797323966908\n",
      "val_loss: 0.16847442821967118\n",
      "Progress: 87.7% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 8765\n",
      "train_loss: 0.05763051219249616\n",
      "val_loss: 0.16693746415532254\n",
      "Progress: 87.7% ... Training loss: 0.065 ... Validation loss: 0.160iteration: 8766\n",
      "train_loss: 0.06521048741082446\n",
      "val_loss: 0.16057093517003287\n",
      "Progress: 87.7% ... Training loss: 0.065 ... Validation loss: 0.149iteration: 8767\n",
      "train_loss: 0.0652956323701606\n",
      "val_loss: 0.14910385973670012\n",
      "Progress: 87.7% ... Training loss: 0.065 ... Validation loss: 0.193iteration: 8768\n",
      "train_loss: 0.06599718364122692\n",
      "val_loss: 0.19332151439172673\n",
      "Progress: 87.7% ... Training loss: 0.070 ... Validation loss: 0.154iteration: 8769\n",
      "train_loss: 0.07031605985133385\n",
      "val_loss: 0.15449279487407835\n",
      "Progress: 87.7% ... Training loss: 0.066 ... Validation loss: 0.190iteration: 8770\n",
      "train_loss: 0.0664688284639002\n",
      "val_loss: 0.19059615407402702\n",
      "Progress: 87.7% ... Training loss: 0.063 ... Validation loss: 0.148iteration: 8771\n",
      "train_loss: 0.06341245843332352\n",
      "val_loss: 0.14827481641882273\n",
      "Progress: 87.7% ... Training loss: 0.057 ... Validation loss: 0.168iteration: 8772\n",
      "train_loss: 0.05752754019724203\n",
      "val_loss: 0.16863421938153664\n",
      "Progress: 87.7% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 8773\n",
      "train_loss: 0.060343958636228094\n",
      "val_loss: 0.15561546128261397\n",
      "Progress: 87.7% ... Training loss: 0.065 ... Validation loss: 0.193iteration: 8774\n",
      "train_loss: 0.06545559300664787\n",
      "val_loss: 0.19322781665646865\n",
      "Progress: 87.8% ... Training loss: 0.062 ... Validation loss: 0.151iteration: 8775\n",
      "train_loss: 0.06267441562319462\n",
      "val_loss: 0.15128245215251074\n",
      "Progress: 87.8% ... Training loss: 0.063 ... Validation loss: 0.185iteration: 8776\n",
      "train_loss: 0.06314462873270871\n",
      "val_loss: 0.18556512869556\n",
      "Progress: 87.8% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 8777\n",
      "train_loss: 0.060173334947546765\n",
      "val_loss: 0.15516840392268552\n",
      "Progress: 87.8% ... Training loss: 0.067 ... Validation loss: 0.190iteration: 8778\n",
      "train_loss: 0.06747252153788243\n",
      "val_loss: 0.1909379173905073\n",
      "Progress: 87.8% ... Training loss: 0.065 ... Validation loss: 0.143iteration: 8779\n",
      "train_loss: 0.06559029581898247\n",
      "val_loss: 0.14313060963078236\n",
      "Progress: 87.8% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 8780\n",
      "train_loss: 0.06001897790152177\n",
      "val_loss: 0.1599069016464978\n",
      "Progress: 87.8% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 8781\n",
      "train_loss: 0.058288664415790704\n",
      "val_loss: 0.15369738052241383\n",
      "Progress: 87.8% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8782\n",
      "train_loss: 0.05758788895315542\n",
      "val_loss: 0.16385730556244363\n",
      "Progress: 87.8% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 8783\n",
      "train_loss: 0.06081800385423369\n",
      "val_loss: 0.16352249442254913\n",
      "Progress: 87.8% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 8784\n",
      "train_loss: 0.0573311927879733\n",
      "val_loss: 0.16092390732270978\n",
      "Progress: 87.8% ... Training loss: 0.057 ... Validation loss: 0.161iteration: 8785\n",
      "train_loss: 0.05785234620453584\n",
      "val_loss: 0.1618269613951148\n",
      "Progress: 87.9% ... Training loss: 0.059 ... Validation loss: 0.150iteration: 8786\n",
      "train_loss: 0.0599429141274864\n",
      "val_loss: 0.15071517040986912\n",
      "Progress: 87.9% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 8787\n",
      "train_loss: 0.05744892781405777\n",
      "val_loss: 0.16762684082408663\n",
      "Progress: 87.9% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8788\n",
      "train_loss: 0.058391623849737576\n",
      "val_loss: 0.17099546824864692\n",
      "Progress: 87.9% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8789\n",
      "train_loss: 0.057223565253629094\n",
      "val_loss: 0.16391054178969316\n",
      "Progress: 87.9% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 8790\n",
      "train_loss: 0.05909997073843032\n",
      "val_loss: 0.16936626048573952\n",
      "Progress: 87.9% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 8791\n",
      "train_loss: 0.058466258062487904\n",
      "val_loss: 0.15382958377915645\n",
      "Progress: 87.9% ... Training loss: 0.058 ... Validation loss: 0.172iteration: 8792\n",
      "train_loss: 0.05843547025464413\n",
      "val_loss: 0.17255581535464207\n",
      "Progress: 87.9% ... Training loss: 0.064 ... Validation loss: 0.146iteration: 8793\n",
      "train_loss: 0.06420085700347972\n",
      "val_loss: 0.14618507026186517\n",
      "Progress: 87.9% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 8794\n",
      "train_loss: 0.059111932655796035\n",
      "val_loss: 0.170721895726635\n",
      "Progress: 88.0% ... Training loss: 0.066 ... Validation loss: 0.146iteration: 8795\n",
      "train_loss: 0.0660528446789972\n",
      "val_loss: 0.1467356743747196\n",
      "Progress: 88.0% ... Training loss: 0.078 ... Validation loss: 0.222iteration: 8796\n",
      "train_loss: 0.07843711663530077\n",
      "val_loss: 0.22239554290614624\n",
      "Progress: 88.0% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 8797\n",
      "train_loss: 0.061756996555718535\n",
      "val_loss: 0.1536506411185506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 88.0% ... Training loss: 0.061 ... Validation loss: 0.186iteration: 8798\n",
      "train_loss: 0.061974828534418436\n",
      "val_loss: 0.186537403896113\n",
      "Progress: 88.0% ... Training loss: 0.070 ... Validation loss: 0.153iteration: 8799\n",
      "train_loss: 0.0700906142564592\n",
      "val_loss: 0.15301821673048666\n",
      "Progress: 88.0% ... Training loss: 0.070 ... Validation loss: 0.181iteration: 8800\n",
      "train_loss: 0.07017367256667213\n",
      "val_loss: 0.18118730170532002\n",
      "Progress: 88.0% ... Training loss: 0.079 ... Validation loss: 0.148iteration: 8801\n",
      "train_loss: 0.07971000245573091\n",
      "val_loss: 0.14867453326911476\n",
      "Progress: 88.0% ... Training loss: 0.075 ... Validation loss: 0.199iteration: 8802\n",
      "train_loss: 0.07577207205918512\n",
      "val_loss: 0.19902270799702693\n",
      "Progress: 88.0% ... Training loss: 0.062 ... Validation loss: 0.143iteration: 8803\n",
      "train_loss: 0.06229877212251328\n",
      "val_loss: 0.1437615908673607\n",
      "Progress: 88.0% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 8804\n",
      "train_loss: 0.059598706076159046\n",
      "val_loss: 0.1684587662354276\n",
      "Progress: 88.0% ... Training loss: 0.058 ... Validation loss: 0.148iteration: 8805\n",
      "train_loss: 0.05843467899106448\n",
      "val_loss: 0.14887045080891342\n",
      "Progress: 88.1% ... Training loss: 0.061 ... Validation loss: 0.175iteration: 8806\n",
      "train_loss: 0.06115651452745483\n",
      "val_loss: 0.17529979126168868\n",
      "Progress: 88.1% ... Training loss: 0.060 ... Validation loss: 0.150iteration: 8807\n",
      "train_loss: 0.06011567398651503\n",
      "val_loss: 0.15090850009696036\n",
      "Progress: 88.1% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8808\n",
      "train_loss: 0.058019253899132935\n",
      "val_loss: 0.16509498859563326\n",
      "Progress: 88.1% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 8809\n",
      "train_loss: 0.05711227915622857\n",
      "val_loss: 0.15929272862359461\n",
      "Progress: 88.1% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 8810\n",
      "train_loss: 0.05885055139518339\n",
      "val_loss: 0.1512447149520987\n",
      "Progress: 88.1% ... Training loss: 0.064 ... Validation loss: 0.148iteration: 8811\n",
      "train_loss: 0.06494639393495298\n",
      "val_loss: 0.14817618707941876\n",
      "Progress: 88.1% ... Training loss: 0.066 ... Validation loss: 0.178iteration: 8812\n",
      "train_loss: 0.06623680752424256\n",
      "val_loss: 0.17863401436638388\n",
      "Progress: 88.1% ... Training loss: 0.063 ... Validation loss: 0.150iteration: 8813\n",
      "train_loss: 0.06324686991103534\n",
      "val_loss: 0.15051009962134065\n",
      "Progress: 88.1% ... Training loss: 0.059 ... Validation loss: 0.179iteration: 8814\n",
      "train_loss: 0.059819269602895624\n",
      "val_loss: 0.1791527961330804\n",
      "Progress: 88.2% ... Training loss: 0.058 ... Validation loss: 0.158iteration: 8815\n",
      "train_loss: 0.05829747636419284\n",
      "val_loss: 0.15876775135644558\n",
      "Progress: 88.2% ... Training loss: 0.057 ... Validation loss: 0.176iteration: 8816\n",
      "train_loss: 0.05776738881663905\n",
      "val_loss: 0.17677675060972356\n",
      "Progress: 88.2% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 8817\n",
      "train_loss: 0.057642028989277574\n",
      "val_loss: 0.1677359559873809\n",
      "Progress: 88.2% ... Training loss: 0.058 ... Validation loss: 0.184iteration: 8818\n",
      "train_loss: 0.05867358683100812\n",
      "val_loss: 0.18441966400620832\n",
      "Progress: 88.2% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 8819\n",
      "train_loss: 0.06068666224539942\n",
      "val_loss: 0.15912760107482132\n",
      "Progress: 88.2% ... Training loss: 0.057 ... Validation loss: 0.172iteration: 8820\n",
      "train_loss: 0.057491859267390526\n",
      "val_loss: 0.1727730516549583\n",
      "Progress: 88.2% ... Training loss: 0.059 ... Validation loss: 0.153iteration: 8821\n",
      "train_loss: 0.05941655197141993\n",
      "val_loss: 0.1533478409352134\n",
      "Progress: 88.2% ... Training loss: 0.061 ... Validation loss: 0.185iteration: 8822\n",
      "train_loss: 0.06190966791439382\n",
      "val_loss: 0.1852201858413496\n",
      "Progress: 88.2% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 8823\n",
      "train_loss: 0.059155145680070936\n",
      "val_loss: 0.15555713610200467\n",
      "Progress: 88.2% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 8824\n",
      "train_loss: 0.05710906108585012\n",
      "val_loss: 0.1678284392330739\n",
      "Progress: 88.2% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 8825\n",
      "train_loss: 0.05821082234004716\n",
      "val_loss: 0.16532731086350694\n",
      "Progress: 88.3% ... Training loss: 0.062 ... Validation loss: 0.154iteration: 8826\n",
      "train_loss: 0.062318884965964154\n",
      "val_loss: 0.15466281717104352\n",
      "Progress: 88.3% ... Training loss: 0.059 ... Validation loss: 0.166iteration: 8827\n",
      "train_loss: 0.059466699973244476\n",
      "val_loss: 0.16688411315975782\n",
      "Progress: 88.3% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 8828\n",
      "train_loss: 0.061033429003123155\n",
      "val_loss: 0.15352982527722475\n",
      "Progress: 88.3% ... Training loss: 0.061 ... Validation loss: 0.179iteration: 8829\n",
      "train_loss: 0.06174457726145005\n",
      "val_loss: 0.17933544254681838\n",
      "Progress: 88.3% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8830\n",
      "train_loss: 0.05806314070135067\n",
      "val_loss: 0.1635516347521424\n",
      "Progress: 88.3% ... Training loss: 0.057 ... Validation loss: 0.171iteration: 8831\n",
      "train_loss: 0.05752728212673903\n",
      "val_loss: 0.1714343228333781\n",
      "Progress: 88.3% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 8832\n",
      "train_loss: 0.05777009416291368\n",
      "val_loss: 0.16726588346061097\n",
      "Progress: 88.3% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8833\n",
      "train_loss: 0.05775975853827655\n",
      "val_loss: 0.16386579387593067\n",
      "Progress: 88.3% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 8834\n",
      "train_loss: 0.05795530371933862\n",
      "val_loss: 0.15934397834810682\n",
      "Progress: 88.3% ... Training loss: 0.057 ... Validation loss: 0.156iteration: 8835\n",
      "train_loss: 0.05767864267101503\n",
      "val_loss: 0.15698891424845457\n",
      "Progress: 88.4% ... Training loss: 0.057 ... Validation loss: 0.158iteration: 8836\n",
      "train_loss: 0.057535716728147834\n",
      "val_loss: 0.15809306385399433\n",
      "Progress: 88.4% ... Training loss: 0.058 ... Validation loss: 0.173iteration: 8837\n",
      "train_loss: 0.05816247561611055\n",
      "val_loss: 0.17332322618319362\n",
      "Progress: 88.4% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 8838\n",
      "train_loss: 0.0586417112877555\n",
      "val_loss: 0.1714881137698034\n",
      "Progress: 88.4% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 8839\n",
      "train_loss: 0.05793194149416281\n",
      "val_loss: 0.1701743864391596\n",
      "Progress: 88.4% ... Training loss: 0.060 ... Validation loss: 0.182iteration: 8840\n",
      "train_loss: 0.060566964028079186\n",
      "val_loss: 0.1824862770506789\n",
      "Progress: 88.4% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 8841\n",
      "train_loss: 0.06046237095954226\n",
      "val_loss: 0.15656709050799392\n",
      "Progress: 88.4% ... Training loss: 0.061 ... Validation loss: 0.185iteration: 8842\n",
      "train_loss: 0.06180980893705184\n",
      "val_loss: 0.18506962894776383\n",
      "Progress: 88.4% ... Training loss: 0.070 ... Validation loss: 0.155iteration: 8843\n",
      "train_loss: 0.07049325927241906\n",
      "val_loss: 0.15557958144431414\n",
      "Progress: 88.4% ... Training loss: 0.071 ... Validation loss: 0.212iteration: 8844\n",
      "train_loss: 0.07171741888581756\n",
      "val_loss: 0.21226253600138997\n",
      "Progress: 88.5% ... Training loss: 0.081 ... Validation loss: 0.149iteration: 8845\n",
      "train_loss: 0.0811192501484201\n",
      "val_loss: 0.1490639777761821\n",
      "Progress: 88.5% ... Training loss: 0.067 ... Validation loss: 0.184iteration: 8846\n",
      "train_loss: 0.06702426729379186\n",
      "val_loss: 0.1842952062164041\n",
      "Progress: 88.5% ... Training loss: 0.079 ... Validation loss: 0.148iteration: 8847\n",
      "train_loss: 0.07903960262598814\n",
      "val_loss: 0.14824171565943323\n",
      "Progress: 88.5% ... Training loss: 0.086 ... Validation loss: 0.209iteration: 8848\n",
      "train_loss: 0.08691359760676659\n",
      "val_loss: 0.2097749415358859\n",
      "Progress: 88.5% ... Training loss: 0.068 ... Validation loss: 0.151iteration: 8849\n",
      "train_loss: 0.06860217921263173\n",
      "val_loss: 0.1511391003197769\n",
      "Progress: 88.5% ... Training loss: 0.058 ... Validation loss: 0.177iteration: 8850\n",
      "train_loss: 0.05868145505904641\n",
      "val_loss: 0.17733379540585972\n",
      "Progress: 88.5% ... Training loss: 0.058 ... Validation loss: 0.162iteration: 8851\n",
      "train_loss: 0.05800800403403193\n",
      "val_loss: 0.16279619777918047\n",
      "Progress: 88.5% ... Training loss: 0.058 ... Validation loss: 0.160iteration: 8852\n",
      "train_loss: 0.05892224562868845\n",
      "val_loss: 0.16014408233029817\n",
      "Progress: 88.5% ... Training loss: 0.057 ... Validation loss: 0.173iteration: 8853\n",
      "train_loss: 0.057686691295161116\n",
      "val_loss: 0.17394459531964937\n",
      "Progress: 88.5% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8854\n",
      "train_loss: 0.058058693402324585\n",
      "val_loss: 0.16396921068402187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 88.5% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 8855\n",
      "train_loss: 0.05838756039341004\n",
      "val_loss: 0.17136919793208644\n",
      "Progress: 88.6% ... Training loss: 0.057 ... Validation loss: 0.164iteration: 8856\n",
      "train_loss: 0.05747816109806486\n",
      "val_loss: 0.16421878736580334\n",
      "Progress: 88.6% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 8857\n",
      "train_loss: 0.057222532462812996\n",
      "val_loss: 0.16096742715703358\n",
      "Progress: 88.6% ... Training loss: 0.064 ... Validation loss: 0.182iteration: 8858\n",
      "train_loss: 0.06421814888461307\n",
      "val_loss: 0.18215754041012247\n",
      "Progress: 88.6% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 8859\n",
      "train_loss: 0.05870813222048815\n",
      "val_loss: 0.1573573715900256\n",
      "Progress: 88.6% ... Training loss: 0.058 ... Validation loss: 0.168iteration: 8860\n",
      "train_loss: 0.05865827425616466\n",
      "val_loss: 0.16839582189619193\n",
      "Progress: 88.6% ... Training loss: 0.058 ... Validation loss: 0.156iteration: 8861\n",
      "train_loss: 0.05868788992770899\n",
      "val_loss: 0.15667615149883665\n",
      "Progress: 88.6% ... Training loss: 0.065 ... Validation loss: 0.187iteration: 8862\n",
      "train_loss: 0.06504540819081564\n",
      "val_loss: 0.18767312136782338\n",
      "Progress: 88.6% ... Training loss: 0.066 ... Validation loss: 0.154iteration: 8863\n",
      "train_loss: 0.06654317577510183\n",
      "val_loss: 0.1543794634102601\n",
      "Progress: 88.6% ... Training loss: 0.067 ... Validation loss: 0.188iteration: 8864\n",
      "train_loss: 0.06701404686625974\n",
      "val_loss: 0.18803729925896673\n",
      "Progress: 88.7% ... Training loss: 0.063 ... Validation loss: 0.146iteration: 8865\n",
      "train_loss: 0.0634765678846715\n",
      "val_loss: 0.14640822858587807\n",
      "Progress: 88.7% ... Training loss: 0.057 ... Validation loss: 0.156iteration: 8866\n",
      "train_loss: 0.05710816181432637\n",
      "val_loss: 0.15630851531710313\n",
      "Progress: 88.7% ... Training loss: 0.058 ... Validation loss: 0.158iteration: 8867\n",
      "train_loss: 0.05881189681831678\n",
      "val_loss: 0.1589245203604078\n",
      "Progress: 88.7% ... Training loss: 0.061 ... Validation loss: 0.187iteration: 8868\n",
      "train_loss: 0.061862519178315284\n",
      "val_loss: 0.18738345103395038\n",
      "Progress: 88.7% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 8869\n",
      "train_loss: 0.05734104363349671\n",
      "val_loss: 0.15951549680791424\n",
      "Progress: 88.7% ... Training loss: 0.057 ... Validation loss: 0.157iteration: 8870\n",
      "train_loss: 0.057354486563966786\n",
      "val_loss: 0.15794329945045604\n",
      "Progress: 88.7% ... Training loss: 0.070 ... Validation loss: 0.198iteration: 8871\n",
      "train_loss: 0.07084911038636103\n",
      "val_loss: 0.19888055876504257\n",
      "Progress: 88.7% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 8872\n",
      "train_loss: 0.059286625869023155\n",
      "val_loss: 0.15174549196511905\n",
      "Progress: 88.7% ... Training loss: 0.060 ... Validation loss: 0.178iteration: 8873\n",
      "train_loss: 0.06025960412130088\n",
      "val_loss: 0.17857521976250254\n",
      "Progress: 88.7% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 8874\n",
      "train_loss: 0.057777864345254545\n",
      "val_loss: 0.16595864097560278\n",
      "Progress: 88.8% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 8875\n",
      "train_loss: 0.05765454341416492\n",
      "val_loss: 0.1654247453790599\n",
      "Progress: 88.8% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 8876\n",
      "train_loss: 0.05875819704034509\n",
      "val_loss: 0.15935986200758295\n",
      "Progress: 88.8% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 8877\n",
      "train_loss: 0.0596761796346816\n",
      "val_loss: 0.16876586318747805\n",
      "Progress: 88.8% ... Training loss: 0.057 ... Validation loss: 0.157iteration: 8878\n",
      "train_loss: 0.057810008923594024\n",
      "val_loss: 0.1579286794853966\n",
      "Progress: 88.8% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 8879\n",
      "train_loss: 0.057261006874328335\n",
      "val_loss: 0.16551486373103202\n",
      "Progress: 88.8% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 8880\n",
      "train_loss: 0.05787326607287118\n",
      "val_loss: 0.16536521310000593\n",
      "Progress: 88.8% ... Training loss: 0.058 ... Validation loss: 0.173iteration: 8881\n",
      "train_loss: 0.05826361910421867\n",
      "val_loss: 0.17351240562179895\n",
      "Progress: 88.8% ... Training loss: 0.057 ... Validation loss: 0.161iteration: 8882\n",
      "train_loss: 0.0574639231983039\n",
      "val_loss: 0.1619162231514739\n",
      "Progress: 88.8% ... Training loss: 0.060 ... Validation loss: 0.157iteration: 8883\n",
      "train_loss: 0.060608141659619495\n",
      "val_loss: 0.15743127501619913\n",
      "Progress: 88.8% ... Training loss: 0.060 ... Validation loss: 0.183iteration: 8884\n",
      "train_loss: 0.0607156412579101\n",
      "val_loss: 0.183355411114156\n",
      "Progress: 88.8% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 8885\n",
      "train_loss: 0.05881685903272341\n",
      "val_loss: 0.16915913298332116\n",
      "Progress: 88.9% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 8886\n",
      "train_loss: 0.05725581513510726\n",
      "val_loss: 0.15941728801766972\n",
      "Progress: 88.9% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8887\n",
      "train_loss: 0.05736004775723676\n",
      "val_loss: 0.16355369770670897\n",
      "Progress: 88.9% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 8888\n",
      "train_loss: 0.05858563781875863\n",
      "val_loss: 0.17130675727032096\n",
      "Progress: 88.9% ... Training loss: 0.058 ... Validation loss: 0.162iteration: 8889\n",
      "train_loss: 0.058448347245341606\n",
      "val_loss: 0.16221528760961232\n",
      "Progress: 88.9% ... Training loss: 0.068 ... Validation loss: 0.182iteration: 8890\n",
      "train_loss: 0.06828253333558046\n",
      "val_loss: 0.18297771554650588\n",
      "Progress: 88.9% ... Training loss: 0.057 ... Validation loss: 0.161iteration: 8891\n",
      "train_loss: 0.057021032259750457\n",
      "val_loss: 0.1614705258145343\n",
      "Progress: 88.9% ... Training loss: 0.058 ... Validation loss: 0.152iteration: 8892\n",
      "train_loss: 0.058709077972956054\n",
      "val_loss: 0.15274159267085247\n",
      "Progress: 88.9% ... Training loss: 0.059 ... Validation loss: 0.172iteration: 8893\n",
      "train_loss: 0.05981116476230398\n",
      "val_loss: 0.172358845838276\n",
      "Progress: 88.9% ... Training loss: 0.061 ... Validation loss: 0.159iteration: 8894\n",
      "train_loss: 0.06182854447929394\n",
      "val_loss: 0.15928254218433027\n",
      "Progress: 89.0% ... Training loss: 0.074 ... Validation loss: 0.217iteration: 8895\n",
      "train_loss: 0.07452297513650612\n",
      "val_loss: 0.21736070154564707\n",
      "Progress: 89.0% ... Training loss: 0.080 ... Validation loss: 0.151iteration: 8896\n",
      "train_loss: 0.08022064332370296\n",
      "val_loss: 0.15112334926812443\n",
      "Progress: 89.0% ... Training loss: 0.067 ... Validation loss: 0.186iteration: 8897\n",
      "train_loss: 0.0670613999279067\n",
      "val_loss: 0.1869959889481157\n",
      "Progress: 89.0% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 8898\n",
      "train_loss: 0.059946515585050875\n",
      "val_loss: 0.1514908494950194\n",
      "Progress: 89.0% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 8899\n",
      "train_loss: 0.06354715092330336\n",
      "val_loss: 0.1780717047805965\n",
      "Progress: 89.0% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 8900\n",
      "train_loss: 0.05764978546223662\n",
      "val_loss: 0.1513392336153093\n",
      "Progress: 89.0% ... Training loss: 0.057 ... Validation loss: 0.150iteration: 8901\n",
      "train_loss: 0.057687343369348804\n",
      "val_loss: 0.15018223318211793\n",
      "Progress: 89.0% ... Training loss: 0.058 ... Validation loss: 0.148iteration: 8902\n",
      "train_loss: 0.05832597737929073\n",
      "val_loss: 0.14840213899902335\n",
      "Progress: 89.0% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 8903\n",
      "train_loss: 0.057949151459416835\n",
      "val_loss: 0.15266104780559223\n",
      "Progress: 89.0% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 8904\n",
      "train_loss: 0.05870850952127362\n",
      "val_loss: 0.16985775834858544\n",
      "Progress: 89.0% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8905\n",
      "train_loss: 0.05778523424740581\n",
      "val_loss: 0.16354005263411436\n",
      "Progress: 89.1% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 8906\n",
      "train_loss: 0.0581641436101797\n",
      "val_loss: 0.1515466579409407\n",
      "Progress: 89.1% ... Training loss: 0.058 ... Validation loss: 0.160iteration: 8907\n",
      "train_loss: 0.0580284533008101\n",
      "val_loss: 0.1607398571971852\n",
      "Progress: 89.1% ... Training loss: 0.061 ... Validation loss: 0.158iteration: 8908\n",
      "train_loss: 0.061827713742019556\n",
      "val_loss: 0.15804509380398224\n",
      "Progress: 89.1% ... Training loss: 0.060 ... Validation loss: 0.184iteration: 8909\n",
      "train_loss: 0.060868266626255975\n",
      "val_loss: 0.18481582700203852\n",
      "Progress: 89.1% ... Training loss: 0.066 ... Validation loss: 0.148iteration: 8910\n",
      "train_loss: 0.06691110140026958\n",
      "val_loss: 0.14806068427451738\n",
      "Progress: 89.1% ... Training loss: 0.063 ... Validation loss: 0.193iteration: 8911\n",
      "train_loss: 0.06317898520574597\n",
      "val_loss: 0.19363755515951167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 89.1% ... Training loss: 0.060 ... Validation loss: 0.162iteration: 8912\n",
      "train_loss: 0.06035961984275922\n",
      "val_loss: 0.1621848077335774\n",
      "Progress: 89.1% ... Training loss: 0.063 ... Validation loss: 0.189iteration: 8913\n",
      "train_loss: 0.06369231265678327\n",
      "val_loss: 0.1892359260477378\n",
      "Progress: 89.1% ... Training loss: 0.071 ... Validation loss: 0.158iteration: 8914\n",
      "train_loss: 0.07119004849280684\n",
      "val_loss: 0.15870732906548626\n",
      "Progress: 89.2% ... Training loss: 0.074 ... Validation loss: 0.223iteration: 8915\n",
      "train_loss: 0.07475961637939808\n",
      "val_loss: 0.2232043892193265\n",
      "Progress: 89.2% ... Training loss: 0.063 ... Validation loss: 0.159iteration: 8916\n",
      "train_loss: 0.0638073275717318\n",
      "val_loss: 0.15999280765584897\n",
      "Progress: 89.2% ... Training loss: 0.074 ... Validation loss: 0.218iteration: 8917\n",
      "train_loss: 0.07436090183093597\n",
      "val_loss: 0.2187395203338113\n",
      "Progress: 89.2% ... Training loss: 0.060 ... Validation loss: 0.156iteration: 8918\n",
      "train_loss: 0.06058554999574335\n",
      "val_loss: 0.15610281235163276\n",
      "Progress: 89.2% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 8919\n",
      "train_loss: 0.05918833020357012\n",
      "val_loss: 0.17192069850868957\n",
      "Progress: 89.2% ... Training loss: 0.061 ... Validation loss: 0.154iteration: 8920\n",
      "train_loss: 0.061547882238602955\n",
      "val_loss: 0.15402881336123656\n",
      "Progress: 89.2% ... Training loss: 0.074 ... Validation loss: 0.204iteration: 8921\n",
      "train_loss: 0.0745025358354699\n",
      "val_loss: 0.2046652068578358\n",
      "Progress: 89.2% ... Training loss: 0.074 ... Validation loss: 0.153iteration: 8922\n",
      "train_loss: 0.07470643292830478\n",
      "val_loss: 0.15333957514909727\n",
      "Progress: 89.2% ... Training loss: 0.073 ... Validation loss: 0.206iteration: 8923\n",
      "train_loss: 0.07390953142011393\n",
      "val_loss: 0.20678486436555624\n",
      "Progress: 89.2% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 8924\n",
      "train_loss: 0.06184547698071749\n",
      "val_loss: 0.15310118779119283\n",
      "Progress: 89.2% ... Training loss: 0.058 ... Validation loss: 0.175iteration: 8925\n",
      "train_loss: 0.058006117335130475\n",
      "val_loss: 0.1758685376506668\n",
      "Progress: 89.3% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 8926\n",
      "train_loss: 0.0570896019695775\n",
      "val_loss: 0.16723220845097847\n",
      "Progress: 89.3% ... Training loss: 0.059 ... Validation loss: 0.165iteration: 8927\n",
      "train_loss: 0.05936161232280786\n",
      "val_loss: 0.16538785898167757\n",
      "Progress: 89.3% ... Training loss: 0.057 ... Validation loss: 0.174iteration: 8928\n",
      "train_loss: 0.057051710616183235\n",
      "val_loss: 0.17429420040852892\n",
      "Progress: 89.3% ... Training loss: 0.057 ... Validation loss: 0.175iteration: 8929\n",
      "train_loss: 0.057429152934652436\n",
      "val_loss: 0.17509202020020126\n",
      "Progress: 89.3% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 8930\n",
      "train_loss: 0.057253978083223425\n",
      "val_loss: 0.16570263029697346\n",
      "Progress: 89.3% ... Training loss: 0.059 ... Validation loss: 0.187iteration: 8931\n",
      "train_loss: 0.05996981564778624\n",
      "val_loss: 0.18737910390344925\n",
      "Progress: 89.3% ... Training loss: 0.073 ... Validation loss: 0.149iteration: 8932\n",
      "train_loss: 0.07340047341177933\n",
      "val_loss: 0.14981351210640412\n",
      "Progress: 89.3% ... Training loss: 0.067 ... Validation loss: 0.205iteration: 8933\n",
      "train_loss: 0.06794475103298948\n",
      "val_loss: 0.2057040111444325\n",
      "Progress: 89.3% ... Training loss: 0.067 ... Validation loss: 0.155iteration: 8934\n",
      "train_loss: 0.06732526248832829\n",
      "val_loss: 0.15518997165045534\n",
      "Progress: 89.3% ... Training loss: 0.060 ... Validation loss: 0.180iteration: 8935\n",
      "train_loss: 0.06056035830812054\n",
      "val_loss: 0.18041720866884997\n",
      "Progress: 89.4% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 8936\n",
      "train_loss: 0.05733133492876788\n",
      "val_loss: 0.16067022104026865\n",
      "Progress: 89.4% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 8937\n",
      "train_loss: 0.05858139077866756\n",
      "val_loss: 0.15371853357873644\n",
      "Progress: 89.4% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 8938\n",
      "train_loss: 0.058499743216011015\n",
      "val_loss: 0.16702313110610661\n",
      "Progress: 89.4% ... Training loss: 0.059 ... Validation loss: 0.191iteration: 8939\n",
      "train_loss: 0.0599530261974142\n",
      "val_loss: 0.1916588550701035\n",
      "Progress: 89.4% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 8940\n",
      "train_loss: 0.05774110871306873\n",
      "val_loss: 0.15474122019878098\n",
      "Progress: 89.4% ... Training loss: 0.067 ... Validation loss: 0.177iteration: 8941\n",
      "train_loss: 0.06770745033849936\n",
      "val_loss: 0.17786366604105228\n",
      "Progress: 89.4% ... Training loss: 0.060 ... Validation loss: 0.150iteration: 8942\n",
      "train_loss: 0.06062905423412972\n",
      "val_loss: 0.15043540083240814\n",
      "Progress: 89.4% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 8943\n",
      "train_loss: 0.05831391179215562\n",
      "val_loss: 0.16386801791846806\n",
      "Progress: 89.4% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 8944\n",
      "train_loss: 0.057588668495158916\n",
      "val_loss: 0.16081852645448297\n",
      "Progress: 89.5% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 8945\n",
      "train_loss: 0.05919997286924769\n",
      "val_loss: 0.16296792920956774\n",
      "Progress: 89.5% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 8946\n",
      "train_loss: 0.057696769829789246\n",
      "val_loss: 0.16729146666913164\n",
      "Progress: 89.5% ... Training loss: 0.068 ... Validation loss: 0.183iteration: 8947\n",
      "train_loss: 0.0684376993927209\n",
      "val_loss: 0.1837815180796654\n",
      "Progress: 89.5% ... Training loss: 0.083 ... Validation loss: 0.153iteration: 8948\n",
      "train_loss: 0.08306725843136047\n",
      "val_loss: 0.15395140831144688\n",
      "Progress: 89.5% ... Training loss: 0.083 ... Validation loss: 0.219iteration: 8949\n",
      "train_loss: 0.08351615144547274\n",
      "val_loss: 0.21968301888285907\n",
      "Progress: 89.5% ... Training loss: 0.073 ... Validation loss: 0.150iteration: 8950\n",
      "train_loss: 0.0738987826827501\n",
      "val_loss: 0.15021213438597544\n",
      "Progress: 89.5% ... Training loss: 0.069 ... Validation loss: 0.185iteration: 8951\n",
      "train_loss: 0.06921128739535423\n",
      "val_loss: 0.18508965941937422\n",
      "Progress: 89.5% ... Training loss: 0.072 ... Validation loss: 0.152iteration: 8952\n",
      "train_loss: 0.0729348905254629\n",
      "val_loss: 0.1524681864840394\n",
      "Progress: 89.5% ... Training loss: 0.076 ... Validation loss: 0.191iteration: 8953\n",
      "train_loss: 0.07650277099499803\n",
      "val_loss: 0.19106648281505245\n",
      "Progress: 89.5% ... Training loss: 0.069 ... Validation loss: 0.151iteration: 8954\n",
      "train_loss: 0.06966936209014622\n",
      "val_loss: 0.15159788031446092\n",
      "Progress: 89.5% ... Training loss: 0.082 ... Validation loss: 0.202iteration: 8955\n",
      "train_loss: 0.08251486459737392\n",
      "val_loss: 0.20257477957989975\n",
      "Progress: 89.6% ... Training loss: 0.079 ... Validation loss: 0.156iteration: 8956\n",
      "train_loss: 0.07905908871697384\n",
      "val_loss: 0.15656251077924968\n",
      "Progress: 89.6% ... Training loss: 0.101 ... Validation loss: 0.230iteration: 8957\n",
      "train_loss: 0.10128638074156888\n",
      "val_loss: 0.23005650838161423\n",
      "Progress: 89.6% ... Training loss: 0.104 ... Validation loss: 0.158iteration: 8958\n",
      "train_loss: 0.1049001750317183\n",
      "val_loss: 0.15850521838472717\n",
      "Progress: 89.6% ... Training loss: 0.090 ... Validation loss: 0.212iteration: 8959\n",
      "train_loss: 0.09044640908085727\n",
      "val_loss: 0.21263294710890063\n",
      "Progress: 89.6% ... Training loss: 0.064 ... Validation loss: 0.146iteration: 8960\n",
      "train_loss: 0.06422515171576944\n",
      "val_loss: 0.14647335790584456\n",
      "Progress: 89.6% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 8961\n",
      "train_loss: 0.06044357484503373\n",
      "val_loss: 0.17491111907257093\n",
      "Progress: 89.6% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 8962\n",
      "train_loss: 0.06045160723818633\n",
      "val_loss: 0.1519072067217917\n",
      "Progress: 89.6% ... Training loss: 0.067 ... Validation loss: 0.185iteration: 8963\n",
      "train_loss: 0.06708746662258162\n",
      "val_loss: 0.1859342102797285\n",
      "Progress: 89.6% ... Training loss: 0.069 ... Validation loss: 0.148iteration: 8964\n",
      "train_loss: 0.06955541952983671\n",
      "val_loss: 0.14861131876310082\n",
      "Progress: 89.7% ... Training loss: 0.063 ... Validation loss: 0.181iteration: 8965\n",
      "train_loss: 0.06326870458361493\n",
      "val_loss: 0.1814410007930992\n",
      "Progress: 89.7% ... Training loss: 0.058 ... Validation loss: 0.155iteration: 8966\n",
      "train_loss: 0.058265292503680656\n",
      "val_loss: 0.15595384959710568\n",
      "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 8967\n",
      "train_loss: 0.0570768776468082\n",
      "val_loss: 0.1670756923367978\n",
      "Progress: 89.7% ... Training loss: 0.056 ... Validation loss: 0.160iteration: 8968\n",
      "train_loss: 0.05674864211710909\n",
      "val_loss: 0.16028475876792836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 8969\n",
      "train_loss: 0.05736454627330991\n",
      "val_loss: 0.1556633954779515\n",
      "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.176iteration: 8970\n",
      "train_loss: 0.057745195328971405\n",
      "val_loss: 0.1762548208980272\n",
      "Progress: 89.7% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 8971\n",
      "train_loss: 0.05853375182650135\n",
      "val_loss: 0.17094048074845014\n",
      "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8972\n",
      "train_loss: 0.057921544527412154\n",
      "val_loss: 0.1630494374388205\n",
      "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.164iteration: 8973\n",
      "train_loss: 0.05729820614035318\n",
      "val_loss: 0.16411035645078287\n",
      "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.175iteration: 8974\n",
      "train_loss: 0.05783499663077323\n",
      "val_loss: 0.1757607758444368\n",
      "Progress: 89.8% ... Training loss: 0.057 ... Validation loss: 0.174iteration: 8975\n",
      "train_loss: 0.05750851659293679\n",
      "val_loss: 0.17423735137837876\n",
      "Progress: 89.8% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 8976\n",
      "train_loss: 0.057579702532051115\n",
      "val_loss: 0.17086577474996037\n",
      "Progress: 89.8% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 8977\n",
      "train_loss: 0.05734187943092477\n",
      "val_loss: 0.1638034191267552\n",
      "Progress: 89.8% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 8978\n",
      "train_loss: 0.05739400290589368\n",
      "val_loss: 0.15980773435846238\n",
      "Progress: 89.8% ... Training loss: 0.057 ... Validation loss: 0.156iteration: 8979\n",
      "train_loss: 0.05726364805471908\n",
      "val_loss: 0.15635156126191352\n",
      "Progress: 89.8% ... Training loss: 0.056 ... Validation loss: 0.158iteration: 8980\n",
      "train_loss: 0.05692581901875185\n",
      "val_loss: 0.1581689541075393\n",
      "Progress: 89.8% ... Training loss: 0.068 ... Validation loss: 0.162iteration: 8981\n",
      "train_loss: 0.06815258698360904\n",
      "val_loss: 0.1626267903601374\n",
      "Progress: 89.8% ... Training loss: 0.061 ... Validation loss: 0.145iteration: 8982\n",
      "train_loss: 0.06186271346653443\n",
      "val_loss: 0.14530242263333185\n",
      "Progress: 89.8% ... Training loss: 0.063 ... Validation loss: 0.172iteration: 8983\n",
      "train_loss: 0.06338278843199635\n",
      "val_loss: 0.17246315088275377\n",
      "Progress: 89.8% ... Training loss: 0.057 ... Validation loss: 0.145iteration: 8984\n",
      "train_loss: 0.057669621050835417\n",
      "val_loss: 0.1453099378537822\n",
      "Progress: 89.8% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 8985\n",
      "train_loss: 0.05912485609947536\n",
      "val_loss: 0.15773547821486322\n",
      "Progress: 89.9% ... Training loss: 0.057 ... Validation loss: 0.147iteration: 8986\n",
      "train_loss: 0.05776039549367372\n",
      "val_loss: 0.147641857718565\n",
      "Progress: 89.9% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 8987\n",
      "train_loss: 0.05830389913405432\n",
      "val_loss: 0.16927513988563178\n",
      "Progress: 89.9% ... Training loss: 0.056 ... Validation loss: 0.158iteration: 8988\n",
      "train_loss: 0.05695107011643453\n",
      "val_loss: 0.15873588157280621\n",
      "Progress: 89.9% ... Training loss: 0.057 ... Validation loss: 0.168iteration: 8989\n",
      "train_loss: 0.05743669665768472\n",
      "val_loss: 0.1680184956082228\n",
      "Progress: 89.9% ... Training loss: 0.059 ... Validation loss: 0.152iteration: 8990\n",
      "train_loss: 0.0591165874111894\n",
      "val_loss: 0.15294471051535244\n",
      "Progress: 89.9% ... Training loss: 0.057 ... Validation loss: 0.157iteration: 8991\n",
      "train_loss: 0.05744104493327363\n",
      "val_loss: 0.15760142913485325\n",
      "Progress: 89.9% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 8992\n",
      "train_loss: 0.05812336320658595\n",
      "val_loss: 0.16681460650570448\n",
      "Progress: 89.9% ... Training loss: 0.060 ... Validation loss: 0.170iteration: 8993\n",
      "train_loss: 0.06089775057138934\n",
      "val_loss: 0.1700911665606149\n",
      "Progress: 89.9% ... Training loss: 0.058 ... Validation loss: 0.154iteration: 8994\n",
      "train_loss: 0.05845327428265746\n",
      "val_loss: 0.15407706003583516\n",
      "Progress: 90.0% ... Training loss: 0.058 ... Validation loss: 0.150iteration: 8995\n",
      "train_loss: 0.05806193731812506\n",
      "val_loss: 0.15066379803797517\n",
      "Progress: 90.0% ... Training loss: 0.058 ... Validation loss: 0.152iteration: 8996\n",
      "train_loss: 0.05827182015070691\n",
      "val_loss: 0.15207390644862914\n",
      "Progress: 90.0% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 8997\n",
      "train_loss: 0.05709345995572034\n",
      "val_loss: 0.1556463352754498\n",
      "Progress: 90.0% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 8998\n",
      "train_loss: 0.057019788755107347\n",
      "val_loss: 0.15138485316224254\n",
      "Progress: 90.0% ... Training loss: 0.057 ... Validation loss: 0.150iteration: 8999\n",
      "train_loss: 0.057615799007501904\n",
      "val_loss: 0.15098616690726366\n",
      "Progress: 90.0% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 9000\n",
      "train_loss: 0.05877855386411967\n",
      "val_loss: 0.1707659229304139\n",
      "Progress: 90.0% ... Training loss: 0.067 ... Validation loss: 0.150iteration: 9001\n",
      "train_loss: 0.06766517864477499\n",
      "val_loss: 0.1501519893951226\n",
      "Progress: 90.0% ... Training loss: 0.061 ... Validation loss: 0.166iteration: 9002\n",
      "train_loss: 0.06199018832028074\n",
      "val_loss: 0.1664343289051762\n",
      "Progress: 90.0% ... Training loss: 0.069 ... Validation loss: 0.149iteration: 9003\n",
      "train_loss: 0.0692276462494631\n",
      "val_loss: 0.14988429890181879\n",
      "Progress: 90.0% ... Training loss: 0.071 ... Validation loss: 0.186iteration: 9004\n",
      "train_loss: 0.07189454057940382\n",
      "val_loss: 0.18690564611363666\n",
      "Progress: 90.0% ... Training loss: 0.066 ... Validation loss: 0.146iteration: 9005\n",
      "train_loss: 0.0661967074981475\n",
      "val_loss: 0.1463374889568174\n",
      "Progress: 90.1% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 9006\n",
      "train_loss: 0.06406369023202724\n",
      "val_loss: 0.17045323233340792\n",
      "Progress: 90.1% ... Training loss: 0.063 ... Validation loss: 0.145iteration: 9007\n",
      "train_loss: 0.06342753337974034\n",
      "val_loss: 0.14512579963936129\n",
      "Progress: 90.1% ... Training loss: 0.059 ... Validation loss: 0.165iteration: 9008\n",
      "train_loss: 0.05983327068216489\n",
      "val_loss: 0.16599318633146634\n",
      "Progress: 90.1% ... Training loss: 0.058 ... Validation loss: 0.145iteration: 9009\n",
      "train_loss: 0.05804836710356488\n",
      "val_loss: 0.14535706975757479\n",
      "Progress: 90.1% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9010\n",
      "train_loss: 0.05716212482305487\n",
      "val_loss: 0.1523209595618264\n",
      "Progress: 90.1% ... Training loss: 0.058 ... Validation loss: 0.154iteration: 9011\n",
      "train_loss: 0.05871165428339744\n",
      "val_loss: 0.1545952624938692\n",
      "Progress: 90.1% ... Training loss: 0.057 ... Validation loss: 0.153iteration: 9012\n",
      "train_loss: 0.057615452735778144\n",
      "val_loss: 0.1535969536657234\n",
      "Progress: 90.1% ... Training loss: 0.057 ... Validation loss: 0.157iteration: 9013\n",
      "train_loss: 0.05707043265451339\n",
      "val_loss: 0.15734612617405783\n",
      "Progress: 90.1% ... Training loss: 0.060 ... Validation loss: 0.147iteration: 9014\n",
      "train_loss: 0.06006806715666052\n",
      "val_loss: 0.14775037999020812\n",
      "Progress: 90.2% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 9015\n",
      "train_loss: 0.05948286414437133\n",
      "val_loss: 0.1518177226447429\n",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.158iteration: 9016\n",
      "train_loss: 0.05846859363458039\n",
      "val_loss: 0.15886056568531112\n",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.149iteration: 9017\n",
      "train_loss: 0.05896322331969662\n",
      "val_loss: 0.14923660614005935\n",
      "Progress: 90.2% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9018\n",
      "train_loss: 0.05756907493860289\n",
      "val_loss: 0.1540167846280252\n",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 9019\n",
      "train_loss: 0.05897381127889843\n",
      "val_loss: 0.17128259746337363\n",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.162iteration: 9020\n",
      "train_loss: 0.058052415305414995\n",
      "val_loss: 0.16210419689839103\n",
      "Progress: 90.2% ... Training loss: 0.059 ... Validation loss: 0.152iteration: 9021\n",
      "train_loss: 0.059078683448563804\n",
      "val_loss: 0.1525259734560334\n",
      "Progress: 90.2% ... Training loss: 0.059 ... Validation loss: 0.172iteration: 9022\n",
      "train_loss: 0.059508071912234076\n",
      "val_loss: 0.17251399723442343\n",
      "Progress: 90.2% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 9023\n",
      "train_loss: 0.06457905446805547\n",
      "val_loss: 0.15635712994228643\n",
      "Progress: 90.2% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 9024\n",
      "train_loss: 0.05773123990901035\n",
      "val_loss: 0.1631179321656604\n",
      "Progress: 90.2% ... Training loss: 0.057 ... Validation loss: 0.162iteration: 9025\n",
      "train_loss: 0.05737844152717266\n",
      "val_loss: 0.16273471466161796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 90.3% ... Training loss: 0.057 ... Validation loss: 0.171iteration: 9026\n",
      "train_loss: 0.057868417296326566\n",
      "val_loss: 0.17124563091207573\n",
      "Progress: 90.3% ... Training loss: 0.061 ... Validation loss: 0.153iteration: 9027\n",
      "train_loss: 0.061927315140649364\n",
      "val_loss: 0.1530821671500456\n",
      "Progress: 90.3% ... Training loss: 0.059 ... Validation loss: 0.179iteration: 9028\n",
      "train_loss: 0.05978369072067113\n",
      "val_loss: 0.17923528952126544\n",
      "Progress: 90.3% ... Training loss: 0.060 ... Validation loss: 0.150iteration: 9029\n",
      "train_loss: 0.06017460781742257\n",
      "val_loss: 0.1504673160351596\n",
      "Progress: 90.3% ... Training loss: 0.056 ... Validation loss: 0.157iteration: 9030\n",
      "train_loss: 0.05663530419879371\n",
      "val_loss: 0.15749124772324954\n",
      "Progress: 90.3% ... Training loss: 0.056 ... Validation loss: 0.164iteration: 9031\n",
      "train_loss: 0.056999677732132166\n",
      "val_loss: 0.16420523315822874\n",
      "Progress: 90.3% ... Training loss: 0.056 ... Validation loss: 0.160iteration: 9032\n",
      "train_loss: 0.05672646154469099\n",
      "val_loss: 0.16020138374365606\n",
      "Progress: 90.3% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 9033\n",
      "train_loss: 0.05913571316838479\n",
      "val_loss: 0.15602208634274922\n",
      "Progress: 90.3% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9034\n",
      "train_loss: 0.05759490121011998\n",
      "val_loss: 0.16578684165266955\n",
      "Progress: 90.3% ... Training loss: 0.061 ... Validation loss: 0.167iteration: 9035\n",
      "train_loss: 0.06197016279821354\n",
      "val_loss: 0.1670906564128179\n",
      "Progress: 90.4% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 9036\n",
      "train_loss: 0.05978253173514685\n",
      "val_loss: 0.15570710530261625\n",
      "Progress: 90.4% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 9037\n",
      "train_loss: 0.05795826252923466\n",
      "val_loss: 0.17044505081633332\n",
      "Progress: 90.4% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9038\n",
      "train_loss: 0.05672817587089592\n",
      "val_loss: 0.15969981316420026\n",
      "Progress: 90.4% ... Training loss: 0.058 ... Validation loss: 0.172iteration: 9039\n",
      "train_loss: 0.05882777733586344\n",
      "val_loss: 0.1723953851764925\n",
      "Progress: 90.4% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 9040\n",
      "train_loss: 0.057426556802198346\n",
      "val_loss: 0.17089344903736778\n",
      "Progress: 90.4% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9041\n",
      "train_loss: 0.05721813418836208\n",
      "val_loss: 0.16689155406189676\n",
      "Progress: 90.4% ... Training loss: 0.061 ... Validation loss: 0.165iteration: 9042\n",
      "train_loss: 0.06118004750501506\n",
      "val_loss: 0.1651546766644045\n",
      "Progress: 90.4% ... Training loss: 0.057 ... Validation loss: 0.158iteration: 9043\n",
      "train_loss: 0.05725195665734309\n",
      "val_loss: 0.15873032238058837\n",
      "Progress: 90.4% ... Training loss: 0.061 ... Validation loss: 0.157iteration: 9044\n",
      "train_loss: 0.06147439027222044\n",
      "val_loss: 0.1571895256389266\n",
      "Progress: 90.5% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9045\n",
      "train_loss: 0.05750339301942362\n",
      "val_loss: 0.16569504588337136\n",
      "Progress: 90.5% ... Training loss: 0.056 ... Validation loss: 0.165iteration: 9046\n",
      "train_loss: 0.05699260245532876\n",
      "val_loss: 0.1650026712591059\n",
      "Progress: 90.5% ... Training loss: 0.056 ... Validation loss: 0.173iteration: 9047\n",
      "train_loss: 0.05696810473154264\n",
      "val_loss: 0.17316646700749305\n",
      "Progress: 90.5% ... Training loss: 0.064 ... Validation loss: 0.158iteration: 9048\n",
      "train_loss: 0.06433203088298985\n",
      "val_loss: 0.15842346227660023\n",
      "Progress: 90.5% ... Training loss: 0.058 ... Validation loss: 0.177iteration: 9049\n",
      "train_loss: 0.05830134699936261\n",
      "val_loss: 0.17711480934709004\n",
      "Progress: 90.5% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 9050\n",
      "train_loss: 0.05878540231429677\n",
      "val_loss: 0.1593025934266473\n",
      "Progress: 90.5% ... Training loss: 0.057 ... Validation loss: 0.179iteration: 9051\n",
      "train_loss: 0.05775243769544471\n",
      "val_loss: 0.1795492766002312\n",
      "Progress: 90.5% ... Training loss: 0.061 ... Validation loss: 0.196iteration: 9052\n",
      "train_loss: 0.061984419152524645\n",
      "val_loss: 0.1963612499651085\n",
      "Progress: 90.5% ... Training loss: 0.056 ... Validation loss: 0.158iteration: 9053\n",
      "train_loss: 0.05695916182684327\n",
      "val_loss: 0.1583674933209761\n",
      "Progress: 90.5% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 9054\n",
      "train_loss: 0.05848181105561679\n",
      "val_loss: 0.16684676800530754\n",
      "Progress: 90.5% ... Training loss: 0.056 ... Validation loss: 0.161iteration: 9055\n",
      "train_loss: 0.05655207345838972\n",
      "val_loss: 0.16194811730152964\n",
      "Progress: 90.6% ... Training loss: 0.057 ... Validation loss: 0.161iteration: 9056\n",
      "train_loss: 0.057998212035060136\n",
      "val_loss: 0.16117696409047758\n",
      "Progress: 90.6% ... Training loss: 0.090 ... Validation loss: 0.215iteration: 9057\n",
      "train_loss: 0.09010046874590369\n",
      "val_loss: 0.21522983854129063\n",
      "Progress: 90.6% ... Training loss: 0.089 ... Validation loss: 0.155iteration: 9058\n",
      "train_loss: 0.08960061604701487\n",
      "val_loss: 0.1555917451335187\n",
      "Progress: 90.6% ... Training loss: 0.064 ... Validation loss: 0.189iteration: 9059\n",
      "train_loss: 0.06428403733201461\n",
      "val_loss: 0.189383795685879\n",
      "Progress: 90.6% ... Training loss: 0.057 ... Validation loss: 0.168iteration: 9060\n",
      "train_loss: 0.05713264514544168\n",
      "val_loss: 0.16851673871747297\n",
      "Progress: 90.6% ... Training loss: 0.057 ... Validation loss: 0.169iteration: 9061\n",
      "train_loss: 0.05711236284681292\n",
      "val_loss: 0.16928916661410034\n",
      "Progress: 90.6% ... Training loss: 0.062 ... Validation loss: 0.151iteration: 9062\n",
      "train_loss: 0.06286986460387016\n",
      "val_loss: 0.15121777604319994\n",
      "Progress: 90.6% ... Training loss: 0.062 ... Validation loss: 0.190iteration: 9063\n",
      "train_loss: 0.06221683445408741\n",
      "val_loss: 0.19022489772649837\n",
      "Progress: 90.6% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 9064\n",
      "train_loss: 0.05720229897375015\n",
      "val_loss: 0.16041527251278104\n",
      "Progress: 90.7% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 9065\n",
      "train_loss: 0.05750891020871154\n",
      "val_loss: 0.1603103289356644\n",
      "Progress: 90.7% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9066\n",
      "train_loss: 0.05770152306038526\n",
      "val_loss: 0.1512429871940708\n",
      "Progress: 90.7% ... Training loss: 0.059 ... Validation loss: 0.154iteration: 9067\n",
      "train_loss: 0.05947279696428273\n",
      "val_loss: 0.15457821507932576\n",
      "Progress: 90.7% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 9068\n",
      "train_loss: 0.06109465824772926\n",
      "val_loss: 0.17381813583662448\n",
      "Progress: 90.7% ... Training loss: 0.057 ... Validation loss: 0.162iteration: 9069\n",
      "train_loss: 0.05795758925157129\n",
      "val_loss: 0.16285842875003936\n",
      "Progress: 90.7% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 9070\n",
      "train_loss: 0.05919046255534259\n",
      "val_loss: 0.168339047708395\n",
      "Progress: 90.7% ... Training loss: 0.080 ... Validation loss: 0.150iteration: 9071\n",
      "train_loss: 0.0805480503762532\n",
      "val_loss: 0.15066759872049992\n",
      "Progress: 90.7% ... Training loss: 0.060 ... Validation loss: 0.179iteration: 9072\n",
      "train_loss: 0.06042733982693455\n",
      "val_loss: 0.1791748242591219\n",
      "Progress: 90.7% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 9073\n",
      "train_loss: 0.06070165562238988\n",
      "val_loss: 0.149869104682094\n",
      "Progress: 90.7% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 9074\n",
      "train_loss: 0.06098510718803396\n",
      "val_loss: 0.17387840636967009\n",
      "Progress: 90.8% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9075\n",
      "train_loss: 0.05694588031207984\n",
      "val_loss: 0.15966713539192312\n",
      "Progress: 90.8% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 9076\n",
      "train_loss: 0.05700047053573536\n",
      "val_loss: 0.1596845551461712\n",
      "Progress: 90.8% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 9077\n",
      "train_loss: 0.058121476032975426\n",
      "val_loss: 0.17027981514196208\n",
      "Progress: 90.8% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 9078\n",
      "train_loss: 0.057916908762853964\n",
      "val_loss: 0.15920024185966067\n",
      "Progress: 90.8% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9079\n",
      "train_loss: 0.05712115047502257\n",
      "val_loss: 0.16581912458149828\n",
      "Progress: 90.8% ... Training loss: 0.057 ... Validation loss: 0.171iteration: 9080\n",
      "train_loss: 0.057594794403353786\n",
      "val_loss: 0.17111121871921714\n",
      "Progress: 90.8% ... Training loss: 0.058 ... Validation loss: 0.154iteration: 9081\n",
      "train_loss: 0.05866038879205936\n",
      "val_loss: 0.15494688035062468\n",
      "Progress: 90.8% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 9082\n",
      "train_loss: 0.05806151293416928\n",
      "val_loss: 0.1633344793060678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 90.8% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 9083\n",
      "train_loss: 0.0615424777197135\n",
      "val_loss: 0.14950246234142991\n",
      "Progress: 90.8% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 9084\n",
      "train_loss: 0.05809004884443656\n",
      "val_loss: 0.16691179437269474\n",
      "Progress: 90.8% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 9085\n",
      "train_loss: 0.05799516240736261\n",
      "val_loss: 0.170427212016723\n",
      "Progress: 90.9% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 9086\n",
      "train_loss: 0.057183781717358495\n",
      "val_loss: 0.15921800230277033\n",
      "Progress: 90.9% ... Training loss: 0.057 ... Validation loss: 0.158iteration: 9087\n",
      "train_loss: 0.05762998617533075\n",
      "val_loss: 0.1582729574577542\n",
      "Progress: 90.9% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 9088\n",
      "train_loss: 0.05711959093964135\n",
      "val_loss: 0.1596257462364031\n",
      "Progress: 90.9% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 9089\n",
      "train_loss: 0.05827818510523096\n",
      "val_loss: 0.16422925791740794\n",
      "Progress: 90.9% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 9090\n",
      "train_loss: 0.06088412056675357\n",
      "val_loss: 0.1517059498268547\n",
      "Progress: 90.9% ... Training loss: 0.077 ... Validation loss: 0.210iteration: 9091\n",
      "train_loss: 0.07788631442139993\n",
      "val_loss: 0.21043306472812343\n",
      "Progress: 90.9% ... Training loss: 0.067 ... Validation loss: 0.145iteration: 9092\n",
      "train_loss: 0.06791956527441555\n",
      "val_loss: 0.1454569855016053\n",
      "Progress: 90.9% ... Training loss: 0.064 ... Validation loss: 0.186iteration: 9093\n",
      "train_loss: 0.064271560874467\n",
      "val_loss: 0.1864577657714238\n",
      "Progress: 90.9% ... Training loss: 0.069 ... Validation loss: 0.146iteration: 9094\n",
      "train_loss: 0.06990496839043368\n",
      "val_loss: 0.1461308063131152\n",
      "Progress: 91.0% ... Training loss: 0.068 ... Validation loss: 0.180iteration: 9095\n",
      "train_loss: 0.06820984105938858\n",
      "val_loss: 0.18054644280816412\n",
      "Progress: 91.0% ... Training loss: 0.065 ... Validation loss: 0.149iteration: 9096\n",
      "train_loss: 0.06525280686733292\n",
      "val_loss: 0.14959202813164918\n",
      "Progress: 91.0% ... Training loss: 0.063 ... Validation loss: 0.188iteration: 9097\n",
      "train_loss: 0.06350965150202811\n",
      "val_loss: 0.18840793591265262\n",
      "Progress: 91.0% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 9098\n",
      "train_loss: 0.0609576356694853\n",
      "val_loss: 0.15116677499103648\n",
      "Progress: 91.0% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 9099\n",
      "train_loss: 0.06069075190479987\n",
      "val_loss: 0.17414218862809203\n",
      "Progress: 91.0% ... Training loss: 0.066 ... Validation loss: 0.147iteration: 9100\n",
      "train_loss: 0.06672445641453317\n",
      "val_loss: 0.14762861557569834\n",
      "Progress: 91.0% ... Training loss: 0.071 ... Validation loss: 0.187iteration: 9101\n",
      "train_loss: 0.0710644683968907\n",
      "val_loss: 0.1873344100364648\n",
      "Progress: 91.0% ... Training loss: 0.075 ... Validation loss: 0.146iteration: 9102\n",
      "train_loss: 0.07512027005344203\n",
      "val_loss: 0.14619567015765583\n",
      "Progress: 91.0% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 9103\n",
      "train_loss: 0.05874190288714628\n",
      "val_loss: 0.16173038818404567\n",
      "Progress: 91.0% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9104\n",
      "train_loss: 0.05772121514811103\n",
      "val_loss: 0.15139819259669973\n",
      "Progress: 91.0% ... Training loss: 0.063 ... Validation loss: 0.153iteration: 9105\n",
      "train_loss: 0.06350514966183528\n",
      "val_loss: 0.15349788875662398\n",
      "Progress: 91.1% ... Training loss: 0.060 ... Validation loss: 0.174iteration: 9106\n",
      "train_loss: 0.06035078371623778\n",
      "val_loss: 0.17415613549337014\n",
      "Progress: 91.1% ... Training loss: 0.062 ... Validation loss: 0.149iteration: 9107\n",
      "train_loss: 0.062306878994097016\n",
      "val_loss: 0.1495643850579783\n",
      "Progress: 91.1% ... Training loss: 0.065 ... Validation loss: 0.172iteration: 9108\n",
      "train_loss: 0.06518300777864137\n",
      "val_loss: 0.1729817860465103\n",
      "Progress: 91.1% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9109\n",
      "train_loss: 0.057801606354409577\n",
      "val_loss: 0.1548862171485153\n",
      "Progress: 91.1% ... Training loss: 0.058 ... Validation loss: 0.149iteration: 9110\n",
      "train_loss: 0.05833925955525069\n",
      "val_loss: 0.1491501061778719\n",
      "Progress: 91.1% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9111\n",
      "train_loss: 0.05714814144711782\n",
      "val_loss: 0.15403481880139472\n",
      "Progress: 91.1% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 9112\n",
      "train_loss: 0.0570842708944139\n",
      "val_loss: 0.16338910685742136\n",
      "Progress: 91.1% ... Training loss: 0.058 ... Validation loss: 0.154iteration: 9113\n",
      "train_loss: 0.058682594126274176\n",
      "val_loss: 0.15400448620978555\n",
      "Progress: 91.1% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 9114\n",
      "train_loss: 0.057504189388030945\n",
      "val_loss: 0.1597293776925683\n",
      "Progress: 91.2% ... Training loss: 0.057 ... Validation loss: 0.176iteration: 9115\n",
      "train_loss: 0.05748810958930991\n",
      "val_loss: 0.1763823331490662\n",
      "Progress: 91.2% ... Training loss: 0.056 ... Validation loss: 0.168iteration: 9116\n",
      "train_loss: 0.05686795175597829\n",
      "val_loss: 0.16810538871701527\n",
      "Progress: 91.2% ... Training loss: 0.063 ... Validation loss: 0.190iteration: 9117\n",
      "train_loss: 0.06341794403042227\n",
      "val_loss: 0.19029846832887248\n",
      "Progress: 91.2% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 9118\n",
      "train_loss: 0.06001146784609702\n",
      "val_loss: 0.15526485032690185\n",
      "Progress: 91.2% ... Training loss: 0.056 ... Validation loss: 0.163iteration: 9119\n",
      "train_loss: 0.05662770469544522\n",
      "val_loss: 0.16319652548138114\n",
      "Progress: 91.2% ... Training loss: 0.060 ... Validation loss: 0.160iteration: 9120\n",
      "train_loss: 0.06022002113918908\n",
      "val_loss: 0.1600753882340143\n",
      "Progress: 91.2% ... Training loss: 0.057 ... Validation loss: 0.169iteration: 9121\n",
      "train_loss: 0.057135851031608405\n",
      "val_loss: 0.16913035123984962\n",
      "Progress: 91.2% ... Training loss: 0.057 ... Validation loss: 0.171iteration: 9122\n",
      "train_loss: 0.057453068811195496\n",
      "val_loss: 0.17187925942722443\n",
      "Progress: 91.2% ... Training loss: 0.057 ... Validation loss: 0.168iteration: 9123\n",
      "train_loss: 0.05740566074822171\n",
      "val_loss: 0.1682213893778856\n",
      "Progress: 91.2% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 9124\n",
      "train_loss: 0.06070101974883604\n",
      "val_loss: 0.14936121171407657\n",
      "Progress: 91.2% ... Training loss: 0.068 ... Validation loss: 0.181iteration: 9125\n",
      "train_loss: 0.0682356615858797\n",
      "val_loss: 0.1819026716455194\n",
      "Progress: 91.3% ... Training loss: 0.060 ... Validation loss: 0.144iteration: 9126\n",
      "train_loss: 0.06083158190866899\n",
      "val_loss: 0.14440010476991189\n",
      "Progress: 91.3% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9127\n",
      "train_loss: 0.05659294110423496\n",
      "val_loss: 0.15686473782806687\n",
      "Progress: 91.3% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9128\n",
      "train_loss: 0.05669767290649214\n",
      "val_loss: 0.15696802959821948\n",
      "Progress: 91.3% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 9129\n",
      "train_loss: 0.05902967225543705\n",
      "val_loss: 0.16031137685356242\n",
      "Progress: 91.3% ... Training loss: 0.060 ... Validation loss: 0.147iteration: 9130\n",
      "train_loss: 0.060196504370904125\n",
      "val_loss: 0.14767645327085285\n",
      "Progress: 91.3% ... Training loss: 0.063 ... Validation loss: 0.184iteration: 9131\n",
      "train_loss: 0.06389227648885287\n",
      "val_loss: 0.18450111537599614\n",
      "Progress: 91.3% ... Training loss: 0.058 ... Validation loss: 0.156iteration: 9132\n",
      "train_loss: 0.058904549715026164\n",
      "val_loss: 0.15612142893089617\n",
      "Progress: 91.3% ... Training loss: 0.058 ... Validation loss: 0.184iteration: 9133\n",
      "train_loss: 0.05893029912276296\n",
      "val_loss: 0.1849274500755582\n",
      "Progress: 91.3% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 9134\n",
      "train_loss: 0.0571166037141472\n",
      "val_loss: 0.16779341211220725\n",
      "Progress: 91.3% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 9135\n",
      "train_loss: 0.058978566149178555\n",
      "val_loss: 0.15382627337554947\n",
      "Progress: 91.4% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9136\n",
      "train_loss: 0.05670513159715511\n",
      "val_loss: 0.15510378255711266\n",
      "Progress: 91.4% ... Training loss: 0.059 ... Validation loss: 0.147iteration: 9137\n",
      "train_loss: 0.059222397207566176\n",
      "val_loss: 0.14734738908512435\n",
      "Progress: 91.4% ... Training loss: 0.061 ... Validation loss: 0.168iteration: 9138\n",
      "train_loss: 0.06162436762120127\n",
      "val_loss: 0.1689774309477782\n",
      "Progress: 91.4% ... Training loss: 0.059 ... Validation loss: 0.149iteration: 9139\n",
      "train_loss: 0.05955542917316816\n",
      "val_loss: 0.1490827379421928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 91.4% ... Training loss: 0.061 ... Validation loss: 0.178iteration: 9140\n",
      "train_loss: 0.06188931523049944\n",
      "val_loss: 0.17864511960890464\n",
      "Progress: 91.4% ... Training loss: 0.056 ... Validation loss: 0.158iteration: 9141\n",
      "train_loss: 0.05699051392289415\n",
      "val_loss: 0.1584514029167573\n",
      "Progress: 91.4% ... Training loss: 0.057 ... Validation loss: 0.149iteration: 9142\n",
      "train_loss: 0.05738339917173895\n",
      "val_loss: 0.14905430040194312\n",
      "Progress: 91.4% ... Training loss: 0.059 ... Validation loss: 0.159iteration: 9143\n",
      "train_loss: 0.05931022598997185\n",
      "val_loss: 0.15973075881868645\n",
      "Progress: 91.4% ... Training loss: 0.059 ... Validation loss: 0.144iteration: 9144\n",
      "train_loss: 0.05997274650315533\n",
      "val_loss: 0.1445038675677391\n",
      "Progress: 91.5% ... Training loss: 0.059 ... Validation loss: 0.145iteration: 9145\n",
      "train_loss: 0.05976019818552358\n",
      "val_loss: 0.14574849172142204\n",
      "Progress: 91.5% ... Training loss: 0.061 ... Validation loss: 0.170iteration: 9146\n",
      "train_loss: 0.06163689167600667\n",
      "val_loss: 0.17085735039776748\n",
      "Progress: 91.5% ... Training loss: 0.066 ... Validation loss: 0.144iteration: 9147\n",
      "train_loss: 0.06603545529670651\n",
      "val_loss: 0.14467572015902902\n",
      "Progress: 91.5% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9148\n",
      "train_loss: 0.05718763044710063\n",
      "val_loss: 0.15413248751867312\n",
      "Progress: 91.5% ... Training loss: 0.057 ... Validation loss: 0.150iteration: 9149\n",
      "train_loss: 0.05702765568586791\n",
      "val_loss: 0.150227896308701\n",
      "Progress: 91.5% ... Training loss: 0.056 ... Validation loss: 0.150iteration: 9150\n",
      "train_loss: 0.05677183225379651\n",
      "val_loss: 0.1504296751550288\n",
      "Progress: 91.5% ... Training loss: 0.057 ... Validation loss: 0.153iteration: 9151\n",
      "train_loss: 0.05741401810106712\n",
      "val_loss: 0.1532864664666168\n",
      "Progress: 91.5% ... Training loss: 0.057 ... Validation loss: 0.149iteration: 9152\n",
      "train_loss: 0.05745783477604457\n",
      "val_loss: 0.14989598739002574\n",
      "Progress: 91.5% ... Training loss: 0.056 ... Validation loss: 0.167iteration: 9153\n",
      "train_loss: 0.05698711034421512\n",
      "val_loss: 0.16711982485506577\n",
      "Progress: 91.5% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 9154\n",
      "train_loss: 0.05802022837265074\n",
      "val_loss: 0.15728856105485736\n",
      "Progress: 91.5% ... Training loss: 0.061 ... Validation loss: 0.177iteration: 9155\n",
      "train_loss: 0.06101802050869724\n",
      "val_loss: 0.1770988530258041\n",
      "Progress: 91.6% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 9156\n",
      "train_loss: 0.05868526162610989\n",
      "val_loss: 0.15715057829326012\n",
      "Progress: 91.6% ... Training loss: 0.059 ... Validation loss: 0.182iteration: 9157\n",
      "train_loss: 0.059565597528406095\n",
      "val_loss: 0.1828307767504195\n",
      "Progress: 91.6% ... Training loss: 0.057 ... Validation loss: 0.153iteration: 9158\n",
      "train_loss: 0.05789494174481738\n",
      "val_loss: 0.15343946534488825\n",
      "Progress: 91.6% ... Training loss: 0.061 ... Validation loss: 0.190iteration: 9159\n",
      "train_loss: 0.06146154366062802\n",
      "val_loss: 0.19054806190246798\n",
      "Progress: 91.6% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9160\n",
      "train_loss: 0.05657001927390569\n",
      "val_loss: 0.15641348575298047\n",
      "Progress: 91.6% ... Training loss: 0.058 ... Validation loss: 0.146iteration: 9161\n",
      "train_loss: 0.0583458558043839\n",
      "val_loss: 0.1466149549311031\n",
      "Progress: 91.6% ... Training loss: 0.057 ... Validation loss: 0.148iteration: 9162\n",
      "train_loss: 0.05748767999115291\n",
      "val_loss: 0.14879127930661457\n",
      "Progress: 91.6% ... Training loss: 0.067 ... Validation loss: 0.196iteration: 9163\n",
      "train_loss: 0.06781702281674178\n",
      "val_loss: 0.19679613080082953\n",
      "Progress: 91.6% ... Training loss: 0.071 ... Validation loss: 0.141iteration: 9164\n",
      "train_loss: 0.0715418068221567\n",
      "val_loss: 0.1411508890855611\n",
      "Progress: 91.7% ... Training loss: 0.064 ... Validation loss: 0.177iteration: 9165\n",
      "train_loss: 0.06451821068470488\n",
      "val_loss: 0.1778806123148026\n",
      "Progress: 91.7% ... Training loss: 0.070 ... Validation loss: 0.144iteration: 9166\n",
      "train_loss: 0.07075084834990955\n",
      "val_loss: 0.14473359019069498\n",
      "Progress: 91.7% ... Training loss: 0.063 ... Validation loss: 0.179iteration: 9167\n",
      "train_loss: 0.06337810417917598\n",
      "val_loss: 0.17935978862198684\n",
      "Progress: 91.7% ... Training loss: 0.069 ... Validation loss: 0.144iteration: 9168\n",
      "train_loss: 0.06974523765378202\n",
      "val_loss: 0.14442611754368564\n",
      "Progress: 91.7% ... Training loss: 0.066 ... Validation loss: 0.181iteration: 9169\n",
      "train_loss: 0.06629260212017009\n",
      "val_loss: 0.18124528683664162\n",
      "Progress: 91.7% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9170\n",
      "train_loss: 0.05796182342456231\n",
      "val_loss: 0.15164083527753308\n",
      "Progress: 91.7% ... Training loss: 0.058 ... Validation loss: 0.147iteration: 9171\n",
      "train_loss: 0.05875819783233689\n",
      "val_loss: 0.14718231387675473\n",
      "Progress: 91.7% ... Training loss: 0.057 ... Validation loss: 0.162iteration: 9172\n",
      "train_loss: 0.057492984116910464\n",
      "val_loss: 0.1622223956124594\n",
      "Progress: 91.7% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 9173\n",
      "train_loss: 0.05850355770115607\n",
      "val_loss: 0.15308861580719627\n",
      "Progress: 91.7% ... Training loss: 0.067 ... Validation loss: 0.177iteration: 9174\n",
      "train_loss: 0.06773612812762639\n",
      "val_loss: 0.1775304239996251\n",
      "Progress: 91.8% ... Training loss: 0.060 ... Validation loss: 0.150iteration: 9175\n",
      "train_loss: 0.0608182191398799\n",
      "val_loss: 0.15055272124984528\n",
      "Progress: 91.8% ... Training loss: 0.058 ... Validation loss: 0.150iteration: 9176\n",
      "train_loss: 0.05840054724684534\n",
      "val_loss: 0.15086896633014754\n",
      "Progress: 91.8% ... Training loss: 0.058 ... Validation loss: 0.152iteration: 9177\n",
      "train_loss: 0.058878892043159214\n",
      "val_loss: 0.15297482427460912\n",
      "Progress: 91.8% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9178\n",
      "train_loss: 0.057723495250942974\n",
      "val_loss: 0.15561247203762346\n",
      "Progress: 91.8% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9179\n",
      "train_loss: 0.05769627183061567\n",
      "val_loss: 0.16663268456499772\n",
      "Progress: 91.8% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 9180\n",
      "train_loss: 0.05851274631708784\n",
      "val_loss: 0.16610777663431123\n",
      "Progress: 91.8% ... Training loss: 0.059 ... Validation loss: 0.158iteration: 9181\n",
      "train_loss: 0.05997796353487471\n",
      "val_loss: 0.15804259765131268\n",
      "Progress: 91.8% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9182\n",
      "train_loss: 0.056888867964193894\n",
      "val_loss: 0.15928819307530467\n",
      "Progress: 91.8% ... Training loss: 0.056 ... Validation loss: 0.153iteration: 9183\n",
      "train_loss: 0.056868177585796716\n",
      "val_loss: 0.15378293265660473\n",
      "Progress: 91.8% ... Training loss: 0.059 ... Validation loss: 0.143iteration: 9184\n",
      "train_loss: 0.059654562606778753\n",
      "val_loss: 0.1437155021977425\n",
      "Progress: 91.8% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 9185\n",
      "train_loss: 0.058107791119349876\n",
      "val_loss: 0.16402484354007132\n",
      "Progress: 91.9% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 9186\n",
      "train_loss: 0.06091267719086663\n",
      "val_loss: 0.14913054961106167\n",
      "Progress: 91.9% ... Training loss: 0.069 ... Validation loss: 0.186iteration: 9187\n",
      "train_loss: 0.06902875102982418\n",
      "val_loss: 0.18618880083136835\n",
      "Progress: 91.9% ... Training loss: 0.070 ... Validation loss: 0.145iteration: 9188\n",
      "train_loss: 0.07046016742717379\n",
      "val_loss: 0.14571427352123015\n",
      "Progress: 91.9% ... Training loss: 0.065 ... Validation loss: 0.183iteration: 9189\n",
      "train_loss: 0.06562258759030658\n",
      "val_loss: 0.1839422073000243\n",
      "Progress: 91.9% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 9190\n",
      "train_loss: 0.060134510343144265\n",
      "val_loss: 0.15218036037022237\n",
      "Progress: 91.9% ... Training loss: 0.063 ... Validation loss: 0.183iteration: 9191\n",
      "train_loss: 0.06392268280302914\n",
      "val_loss: 0.18376522068938222\n",
      "Progress: 91.9% ... Training loss: 0.059 ... Validation loss: 0.144iteration: 9192\n",
      "train_loss: 0.05943500729829448\n",
      "val_loss: 0.1445491952054438\n",
      "Progress: 91.9% ... Training loss: 0.064 ... Validation loss: 0.183iteration: 9193\n",
      "train_loss: 0.0644203643115716\n",
      "val_loss: 0.18361031000872863\n",
      "Progress: 91.9% ... Training loss: 0.059 ... Validation loss: 0.145iteration: 9194\n",
      "train_loss: 0.05912624189637217\n",
      "val_loss: 0.1457735441513281\n",
      "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9195\n",
      "train_loss: 0.05723178151537907\n",
      "val_loss: 0.16579434980757746\n",
      "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9196\n",
      "train_loss: 0.057920227049495926\n",
      "val_loss: 0.1667854127143172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 92.0% ... Training loss: 0.060 ... Validation loss: 0.159iteration: 9197\n",
      "train_loss: 0.06026017839492277\n",
      "val_loss: 0.1594537247011176\n",
      "Progress: 92.0% ... Training loss: 0.065 ... Validation loss: 0.189iteration: 9198\n",
      "train_loss: 0.06593693099798163\n",
      "val_loss: 0.18979735261339578\n",
      "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9199\n",
      "train_loss: 0.057466927536809945\n",
      "val_loss: 0.15571331077348935\n",
      "Progress: 92.0% ... Training loss: 0.060 ... Validation loss: 0.176iteration: 9200\n",
      "train_loss: 0.06085435745087469\n",
      "val_loss: 0.1765936028379673\n",
      "Progress: 92.0% ... Training loss: 0.061 ... Validation loss: 0.150iteration: 9201\n",
      "train_loss: 0.06182900875937658\n",
      "val_loss: 0.15019632771410865\n",
      "Progress: 92.0% ... Training loss: 0.060 ... Validation loss: 0.175iteration: 9202\n",
      "train_loss: 0.060580440077007816\n",
      "val_loss: 0.17518601929295416\n",
      "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9203\n",
      "train_loss: 0.05793436315712626\n",
      "val_loss: 0.15555673751865917\n",
      "Progress: 92.0% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 9204\n",
      "train_loss: 0.05953749718061867\n",
      "val_loss: 0.16820067089396845\n",
      "Progress: 92.0% ... Training loss: 0.062 ... Validation loss: 0.145iteration: 9205\n",
      "train_loss: 0.062008918643174135\n",
      "val_loss: 0.1456915130970225\n",
      "Progress: 92.1% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 9206\n",
      "train_loss: 0.06272906298939851\n",
      "val_loss: 0.1724761549383634\n",
      "Progress: 92.1% ... Training loss: 0.056 ... Validation loss: 0.149iteration: 9207\n",
      "train_loss: 0.05685702245185756\n",
      "val_loss: 0.1499758698471156\n",
      "Progress: 92.1% ... Training loss: 0.057 ... Validation loss: 0.162iteration: 9208\n",
      "train_loss: 0.05715458858084908\n",
      "val_loss: 0.162861267276913\n",
      "Progress: 92.1% ... Training loss: 0.062 ... Validation loss: 0.147iteration: 9209\n",
      "train_loss: 0.062367436664403024\n",
      "val_loss: 0.14753998875437568\n",
      "Progress: 92.1% ... Training loss: 0.059 ... Validation loss: 0.176iteration: 9210\n",
      "train_loss: 0.05935934603866766\n",
      "val_loss: 0.17610026416841448\n",
      "Progress: 92.1% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9211\n",
      "train_loss: 0.05736082305706797\n",
      "val_loss: 0.15431593524370715\n",
      "Progress: 92.1% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9212\n",
      "train_loss: 0.05708846597325775\n",
      "val_loss: 0.1515948578305059\n",
      "Progress: 92.1% ... Training loss: 0.058 ... Validation loss: 0.168iteration: 9213\n",
      "train_loss: 0.05831309054043147\n",
      "val_loss: 0.16866906006445223\n",
      "Progress: 92.1% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9214\n",
      "train_loss: 0.057369636304478035\n",
      "val_loss: 0.15251072314493946\n",
      "Progress: 92.2% ... Training loss: 0.056 ... Validation loss: 0.165iteration: 9215\n",
      "train_loss: 0.05685079874578939\n",
      "val_loss: 0.16507676803221416\n",
      "Progress: 92.2% ... Training loss: 0.057 ... Validation loss: 0.153iteration: 9216\n",
      "train_loss: 0.0572535918219344\n",
      "val_loss: 0.1535359574741739\n",
      "Progress: 92.2% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9217\n",
      "train_loss: 0.05684090275119528\n",
      "val_loss: 0.1595069978913785\n",
      "Progress: 92.2% ... Training loss: 0.058 ... Validation loss: 0.176iteration: 9218\n",
      "train_loss: 0.05852412816940002\n",
      "val_loss: 0.17626060308988817\n",
      "Progress: 92.2% ... Training loss: 0.058 ... Validation loss: 0.160iteration: 9219\n",
      "train_loss: 0.058135401791885415\n",
      "val_loss: 0.16041299985413035\n",
      "Progress: 92.2% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9220\n",
      "train_loss: 0.05780878443639062\n",
      "val_loss: 0.1660773434088666\n",
      "Progress: 92.2% ... Training loss: 0.061 ... Validation loss: 0.146iteration: 9221\n",
      "train_loss: 0.06161116051051044\n",
      "val_loss: 0.14675831196193623\n",
      "Progress: 92.2% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9222\n",
      "train_loss: 0.05719575277835081\n",
      "val_loss: 0.16564154854785787\n",
      "Progress: 92.2% ... Training loss: 0.056 ... Validation loss: 0.161iteration: 9223\n",
      "train_loss: 0.056566511265183604\n",
      "val_loss: 0.16192629627594365\n",
      "Progress: 92.2% ... Training loss: 0.061 ... Validation loss: 0.175iteration: 9224\n",
      "train_loss: 0.061243063345423704\n",
      "val_loss: 0.17511227286303438\n",
      "Progress: 92.2% ... Training loss: 0.056 ... Validation loss: 0.157iteration: 9225\n",
      "train_loss: 0.056797515831305684\n",
      "val_loss: 0.15734999190458127\n",
      "Progress: 92.3% ... Training loss: 0.056 ... Validation loss: 0.151iteration: 9226\n",
      "train_loss: 0.05632053721155521\n",
      "val_loss: 0.15187855407183387\n",
      "Progress: 92.3% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 9227\n",
      "train_loss: 0.05972263723891858\n",
      "val_loss: 0.15789395665710584\n",
      "Progress: 92.3% ... Training loss: 0.060 ... Validation loss: 0.158iteration: 9228\n",
      "train_loss: 0.06022280865237802\n",
      "val_loss: 0.15887268734716048\n",
      "Progress: 92.3% ... Training loss: 0.061 ... Validation loss: 0.183iteration: 9229\n",
      "train_loss: 0.06168527036927368\n",
      "val_loss: 0.1831883756489045\n",
      "Progress: 92.3% ... Training loss: 0.059 ... Validation loss: 0.146iteration: 9230\n",
      "train_loss: 0.05987876067252344\n",
      "val_loss: 0.14666816610530842\n",
      "Progress: 92.3% ... Training loss: 0.060 ... Validation loss: 0.170iteration: 9231\n",
      "train_loss: 0.0604178467972281\n",
      "val_loss: 0.17031101214954966\n",
      "Progress: 92.3% ... Training loss: 0.056 ... Validation loss: 0.164iteration: 9232\n",
      "train_loss: 0.05673190156678287\n",
      "val_loss: 0.16472817544872176\n",
      "Progress: 92.3% ... Training loss: 0.062 ... Validation loss: 0.153iteration: 9233\n",
      "train_loss: 0.06284501527569174\n",
      "val_loss: 0.15395529308976638\n",
      "Progress: 92.3% ... Training loss: 0.065 ... Validation loss: 0.197iteration: 9234\n",
      "train_loss: 0.06569642379936906\n",
      "val_loss: 0.19745712117540773\n",
      "Progress: 92.3% ... Training loss: 0.073 ... Validation loss: 0.149iteration: 9235\n",
      "train_loss: 0.0739579416534776\n",
      "val_loss: 0.1494197686552236\n",
      "Progress: 92.4% ... Training loss: 0.067 ... Validation loss: 0.206iteration: 9236\n",
      "train_loss: 0.06759145854970361\n",
      "val_loss: 0.20677332717938776\n",
      "Progress: 92.4% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9237\n",
      "train_loss: 0.05702445864591287\n",
      "val_loss: 0.15174324654914498\n",
      "Progress: 92.4% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9238\n",
      "train_loss: 0.057593526784798836\n",
      "val_loss: 0.16600072961106893\n",
      "Progress: 92.4% ... Training loss: 0.057 ... Validation loss: 0.149iteration: 9239\n",
      "train_loss: 0.0576500541838894\n",
      "val_loss: 0.14903863445812135\n",
      "Progress: 92.4% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 9240\n",
      "train_loss: 0.057552676560961226\n",
      "val_loss: 0.15967006464966238\n",
      "Progress: 92.4% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9241\n",
      "train_loss: 0.05646316353988539\n",
      "val_loss: 0.1593656045487674\n",
      "Progress: 92.4% ... Training loss: 0.064 ... Validation loss: 0.141iteration: 9242\n",
      "train_loss: 0.0648081094528041\n",
      "val_loss: 0.14146367305978697\n",
      "Progress: 92.4% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9243\n",
      "train_loss: 0.05753416485029837\n",
      "val_loss: 0.16693198838537796\n",
      "Progress: 92.4% ... Training loss: 0.058 ... Validation loss: 0.147iteration: 9244\n",
      "train_loss: 0.058664105563084885\n",
      "val_loss: 0.1479202792145429\n",
      "Progress: 92.5% ... Training loss: 0.064 ... Validation loss: 0.173iteration: 9245\n",
      "train_loss: 0.06483521913884184\n",
      "val_loss: 0.17344659827128234\n",
      "Progress: 92.5% ... Training loss: 0.092 ... Validation loss: 0.150iteration: 9246\n",
      "train_loss: 0.09290976599948643\n",
      "val_loss: 0.15092732542977813\n",
      "Progress: 92.5% ... Training loss: 0.092 ... Validation loss: 0.212iteration: 9247\n",
      "train_loss: 0.09208639121344947\n",
      "val_loss: 0.2123338126970261\n",
      "Progress: 92.5% ... Training loss: 0.115 ... Validation loss: 0.160iteration: 9248\n",
      "train_loss: 0.1153228171178029\n",
      "val_loss: 0.1608798191088607\n",
      "Progress: 92.5% ... Training loss: 0.087 ... Validation loss: 0.204iteration: 9249\n",
      "train_loss: 0.08776780949212497\n",
      "val_loss: 0.20415153258246904\n",
      "Progress: 92.5% ... Training loss: 0.082 ... Validation loss: 0.145iteration: 9250\n",
      "train_loss: 0.08209628083203603\n",
      "val_loss: 0.14593344971026684\n",
      "Progress: 92.5% ... Training loss: 0.071 ... Validation loss: 0.208iteration: 9251\n",
      "train_loss: 0.07149002750009822\n",
      "val_loss: 0.20884258684621518\n",
      "Progress: 92.5% ... Training loss: 0.086 ... Validation loss: 0.152iteration: 9252\n",
      "train_loss: 0.0869363678604229\n",
      "val_loss: 0.15267539015479226\n",
      "Progress: 92.5% ... Training loss: 0.068 ... Validation loss: 0.208iteration: 9253\n",
      "train_loss: 0.06825518058126163\n",
      "val_loss: 0.2086065034885944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 92.5% ... Training loss: 0.067 ... Validation loss: 0.148iteration: 9254\n",
      "train_loss: 0.06738501306651205\n",
      "val_loss: 0.14850332777546996\n",
      "Progress: 92.5% ... Training loss: 0.068 ... Validation loss: 0.207iteration: 9255\n",
      "train_loss: 0.06860280915473402\n",
      "val_loss: 0.20777809088516522\n",
      "Progress: 92.6% ... Training loss: 0.059 ... Validation loss: 0.152iteration: 9256\n",
      "train_loss: 0.0597637709399587\n",
      "val_loss: 0.15226591234707348\n",
      "Progress: 92.6% ... Training loss: 0.059 ... Validation loss: 0.193iteration: 9257\n",
      "train_loss: 0.05968798751880438\n",
      "val_loss: 0.1933927401455997\n",
      "Progress: 92.6% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9258\n",
      "train_loss: 0.05797266804601286\n",
      "val_loss: 0.16610351652534924\n",
      "Progress: 92.6% ... Training loss: 0.056 ... Validation loss: 0.169iteration: 9259\n",
      "train_loss: 0.05676485127652623\n",
      "val_loss: 0.16902454341581716\n",
      "Progress: 92.6% ... Training loss: 0.061 ... Validation loss: 0.188iteration: 9260\n",
      "train_loss: 0.061704571987857254\n",
      "val_loss: 0.18805139254308495\n",
      "Progress: 92.6% ... Training loss: 0.082 ... Validation loss: 0.148iteration: 9261\n",
      "train_loss: 0.08290605577382953\n",
      "val_loss: 0.14840544228121558\n",
      "Progress: 92.6% ... Training loss: 0.075 ... Validation loss: 0.241iteration: 9262\n",
      "train_loss: 0.07586756132321437\n",
      "val_loss: 0.2416815518466053\n",
      "Progress: 92.6% ... Training loss: 0.063 ... Validation loss: 0.149iteration: 9263\n",
      "train_loss: 0.06312226094914307\n",
      "val_loss: 0.14994116073361582\n",
      "Progress: 92.6% ... Training loss: 0.073 ... Validation loss: 0.206iteration: 9264\n",
      "train_loss: 0.07306786506651285\n",
      "val_loss: 0.20620680723170964\n",
      "Progress: 92.7% ... Training loss: 0.080 ... Validation loss: 0.151iteration: 9265\n",
      "train_loss: 0.08041692572860211\n",
      "val_loss: 0.15134773600308532\n",
      "Progress: 92.7% ... Training loss: 0.059 ... Validation loss: 0.184iteration: 9266\n",
      "train_loss: 0.059359207769302286\n",
      "val_loss: 0.1847247861303147\n",
      "Progress: 92.7% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 9267\n",
      "train_loss: 0.05769353984142546\n",
      "val_loss: 0.1630283468252068\n",
      "Progress: 92.7% ... Training loss: 0.066 ... Validation loss: 0.193iteration: 9268\n",
      "train_loss: 0.06648747449650003\n",
      "val_loss: 0.19313308087136602\n",
      "Progress: 92.7% ... Training loss: 0.075 ... Validation loss: 0.147iteration: 9269\n",
      "train_loss: 0.07501208601574268\n",
      "val_loss: 0.1476459996838514\n",
      "Progress: 92.7% ... Training loss: 0.063 ... Validation loss: 0.185iteration: 9270\n",
      "train_loss: 0.06327669499620645\n",
      "val_loss: 0.1857506446497206\n",
      "Progress: 92.7% ... Training loss: 0.071 ... Validation loss: 0.155iteration: 9271\n",
      "train_loss: 0.07123448734825578\n",
      "val_loss: 0.15514965692593116\n",
      "Progress: 92.7% ... Training loss: 0.059 ... Validation loss: 0.185iteration: 9272\n",
      "train_loss: 0.05991396685234372\n",
      "val_loss: 0.1851366864489853\n",
      "Progress: 92.7% ... Training loss: 0.058 ... Validation loss: 0.156iteration: 9273\n",
      "train_loss: 0.058942916700021074\n",
      "val_loss: 0.156782310387752\n",
      "Progress: 92.7% ... Training loss: 0.058 ... Validation loss: 0.175iteration: 9274\n",
      "train_loss: 0.0580333716459392\n",
      "val_loss: 0.17596557731326873\n",
      "Progress: 92.8% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 9275\n",
      "train_loss: 0.05819497563195802\n",
      "val_loss: 0.1512930431600736\n",
      "Progress: 92.8% ... Training loss: 0.057 ... Validation loss: 0.174iteration: 9276\n",
      "train_loss: 0.05799836828536957\n",
      "val_loss: 0.17481311157136373\n",
      "Progress: 92.8% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 9277\n",
      "train_loss: 0.0588246198148163\n",
      "val_loss: 0.16949173965976852\n",
      "Progress: 92.8% ... Training loss: 0.057 ... Validation loss: 0.162iteration: 9278\n",
      "train_loss: 0.057778997444046756\n",
      "val_loss: 0.16242707546511934\n",
      "Progress: 92.8% ... Training loss: 0.057 ... Validation loss: 0.174iteration: 9279\n",
      "train_loss: 0.05773938905569903\n",
      "val_loss: 0.17499552855050882\n",
      "Progress: 92.8% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 9280\n",
      "train_loss: 0.058115725389589194\n",
      "val_loss: 0.1594748348390089\n",
      "Progress: 92.8% ... Training loss: 0.061 ... Validation loss: 0.145iteration: 9281\n",
      "train_loss: 0.06129811990844844\n",
      "val_loss: 0.14533446911345618\n",
      "Progress: 92.8% ... Training loss: 0.063 ... Validation loss: 0.180iteration: 9282\n",
      "train_loss: 0.06354417367562104\n",
      "val_loss: 0.18045123481679834\n",
      "Progress: 92.8% ... Training loss: 0.056 ... Validation loss: 0.158iteration: 9283\n",
      "train_loss: 0.056741782875329896\n",
      "val_loss: 0.15805879900167752\n",
      "Progress: 92.8% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 9284\n",
      "train_loss: 0.05944874692297916\n",
      "val_loss: 0.17041961300874947\n",
      "Progress: 92.8% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 9285\n",
      "train_loss: 0.05978877636234078\n",
      "val_loss: 0.15616948851571302\n",
      "Progress: 92.9% ... Training loss: 0.059 ... Validation loss: 0.182iteration: 9286\n",
      "train_loss: 0.05915951977477349\n",
      "val_loss: 0.18228294840824324\n",
      "Progress: 92.9% ... Training loss: 0.068 ... Validation loss: 0.151iteration: 9287\n",
      "train_loss: 0.0687792357533663\n",
      "val_loss: 0.1518863820181759\n",
      "Progress: 92.9% ... Training loss: 0.075 ... Validation loss: 0.209iteration: 9288\n",
      "train_loss: 0.07579432703471967\n",
      "val_loss: 0.20900269254162482\n",
      "Progress: 92.9% ... Training loss: 0.086 ... Validation loss: 0.150iteration: 9289\n",
      "train_loss: 0.0861660668732356\n",
      "val_loss: 0.1509204697969344\n",
      "Progress: 92.9% ... Training loss: 0.079 ... Validation loss: 0.203iteration: 9290\n",
      "train_loss: 0.0791196225832545\n",
      "val_loss: 0.20377921154480352\n",
      "Progress: 92.9% ... Training loss: 0.075 ... Validation loss: 0.146iteration: 9291\n",
      "train_loss: 0.07564990193347144\n",
      "val_loss: 0.14677202460065603\n",
      "Progress: 92.9% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 9292\n",
      "train_loss: 0.06451094068971706\n",
      "val_loss: 0.1789467594859811\n",
      "Progress: 92.9% ... Training loss: 0.076 ... Validation loss: 0.155iteration: 9293\n",
      "train_loss: 0.07630537564767222\n",
      "val_loss: 0.15502161041564977\n",
      "Progress: 92.9% ... Training loss: 0.068 ... Validation loss: 0.183iteration: 9294\n",
      "train_loss: 0.06821823283112506\n",
      "val_loss: 0.18329447147995137\n",
      "Progress: 93.0% ... Training loss: 0.068 ... Validation loss: 0.146iteration: 9295\n",
      "train_loss: 0.06861752378281837\n",
      "val_loss: 0.14626621452490346\n",
      "Progress: 93.0% ... Training loss: 0.063 ... Validation loss: 0.177iteration: 9296\n",
      "train_loss: 0.06337517468466942\n",
      "val_loss: 0.17787421803566414\n",
      "Progress: 93.0% ... Training loss: 0.063 ... Validation loss: 0.147iteration: 9297\n",
      "train_loss: 0.06367680793471359\n",
      "val_loss: 0.14728902130645638\n",
      "Progress: 93.0% ... Training loss: 0.069 ... Validation loss: 0.197iteration: 9298\n",
      "train_loss: 0.06999968459923936\n",
      "val_loss: 0.1976352413856604\n",
      "Progress: 93.0% ... Training loss: 0.064 ... Validation loss: 0.145iteration: 9299\n",
      "train_loss: 0.06475030472098742\n",
      "val_loss: 0.1455663346391942\n",
      "Progress: 93.0% ... Training loss: 0.056 ... Validation loss: 0.167iteration: 9300\n",
      "train_loss: 0.05665081855393034\n",
      "val_loss: 0.16779306203074454\n",
      "Progress: 93.0% ... Training loss: 0.058 ... Validation loss: 0.162iteration: 9301\n",
      "train_loss: 0.058591562839221595\n",
      "val_loss: 0.16200357834270435\n",
      "Progress: 93.0% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9302\n",
      "train_loss: 0.057070235347165855\n",
      "val_loss: 0.15187110395935957\n",
      "Progress: 93.0% ... Training loss: 0.056 ... Validation loss: 0.161iteration: 9303\n",
      "train_loss: 0.05697389396914033\n",
      "val_loss: 0.16197857292165602\n",
      "Progress: 93.0% ... Training loss: 0.061 ... Validation loss: 0.175iteration: 9304\n",
      "train_loss: 0.061911801838325696\n",
      "val_loss: 0.17540625667987964\n",
      "Progress: 93.0% ... Training loss: 0.059 ... Validation loss: 0.149iteration: 9305\n",
      "train_loss: 0.05931518353033921\n",
      "val_loss: 0.14926880221691805\n",
      "Progress: 93.1% ... Training loss: 0.058 ... Validation loss: 0.172iteration: 9306\n",
      "train_loss: 0.058522659865816416\n",
      "val_loss: 0.17240516990507546\n",
      "Progress: 93.1% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9307\n",
      "train_loss: 0.057116541842892894\n",
      "val_loss: 0.15563287962670838\n",
      "Progress: 93.1% ... Training loss: 0.059 ... Validation loss: 0.166iteration: 9308\n",
      "train_loss: 0.059737417220029836\n",
      "val_loss: 0.1661145273306036\n",
      "Progress: 93.1% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 9309\n",
      "train_loss: 0.06055783631776642\n",
      "val_loss: 0.15167746624319892\n",
      "Progress: 93.1% ... Training loss: 0.065 ... Validation loss: 0.197iteration: 9310\n",
      "train_loss: 0.0658480658290682\n",
      "val_loss: 0.19776795549279508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 93.1% ... Training loss: 0.060 ... Validation loss: 0.158iteration: 9311\n",
      "train_loss: 0.0603706228750298\n",
      "val_loss: 0.15866686727033585\n",
      "Progress: 93.1% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 9312\n",
      "train_loss: 0.058234851009843026\n",
      "val_loss: 0.17175109930894505\n",
      "Progress: 93.1% ... Training loss: 0.064 ... Validation loss: 0.142iteration: 9313\n",
      "train_loss: 0.06402777027717353\n",
      "val_loss: 0.14239682745930063\n",
      "Progress: 93.1% ... Training loss: 0.063 ... Validation loss: 0.174iteration: 9314\n",
      "train_loss: 0.06378485304130099\n",
      "val_loss: 0.174097916692553\n",
      "Progress: 93.2% ... Training loss: 0.062 ... Validation loss: 0.145iteration: 9315\n",
      "train_loss: 0.062020773177693916\n",
      "val_loss: 0.14595781603355437\n",
      "Progress: 93.2% ... Training loss: 0.060 ... Validation loss: 0.166iteration: 9316\n",
      "train_loss: 0.06005735436283364\n",
      "val_loss: 0.16684856837944284\n",
      "Progress: 93.2% ... Training loss: 0.059 ... Validation loss: 0.142iteration: 9317\n",
      "train_loss: 0.05934489814279313\n",
      "val_loss: 0.14290549886179704\n",
      "Progress: 93.2% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9318\n",
      "train_loss: 0.0575734107799608\n",
      "val_loss: 0.15590706733060172\n",
      "Progress: 93.2% ... Training loss: 0.058 ... Validation loss: 0.152iteration: 9319\n",
      "train_loss: 0.05888457473722769\n",
      "val_loss: 0.15270000089924837\n",
      "Progress: 93.2% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 9320\n",
      "train_loss: 0.061252394090848164\n",
      "val_loss: 0.17350490673478614\n",
      "Progress: 93.2% ... Training loss: 0.060 ... Validation loss: 0.144iteration: 9321\n",
      "train_loss: 0.060728415126988906\n",
      "val_loss: 0.14458117591377667\n",
      "Progress: 93.2% ... Training loss: 0.074 ... Validation loss: 0.197iteration: 9322\n",
      "train_loss: 0.07468324068449432\n",
      "val_loss: 0.19786571283646587\n",
      "Progress: 93.2% ... Training loss: 0.071 ... Validation loss: 0.144iteration: 9323\n",
      "train_loss: 0.07111901081547523\n",
      "val_loss: 0.14441309488555995\n",
      "Progress: 93.2% ... Training loss: 0.066 ... Validation loss: 0.183iteration: 9324\n",
      "train_loss: 0.06648106504593641\n",
      "val_loss: 0.18338582465770797\n",
      "Progress: 93.2% ... Training loss: 0.067 ... Validation loss: 0.143iteration: 9325\n",
      "train_loss: 0.06764658200424974\n",
      "val_loss: 0.14318022436352426\n",
      "Progress: 93.3% ... Training loss: 0.061 ... Validation loss: 0.184iteration: 9326\n",
      "train_loss: 0.06106467666171499\n",
      "val_loss: 0.18442924969791988\n",
      "Progress: 93.3% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9327\n",
      "train_loss: 0.056879018421154844\n",
      "val_loss: 0.15428437842332496\n",
      "Progress: 93.3% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9328\n",
      "train_loss: 0.05780239899820848\n",
      "val_loss: 0.151506135290163\n",
      "Progress: 93.3% ... Training loss: 0.062 ... Validation loss: 0.170iteration: 9329\n",
      "train_loss: 0.0625030127904725\n",
      "val_loss: 0.1709814284821511\n",
      "Progress: 93.3% ... Training loss: 0.060 ... Validation loss: 0.144iteration: 9330\n",
      "train_loss: 0.060184857804405946\n",
      "val_loss: 0.14427943154882578\n",
      "Progress: 93.3% ... Training loss: 0.060 ... Validation loss: 0.164iteration: 9331\n",
      "train_loss: 0.06060733558827699\n",
      "val_loss: 0.16439795368444626\n",
      "Progress: 93.3% ... Training loss: 0.063 ... Validation loss: 0.142iteration: 9332\n",
      "train_loss: 0.06345612151598289\n",
      "val_loss: 0.1422326642968016\n",
      "Progress: 93.3% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9333\n",
      "train_loss: 0.05684469573946453\n",
      "val_loss: 0.15693502038361862\n",
      "Progress: 93.3% ... Training loss: 0.059 ... Validation loss: 0.148iteration: 9334\n",
      "train_loss: 0.05924357973385503\n",
      "val_loss: 0.14881213288563863\n",
      "Progress: 93.3% ... Training loss: 0.056 ... Validation loss: 0.150iteration: 9335\n",
      "train_loss: 0.056893360869252126\n",
      "val_loss: 0.15026648051480093\n",
      "Progress: 93.4% ... Training loss: 0.061 ... Validation loss: 0.160iteration: 9336\n",
      "train_loss: 0.06186271066968212\n",
      "val_loss: 0.16095446496948845\n",
      "Progress: 93.4% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9337\n",
      "train_loss: 0.05682084161266581\n",
      "val_loss: 0.1522362659666724\n",
      "Progress: 93.4% ... Training loss: 0.061 ... Validation loss: 0.154iteration: 9338\n",
      "train_loss: 0.06192903372310083\n",
      "val_loss: 0.15459183480677166\n",
      "Progress: 93.4% ... Training loss: 0.059 ... Validation loss: 0.146iteration: 9339\n",
      "train_loss: 0.059879111644028024\n",
      "val_loss: 0.14621188603365512\n",
      "Progress: 93.4% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9340\n",
      "train_loss: 0.0570925181317923\n",
      "val_loss: 0.15245981986935458\n",
      "Progress: 93.4% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9341\n",
      "train_loss: 0.05709350872033053\n",
      "val_loss: 0.15242280060465382\n",
      "Progress: 93.4% ... Training loss: 0.062 ... Validation loss: 0.150iteration: 9342\n",
      "train_loss: 0.06284945196735287\n",
      "val_loss: 0.1508480813783499\n",
      "Progress: 93.4% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9343\n",
      "train_loss: 0.057802925732338035\n",
      "val_loss: 0.1526211988131921\n",
      "Progress: 93.4% ... Training loss: 0.064 ... Validation loss: 0.164iteration: 9344\n",
      "train_loss: 0.06493028662227934\n",
      "val_loss: 0.16446478848068918\n",
      "Progress: 93.5% ... Training loss: 0.060 ... Validation loss: 0.145iteration: 9345\n",
      "train_loss: 0.06075707117396979\n",
      "val_loss: 0.14502806760874226\n",
      "Progress: 93.5% ... Training loss: 0.068 ... Validation loss: 0.188iteration: 9346\n",
      "train_loss: 0.06805958748904639\n",
      "val_loss: 0.18879383697488733\n",
      "Progress: 93.5% ... Training loss: 0.059 ... Validation loss: 0.150iteration: 9347\n",
      "train_loss: 0.05995881964731032\n",
      "val_loss: 0.15038707988454675\n",
      "Progress: 93.5% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 9348\n",
      "train_loss: 0.06005589418650174\n",
      "val_loss: 0.1614391776762652\n",
      "Progress: 93.5% ... Training loss: 0.060 ... Validation loss: 0.143iteration: 9349\n",
      "train_loss: 0.06016825697964185\n",
      "val_loss: 0.14390947015630579\n",
      "Progress: 93.5% ... Training loss: 0.064 ... Validation loss: 0.174iteration: 9350\n",
      "train_loss: 0.06418828117216255\n",
      "val_loss: 0.174281192234764\n",
      "Progress: 93.5% ... Training loss: 0.061 ... Validation loss: 0.144iteration: 9351\n",
      "train_loss: 0.061218032123615686\n",
      "val_loss: 0.14441588307149456\n",
      "Progress: 93.5% ... Training loss: 0.062 ... Validation loss: 0.167iteration: 9352\n",
      "train_loss: 0.06298646252350698\n",
      "val_loss: 0.16777008008951164\n",
      "Progress: 93.5% ... Training loss: 0.064 ... Validation loss: 0.145iteration: 9353\n",
      "train_loss: 0.06492152872265189\n",
      "val_loss: 0.14594929907229093\n",
      "Progress: 93.5% ... Training loss: 0.065 ... Validation loss: 0.176iteration: 9354\n",
      "train_loss: 0.06511744657968721\n",
      "val_loss: 0.1769937192851729\n",
      "Progress: 93.5% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 9355\n",
      "train_loss: 0.061685237032945456\n",
      "val_loss: 0.14929903555217766\n",
      "Progress: 93.6% ... Training loss: 0.066 ... Validation loss: 0.183iteration: 9356\n",
      "train_loss: 0.06626930663458956\n",
      "val_loss: 0.18314015841334788\n",
      "Progress: 93.6% ... Training loss: 0.095 ... Validation loss: 0.145iteration: 9357\n",
      "train_loss: 0.09529587609147841\n",
      "val_loss: 0.14541312782106208\n",
      "Progress: 93.6% ... Training loss: 0.111 ... Validation loss: 0.231iteration: 9358\n",
      "train_loss: 0.1116928246932091\n",
      "val_loss: 0.23149214671206486\n",
      "Progress: 93.6% ... Training loss: 0.109 ... Validation loss: 0.149iteration: 9359\n",
      "train_loss: 0.10949622539161898\n",
      "val_loss: 0.14953396834341942\n",
      "Progress: 93.6% ... Training loss: 0.135 ... Validation loss: 0.277iteration: 9360\n",
      "train_loss: 0.13515314024454408\n",
      "val_loss: 0.27775784307980134\n",
      "Progress: 93.6% ... Training loss: 0.159 ... Validation loss: 0.178iteration: 9361\n",
      "train_loss: 0.15948379661954432\n",
      "val_loss: 0.17804645608323963\n",
      "Progress: 93.6% ... Training loss: 0.087 ... Validation loss: 0.224iteration: 9362\n",
      "train_loss: 0.0870070900207799\n",
      "val_loss: 0.22437270173066357\n",
      "Progress: 93.6% ... Training loss: 0.075 ... Validation loss: 0.145iteration: 9363\n",
      "train_loss: 0.0756821090706802\n",
      "val_loss: 0.14590783618120698\n",
      "Progress: 93.6% ... Training loss: 0.098 ... Validation loss: 0.214iteration: 9364\n",
      "train_loss: 0.0983366903796904\n",
      "val_loss: 0.2140473464983987\n",
      "Progress: 93.7% ... Training loss: 0.067 ... Validation loss: 0.138iteration: 9365\n",
      "train_loss: 0.06735760658166746\n",
      "val_loss: 0.13840657582598673\n",
      "Progress: 93.7% ... Training loss: 0.059 ... Validation loss: 0.148iteration: 9366\n",
      "train_loss: 0.05951379083731442\n",
      "val_loss: 0.14880948315748926\n",
      "Progress: 93.7% ... Training loss: 0.057 ... Validation loss: 0.158iteration: 9367\n",
      "train_loss: 0.0576898371818841\n",
      "val_loss: 0.15851608496484662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 93.7% ... Training loss: 0.057 ... Validation loss: 0.161iteration: 9368\n",
      "train_loss: 0.05765720362161037\n",
      "val_loss: 0.16143386040701707\n",
      "Progress: 93.7% ... Training loss: 0.066 ... Validation loss: 0.176iteration: 9369\n",
      "train_loss: 0.06677217349304687\n",
      "val_loss: 0.17633466687438276\n",
      "Progress: 93.7% ... Training loss: 0.058 ... Validation loss: 0.144iteration: 9370\n",
      "train_loss: 0.058371537102959815\n",
      "val_loss: 0.14450595005071512\n",
      "Progress: 93.7% ... Training loss: 0.059 ... Validation loss: 0.176iteration: 9371\n",
      "train_loss: 0.0598496289568222\n",
      "val_loss: 0.176128389652679\n",
      "Progress: 93.7% ... Training loss: 0.076 ... Validation loss: 0.147iteration: 9372\n",
      "train_loss: 0.0764149245034408\n",
      "val_loss: 0.14766336889604703\n",
      "Progress: 93.7% ... Training loss: 0.062 ... Validation loss: 0.186iteration: 9373\n",
      "train_loss: 0.062819148486748\n",
      "val_loss: 0.18669318346525923\n",
      "Progress: 93.7% ... Training loss: 0.062 ... Validation loss: 0.150iteration: 9374\n",
      "train_loss: 0.06275889606631856\n",
      "val_loss: 0.15097515386172483\n",
      "Progress: 93.8% ... Training loss: 0.070 ... Validation loss: 0.198iteration: 9375\n",
      "train_loss: 0.0707485506999751\n",
      "val_loss: 0.19872886045465363\n",
      "Progress: 93.8% ... Training loss: 0.078 ... Validation loss: 0.138iteration: 9376\n",
      "train_loss: 0.07885293112314523\n",
      "val_loss: 0.13879938811180573\n",
      "Progress: 93.8% ... Training loss: 0.077 ... Validation loss: 0.219iteration: 9377\n",
      "train_loss: 0.07746798711709221\n",
      "val_loss: 0.21989309711655752\n",
      "Progress: 93.8% ... Training loss: 0.085 ... Validation loss: 0.145iteration: 9378\n",
      "train_loss: 0.08552921598576395\n",
      "val_loss: 0.14500128525873804\n",
      "Progress: 93.8% ... Training loss: 0.071 ... Validation loss: 0.200iteration: 9379\n",
      "train_loss: 0.0711702399247638\n",
      "val_loss: 0.20075045654808\n",
      "Progress: 93.8% ... Training loss: 0.082 ... Validation loss: 0.145iteration: 9380\n",
      "train_loss: 0.08260096848808833\n",
      "val_loss: 0.1457229889643043\n",
      "Progress: 93.8% ... Training loss: 0.113 ... Validation loss: 0.240iteration: 9381\n",
      "train_loss: 0.11323224716905848\n",
      "val_loss: 0.24040288689767675\n",
      "Progress: 93.8% ... Training loss: 0.111 ... Validation loss: 0.151iteration: 9382\n",
      "train_loss: 0.11165147503985468\n",
      "val_loss: 0.1517690106443211\n",
      "Progress: 93.8% ... Training loss: 0.120 ... Validation loss: 0.261iteration: 9383\n",
      "train_loss: 0.12074572325792976\n",
      "val_loss: 0.2615100991494204\n",
      "Progress: 93.8% ... Training loss: 0.121 ... Validation loss: 0.161iteration: 9384\n",
      "train_loss: 0.1217302609191598\n",
      "val_loss: 0.16140451359019217\n",
      "Progress: 93.8% ... Training loss: 0.132 ... Validation loss: 0.246iteration: 9385\n",
      "train_loss: 0.13257083109455564\n",
      "val_loss: 0.24649002806728454\n",
      "Progress: 93.9% ... Training loss: 0.117 ... Validation loss: 0.159iteration: 9386\n",
      "train_loss: 0.11739273293818293\n",
      "val_loss: 0.15949459598620228\n",
      "Progress: 93.9% ... Training loss: 0.157 ... Validation loss: 0.295iteration: 9387\n",
      "train_loss: 0.15742095265984266\n",
      "val_loss: 0.29522291767722214\n",
      "Progress: 93.9% ... Training loss: 0.179 ... Validation loss: 0.202iteration: 9388\n",
      "train_loss: 0.17930893995289046\n",
      "val_loss: 0.2029218221743931\n",
      "Progress: 93.9% ... Training loss: 0.151 ... Validation loss: 0.272iteration: 9389\n",
      "train_loss: 0.1519916277266472\n",
      "val_loss: 0.2729859650819725\n",
      "Progress: 93.9% ... Training loss: 0.121 ... Validation loss: 0.166iteration: 9390\n",
      "train_loss: 0.12144969475385174\n",
      "val_loss: 0.16669343615589066\n",
      "Progress: 93.9% ... Training loss: 0.131 ... Validation loss: 0.248iteration: 9391\n",
      "train_loss: 0.13128536805536134\n",
      "val_loss: 0.24800373989275606\n",
      "Progress: 93.9% ... Training loss: 0.104 ... Validation loss: 0.151iteration: 9392\n",
      "train_loss: 0.10485317166946731\n",
      "val_loss: 0.15142812144875922\n",
      "Progress: 93.9% ... Training loss: 0.115 ... Validation loss: 0.237iteration: 9393\n",
      "train_loss: 0.11576694801430182\n",
      "val_loss: 0.23757451318352396\n",
      "Progress: 93.9% ... Training loss: 0.118 ... Validation loss: 0.161iteration: 9394\n",
      "train_loss: 0.11866520224569928\n",
      "val_loss: 0.16114819411739126\n",
      "Progress: 94.0% ... Training loss: 0.085 ... Validation loss: 0.210iteration: 9395\n",
      "train_loss: 0.08532141702419577\n",
      "val_loss: 0.2100787265323252\n",
      "Progress: 94.0% ... Training loss: 0.075 ... Validation loss: 0.147iteration: 9396\n",
      "train_loss: 0.0754267152615229\n",
      "val_loss: 0.14707706326481904\n",
      "Progress: 94.0% ... Training loss: 0.068 ... Validation loss: 0.189iteration: 9397\n",
      "train_loss: 0.0682707316609275\n",
      "val_loss: 0.1896849267400505\n",
      "Progress: 94.0% ... Training loss: 0.075 ... Validation loss: 0.147iteration: 9398\n",
      "train_loss: 0.07538219612902174\n",
      "val_loss: 0.14717041219075117\n",
      "Progress: 94.0% ... Training loss: 0.063 ... Validation loss: 0.163iteration: 9399\n",
      "train_loss: 0.06340667248651585\n",
      "val_loss: 0.16361940345869047\n",
      "Progress: 94.0% ... Training loss: 0.058 ... Validation loss: 0.146iteration: 9400\n",
      "train_loss: 0.05837688421891046\n",
      "val_loss: 0.14681724503012308\n",
      "Progress: 94.0% ... Training loss: 0.057 ... Validation loss: 0.153iteration: 9401\n",
      "train_loss: 0.05784270263192223\n",
      "val_loss: 0.15372702807157007\n",
      "Progress: 94.0% ... Training loss: 0.061 ... Validation loss: 0.145iteration: 9402\n",
      "train_loss: 0.06188076545434053\n",
      "val_loss: 0.14525035835728328\n",
      "Progress: 94.0% ... Training loss: 0.064 ... Validation loss: 0.156iteration: 9403\n",
      "train_loss: 0.06418346130836285\n",
      "val_loss: 0.15622702498174204\n",
      "Progress: 94.0% ... Training loss: 0.063 ... Validation loss: 0.144iteration: 9404\n",
      "train_loss: 0.06360534869369283\n",
      "val_loss: 0.14478373897425076\n",
      "Progress: 94.0% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 9405\n",
      "train_loss: 0.05984980543006691\n",
      "val_loss: 0.15584553974598572\n",
      "Progress: 94.1% ... Training loss: 0.057 ... Validation loss: 0.150iteration: 9406\n",
      "train_loss: 0.057687279185875094\n",
      "val_loss: 0.15060791284020403\n",
      "Progress: 94.1% ... Training loss: 0.056 ... Validation loss: 0.148iteration: 9407\n",
      "train_loss: 0.056956193564172294\n",
      "val_loss: 0.14855738175837604\n",
      "Progress: 94.1% ... Training loss: 0.058 ... Validation loss: 0.156iteration: 9408\n",
      "train_loss: 0.05810378663418351\n",
      "val_loss: 0.15675781929153537\n",
      "Progress: 94.1% ... Training loss: 0.059 ... Validation loss: 0.143iteration: 9409\n",
      "train_loss: 0.05932286357323226\n",
      "val_loss: 0.14339854367503965\n",
      "Progress: 94.1% ... Training loss: 0.070 ... Validation loss: 0.168iteration: 9410\n",
      "train_loss: 0.07035963048033086\n",
      "val_loss: 0.16862631484880314\n",
      "Progress: 94.1% ... Training loss: 0.068 ... Validation loss: 0.140iteration: 9411\n",
      "train_loss: 0.06867801861347644\n",
      "val_loss: 0.1406153796864164\n",
      "Progress: 94.1% ... Training loss: 0.071 ... Validation loss: 0.178iteration: 9412\n",
      "train_loss: 0.07137714012641709\n",
      "val_loss: 0.1782796463314024\n",
      "Progress: 94.1% ... Training loss: 0.064 ... Validation loss: 0.146iteration: 9413\n",
      "train_loss: 0.06459034838330172\n",
      "val_loss: 0.1460575009374099\n",
      "Progress: 94.1% ... Training loss: 0.063 ... Validation loss: 0.188iteration: 9414\n",
      "train_loss: 0.0630512695054861\n",
      "val_loss: 0.1880961604849594\n",
      "Progress: 94.2% ... Training loss: 0.062 ... Validation loss: 0.147iteration: 9415\n",
      "train_loss: 0.06291512175509362\n",
      "val_loss: 0.14773792283588577\n",
      "Progress: 94.2% ... Training loss: 0.072 ... Validation loss: 0.206iteration: 9416\n",
      "train_loss: 0.07230691768732204\n",
      "val_loss: 0.2069187276250083\n",
      "Progress: 94.2% ... Training loss: 0.114 ... Validation loss: 0.153iteration: 9417\n",
      "train_loss: 0.1142165572929743\n",
      "val_loss: 0.153823014417797\n",
      "Progress: 94.2% ... Training loss: 0.124 ... Validation loss: 0.312iteration: 9418\n",
      "train_loss: 0.12483779574534032\n",
      "val_loss: 0.3125638426025397\n",
      "Progress: 94.2% ... Training loss: 0.103 ... Validation loss: 0.152iteration: 9419\n",
      "train_loss: 0.10319966538328687\n",
      "val_loss: 0.15202268758196472\n",
      "Progress: 94.2% ... Training loss: 0.101 ... Validation loss: 0.262iteration: 9420\n",
      "train_loss: 0.10174100151238093\n",
      "val_loss: 0.262084756107597\n",
      "Progress: 94.2% ... Training loss: 0.087 ... Validation loss: 0.151iteration: 9421\n",
      "train_loss: 0.0871586097120018\n",
      "val_loss: 0.15139395910026968\n",
      "Progress: 94.2% ... Training loss: 0.069 ... Validation loss: 0.215iteration: 9422\n",
      "train_loss: 0.06948692620262471\n",
      "val_loss: 0.2155577808271308\n",
      "Progress: 94.2% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 9423\n",
      "train_loss: 0.0617648283047999\n",
      "val_loss: 0.14948220534436363\n",
      "Progress: 94.2% ... Training loss: 0.062 ... Validation loss: 0.182iteration: 9424\n",
      "train_loss: 0.06231012387994009\n",
      "val_loss: 0.1824999597171134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 94.2% ... Training loss: 0.061 ... Validation loss: 0.144iteration: 9425\n",
      "train_loss: 0.06129919003941345\n",
      "val_loss: 0.1447671450428592\n",
      "Progress: 94.3% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 9426\n",
      "train_loss: 0.05889794451868676\n",
      "val_loss: 0.16906787793091643\n",
      "Progress: 94.3% ... Training loss: 0.058 ... Validation loss: 0.155iteration: 9427\n",
      "train_loss: 0.058100857107090564\n",
      "val_loss: 0.1556681788041424\n",
      "Progress: 94.3% ... Training loss: 0.056 ... Validation loss: 0.167iteration: 9428\n",
      "train_loss: 0.05689302139320733\n",
      "val_loss: 0.1673112780980258\n",
      "Progress: 94.3% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 9429\n",
      "train_loss: 0.05720313298459346\n",
      "val_loss: 0.17010460055672327\n",
      "Progress: 94.3% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 9430\n",
      "train_loss: 0.05901112210778457\n",
      "val_loss: 0.15621258058835003\n",
      "Progress: 94.3% ... Training loss: 0.061 ... Validation loss: 0.188iteration: 9431\n",
      "train_loss: 0.06173028914595585\n",
      "val_loss: 0.1882294069187583\n",
      "Progress: 94.3% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 9432\n",
      "train_loss: 0.05802257526349717\n",
      "val_loss: 0.16380776200071068\n",
      "Progress: 94.3% ... Training loss: 0.065 ... Validation loss: 0.185iteration: 9433\n",
      "train_loss: 0.0650340505317853\n",
      "val_loss: 0.1856665414232017\n",
      "Progress: 94.3% ... Training loss: 0.064 ... Validation loss: 0.152iteration: 9434\n",
      "train_loss: 0.06413570956280137\n",
      "val_loss: 0.15201084072241094\n",
      "Progress: 94.3% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 9435\n",
      "train_loss: 0.06036694722329492\n",
      "val_loss: 0.16353351471309996\n",
      "Progress: 94.4% ... Training loss: 0.062 ... Validation loss: 0.145iteration: 9436\n",
      "train_loss: 0.0626358678178794\n",
      "val_loss: 0.14535175275750542\n",
      "Progress: 94.4% ... Training loss: 0.058 ... Validation loss: 0.172iteration: 9437\n",
      "train_loss: 0.05810784129391367\n",
      "val_loss: 0.17261268797649965\n",
      "Progress: 94.4% ... Training loss: 0.058 ... Validation loss: 0.146iteration: 9438\n",
      "train_loss: 0.05877368439511268\n",
      "val_loss: 0.14666257782463268\n",
      "Progress: 94.4% ... Training loss: 0.057 ... Validation loss: 0.156iteration: 9439\n",
      "train_loss: 0.05731598860991911\n",
      "val_loss: 0.15667361160589927\n",
      "Progress: 94.4% ... Training loss: 0.057 ... Validation loss: 0.158iteration: 9440\n",
      "train_loss: 0.057539465781028414\n",
      "val_loss: 0.1586941412207533\n",
      "Progress: 94.4% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9441\n",
      "train_loss: 0.0567848991948349\n",
      "val_loss: 0.1545633954231428\n",
      "Progress: 94.4% ... Training loss: 0.057 ... Validation loss: 0.147iteration: 9442\n",
      "train_loss: 0.05763564138541565\n",
      "val_loss: 0.14746277483431214\n",
      "Progress: 94.4% ... Training loss: 0.069 ... Validation loss: 0.178iteration: 9443\n",
      "train_loss: 0.06943003416543721\n",
      "val_loss: 0.1787328849081624\n",
      "Progress: 94.4% ... Training loss: 0.058 ... Validation loss: 0.147iteration: 9444\n",
      "train_loss: 0.05878399884725312\n",
      "val_loss: 0.14752217837862183\n",
      "Progress: 94.5% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9445\n",
      "train_loss: 0.05662502382987753\n",
      "val_loss: 0.15247977787646583\n",
      "Progress: 94.5% ... Training loss: 0.057 ... Validation loss: 0.149iteration: 9446\n",
      "train_loss: 0.05767640554871783\n",
      "val_loss: 0.14961868981157156\n",
      "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.168iteration: 9447\n",
      "train_loss: 0.058297950196211104\n",
      "val_loss: 0.16866024142349118\n",
      "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.150iteration: 9448\n",
      "train_loss: 0.05850052012547706\n",
      "val_loss: 0.15027902852695543\n",
      "Progress: 94.5% ... Training loss: 0.065 ... Validation loss: 0.195iteration: 9449\n",
      "train_loss: 0.06535690144365491\n",
      "val_loss: 0.19505222586754176\n",
      "Progress: 94.5% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 9450\n",
      "train_loss: 0.06035880115122222\n",
      "val_loss: 0.15574837447336842\n",
      "Progress: 94.5% ... Training loss: 0.057 ... Validation loss: 0.163iteration: 9451\n",
      "train_loss: 0.057255946839312316\n",
      "val_loss: 0.1635247195791737\n",
      "Progress: 94.5% ... Training loss: 0.057 ... Validation loss: 0.153iteration: 9452\n",
      "train_loss: 0.05703253171651792\n",
      "val_loss: 0.15312118824346937\n",
      "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.147iteration: 9453\n",
      "train_loss: 0.058061848330667534\n",
      "val_loss: 0.14743304160749784\n",
      "Progress: 94.5% ... Training loss: 0.059 ... Validation loss: 0.146iteration: 9454\n",
      "train_loss: 0.05968676401862093\n",
      "val_loss: 0.1462280907609096\n",
      "Progress: 94.5% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 9455\n",
      "train_loss: 0.05903322912087313\n",
      "val_loss: 0.17046065215434553\n",
      "Progress: 94.6% ... Training loss: 0.059 ... Validation loss: 0.148iteration: 9456\n",
      "train_loss: 0.05991595466249861\n",
      "val_loss: 0.1488700719985808\n",
      "Progress: 94.6% ... Training loss: 0.060 ... Validation loss: 0.160iteration: 9457\n",
      "train_loss: 0.060500314617984566\n",
      "val_loss: 0.1607885556474981\n",
      "Progress: 94.6% ... Training loss: 0.077 ... Validation loss: 0.150iteration: 9458\n",
      "train_loss: 0.07769401438574859\n",
      "val_loss: 0.15021442143871194\n",
      "Progress: 94.6% ... Training loss: 0.075 ... Validation loss: 0.199iteration: 9459\n",
      "train_loss: 0.0753272090167531\n",
      "val_loss: 0.19919368530506587\n",
      "Progress: 94.6% ... Training loss: 0.080 ... Validation loss: 0.147iteration: 9460\n",
      "train_loss: 0.08071171206007012\n",
      "val_loss: 0.14793162614822666\n",
      "Progress: 94.6% ... Training loss: 0.064 ... Validation loss: 0.182iteration: 9461\n",
      "train_loss: 0.06433023060830935\n",
      "val_loss: 0.18255107088117126\n",
      "Progress: 94.6% ... Training loss: 0.072 ... Validation loss: 0.148iteration: 9462\n",
      "train_loss: 0.07219257506328551\n",
      "val_loss: 0.1484676918219769\n",
      "Progress: 94.6% ... Training loss: 0.060 ... Validation loss: 0.170iteration: 9463\n",
      "train_loss: 0.06092198584924677\n",
      "val_loss: 0.17029252179349708\n",
      "Progress: 94.6% ... Training loss: 0.061 ... Validation loss: 0.148iteration: 9464\n",
      "train_loss: 0.06113283247009687\n",
      "val_loss: 0.14826676798823246\n",
      "Progress: 94.7% ... Training loss: 0.056 ... Validation loss: 0.165iteration: 9465\n",
      "train_loss: 0.0567576994030333\n",
      "val_loss: 0.16540336925304713\n",
      "Progress: 94.7% ... Training loss: 0.059 ... Validation loss: 0.149iteration: 9466\n",
      "train_loss: 0.059673674356318346\n",
      "val_loss: 0.14912906630926837\n",
      "Progress: 94.7% ... Training loss: 0.056 ... Validation loss: 0.151iteration: 9467\n",
      "train_loss: 0.05691356441217363\n",
      "val_loss: 0.15162050088866502\n",
      "Progress: 94.7% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 9468\n",
      "train_loss: 0.058221078800540026\n",
      "val_loss: 0.16576861762376904\n",
      "Progress: 94.7% ... Training loss: 0.060 ... Validation loss: 0.150iteration: 9469\n",
      "train_loss: 0.06047980021413783\n",
      "val_loss: 0.15003245100909066\n",
      "Progress: 94.7% ... Training loss: 0.063 ... Validation loss: 0.183iteration: 9470\n",
      "train_loss: 0.06317318471151005\n",
      "val_loss: 0.1835810100146434\n",
      "Progress: 94.7% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9471\n",
      "train_loss: 0.057946521495639856\n",
      "val_loss: 0.15536432462444397\n",
      "Progress: 94.7% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9472\n",
      "train_loss: 0.057264019814264266\n",
      "val_loss: 0.1650610232415405\n",
      "Progress: 94.7% ... Training loss: 0.058 ... Validation loss: 0.159iteration: 9473\n",
      "train_loss: 0.058005210060755194\n",
      "val_loss: 0.15942171350377815\n",
      "Progress: 94.7% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9474\n",
      "train_loss: 0.05788071902257753\n",
      "val_loss: 0.15476134206576483\n",
      "Progress: 94.8% ... Training loss: 0.059 ... Validation loss: 0.178iteration: 9475\n",
      "train_loss: 0.059189610729111525\n",
      "val_loss: 0.1784139790542872\n",
      "Progress: 94.8% ... Training loss: 0.063 ... Validation loss: 0.155iteration: 9476\n",
      "train_loss: 0.06332469147890167\n",
      "val_loss: 0.15537902677203222\n",
      "Progress: 94.8% ... Training loss: 0.061 ... Validation loss: 0.173iteration: 9477\n",
      "train_loss: 0.06144563977817979\n",
      "val_loss: 0.17399183611399455\n",
      "Progress: 94.8% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 9478\n",
      "train_loss: 0.059583328686625\n",
      "val_loss: 0.16246639586111453\n",
      "Progress: 94.8% ... Training loss: 0.074 ... Validation loss: 0.193iteration: 9479\n",
      "train_loss: 0.0745545955199108\n",
      "val_loss: 0.1934658572681639\n",
      "Progress: 94.8% ... Training loss: 0.063 ... Validation loss: 0.148iteration: 9480\n",
      "train_loss: 0.06360792388794338\n",
      "val_loss: 0.14838655916322321\n",
      "Progress: 94.8% ... Training loss: 0.070 ... Validation loss: 0.196iteration: 9481\n",
      "train_loss: 0.07057171305560336\n",
      "val_loss: 0.19698199864559265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 94.8% ... Training loss: 0.086 ... Validation loss: 0.154iteration: 9482\n",
      "train_loss: 0.08609042871116081\n",
      "val_loss: 0.15434621617901742\n",
      "Progress: 94.8% ... Training loss: 0.066 ... Validation loss: 0.197iteration: 9483\n",
      "train_loss: 0.0663818670717899\n",
      "val_loss: 0.1970655033148587\n",
      "Progress: 94.8% ... Training loss: 0.067 ... Validation loss: 0.149iteration: 9484\n",
      "train_loss: 0.06794069217679194\n",
      "val_loss: 0.14998157685363944\n",
      "Progress: 94.8% ... Training loss: 0.070 ... Validation loss: 0.168iteration: 9485\n",
      "train_loss: 0.07021663795793477\n",
      "val_loss: 0.16800948014615846\n",
      "Progress: 94.9% ... Training loss: 0.059 ... Validation loss: 0.149iteration: 9486\n",
      "train_loss: 0.05920143707717638\n",
      "val_loss: 0.14984444275164016\n",
      "Progress: 94.9% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9487\n",
      "train_loss: 0.05641341383468829\n",
      "val_loss: 0.15565936725690124\n",
      "Progress: 94.9% ... Training loss: 0.060 ... Validation loss: 0.181iteration: 9488\n",
      "train_loss: 0.06027261177222728\n",
      "val_loss: 0.1811280250658983\n",
      "Progress: 94.9% ... Training loss: 0.060 ... Validation loss: 0.154iteration: 9489\n",
      "train_loss: 0.06022902733905532\n",
      "val_loss: 0.15449874139657896\n",
      "Progress: 94.9% ... Training loss: 0.057 ... Validation loss: 0.170iteration: 9490\n",
      "train_loss: 0.05792781740448669\n",
      "val_loss: 0.17082015858798905\n",
      "Progress: 94.9% ... Training loss: 0.057 ... Validation loss: 0.175iteration: 9491\n",
      "train_loss: 0.05718385946326722\n",
      "val_loss: 0.17589972518746247\n",
      "Progress: 94.9% ... Training loss: 0.057 ... Validation loss: 0.161iteration: 9492\n",
      "train_loss: 0.05752114041345129\n",
      "val_loss: 0.16155502867628493\n",
      "Progress: 94.9% ... Training loss: 0.062 ... Validation loss: 0.191iteration: 9493\n",
      "train_loss: 0.06216846264477693\n",
      "val_loss: 0.19146004474899417\n",
      "Progress: 94.9% ... Training loss: 0.058 ... Validation loss: 0.160iteration: 9494\n",
      "train_loss: 0.058616063391546534\n",
      "val_loss: 0.16005418813297265\n",
      "Progress: 95.0% ... Training loss: 0.059 ... Validation loss: 0.177iteration: 9495\n",
      "train_loss: 0.05925707617993509\n",
      "val_loss: 0.17786679979317432\n",
      "Progress: 95.0% ... Training loss: 0.057 ... Validation loss: 0.157iteration: 9496\n",
      "train_loss: 0.05727426238120237\n",
      "val_loss: 0.15726986708977456\n",
      "Progress: 95.0% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9497\n",
      "train_loss: 0.05745135839040908\n",
      "val_loss: 0.1652832214811342\n",
      "Progress: 95.0% ... Training loss: 0.056 ... Validation loss: 0.170iteration: 9498\n",
      "train_loss: 0.05664919016511211\n",
      "val_loss: 0.1700283057147001\n",
      "Progress: 95.0% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9499\n",
      "train_loss: 0.05618178146947819\n",
      "val_loss: 0.1552782678288309\n",
      "Progress: 95.0% ... Training loss: 0.059 ... Validation loss: 0.147iteration: 9500\n",
      "train_loss: 0.05902308350951435\n",
      "val_loss: 0.14796401118866323\n",
      "Progress: 95.0% ... Training loss: 0.066 ... Validation loss: 0.191iteration: 9501\n",
      "train_loss: 0.06651389581399941\n",
      "val_loss: 0.19142766634436156\n",
      "Progress: 95.0% ... Training loss: 0.064 ... Validation loss: 0.150iteration: 9502\n",
      "train_loss: 0.06436126526265025\n",
      "val_loss: 0.15084369882433102\n",
      "Progress: 95.0% ... Training loss: 0.066 ... Validation loss: 0.200iteration: 9503\n",
      "train_loss: 0.06688101495358474\n",
      "val_loss: 0.20044254307417309\n",
      "Progress: 95.0% ... Training loss: 0.061 ... Validation loss: 0.148iteration: 9504\n",
      "train_loss: 0.06197002794067475\n",
      "val_loss: 0.1482898580976808\n",
      "Progress: 95.0% ... Training loss: 0.058 ... Validation loss: 0.177iteration: 9505\n",
      "train_loss: 0.058234937796941146\n",
      "val_loss: 0.17713753807843638\n",
      "Progress: 95.1% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9506\n",
      "train_loss: 0.05745173627966158\n",
      "val_loss: 0.16606427388653056\n",
      "Progress: 95.1% ... Training loss: 0.058 ... Validation loss: 0.158iteration: 9507\n",
      "train_loss: 0.05809045042459518\n",
      "val_loss: 0.1582575876272163\n",
      "Progress: 95.1% ... Training loss: 0.066 ... Validation loss: 0.180iteration: 9508\n",
      "train_loss: 0.06681757663829108\n",
      "val_loss: 0.1804275498014377\n",
      "Progress: 95.1% ... Training loss: 0.060 ... Validation loss: 0.140iteration: 9509\n",
      "train_loss: 0.06025791632561643\n",
      "val_loss: 0.1406367321078682\n",
      "Progress: 95.1% ... Training loss: 0.066 ... Validation loss: 0.168iteration: 9510\n",
      "train_loss: 0.06618126311264978\n",
      "val_loss: 0.1685459794958505\n",
      "Progress: 95.1% ... Training loss: 0.062 ... Validation loss: 0.142iteration: 9511\n",
      "train_loss: 0.06248688603612165\n",
      "val_loss: 0.1422570288824324\n",
      "Progress: 95.1% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 9512\n",
      "train_loss: 0.058278525072735884\n",
      "val_loss: 0.1638012255427421\n",
      "Progress: 95.1% ... Training loss: 0.060 ... Validation loss: 0.155iteration: 9513\n",
      "train_loss: 0.0603995885238569\n",
      "val_loss: 0.1559144094316286\n",
      "Progress: 95.1% ... Training loss: 0.063 ... Validation loss: 0.190iteration: 9514\n",
      "train_loss: 0.06369402729173175\n",
      "val_loss: 0.19065408355701266\n",
      "Progress: 95.2% ... Training loss: 0.079 ... Validation loss: 0.146iteration: 9515\n",
      "train_loss: 0.07917326640443109\n",
      "val_loss: 0.1461810725697541\n",
      "Progress: 95.2% ... Training loss: 0.081 ... Validation loss: 0.213iteration: 9516\n",
      "train_loss: 0.08116503770431392\n",
      "val_loss: 0.21393018999330413\n",
      "Progress: 95.2% ... Training loss: 0.063 ... Validation loss: 0.153iteration: 9517\n",
      "train_loss: 0.06306110754304568\n",
      "val_loss: 0.15363967034341297\n",
      "Progress: 95.2% ... Training loss: 0.083 ... Validation loss: 0.223iteration: 9518\n",
      "train_loss: 0.08330583365321313\n",
      "val_loss: 0.22384535104572584\n",
      "Progress: 95.2% ... Training loss: 0.066 ... Validation loss: 0.146iteration: 9519\n",
      "train_loss: 0.06671895526633982\n",
      "val_loss: 0.1469868399603504\n",
      "Progress: 95.2% ... Training loss: 0.071 ... Validation loss: 0.196iteration: 9520\n",
      "train_loss: 0.07188332064054533\n",
      "val_loss: 0.1968789123026341\n",
      "Progress: 95.2% ... Training loss: 0.063 ... Validation loss: 0.148iteration: 9521\n",
      "train_loss: 0.0638678604191352\n",
      "val_loss: 0.1484963494235751\n",
      "Progress: 95.2% ... Training loss: 0.060 ... Validation loss: 0.188iteration: 9522\n",
      "train_loss: 0.06009291537085233\n",
      "val_loss: 0.18859061442031708\n",
      "Progress: 95.2% ... Training loss: 0.077 ... Validation loss: 0.146iteration: 9523\n",
      "train_loss: 0.07798692382447045\n",
      "val_loss: 0.14645425748493932\n",
      "Progress: 95.2% ... Training loss: 0.126 ... Validation loss: 0.229iteration: 9524\n",
      "train_loss: 0.12604086115983978\n",
      "val_loss: 0.22980294635042944\n",
      "Progress: 95.2% ... Training loss: 0.128 ... Validation loss: 0.160iteration: 9525\n",
      "train_loss: 0.12851097830854674\n",
      "val_loss: 0.160798129962878\n",
      "Progress: 95.3% ... Training loss: 0.099 ... Validation loss: 0.251iteration: 9526\n",
      "train_loss: 0.09947241281450576\n",
      "val_loss: 0.2515714395274343\n",
      "Progress: 95.3% ... Training loss: 0.091 ... Validation loss: 0.152iteration: 9527\n",
      "train_loss: 0.09101785487453268\n",
      "val_loss: 0.1523357043894517\n",
      "Progress: 95.3% ... Training loss: 0.079 ... Validation loss: 0.220iteration: 9528\n",
      "train_loss: 0.07908472842380689\n",
      "val_loss: 0.22085242924654866\n",
      "Progress: 95.3% ... Training loss: 0.065 ... Validation loss: 0.147iteration: 9529\n",
      "train_loss: 0.06592490907478095\n",
      "val_loss: 0.1478128544290362\n",
      "Progress: 95.3% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9530\n",
      "train_loss: 0.05671221549494214\n",
      "val_loss: 0.15597504254783956\n",
      "Progress: 95.3% ... Training loss: 0.056 ... Validation loss: 0.160iteration: 9531\n",
      "train_loss: 0.05635017738022923\n",
      "val_loss: 0.16036824731812088\n",
      "Progress: 95.3% ... Training loss: 0.057 ... Validation loss: 0.156iteration: 9532\n",
      "train_loss: 0.05710906541556218\n",
      "val_loss: 0.15668878831502925\n",
      "Progress: 95.3% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9533\n",
      "train_loss: 0.05695506083531147\n",
      "val_loss: 0.15935836121538247\n",
      "Progress: 95.3% ... Training loss: 0.056 ... Validation loss: 0.160iteration: 9534\n",
      "train_loss: 0.05667541990797589\n",
      "val_loss: 0.16042599746466762\n",
      "Progress: 95.3% ... Training loss: 0.059 ... Validation loss: 0.149iteration: 9535\n",
      "train_loss: 0.05952528577169572\n",
      "val_loss: 0.14930121293761514\n",
      "Progress: 95.4% ... Training loss: 0.061 ... Validation loss: 0.178iteration: 9536\n",
      "train_loss: 0.061594148117353906\n",
      "val_loss: 0.17882845637949688\n",
      "Progress: 95.4% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 9537\n",
      "train_loss: 0.05857067075786893\n",
      "val_loss: 0.15334916575389507\n",
      "Progress: 95.4% ... Training loss: 0.057 ... Validation loss: 0.168iteration: 9538\n",
      "train_loss: 0.05711318702490595\n",
      "val_loss: 0.16832076193810766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 95.4% ... Training loss: 0.062 ... Validation loss: 0.197iteration: 9539\n",
      "train_loss: 0.06259019258186653\n",
      "val_loss: 0.19787032984343184\n",
      "Progress: 95.4% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 9540\n",
      "train_loss: 0.058396224012564356\n",
      "val_loss: 0.16743234163372994\n",
      "Progress: 95.4% ... Training loss: 0.059 ... Validation loss: 0.178iteration: 9541\n",
      "train_loss: 0.05916100945063587\n",
      "val_loss: 0.17842988462931086\n",
      "Progress: 95.4% ... Training loss: 0.056 ... Validation loss: 0.158iteration: 9542\n",
      "train_loss: 0.05614140288820908\n",
      "val_loss: 0.1589959709428978\n",
      "Progress: 95.4% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9543\n",
      "train_loss: 0.05653404731907875\n",
      "val_loss: 0.15988500881801485\n",
      "Progress: 95.4% ... Training loss: 0.056 ... Validation loss: 0.165iteration: 9544\n",
      "train_loss: 0.056598511608344435\n",
      "val_loss: 0.16570028730508285\n",
      "Progress: 95.5% ... Training loss: 0.056 ... Validation loss: 0.163iteration: 9545\n",
      "train_loss: 0.056639331610061336\n",
      "val_loss: 0.1632745157610081\n",
      "Progress: 95.5% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 9546\n",
      "train_loss: 0.05927123161616211\n",
      "val_loss: 0.16097570571951383\n",
      "Progress: 95.5% ... Training loss: 0.061 ... Validation loss: 0.185iteration: 9547\n",
      "train_loss: 0.061055132415814\n",
      "val_loss: 0.18593288275911138\n",
      "Progress: 95.5% ... Training loss: 0.075 ... Validation loss: 0.151iteration: 9548\n",
      "train_loss: 0.07519837601366185\n",
      "val_loss: 0.15173602388038399\n",
      "Progress: 95.5% ... Training loss: 0.070 ... Validation loss: 0.188iteration: 9549\n",
      "train_loss: 0.07005124806905402\n",
      "val_loss: 0.18817068577479218\n",
      "Progress: 95.5% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 9550\n",
      "train_loss: 0.058333241673679696\n",
      "val_loss: 0.15718854120198933\n",
      "Progress: 95.5% ... Training loss: 0.060 ... Validation loss: 0.176iteration: 9551\n",
      "train_loss: 0.06040627972065576\n",
      "val_loss: 0.17625775726480566\n",
      "Progress: 95.5% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 9552\n",
      "train_loss: 0.060682232288416224\n",
      "val_loss: 0.152036071680258\n",
      "Progress: 95.5% ... Training loss: 0.073 ... Validation loss: 0.216iteration: 9553\n",
      "train_loss: 0.07385963891656375\n",
      "val_loss: 0.216324053747269\n",
      "Progress: 95.5% ... Training loss: 0.067 ... Validation loss: 0.148iteration: 9554\n",
      "train_loss: 0.06716409931767141\n",
      "val_loss: 0.14872277352892355\n",
      "Progress: 95.5% ... Training loss: 0.068 ... Validation loss: 0.180iteration: 9555\n",
      "train_loss: 0.06816591804930441\n",
      "val_loss: 0.18081285789733914\n",
      "Progress: 95.6% ... Training loss: 0.059 ... Validation loss: 0.148iteration: 9556\n",
      "train_loss: 0.0592953948063672\n",
      "val_loss: 0.1482681825021362\n",
      "Progress: 95.6% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 9557\n",
      "train_loss: 0.05724445319837187\n",
      "val_loss: 0.16070060425612948\n",
      "Progress: 95.6% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 9558\n",
      "train_loss: 0.05732430181025067\n",
      "val_loss: 0.1675244213635476\n",
      "Progress: 95.6% ... Training loss: 0.057 ... Validation loss: 0.168iteration: 9559\n",
      "train_loss: 0.05701527739761717\n",
      "val_loss: 0.1683584213203254\n",
      "Progress: 95.6% ... Training loss: 0.058 ... Validation loss: 0.156iteration: 9560\n",
      "train_loss: 0.05805285692121967\n",
      "val_loss: 0.15646959557611742\n",
      "Progress: 95.6% ... Training loss: 0.062 ... Validation loss: 0.179iteration: 9561\n",
      "train_loss: 0.06225190215299602\n",
      "val_loss: 0.17958420161360963\n",
      "Progress: 95.6% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9562\n",
      "train_loss: 0.0568669774677969\n",
      "val_loss: 0.1540023437875067\n",
      "Progress: 95.6% ... Training loss: 0.060 ... Validation loss: 0.179iteration: 9563\n",
      "train_loss: 0.06014966848126682\n",
      "val_loss: 0.17939973854406316\n",
      "Progress: 95.6% ... Training loss: 0.069 ... Validation loss: 0.148iteration: 9564\n",
      "train_loss: 0.06992639412250665\n",
      "val_loss: 0.14887303114800957\n",
      "Progress: 95.7% ... Training loss: 0.059 ... Validation loss: 0.183iteration: 9565\n",
      "train_loss: 0.059911063872638434\n",
      "val_loss: 0.18391907911252275\n",
      "Progress: 95.7% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 9566\n",
      "train_loss: 0.05826897752814935\n",
      "val_loss: 0.151650965525821\n",
      "Progress: 95.7% ... Training loss: 0.062 ... Validation loss: 0.182iteration: 9567\n",
      "train_loss: 0.06283237018337948\n",
      "val_loss: 0.18264240149000616\n",
      "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9568\n",
      "train_loss: 0.05661790954992904\n",
      "val_loss: 0.15415393218976475\n",
      "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.164iteration: 9569\n",
      "train_loss: 0.056783504989757455\n",
      "val_loss: 0.16494076573265234\n",
      "Progress: 95.7% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 9570\n",
      "train_loss: 0.05747824039158098\n",
      "val_loss: 0.15935221697293436\n",
      "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.164iteration: 9571\n",
      "train_loss: 0.05614110875015562\n",
      "val_loss: 0.16434985620144443\n",
      "Progress: 95.7% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 9572\n",
      "train_loss: 0.058679039275397485\n",
      "val_loss: 0.17098914160128048\n",
      "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.158iteration: 9573\n",
      "train_loss: 0.056286371453866856\n",
      "val_loss: 0.15869753798583083\n",
      "Progress: 95.7% ... Training loss: 0.059 ... Validation loss: 0.168iteration: 9574\n",
      "train_loss: 0.05915436100883611\n",
      "val_loss: 0.16893168939223305\n",
      "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 9575\n",
      "train_loss: 0.05812546497832179\n",
      "val_loss: 0.15343802411723187\n",
      "Progress: 95.8% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9576\n",
      "train_loss: 0.057046057602505355\n",
      "val_loss: 0.15461210362293787\n",
      "Progress: 95.8% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9577\n",
      "train_loss: 0.05606969729965712\n",
      "val_loss: 0.15614822966358977\n",
      "Progress: 95.8% ... Training loss: 0.057 ... Validation loss: 0.168iteration: 9578\n",
      "train_loss: 0.057304693934437365\n",
      "val_loss: 0.16874147110539145\n",
      "Progress: 95.8% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 9579\n",
      "train_loss: 0.06034075042714989\n",
      "val_loss: 0.15176179913677812\n",
      "Progress: 95.8% ... Training loss: 0.070 ... Validation loss: 0.200iteration: 9580\n",
      "train_loss: 0.07081489468653418\n",
      "val_loss: 0.20042209933865898\n",
      "Progress: 95.8% ... Training loss: 0.078 ... Validation loss: 0.149iteration: 9581\n",
      "train_loss: 0.07832167413314509\n",
      "val_loss: 0.14994985661283536\n",
      "Progress: 95.8% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 9582\n",
      "train_loss: 0.0637703146692729\n",
      "val_loss: 0.17801843664256214\n",
      "Progress: 95.8% ... Training loss: 0.060 ... Validation loss: 0.146iteration: 9583\n",
      "train_loss: 0.06077159998151227\n",
      "val_loss: 0.14689560074660413\n",
      "Progress: 95.8% ... Training loss: 0.057 ... Validation loss: 0.148iteration: 9584\n",
      "train_loss: 0.05717682223804105\n",
      "val_loss: 0.1485935084009611\n",
      "Progress: 95.8% ... Training loss: 0.056 ... Validation loss: 0.160iteration: 9585\n",
      "train_loss: 0.056435022644555084\n",
      "val_loss: 0.16034554885648916\n",
      "Progress: 95.9% ... Training loss: 0.056 ... Validation loss: 0.158iteration: 9586\n",
      "train_loss: 0.05616315707655431\n",
      "val_loss: 0.15890019479102613\n",
      "Progress: 95.9% ... Training loss: 0.060 ... Validation loss: 0.178iteration: 9587\n",
      "train_loss: 0.060564607396533576\n",
      "val_loss: 0.17814549361553386\n",
      "Progress: 95.9% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 9588\n",
      "train_loss: 0.05954900010531048\n",
      "val_loss: 0.15616649994108636\n",
      "Progress: 95.9% ... Training loss: 0.064 ... Validation loss: 0.178iteration: 9589\n",
      "train_loss: 0.06491686758917163\n",
      "val_loss: 0.17895502934803736\n",
      "Progress: 95.9% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 9590\n",
      "train_loss: 0.059029098961655105\n",
      "val_loss: 0.15107749104737964\n",
      "Progress: 95.9% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9591\n",
      "train_loss: 0.056016314353378616\n",
      "val_loss: 0.15422750370293864\n",
      "Progress: 95.9% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9592\n",
      "train_loss: 0.05605517083401285\n",
      "val_loss: 0.15262150557535265\n",
      "Progress: 95.9% ... Training loss: 0.057 ... Validation loss: 0.159iteration: 9593\n",
      "train_loss: 0.05768655418346473\n",
      "val_loss: 0.15937144632669403\n",
      "Progress: 95.9% ... Training loss: 0.058 ... Validation loss: 0.146iteration: 9594\n",
      "train_loss: 0.05808464774912641\n",
      "val_loss: 0.14648831662376666\n",
      "Progress: 96.0% ... Training loss: 0.077 ... Validation loss: 0.208iteration: 9595\n",
      "train_loss: 0.07703844196480214\n",
      "val_loss: 0.20845361045915323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 96.0% ... Training loss: 0.083 ... Validation loss: 0.147iteration: 9596\n",
      "train_loss: 0.08307878872117098\n",
      "val_loss: 0.14781173775662915\n",
      "Progress: 96.0% ... Training loss: 0.068 ... Validation loss: 0.212iteration: 9597\n",
      "train_loss: 0.0685041193854507\n",
      "val_loss: 0.21264430478468302\n",
      "Progress: 96.0% ... Training loss: 0.063 ... Validation loss: 0.146iteration: 9598\n",
      "train_loss: 0.06332421487275253\n",
      "val_loss: 0.14659229838651042\n",
      "Progress: 96.0% ... Training loss: 0.064 ... Validation loss: 0.182iteration: 9599\n",
      "train_loss: 0.06407199532890569\n",
      "val_loss: 0.18266919439592724\n",
      "Progress: 96.0% ... Training loss: 0.056 ... Validation loss: 0.149iteration: 9600\n",
      "train_loss: 0.056633084713632244\n",
      "val_loss: 0.14987957121106033\n",
      "Progress: 96.0% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 9601\n",
      "train_loss: 0.05736684029617832\n",
      "val_loss: 0.16789808668178804\n",
      "Progress: 96.0% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9602\n",
      "train_loss: 0.05621227152164533\n",
      "val_loss: 0.15655639508696353\n",
      "Progress: 96.0% ... Training loss: 0.056 ... Validation loss: 0.167iteration: 9603\n",
      "train_loss: 0.056613971292122235\n",
      "val_loss: 0.16742278972471933\n",
      "Progress: 96.0% ... Training loss: 0.056 ... Validation loss: 0.162iteration: 9604\n",
      "train_loss: 0.056141057082378584\n",
      "val_loss: 0.16265716010014172\n",
      "Progress: 96.0% ... Training loss: 0.059 ... Validation loss: 0.153iteration: 9605\n",
      "train_loss: 0.059387145795240355\n",
      "val_loss: 0.15354371974691194\n",
      "Progress: 96.1% ... Training loss: 0.063 ... Validation loss: 0.186iteration: 9606\n",
      "train_loss: 0.0631092358020768\n",
      "val_loss: 0.18667109874576443\n",
      "Progress: 96.1% ... Training loss: 0.058 ... Validation loss: 0.156iteration: 9607\n",
      "train_loss: 0.058226121100590535\n",
      "val_loss: 0.15652633547430977\n",
      "Progress: 96.1% ... Training loss: 0.056 ... Validation loss: 0.172iteration: 9608\n",
      "train_loss: 0.05661073107908588\n",
      "val_loss: 0.17282352925179884\n",
      "Progress: 96.1% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9609\n",
      "train_loss: 0.05613610025424889\n",
      "val_loss: 0.1551110063549261\n",
      "Progress: 96.1% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9610\n",
      "train_loss: 0.0561161538623715\n",
      "val_loss: 0.15224436330888513\n",
      "Progress: 96.1% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9611\n",
      "train_loss: 0.05645307186180763\n",
      "val_loss: 0.15543171738512804\n",
      "Progress: 96.1% ... Training loss: 0.064 ... Validation loss: 0.151iteration: 9612\n",
      "train_loss: 0.0645355985367308\n",
      "val_loss: 0.15184890830973624\n",
      "Progress: 96.1% ... Training loss: 0.073 ... Validation loss: 0.190iteration: 9613\n",
      "train_loss: 0.07302736712652001\n",
      "val_loss: 0.19023682728931932\n",
      "Progress: 96.1% ... Training loss: 0.064 ... Validation loss: 0.149iteration: 9614\n",
      "train_loss: 0.06485593376231376\n",
      "val_loss: 0.14917687030264523\n",
      "Progress: 96.2% ... Training loss: 0.068 ... Validation loss: 0.185iteration: 9615\n",
      "train_loss: 0.06893842915447172\n",
      "val_loss: 0.18590061849599798\n",
      "Progress: 96.2% ... Training loss: 0.068 ... Validation loss: 0.148iteration: 9616\n",
      "train_loss: 0.06875254318509294\n",
      "val_loss: 0.1481324979976856\n",
      "Progress: 96.2% ... Training loss: 0.059 ... Validation loss: 0.171iteration: 9617\n",
      "train_loss: 0.05975977908661388\n",
      "val_loss: 0.1716094898737854\n",
      "Progress: 96.2% ... Training loss: 0.058 ... Validation loss: 0.145iteration: 9618\n",
      "train_loss: 0.058162159735184744\n",
      "val_loss: 0.1459910539169314\n",
      "Progress: 96.2% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 9619\n",
      "train_loss: 0.05887651997735939\n",
      "val_loss: 0.16355938287725727\n",
      "Progress: 96.2% ... Training loss: 0.061 ... Validation loss: 0.152iteration: 9620\n",
      "train_loss: 0.06124298226503498\n",
      "val_loss: 0.15258249682631989\n",
      "Progress: 96.2% ... Training loss: 0.058 ... Validation loss: 0.173iteration: 9621\n",
      "train_loss: 0.0585971800767154\n",
      "val_loss: 0.1736221462513728\n",
      "Progress: 96.2% ... Training loss: 0.058 ... Validation loss: 0.147iteration: 9622\n",
      "train_loss: 0.05800884858855159\n",
      "val_loss: 0.1472585150696861\n",
      "Progress: 96.2% ... Training loss: 0.056 ... Validation loss: 0.162iteration: 9623\n",
      "train_loss: 0.05662881282601768\n",
      "val_loss: 0.16225288627731757\n",
      "Progress: 96.2% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9624\n",
      "train_loss: 0.056168732229589666\n",
      "val_loss: 0.15284993680555206\n",
      "Progress: 96.2% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9625\n",
      "train_loss: 0.05684538285408649\n",
      "val_loss: 0.15273966235876604\n",
      "Progress: 96.3% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9626\n",
      "train_loss: 0.0565670405365604\n",
      "val_loss: 0.15474739532726156\n",
      "Progress: 96.3% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 9627\n",
      "train_loss: 0.058040021882784336\n",
      "val_loss: 0.1576750311784609\n",
      "Progress: 96.3% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9628\n",
      "train_loss: 0.05621162111766729\n",
      "val_loss: 0.15591995702703712\n",
      "Progress: 96.3% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9629\n",
      "train_loss: 0.05641805255239875\n",
      "val_loss: 0.15699065904384252\n",
      "Progress: 96.3% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9630\n",
      "train_loss: 0.05673106172727756\n",
      "val_loss: 0.15574179513790185\n",
      "Progress: 96.3% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 9631\n",
      "train_loss: 0.06022383111400057\n",
      "val_loss: 0.14907309515725808\n",
      "Progress: 96.3% ... Training loss: 0.057 ... Validation loss: 0.157iteration: 9632\n",
      "train_loss: 0.0571194943681974\n",
      "val_loss: 0.15730600119754148\n",
      "Progress: 96.3% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 9633\n",
      "train_loss: 0.05707732386705229\n",
      "val_loss: 0.16087037936976473\n",
      "Progress: 96.3% ... Training loss: 0.063 ... Validation loss: 0.176iteration: 9634\n",
      "train_loss: 0.06362167673287328\n",
      "val_loss: 0.17613141437741436\n",
      "Progress: 96.3% ... Training loss: 0.064 ... Validation loss: 0.151iteration: 9635\n",
      "train_loss: 0.06419100996067892\n",
      "val_loss: 0.15196222625954275\n",
      "Progress: 96.4% ... Training loss: 0.066 ... Validation loss: 0.192iteration: 9636\n",
      "train_loss: 0.06631468645850207\n",
      "val_loss: 0.19281180976376752\n",
      "Progress: 96.4% ... Training loss: 0.068 ... Validation loss: 0.147iteration: 9637\n",
      "train_loss: 0.0680369407648417\n",
      "val_loss: 0.14751785082916363\n",
      "Progress: 96.4% ... Training loss: 0.059 ... Validation loss: 0.180iteration: 9638\n",
      "train_loss: 0.059718927029171746\n",
      "val_loss: 0.18037587310427192\n",
      "Progress: 96.4% ... Training loss: 0.059 ... Validation loss: 0.152iteration: 9639\n",
      "train_loss: 0.05948520189138418\n",
      "val_loss: 0.15200550425671341\n",
      "Progress: 96.4% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9640\n",
      "train_loss: 0.056574760211441026\n",
      "val_loss: 0.15949772575437662\n",
      "Progress: 96.4% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9641\n",
      "train_loss: 0.056079102240225644\n",
      "val_loss: 0.15417954848738977\n",
      "Progress: 96.4% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9642\n",
      "train_loss: 0.05650084645467732\n",
      "val_loss: 0.15638239876231194\n",
      "Progress: 96.4% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 9643\n",
      "train_loss: 0.05706910773752802\n",
      "val_loss: 0.1678732562949694\n",
      "Progress: 96.4% ... Training loss: 0.057 ... Validation loss: 0.153iteration: 9644\n",
      "train_loss: 0.05776353910623082\n",
      "val_loss: 0.15315010566506532\n",
      "Progress: 96.5% ... Training loss: 0.059 ... Validation loss: 0.174iteration: 9645\n",
      "train_loss: 0.059069660249015436\n",
      "val_loss: 0.17418085924032628\n",
      "Progress: 96.5% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 9646\n",
      "train_loss: 0.0604719718966517\n",
      "val_loss: 0.14937950538997039\n",
      "Progress: 96.5% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 9647\n",
      "train_loss: 0.06377911850549267\n",
      "val_loss: 0.17865489063888554\n",
      "Progress: 96.5% ... Training loss: 0.065 ... Validation loss: 0.147iteration: 9648\n",
      "train_loss: 0.06507485486561859\n",
      "val_loss: 0.14788047296067838\n",
      "Progress: 96.5% ... Training loss: 0.089 ... Validation loss: 0.230iteration: 9649\n",
      "train_loss: 0.08959367881330975\n",
      "val_loss: 0.23038621686587185\n",
      "Progress: 96.5% ... Training loss: 0.104 ... Validation loss: 0.155iteration: 9650\n",
      "train_loss: 0.104425145624553\n",
      "val_loss: 0.15515412715997381\n",
      "Progress: 96.5% ... Training loss: 0.083 ... Validation loss: 0.214iteration: 9651\n",
      "train_loss: 0.08372636163212459\n",
      "val_loss: 0.2146095219548344\n",
      "Progress: 96.5% ... Training loss: 0.067 ... Validation loss: 0.149iteration: 9652\n",
      "train_loss: 0.06714038100616986\n",
      "val_loss: 0.1496856576273948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 96.5% ... Training loss: 0.069 ... Validation loss: 0.193iteration: 9653\n",
      "train_loss: 0.06922539152366723\n",
      "val_loss: 0.19322901243068888\n",
      "Progress: 96.5% ... Training loss: 0.060 ... Validation loss: 0.149iteration: 9654\n",
      "train_loss: 0.060823213059569545\n",
      "val_loss: 0.14903802045213407\n",
      "Progress: 96.5% ... Training loss: 0.059 ... Validation loss: 0.181iteration: 9655\n",
      "train_loss: 0.05992749075743139\n",
      "val_loss: 0.18111565053103007\n",
      "Progress: 96.6% ... Training loss: 0.068 ... Validation loss: 0.147iteration: 9656\n",
      "train_loss: 0.06867846163946265\n",
      "val_loss: 0.14797512931098417\n",
      "Progress: 96.6% ... Training loss: 0.061 ... Validation loss: 0.169iteration: 9657\n",
      "train_loss: 0.061410486510447046\n",
      "val_loss: 0.16965780471881334\n",
      "Progress: 96.6% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9658\n",
      "train_loss: 0.05654161978611528\n",
      "val_loss: 0.15606685394292413\n",
      "Progress: 96.6% ... Training loss: 0.061 ... Validation loss: 0.180iteration: 9659\n",
      "train_loss: 0.061026333220210456\n",
      "val_loss: 0.18014484247722026\n",
      "Progress: 96.6% ... Training loss: 0.060 ... Validation loss: 0.147iteration: 9660\n",
      "train_loss: 0.06022157152516476\n",
      "val_loss: 0.14773483836032064\n",
      "Progress: 96.6% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9661\n",
      "train_loss: 0.05789644693502346\n",
      "val_loss: 0.15280928418579734\n",
      "Progress: 96.6% ... Training loss: 0.056 ... Validation loss: 0.148iteration: 9662\n",
      "train_loss: 0.05654134896158792\n",
      "val_loss: 0.14894957084985078\n",
      "Progress: 96.6% ... Training loss: 0.063 ... Validation loss: 0.181iteration: 9663\n",
      "train_loss: 0.06342977751880266\n",
      "val_loss: 0.18131465338505737\n",
      "Progress: 96.6% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 9664\n",
      "train_loss: 0.058506389931876186\n",
      "val_loss: 0.15385062319772555\n",
      "Progress: 96.7% ... Training loss: 0.055 ... Validation loss: 0.157iteration: 9665\n",
      "train_loss: 0.05596590985655757\n",
      "val_loss: 0.1579667575741684\n",
      "Progress: 96.7% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9666\n",
      "train_loss: 0.0563767616858775\n",
      "val_loss: 0.15699169545763414\n",
      "Progress: 96.7% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9667\n",
      "train_loss: 0.05628758983700292\n",
      "val_loss: 0.15945657729175003\n",
      "Progress: 96.7% ... Training loss: 0.055 ... Validation loss: 0.163iteration: 9668\n",
      "train_loss: 0.05582416226125827\n",
      "val_loss: 0.16305343386552004\n",
      "Progress: 96.7% ... Training loss: 0.057 ... Validation loss: 0.172iteration: 9669\n",
      "train_loss: 0.05721853990240693\n",
      "val_loss: 0.17241694770801258\n",
      "Progress: 96.7% ... Training loss: 0.073 ... Validation loss: 0.144iteration: 9670\n",
      "train_loss: 0.0739966811422106\n",
      "val_loss: 0.1445935922073225\n",
      "Progress: 96.7% ... Training loss: 0.064 ... Validation loss: 0.176iteration: 9671\n",
      "train_loss: 0.06446487939841228\n",
      "val_loss: 0.17697232098991672\n",
      "Progress: 96.7% ... Training loss: 0.060 ... Validation loss: 0.142iteration: 9672\n",
      "train_loss: 0.0604368452969277\n",
      "val_loss: 0.14256799361708072\n",
      "Progress: 96.7% ... Training loss: 0.061 ... Validation loss: 0.162iteration: 9673\n",
      "train_loss: 0.061369265322019986\n",
      "val_loss: 0.1622409566441462\n",
      "Progress: 96.7% ... Training loss: 0.056 ... Validation loss: 0.143iteration: 9674\n",
      "train_loss: 0.05684563600471992\n",
      "val_loss: 0.143874428541128\n",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.164iteration: 9675\n",
      "train_loss: 0.05824162059143099\n",
      "val_loss: 0.16431191185123645\n",
      "Progress: 96.8% ... Training loss: 0.057 ... Validation loss: 0.150iteration: 9676\n",
      "train_loss: 0.05727328475552533\n",
      "val_loss: 0.15090707195157532\n",
      "Progress: 96.8% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9677\n",
      "train_loss: 0.057317812719506256\n",
      "val_loss: 0.1520455903704582\n",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.143iteration: 9678\n",
      "train_loss: 0.058865554340344704\n",
      "val_loss: 0.14301651130761767\n",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.161iteration: 9679\n",
      "train_loss: 0.05871762653361322\n",
      "val_loss: 0.16189851640325825\n",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.163iteration: 9680\n",
      "train_loss: 0.05802090938887586\n",
      "val_loss: 0.16360756008707572\n",
      "Progress: 96.8% ... Training loss: 0.057 ... Validation loss: 0.148iteration: 9681\n",
      "train_loss: 0.05733985137522986\n",
      "val_loss: 0.14893557169133542\n",
      "Progress: 96.8% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9682\n",
      "train_loss: 0.057973364982255485\n",
      "val_loss: 0.15403601101434422\n",
      "Progress: 96.8% ... Training loss: 0.059 ... Validation loss: 0.173iteration: 9683\n",
      "train_loss: 0.05968947937693681\n",
      "val_loss: 0.17384781030392285\n",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.147iteration: 9684\n",
      "train_loss: 0.05866492616267448\n",
      "val_loss: 0.1471209314379333\n",
      "Progress: 96.8% ... Training loss: 0.066 ... Validation loss: 0.186iteration: 9685\n",
      "train_loss: 0.06672640811431028\n",
      "val_loss: 0.18693428745721907\n",
      "Progress: 96.9% ... Training loss: 0.066 ... Validation loss: 0.140iteration: 9686\n",
      "train_loss: 0.06608740887602921\n",
      "val_loss: 0.1407971978242158\n",
      "Progress: 96.9% ... Training loss: 0.058 ... Validation loss: 0.174iteration: 9687\n",
      "train_loss: 0.05841183058021142\n",
      "val_loss: 0.17403845269589874\n",
      "Progress: 96.9% ... Training loss: 0.056 ... Validation loss: 0.157iteration: 9688\n",
      "train_loss: 0.05665437084981514\n",
      "val_loss: 0.15788379771791305\n",
      "Progress: 96.9% ... Training loss: 0.057 ... Validation loss: 0.168iteration: 9689\n",
      "train_loss: 0.05715566197910427\n",
      "val_loss: 0.16885944865209043\n",
      "Progress: 96.9% ... Training loss: 0.056 ... Validation loss: 0.161iteration: 9690\n",
      "train_loss: 0.056681680575212806\n",
      "val_loss: 0.16191503171657276\n",
      "Progress: 96.9% ... Training loss: 0.060 ... Validation loss: 0.165iteration: 9691\n",
      "train_loss: 0.06023562680478359\n",
      "val_loss: 0.1654315823587851\n",
      "Progress: 96.9% ... Training loss: 0.065 ... Validation loss: 0.157iteration: 9692\n",
      "train_loss: 0.0650021725716712\n",
      "val_loss: 0.15766076535566054\n",
      "Progress: 96.9% ... Training loss: 0.064 ... Validation loss: 0.170iteration: 9693\n",
      "train_loss: 0.06410210980843292\n",
      "val_loss: 0.17081481659113887\n",
      "Progress: 96.9% ... Training loss: 0.080 ... Validation loss: 0.146iteration: 9694\n",
      "train_loss: 0.08050837128760151\n",
      "val_loss: 0.14670920832504888\n",
      "Progress: 97.0% ... Training loss: 0.069 ... Validation loss: 0.187iteration: 9695\n",
      "train_loss: 0.0697129018560774\n",
      "val_loss: 0.18740617796675596\n",
      "Progress: 97.0% ... Training loss: 0.090 ... Validation loss: 0.153iteration: 9696\n",
      "train_loss: 0.09043829683938122\n",
      "val_loss: 0.15367374669866668\n",
      "Progress: 97.0% ... Training loss: 0.129 ... Validation loss: 0.242iteration: 9697\n",
      "train_loss: 0.12981954226898665\n",
      "val_loss: 0.242984255819227\n",
      "Progress: 97.0% ... Training loss: 0.095 ... Validation loss: 0.158iteration: 9698\n",
      "train_loss: 0.09551637776061937\n",
      "val_loss: 0.158463446275996\n",
      "Progress: 97.0% ... Training loss: 0.094 ... Validation loss: 0.217iteration: 9699\n",
      "train_loss: 0.09488007881121004\n",
      "val_loss: 0.217265805934781\n",
      "Progress: 97.0% ... Training loss: 0.073 ... Validation loss: 0.149iteration: 9700\n",
      "train_loss: 0.07357328034389386\n",
      "val_loss: 0.14975389803096442\n",
      "Progress: 97.0% ... Training loss: 0.079 ... Validation loss: 0.215iteration: 9701\n",
      "train_loss: 0.07929875134871535\n",
      "val_loss: 0.21531306328848349\n",
      "Progress: 97.0% ... Training loss: 0.064 ... Validation loss: 0.148iteration: 9702\n",
      "train_loss: 0.06465877866464977\n",
      "val_loss: 0.1486473007637607\n",
      "Progress: 97.0% ... Training loss: 0.074 ... Validation loss: 0.192iteration: 9703\n",
      "train_loss: 0.07471716040905514\n",
      "val_loss: 0.19275307063334002\n",
      "Progress: 97.0% ... Training loss: 0.066 ... Validation loss: 0.145iteration: 9704\n",
      "train_loss: 0.06697554188926627\n",
      "val_loss: 0.14526591822577284\n",
      "Progress: 97.0% ... Training loss: 0.062 ... Validation loss: 0.186iteration: 9705\n",
      "train_loss: 0.06258258902349399\n",
      "val_loss: 0.18649348966528392\n",
      "Progress: 97.1% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 9706\n",
      "train_loss: 0.05974238202327414\n",
      "val_loss: 0.15582106438236357\n",
      "Progress: 97.1% ... Training loss: 0.061 ... Validation loss: 0.184iteration: 9707\n",
      "train_loss: 0.06103747703671737\n",
      "val_loss: 0.18463035600733255\n",
      "Progress: 97.1% ... Training loss: 0.066 ... Validation loss: 0.152iteration: 9708\n",
      "train_loss: 0.06657685054995557\n",
      "val_loss: 0.15295619647874803\n",
      "Progress: 97.1% ... Training loss: 0.057 ... Validation loss: 0.173iteration: 9709\n",
      "train_loss: 0.05779284480152238\n",
      "val_loss: 0.17304265183686784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 97.1% ... Training loss: 0.058 ... Validation loss: 0.152iteration: 9710\n",
      "train_loss: 0.058943963001363016\n",
      "val_loss: 0.15238249034964765\n",
      "Progress: 97.1% ... Training loss: 0.062 ... Validation loss: 0.190iteration: 9711\n",
      "train_loss: 0.06261662779353029\n",
      "val_loss: 0.19093525438188985\n",
      "Progress: 97.1% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 9712\n",
      "train_loss: 0.057596326669878094\n",
      "val_loss: 0.16027067088784086\n",
      "Progress: 97.1% ... Training loss: 0.056 ... Validation loss: 0.171iteration: 9713\n",
      "train_loss: 0.0565134216090027\n",
      "val_loss: 0.1715590788727783\n",
      "Progress: 97.1% ... Training loss: 0.056 ... Validation loss: 0.164iteration: 9714\n",
      "train_loss: 0.05648454639088783\n",
      "val_loss: 0.16451179518013764\n",
      "Progress: 97.2% ... Training loss: 0.059 ... Validation loss: 0.148iteration: 9715\n",
      "train_loss: 0.05982966070277965\n",
      "val_loss: 0.14831082066103246\n",
      "Progress: 97.2% ... Training loss: 0.058 ... Validation loss: 0.181iteration: 9716\n",
      "train_loss: 0.05846935501689272\n",
      "val_loss: 0.18129678894759793\n",
      "Progress: 97.2% ... Training loss: 0.058 ... Validation loss: 0.153iteration: 9717\n",
      "train_loss: 0.05849088627148555\n",
      "val_loss: 0.1538099923542415\n",
      "Progress: 97.2% ... Training loss: 0.059 ... Validation loss: 0.177iteration: 9718\n",
      "train_loss: 0.05973263897328048\n",
      "val_loss: 0.17753135552206206\n",
      "Progress: 97.2% ... Training loss: 0.061 ... Validation loss: 0.147iteration: 9719\n",
      "train_loss: 0.061923665205048925\n",
      "val_loss: 0.14784240638521204\n",
      "Progress: 97.2% ... Training loss: 0.067 ... Validation loss: 0.196iteration: 9720\n",
      "train_loss: 0.06780061351891993\n",
      "val_loss: 0.19687471247730734\n",
      "Progress: 97.2% ... Training loss: 0.070 ... Validation loss: 0.149iteration: 9721\n",
      "train_loss: 0.07055306507847252\n",
      "val_loss: 0.1492978451422811\n",
      "Progress: 97.2% ... Training loss: 0.079 ... Validation loss: 0.204iteration: 9722\n",
      "train_loss: 0.07999264624155497\n",
      "val_loss: 0.20400783004061532\n",
      "Progress: 97.2% ... Training loss: 0.071 ... Validation loss: 0.145iteration: 9723\n",
      "train_loss: 0.07183500977154882\n",
      "val_loss: 0.14585315636327326\n",
      "Progress: 97.2% ... Training loss: 0.077 ... Validation loss: 0.210iteration: 9724\n",
      "train_loss: 0.07778639834207751\n",
      "val_loss: 0.2108554911153551\n",
      "Progress: 97.2% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 9725\n",
      "train_loss: 0.05985912869495997\n",
      "val_loss: 0.1554942932817313\n",
      "Progress: 97.3% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 9726\n",
      "train_loss: 0.0585070508223009\n",
      "val_loss: 0.16933214907760843\n",
      "Progress: 97.3% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9727\n",
      "train_loss: 0.057729927278140254\n",
      "val_loss: 0.1546973653023967\n",
      "Progress: 97.3% ... Training loss: 0.058 ... Validation loss: 0.180iteration: 9728\n",
      "train_loss: 0.058876639965762756\n",
      "val_loss: 0.18022481120332756\n",
      "Progress: 97.3% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9729\n",
      "train_loss: 0.05715429103863326\n",
      "val_loss: 0.15592930936177843\n",
      "Progress: 97.3% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 9730\n",
      "train_loss: 0.06338692708078313\n",
      "val_loss: 0.17888442723446296\n",
      "Progress: 97.3% ... Training loss: 0.064 ... Validation loss: 0.146iteration: 9731\n",
      "train_loss: 0.06430092522270892\n",
      "val_loss: 0.14652576340973417\n",
      "Progress: 97.3% ... Training loss: 0.074 ... Validation loss: 0.199iteration: 9732\n",
      "train_loss: 0.07446855569497296\n",
      "val_loss: 0.19931173386526402\n",
      "Progress: 97.3% ... Training loss: 0.087 ... Validation loss: 0.155iteration: 9733\n",
      "train_loss: 0.08739776504786227\n",
      "val_loss: 0.15598807494678646\n",
      "Progress: 97.3% ... Training loss: 0.062 ... Validation loss: 0.188iteration: 9734\n",
      "train_loss: 0.06272143974410742\n",
      "val_loss: 0.18856845907288391\n",
      "Progress: 97.3% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9735\n",
      "train_loss: 0.05769068435288158\n",
      "val_loss: 0.15521134097991457\n",
      "Progress: 97.4% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9736\n",
      "train_loss: 0.057940691714913145\n",
      "val_loss: 0.16512781201179885\n",
      "Progress: 97.4% ... Training loss: 0.057 ... Validation loss: 0.149iteration: 9737\n",
      "train_loss: 0.057203921485454105\n",
      "val_loss: 0.1497281265384357\n",
      "Progress: 97.4% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9738\n",
      "train_loss: 0.057646348782655434\n",
      "val_loss: 0.154032582572085\n",
      "Progress: 97.4% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9739\n",
      "train_loss: 0.05722661745836502\n",
      "val_loss: 0.1655212438516933\n",
      "Progress: 97.4% ... Training loss: 0.059 ... Validation loss: 0.147iteration: 9740\n",
      "train_loss: 0.05991665490564285\n",
      "val_loss: 0.14723773155241796\n",
      "Progress: 97.4% ... Training loss: 0.059 ... Validation loss: 0.167iteration: 9741\n",
      "train_loss: 0.05936817253706848\n",
      "val_loss: 0.16742133145619656\n",
      "Progress: 97.4% ... Training loss: 0.070 ... Validation loss: 0.146iteration: 9742\n",
      "train_loss: 0.07083902302316619\n",
      "val_loss: 0.14664847865630876\n",
      "Progress: 97.4% ... Training loss: 0.059 ... Validation loss: 0.157iteration: 9743\n",
      "train_loss: 0.059222042396688414\n",
      "val_loss: 0.15787875454031208\n",
      "Progress: 97.4% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9744\n",
      "train_loss: 0.05653302384194197\n",
      "val_loss: 0.15440662847076433\n",
      "Progress: 97.5% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9745\n",
      "train_loss: 0.056241035352697366\n",
      "val_loss: 0.15234430581287528\n",
      "Progress: 97.5% ... Training loss: 0.058 ... Validation loss: 0.162iteration: 9746\n",
      "train_loss: 0.058830821120313716\n",
      "val_loss: 0.16297148884259277\n",
      "Progress: 97.5% ... Training loss: 0.063 ... Validation loss: 0.144iteration: 9747\n",
      "train_loss: 0.06317040434485915\n",
      "val_loss: 0.14483126005888533\n",
      "Progress: 97.5% ... Training loss: 0.067 ... Validation loss: 0.184iteration: 9748\n",
      "train_loss: 0.06717775273470097\n",
      "val_loss: 0.18478375800997246\n",
      "Progress: 97.5% ... Training loss: 0.067 ... Validation loss: 0.147iteration: 9749\n",
      "train_loss: 0.06797170482202947\n",
      "val_loss: 0.1473624754737795\n",
      "Progress: 97.5% ... Training loss: 0.059 ... Validation loss: 0.178iteration: 9750\n",
      "train_loss: 0.05905134999787811\n",
      "val_loss: 0.17832324072652045\n",
      "Progress: 97.5% ... Training loss: 0.056 ... Validation loss: 0.167iteration: 9751\n",
      "train_loss: 0.0569123165642778\n",
      "val_loss: 0.16725912801187803\n",
      "Progress: 97.5% ... Training loss: 0.058 ... Validation loss: 0.150iteration: 9752\n",
      "train_loss: 0.058131991612521416\n",
      "val_loss: 0.15048363291897204\n",
      "Progress: 97.5% ... Training loss: 0.063 ... Validation loss: 0.173iteration: 9753\n",
      "train_loss: 0.06324765728838803\n",
      "val_loss: 0.1730912570515791\n",
      "Progress: 97.5% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9754\n",
      "train_loss: 0.0571617212985247\n",
      "val_loss: 0.15175127745811057\n",
      "Progress: 97.5% ... Training loss: 0.056 ... Validation loss: 0.153iteration: 9755\n",
      "train_loss: 0.056996244453945064\n",
      "val_loss: 0.15326257033701216\n",
      "Progress: 97.6% ... Training loss: 0.060 ... Validation loss: 0.186iteration: 9756\n",
      "train_loss: 0.060354201793602985\n",
      "val_loss: 0.18605745969215595\n",
      "Progress: 97.6% ... Training loss: 0.063 ... Validation loss: 0.143iteration: 9757\n",
      "train_loss: 0.06332280201776017\n",
      "val_loss: 0.14327782767847688\n",
      "Progress: 97.6% ... Training loss: 0.058 ... Validation loss: 0.169iteration: 9758\n",
      "train_loss: 0.05870984721027516\n",
      "val_loss: 0.16995480512089825\n",
      "Progress: 97.6% ... Training loss: 0.058 ... Validation loss: 0.145iteration: 9759\n",
      "train_loss: 0.0589650289374284\n",
      "val_loss: 0.14587787519656414\n",
      "Progress: 97.6% ... Training loss: 0.056 ... Validation loss: 0.149iteration: 9760\n",
      "train_loss: 0.05611484207734221\n",
      "val_loss: 0.14935907500592435\n",
      "Progress: 97.6% ... Training loss: 0.057 ... Validation loss: 0.143iteration: 9761\n",
      "train_loss: 0.057572780801260645\n",
      "val_loss: 0.143683503436468\n",
      "Progress: 97.6% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9762\n",
      "train_loss: 0.05707750075437521\n",
      "val_loss: 0.1662964374009761\n",
      "Progress: 97.6% ... Training loss: 0.065 ... Validation loss: 0.145iteration: 9763\n",
      "train_loss: 0.06585833522848561\n",
      "val_loss: 0.14523709062364112\n",
      "Progress: 97.6% ... Training loss: 0.062 ... Validation loss: 0.167iteration: 9764\n",
      "train_loss: 0.062094092466254136\n",
      "val_loss: 0.16741549615678458\n",
      "Progress: 97.7% ... Training loss: 0.070 ... Validation loss: 0.145iteration: 9765\n",
      "train_loss: 0.07050748685784518\n",
      "val_loss: 0.1450838248320708\n",
      "Progress: 97.7% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9766\n",
      "train_loss: 0.05647523686272361\n",
      "val_loss: 0.15959724735900563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 97.7% ... Training loss: 0.062 ... Validation loss: 0.180iteration: 9767\n",
      "train_loss: 0.06278387213365993\n",
      "val_loss: 0.1809881412103458\n",
      "Progress: 97.7% ... Training loss: 0.059 ... Validation loss: 0.147iteration: 9768\n",
      "train_loss: 0.05945823481377972\n",
      "val_loss: 0.14759401407791034\n",
      "Progress: 97.7% ... Training loss: 0.065 ... Validation loss: 0.184iteration: 9769\n",
      "train_loss: 0.06520581213227793\n",
      "val_loss: 0.18489028832232085\n",
      "Progress: 97.7% ... Training loss: 0.081 ... Validation loss: 0.150iteration: 9770\n",
      "train_loss: 0.08141792267784229\n",
      "val_loss: 0.15087223943624828\n",
      "Progress: 97.7% ... Training loss: 0.082 ... Validation loss: 0.181iteration: 9771\n",
      "train_loss: 0.082316151352895\n",
      "val_loss: 0.18195966156529977\n",
      "Progress: 97.7% ... Training loss: 0.098 ... Validation loss: 0.156iteration: 9772\n",
      "train_loss: 0.09850774593500375\n",
      "val_loss: 0.15633939018922138\n",
      "Progress: 97.7% ... Training loss: 0.077 ... Validation loss: 0.213iteration: 9773\n",
      "train_loss: 0.07706029404797807\n",
      "val_loss: 0.21392398452809122\n",
      "Progress: 97.7% ... Training loss: 0.073 ... Validation loss: 0.150iteration: 9774\n",
      "train_loss: 0.07367853118684793\n",
      "val_loss: 0.150719247661567\n",
      "Progress: 97.8% ... Training loss: 0.082 ... Validation loss: 0.230iteration: 9775\n",
      "train_loss: 0.08254690654475517\n",
      "val_loss: 0.23082263986796855\n",
      "Progress: 97.8% ... Training loss: 0.064 ... Validation loss: 0.142iteration: 9776\n",
      "train_loss: 0.06447728331556134\n",
      "val_loss: 0.14218344104519584\n",
      "Progress: 97.8% ... Training loss: 0.058 ... Validation loss: 0.152iteration: 9777\n",
      "train_loss: 0.05842869022920887\n",
      "val_loss: 0.1521108695168949\n",
      "Progress: 97.8% ... Training loss: 0.058 ... Validation loss: 0.141iteration: 9778\n",
      "train_loss: 0.05864170959601657\n",
      "val_loss: 0.1410322136605798\n",
      "Progress: 97.8% ... Training loss: 0.058 ... Validation loss: 0.157iteration: 9779\n",
      "train_loss: 0.058252845149785425\n",
      "val_loss: 0.15738806226829166\n",
      "Progress: 97.8% ... Training loss: 0.060 ... Validation loss: 0.141iteration: 9780\n",
      "train_loss: 0.060769920758466815\n",
      "val_loss: 0.14100059595559766\n",
      "Progress: 97.8% ... Training loss: 0.060 ... Validation loss: 0.168iteration: 9781\n",
      "train_loss: 0.06015224985715307\n",
      "val_loss: 0.16830701780170693\n",
      "Progress: 97.8% ... Training loss: 0.056 ... Validation loss: 0.151iteration: 9782\n",
      "train_loss: 0.056097104243002655\n",
      "val_loss: 0.15104538611341609\n",
      "Progress: 97.8% ... Training loss: 0.059 ... Validation loss: 0.160iteration: 9783\n",
      "train_loss: 0.05982422906967257\n",
      "val_loss: 0.16038952893101902\n",
      "Progress: 97.8% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9784\n",
      "train_loss: 0.05704238889315299\n",
      "val_loss: 0.15579557857333187\n",
      "Progress: 97.8% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9785\n",
      "train_loss: 0.056551007186117434\n",
      "val_loss: 0.15970898704813105\n",
      "Progress: 97.9% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9786\n",
      "train_loss: 0.05709776378131713\n",
      "val_loss: 0.16640389853707924\n",
      "Progress: 97.9% ... Training loss: 0.063 ... Validation loss: 0.143iteration: 9787\n",
      "train_loss: 0.06360572646877759\n",
      "val_loss: 0.1431888461265067\n",
      "Progress: 97.9% ... Training loss: 0.072 ... Validation loss: 0.184iteration: 9788\n",
      "train_loss: 0.07204014058118605\n",
      "val_loss: 0.1843906673035711\n",
      "Progress: 97.9% ... Training loss: 0.067 ... Validation loss: 0.146iteration: 9789\n",
      "train_loss: 0.06763474041950017\n",
      "val_loss: 0.14611631087660343\n",
      "Progress: 97.9% ... Training loss: 0.067 ... Validation loss: 0.172iteration: 9790\n",
      "train_loss: 0.06777260499191427\n",
      "val_loss: 0.17271297647725264\n",
      "Progress: 97.9% ... Training loss: 0.058 ... Validation loss: 0.146iteration: 9791\n",
      "train_loss: 0.05888308184469419\n",
      "val_loss: 0.14616634943923185\n",
      "Progress: 97.9% ... Training loss: 0.060 ... Validation loss: 0.161iteration: 9792\n",
      "train_loss: 0.06011436022162969\n",
      "val_loss: 0.1618624518221113\n",
      "Progress: 97.9% ... Training loss: 0.057 ... Validation loss: 0.143iteration: 9793\n",
      "train_loss: 0.057406039713053816\n",
      "val_loss: 0.14388482147012602\n",
      "Progress: 97.9% ... Training loss: 0.071 ... Validation loss: 0.185iteration: 9794\n",
      "train_loss: 0.07165662056575187\n",
      "val_loss: 0.18545692369965705\n",
      "Progress: 98.0% ... Training loss: 0.057 ... Validation loss: 0.148iteration: 9795\n",
      "train_loss: 0.05748850690068517\n",
      "val_loss: 0.1486759980385795\n",
      "Progress: 98.0% ... Training loss: 0.056 ... Validation loss: 0.148iteration: 9796\n",
      "train_loss: 0.05686709628950903\n",
      "val_loss: 0.14824150149720058\n",
      "Progress: 98.0% ... Training loss: 0.056 ... Validation loss: 0.157iteration: 9797\n",
      "train_loss: 0.056973866389561666\n",
      "val_loss: 0.15717298054823606\n",
      "Progress: 98.0% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9798\n",
      "train_loss: 0.05705305772844536\n",
      "val_loss: 0.15170866260265806\n",
      "Progress: 98.0% ... Training loss: 0.062 ... Validation loss: 0.154iteration: 9799\n",
      "train_loss: 0.06264355867358887\n",
      "val_loss: 0.1549349048266533\n",
      "Progress: 98.0% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 9800\n",
      "train_loss: 0.06212963341624744\n",
      "val_loss: 0.1725033808989728\n",
      "Progress: 98.0% ... Training loss: 0.062 ... Validation loss: 0.143iteration: 9801\n",
      "train_loss: 0.06259607073657777\n",
      "val_loss: 0.14302849890700275\n",
      "Progress: 98.0% ... Training loss: 0.062 ... Validation loss: 0.177iteration: 9802\n",
      "train_loss: 0.06269794161528798\n",
      "val_loss: 0.1779763680879455\n",
      "Progress: 98.0% ... Training loss: 0.063 ... Validation loss: 0.144iteration: 9803\n",
      "train_loss: 0.06325624958954947\n",
      "val_loss: 0.14406572748017185\n",
      "Progress: 98.0% ... Training loss: 0.075 ... Validation loss: 0.180iteration: 9804\n",
      "train_loss: 0.07575625755719909\n",
      "val_loss: 0.18091143349630676\n",
      "Progress: 98.0% ... Training loss: 0.060 ... Validation loss: 0.143iteration: 9805\n",
      "train_loss: 0.06043311595750888\n",
      "val_loss: 0.14306656035570567\n",
      "Progress: 98.1% ... Training loss: 0.056 ... Validation loss: 0.162iteration: 9806\n",
      "train_loss: 0.05695387753994909\n",
      "val_loss: 0.1622347739922416\n",
      "Progress: 98.1% ... Training loss: 0.059 ... Validation loss: 0.149iteration: 9807\n",
      "train_loss: 0.05909138350668065\n",
      "val_loss: 0.14905967932464884\n",
      "Progress: 98.1% ... Training loss: 0.057 ... Validation loss: 0.154iteration: 9808\n",
      "train_loss: 0.057433503542416685\n",
      "val_loss: 0.15491988520354205\n",
      "Progress: 98.1% ... Training loss: 0.056 ... Validation loss: 0.163iteration: 9809\n",
      "train_loss: 0.05657628749778319\n",
      "val_loss: 0.16307474447699838\n",
      "Progress: 98.1% ... Training loss: 0.056 ... Validation loss: 0.164iteration: 9810\n",
      "train_loss: 0.05652753643275267\n",
      "val_loss: 0.16420091962437852\n",
      "Progress: 98.1% ... Training loss: 0.061 ... Validation loss: 0.174iteration: 9811\n",
      "train_loss: 0.06119941653849179\n",
      "val_loss: 0.17470498791352046\n",
      "Progress: 98.1% ... Training loss: 0.069 ... Validation loss: 0.141iteration: 9812\n",
      "train_loss: 0.06913877587735479\n",
      "val_loss: 0.1419494865542255\n",
      "Progress: 98.1% ... Training loss: 0.061 ... Validation loss: 0.174iteration: 9813\n",
      "train_loss: 0.06166307085552729\n",
      "val_loss: 0.17476352679117746\n",
      "Progress: 98.1% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9814\n",
      "train_loss: 0.056044701543275746\n",
      "val_loss: 0.15491908381112088\n",
      "Progress: 98.2% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9815\n",
      "train_loss: 0.056538895467148666\n",
      "val_loss: 0.15517733000736286\n",
      "Progress: 98.2% ... Training loss: 0.056 ... Validation loss: 0.164iteration: 9816\n",
      "train_loss: 0.056242921168292506\n",
      "val_loss: 0.1642956047444177\n",
      "Progress: 98.2% ... Training loss: 0.058 ... Validation loss: 0.148iteration: 9817\n",
      "train_loss: 0.0585364685868921\n",
      "val_loss: 0.14882198499652274\n",
      "Progress: 98.2% ... Training loss: 0.065 ... Validation loss: 0.164iteration: 9818\n",
      "train_loss: 0.06560524407929767\n",
      "val_loss: 0.16456658059701873\n",
      "Progress: 98.2% ... Training loss: 0.056 ... Validation loss: 0.148iteration: 9819\n",
      "train_loss: 0.05679717687876882\n",
      "val_loss: 0.14886895450067644\n",
      "Progress: 98.2% ... Training loss: 0.056 ... Validation loss: 0.168iteration: 9820\n",
      "train_loss: 0.05693668141503038\n",
      "val_loss: 0.16840647678208873\n",
      "Progress: 98.2% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9821\n",
      "train_loss: 0.05753767342747369\n",
      "val_loss: 0.15264889195477582\n",
      "Progress: 98.2% ... Training loss: 0.058 ... Validation loss: 0.171iteration: 9822\n",
      "train_loss: 0.05863835742342607\n",
      "val_loss: 0.17156666302874846\n",
      "Progress: 98.2% ... Training loss: 0.059 ... Validation loss: 0.155iteration: 9823\n",
      "train_loss: 0.05926470757147663\n",
      "val_loss: 0.1553459323885643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 98.2% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 9824\n",
      "train_loss: 0.05901605249953345\n",
      "val_loss: 0.17088151515794417\n",
      "Progress: 98.2% ... Training loss: 0.058 ... Validation loss: 0.148iteration: 9825\n",
      "train_loss: 0.05874210179918549\n",
      "val_loss: 0.1480513621656465\n",
      "Progress: 98.3% ... Training loss: 0.065 ... Validation loss: 0.176iteration: 9826\n",
      "train_loss: 0.0652218099580746\n",
      "val_loss: 0.17606654850959672\n",
      "Progress: 98.3% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9827\n",
      "train_loss: 0.05623446743104562\n",
      "val_loss: 0.15508018899946682\n",
      "Progress: 98.3% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 9828\n",
      "train_loss: 0.0602870012076853\n",
      "val_loss: 0.17277411877793708\n",
      "Progress: 98.3% ... Training loss: 0.057 ... Validation loss: 0.153iteration: 9829\n",
      "train_loss: 0.05791489682355832\n",
      "val_loss: 0.15374425423143212\n",
      "Progress: 98.3% ... Training loss: 0.055 ... Validation loss: 0.154iteration: 9830\n",
      "train_loss: 0.055869622370077913\n",
      "val_loss: 0.1543194208182499\n",
      "Progress: 98.3% ... Training loss: 0.058 ... Validation loss: 0.146iteration: 9831\n",
      "train_loss: 0.058396728353111174\n",
      "val_loss: 0.14654438580465726\n",
      "Progress: 98.3% ... Training loss: 0.058 ... Validation loss: 0.170iteration: 9832\n",
      "train_loss: 0.05828963052025148\n",
      "val_loss: 0.17037808879069738\n",
      "Progress: 98.3% ... Training loss: 0.056 ... Validation loss: 0.142iteration: 9833\n",
      "train_loss: 0.05698129417458675\n",
      "val_loss: 0.14241984155411463\n",
      "Progress: 98.3% ... Training loss: 0.055 ... Validation loss: 0.151iteration: 9834\n",
      "train_loss: 0.05582077053672732\n",
      "val_loss: 0.15193044507994927\n",
      "Progress: 98.3% ... Training loss: 0.055 ... Validation loss: 0.153iteration: 9835\n",
      "train_loss: 0.055970097195994314\n",
      "val_loss: 0.1536131039305591\n",
      "Progress: 98.4% ... Training loss: 0.058 ... Validation loss: 0.162iteration: 9836\n",
      "train_loss: 0.05847622846107575\n",
      "val_loss: 0.16277301142367362\n",
      "Progress: 98.4% ... Training loss: 0.066 ... Validation loss: 0.140iteration: 9837\n",
      "train_loss: 0.0664233931094042\n",
      "val_loss: 0.14088759631754338\n",
      "Progress: 98.4% ... Training loss: 0.064 ... Validation loss: 0.184iteration: 9838\n",
      "train_loss: 0.06494320316624796\n",
      "val_loss: 0.18442076320470963\n",
      "Progress: 98.4% ... Training loss: 0.069 ... Validation loss: 0.145iteration: 9839\n",
      "train_loss: 0.06983523346169647\n",
      "val_loss: 0.14581256117964556\n",
      "Progress: 98.4% ... Training loss: 0.059 ... Validation loss: 0.169iteration: 9840\n",
      "train_loss: 0.05942775614498533\n",
      "val_loss: 0.16992562102980333\n",
      "Progress: 98.4% ... Training loss: 0.057 ... Validation loss: 0.150iteration: 9841\n",
      "train_loss: 0.057909779004414345\n",
      "val_loss: 0.150242278082228\n",
      "Progress: 98.4% ... Training loss: 0.059 ... Validation loss: 0.174iteration: 9842\n",
      "train_loss: 0.059842513376183654\n",
      "val_loss: 0.17447494218387977\n",
      "Progress: 98.4% ... Training loss: 0.067 ... Validation loss: 0.148iteration: 9843\n",
      "train_loss: 0.0670976663790234\n",
      "val_loss: 0.14891470359079348\n",
      "Progress: 98.4% ... Training loss: 0.065 ... Validation loss: 0.176iteration: 9844\n",
      "train_loss: 0.06528962479609719\n",
      "val_loss: 0.17676453313342566\n",
      "Progress: 98.5% ... Training loss: 0.063 ... Validation loss: 0.152iteration: 9845\n",
      "train_loss: 0.063906743616996\n",
      "val_loss: 0.15210028306350598\n",
      "Progress: 98.5% ... Training loss: 0.066 ... Validation loss: 0.180iteration: 9846\n",
      "train_loss: 0.06624452881630072\n",
      "val_loss: 0.18048404605205345\n",
      "Progress: 98.5% ... Training loss: 0.060 ... Validation loss: 0.152iteration: 9847\n",
      "train_loss: 0.06068164279332286\n",
      "val_loss: 0.15250294701183936\n",
      "Progress: 98.5% ... Training loss: 0.059 ... Validation loss: 0.144iteration: 9848\n",
      "train_loss: 0.05950342966103873\n",
      "val_loss: 0.1448869173855594\n",
      "Progress: 98.5% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9849\n",
      "train_loss: 0.05682389989061036\n",
      "val_loss: 0.15946300013081371\n",
      "Progress: 98.5% ... Training loss: 0.055 ... Validation loss: 0.152iteration: 9850\n",
      "train_loss: 0.055840136609012055\n",
      "val_loss: 0.15273770986851162\n",
      "Progress: 98.5% ... Training loss: 0.057 ... Validation loss: 0.167iteration: 9851\n",
      "train_loss: 0.05794661964613876\n",
      "val_loss: 0.16723975396873167\n",
      "Progress: 98.5% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9852\n",
      "train_loss: 0.05669891610071086\n",
      "val_loss: 0.15298875602182826\n",
      "Progress: 98.5% ... Training loss: 0.061 ... Validation loss: 0.171iteration: 9853\n",
      "train_loss: 0.061295954857378473\n",
      "val_loss: 0.1715424651933749\n",
      "Progress: 98.5% ... Training loss: 0.068 ... Validation loss: 0.148iteration: 9854\n",
      "train_loss: 0.0687207650607425\n",
      "val_loss: 0.1486228582813175\n",
      "Progress: 98.5% ... Training loss: 0.063 ... Validation loss: 0.172iteration: 9855\n",
      "train_loss: 0.06382937187101849\n",
      "val_loss: 0.1728508124576637\n",
      "Progress: 98.6% ... Training loss: 0.061 ... Validation loss: 0.149iteration: 9856\n",
      "train_loss: 0.061894487189900725\n",
      "val_loss: 0.14920399379051588\n",
      "Progress: 98.6% ... Training loss: 0.056 ... Validation loss: 0.167iteration: 9857\n",
      "train_loss: 0.056864518656334274\n",
      "val_loss: 0.16737260346601535\n",
      "Progress: 98.6% ... Training loss: 0.056 ... Validation loss: 0.151iteration: 9858\n",
      "train_loss: 0.0565726597126748\n",
      "val_loss: 0.15185855834008452\n",
      "Progress: 98.6% ... Training loss: 0.064 ... Validation loss: 0.179iteration: 9859\n",
      "train_loss: 0.06473587557843671\n",
      "val_loss: 0.1791654651269168\n",
      "Progress: 98.6% ... Training loss: 0.067 ... Validation loss: 0.144iteration: 9860\n",
      "train_loss: 0.06705950125080924\n",
      "val_loss: 0.14439414692485902\n",
      "Progress: 98.6% ... Training loss: 0.069 ... Validation loss: 0.193iteration: 9861\n",
      "train_loss: 0.0693521821652275\n",
      "val_loss: 0.19319854203398137\n",
      "Progress: 98.6% ... Training loss: 0.072 ... Validation loss: 0.145iteration: 9862\n",
      "train_loss: 0.07225655225027813\n",
      "val_loss: 0.14571577559472296\n",
      "Progress: 98.6% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 9863\n",
      "train_loss: 0.059423153464173616\n",
      "val_loss: 0.1613706279694241\n",
      "Progress: 98.6% ... Training loss: 0.056 ... Validation loss: 0.145iteration: 9864\n",
      "train_loss: 0.05667378421320969\n",
      "val_loss: 0.1457679402859004\n",
      "Progress: 98.7% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9865\n",
      "train_loss: 0.05699019622792544\n",
      "val_loss: 0.15441837812542994\n",
      "Progress: 98.7% ... Training loss: 0.062 ... Validation loss: 0.142iteration: 9866\n",
      "train_loss: 0.062207017916491684\n",
      "val_loss: 0.14239198327845182\n",
      "Progress: 98.7% ... Training loss: 0.059 ... Validation loss: 0.162iteration: 9867\n",
      "train_loss: 0.05931729523653872\n",
      "val_loss: 0.16211792325227817\n",
      "Progress: 98.7% ... Training loss: 0.062 ... Validation loss: 0.144iteration: 9868\n",
      "train_loss: 0.0629141288390627\n",
      "val_loss: 0.14469704291884955\n",
      "Progress: 98.7% ... Training loss: 0.066 ... Validation loss: 0.171iteration: 9869\n",
      "train_loss: 0.06687211580152934\n",
      "val_loss: 0.17129972854930842\n",
      "Progress: 98.7% ... Training loss: 0.078 ... Validation loss: 0.148iteration: 9870\n",
      "train_loss: 0.07864525933719158\n",
      "val_loss: 0.14838566418341942\n",
      "Progress: 98.7% ... Training loss: 0.078 ... Validation loss: 0.206iteration: 9871\n",
      "train_loss: 0.0785678333790764\n",
      "val_loss: 0.20607573496355355\n",
      "Progress: 98.7% ... Training loss: 0.086 ... Validation loss: 0.149iteration: 9872\n",
      "train_loss: 0.08610496536242093\n",
      "val_loss: 0.14931451672874393\n",
      "Progress: 98.7% ... Training loss: 0.102 ... Validation loss: 0.224iteration: 9873\n",
      "train_loss: 0.10293504898150503\n",
      "val_loss: 0.22475724243058753\n",
      "Progress: 98.7% ... Training loss: 0.113 ... Validation loss: 0.165iteration: 9874\n",
      "train_loss: 0.11347395934914545\n",
      "val_loss: 0.16529553535362895\n",
      "Progress: 98.8% ... Training loss: 0.083 ... Validation loss: 0.192iteration: 9875\n",
      "train_loss: 0.08360088472206592\n",
      "val_loss: 0.1926063115417104\n",
      "Progress: 98.8% ... Training loss: 0.079 ... Validation loss: 0.148iteration: 9876\n",
      "train_loss: 0.07910065431840826\n",
      "val_loss: 0.14854885417930758\n",
      "Progress: 98.8% ... Training loss: 0.072 ... Validation loss: 0.175iteration: 9877\n",
      "train_loss: 0.07273878457179213\n",
      "val_loss: 0.17562258516393442\n",
      "Progress: 98.8% ... Training loss: 0.082 ... Validation loss: 0.154iteration: 9878\n",
      "train_loss: 0.08262319347224384\n",
      "val_loss: 0.1541249365497906\n",
      "Progress: 98.8% ... Training loss: 0.071 ... Validation loss: 0.187iteration: 9879\n",
      "train_loss: 0.07159733774627386\n",
      "val_loss: 0.18775980532472616\n",
      "Progress: 98.8% ... Training loss: 0.105 ... Validation loss: 0.161iteration: 9880\n",
      "train_loss: 0.10534997148303936\n",
      "val_loss: 0.16149329197991896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 98.8% ... Training loss: 0.069 ... Validation loss: 0.193iteration: 9881\n",
      "train_loss: 0.0699986926179486\n",
      "val_loss: 0.19355306106100126\n",
      "Progress: 98.8% ... Training loss: 0.072 ... Validation loss: 0.152iteration: 9882\n",
      "train_loss: 0.07252949925898933\n",
      "val_loss: 0.1524710268933413\n",
      "Progress: 98.8% ... Training loss: 0.064 ... Validation loss: 0.192iteration: 9883\n",
      "train_loss: 0.06494511234697381\n",
      "val_loss: 0.1926990344215141\n",
      "Progress: 98.8% ... Training loss: 0.066 ... Validation loss: 0.149iteration: 9884\n",
      "train_loss: 0.06693734235509406\n",
      "val_loss: 0.14960563964059262\n",
      "Progress: 98.8% ... Training loss: 0.059 ... Validation loss: 0.170iteration: 9885\n",
      "train_loss: 0.05943545450629215\n",
      "val_loss: 0.17024706724613262\n",
      "Progress: 98.9% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 9886\n",
      "train_loss: 0.058675377755141436\n",
      "val_loss: 0.15111511973225994\n",
      "Progress: 98.9% ... Training loss: 0.056 ... Validation loss: 0.157iteration: 9887\n",
      "train_loss: 0.05617657113734874\n",
      "val_loss: 0.15730777425269638\n",
      "Progress: 98.9% ... Training loss: 0.058 ... Validation loss: 0.167iteration: 9888\n",
      "train_loss: 0.05897486300256625\n",
      "val_loss: 0.16705346111188696\n",
      "Progress: 98.9% ... Training loss: 0.056 ... Validation loss: 0.153iteration: 9889\n",
      "train_loss: 0.056670477540354175\n",
      "val_loss: 0.15356943242436263\n",
      "Progress: 98.9% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9890\n",
      "train_loss: 0.05722588652460514\n",
      "val_loss: 0.16675822143448965\n",
      "Progress: 98.9% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9891\n",
      "train_loss: 0.057167646231385365\n",
      "val_loss: 0.15131206618476817\n",
      "Progress: 98.9% ... Training loss: 0.058 ... Validation loss: 0.173iteration: 9892\n",
      "train_loss: 0.058027044591357664\n",
      "val_loss: 0.17320132741333832\n",
      "Progress: 98.9% ... Training loss: 0.057 ... Validation loss: 0.145iteration: 9893\n",
      "train_loss: 0.05746404825698137\n",
      "val_loss: 0.14597170419917457\n",
      "Progress: 98.9% ... Training loss: 0.056 ... Validation loss: 0.165iteration: 9894\n",
      "train_loss: 0.05695766343390853\n",
      "val_loss: 0.16502449859110083\n",
      "Progress: 99.0% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 9895\n",
      "train_loss: 0.05837086146697512\n",
      "val_loss: 0.15193951548025553\n",
      "Progress: 99.0% ... Training loss: 0.063 ... Validation loss: 0.178iteration: 9896\n",
      "train_loss: 0.06345454467089454\n",
      "val_loss: 0.17853937179545282\n",
      "Progress: 99.0% ... Training loss: 0.058 ... Validation loss: 0.151iteration: 9897\n",
      "train_loss: 0.058114402625924065\n",
      "val_loss: 0.15142413824845585\n",
      "Progress: 99.0% ... Training loss: 0.055 ... Validation loss: 0.154iteration: 9898\n",
      "train_loss: 0.05582350122435186\n",
      "val_loss: 0.15430638219981907\n",
      "Progress: 99.0% ... Training loss: 0.058 ... Validation loss: 0.166iteration: 9899\n",
      "train_loss: 0.05835349624453845\n",
      "val_loss: 0.1661602992276029\n",
      "Progress: 99.0% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9900\n",
      "train_loss: 0.05627939215091757\n",
      "val_loss: 0.15204394505732213\n",
      "Progress: 99.0% ... Training loss: 0.057 ... Validation loss: 0.160iteration: 9901\n",
      "train_loss: 0.05743521673008544\n",
      "val_loss: 0.16007653322753684\n",
      "Progress: 99.0% ... Training loss: 0.056 ... Validation loss: 0.164iteration: 9902\n",
      "train_loss: 0.056657939289064554\n",
      "val_loss: 0.1645636203090232\n",
      "Progress: 99.0% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9903\n",
      "train_loss: 0.05648698214069646\n",
      "val_loss: 0.1543025794394953\n",
      "Progress: 99.0% ... Training loss: 0.059 ... Validation loss: 0.145iteration: 9904\n",
      "train_loss: 0.05931686262837973\n",
      "val_loss: 0.1459512733063275\n",
      "Progress: 99.0% ... Training loss: 0.056 ... Validation loss: 0.168iteration: 9905\n",
      "train_loss: 0.056451066949577154\n",
      "val_loss: 0.16829899611723403\n",
      "Progress: 99.1% ... Training loss: 0.056 ... Validation loss: 0.157iteration: 9906\n",
      "train_loss: 0.056424657920331994\n",
      "val_loss: 0.15774907768697405\n",
      "Progress: 99.1% ... Training loss: 0.057 ... Validation loss: 0.165iteration: 9907\n",
      "train_loss: 0.0574258754858988\n",
      "val_loss: 0.16564595000063598\n",
      "Progress: 99.1% ... Training loss: 0.061 ... Validation loss: 0.151iteration: 9908\n",
      "train_loss: 0.06157759193014956\n",
      "val_loss: 0.1512345850109646\n",
      "Progress: 99.1% ... Training loss: 0.058 ... Validation loss: 0.179iteration: 9909\n",
      "train_loss: 0.05803592867082738\n",
      "val_loss: 0.17912522983907742\n",
      "Progress: 99.1% ... Training loss: 0.056 ... Validation loss: 0.160iteration: 9910\n",
      "train_loss: 0.05664440207237826\n",
      "val_loss: 0.16053720875023778\n",
      "Progress: 99.1% ... Training loss: 0.069 ... Validation loss: 0.195iteration: 9911\n",
      "train_loss: 0.06914808413698159\n",
      "val_loss: 0.1952713997891984\n",
      "Progress: 99.1% ... Training loss: 0.069 ... Validation loss: 0.148iteration: 9912\n",
      "train_loss: 0.06940769555067516\n",
      "val_loss: 0.1483738227042154\n",
      "Progress: 99.1% ... Training loss: 0.064 ... Validation loss: 0.189iteration: 9913\n",
      "train_loss: 0.06425476642560499\n",
      "val_loss: 0.18903392841964906\n",
      "Progress: 99.1% ... Training loss: 0.058 ... Validation loss: 0.147iteration: 9914\n",
      "train_loss: 0.05840911487085249\n",
      "val_loss: 0.1470151086165567\n",
      "Progress: 99.2% ... Training loss: 0.057 ... Validation loss: 0.171iteration: 9915\n",
      "train_loss: 0.0574259172044055\n",
      "val_loss: 0.17189295692733206\n",
      "Progress: 99.2% ... Training loss: 0.056 ... Validation loss: 0.161iteration: 9916\n",
      "train_loss: 0.05642225385203285\n",
      "val_loss: 0.16135611000801525\n",
      "Progress: 99.2% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9917\n",
      "train_loss: 0.05724130595624136\n",
      "val_loss: 0.15281103029305113\n",
      "Progress: 99.2% ... Training loss: 0.060 ... Validation loss: 0.173iteration: 9918\n",
      "train_loss: 0.06049972009867802\n",
      "val_loss: 0.17349738551867516\n",
      "Progress: 99.2% ... Training loss: 0.068 ... Validation loss: 0.156iteration: 9919\n",
      "train_loss: 0.06868550776937944\n",
      "val_loss: 0.1566769922963419\n",
      "Progress: 99.2% ... Training loss: 0.060 ... Validation loss: 0.163iteration: 9920\n",
      "train_loss: 0.060889885738101425\n",
      "val_loss: 0.16329401619208753\n",
      "Progress: 99.2% ... Training loss: 0.059 ... Validation loss: 0.151iteration: 9921\n",
      "train_loss: 0.059238255281483805\n",
      "val_loss: 0.151481562661709\n",
      "Progress: 99.2% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9922\n",
      "train_loss: 0.05646790145373033\n",
      "val_loss: 0.15665893139955103\n",
      "Progress: 99.2% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9923\n",
      "train_loss: 0.05632100985027225\n",
      "val_loss: 0.15928244373317588\n",
      "Progress: 99.2% ... Training loss: 0.056 ... Validation loss: 0.156iteration: 9924\n",
      "train_loss: 0.056702689745512985\n",
      "val_loss: 0.15673077148223133\n",
      "Progress: 99.2% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9925\n",
      "train_loss: 0.05771083212520593\n",
      "val_loss: 0.1526530268990053\n",
      "Progress: 99.3% ... Training loss: 0.064 ... Validation loss: 0.166iteration: 9926\n",
      "train_loss: 0.0648715648504912\n",
      "val_loss: 0.16658119533787938\n",
      "Progress: 99.3% ... Training loss: 0.066 ... Validation loss: 0.148iteration: 9927\n",
      "train_loss: 0.06617576672416288\n",
      "val_loss: 0.14803892381291522\n",
      "Progress: 99.3% ... Training loss: 0.070 ... Validation loss: 0.195iteration: 9928\n",
      "train_loss: 0.07020540690746244\n",
      "val_loss: 0.19525253169665954\n",
      "Progress: 99.3% ... Training loss: 0.062 ... Validation loss: 0.147iteration: 9929\n",
      "train_loss: 0.062419361229846676\n",
      "val_loss: 0.14719742445093853\n",
      "Progress: 99.3% ... Training loss: 0.061 ... Validation loss: 0.181iteration: 9930\n",
      "train_loss: 0.0614580692210103\n",
      "val_loss: 0.1817576584401778\n",
      "Progress: 99.3% ... Training loss: 0.074 ... Validation loss: 0.150iteration: 9931\n",
      "train_loss: 0.07484522611024866\n",
      "val_loss: 0.15014054455159717\n",
      "Progress: 99.3% ... Training loss: 0.070 ... Validation loss: 0.186iteration: 9932\n",
      "train_loss: 0.07040650005332612\n",
      "val_loss: 0.18602975363342059\n",
      "Progress: 99.3% ... Training loss: 0.059 ... Validation loss: 0.146iteration: 9933\n",
      "train_loss: 0.05902399478023403\n",
      "val_loss: 0.14676079944478268\n",
      "Progress: 99.3% ... Training loss: 0.076 ... Validation loss: 0.184iteration: 9934\n",
      "train_loss: 0.07636983263703065\n",
      "val_loss: 0.1843944929413384\n",
      "Progress: 99.3% ... Training loss: 0.126 ... Validation loss: 0.170iteration: 9935\n",
      "train_loss: 0.1266442626605341\n",
      "val_loss: 0.17000654764978484\n",
      "Progress: 99.4% ... Training loss: 0.106 ... Validation loss: 0.253iteration: 9936\n",
      "train_loss: 0.10674160744114167\n",
      "val_loss: 0.25384341165581803\n",
      "Progress: 99.4% ... Training loss: 0.087 ... Validation loss: 0.155iteration: 9937\n",
      "train_loss: 0.08757413212029119\n",
      "val_loss: 0.15520549210011644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.4% ... Training loss: 0.077 ... Validation loss: 0.206iteration: 9938\n",
      "train_loss: 0.0779857546482142\n",
      "val_loss: 0.20618381385988538\n",
      "Progress: 99.4% ... Training loss: 0.077 ... Validation loss: 0.150iteration: 9939\n",
      "train_loss: 0.07765821168351175\n",
      "val_loss: 0.15044894890329616\n",
      "Progress: 99.4% ... Training loss: 0.070 ... Validation loss: 0.214iteration: 9940\n",
      "train_loss: 0.07016267200998111\n",
      "val_loss: 0.2148822584547918\n",
      "Progress: 99.4% ... Training loss: 0.065 ... Validation loss: 0.149iteration: 9941\n",
      "train_loss: 0.06554269171741306\n",
      "val_loss: 0.14971540717893253\n",
      "Progress: 99.4% ... Training loss: 0.069 ... Validation loss: 0.188iteration: 9942\n",
      "train_loss: 0.06945085117454197\n",
      "val_loss: 0.18864031865147374\n",
      "Progress: 99.4% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 9943\n",
      "train_loss: 0.059275285303059345\n",
      "val_loss: 0.15615237625601222\n",
      "Progress: 99.4% ... Training loss: 0.057 ... Validation loss: 0.164iteration: 9944\n",
      "train_loss: 0.05754835719684368\n",
      "val_loss: 0.16483571806845917\n",
      "Progress: 99.5% ... Training loss: 0.060 ... Validation loss: 0.172iteration: 9945\n",
      "train_loss: 0.06046618840611811\n",
      "val_loss: 0.1726196202513559\n",
      "Progress: 99.5% ... Training loss: 0.064 ... Validation loss: 0.155iteration: 9946\n",
      "train_loss: 0.0646686493423099\n",
      "val_loss: 0.1559294739364686\n",
      "Progress: 99.5% ... Training loss: 0.070 ... Validation loss: 0.211iteration: 9947\n",
      "train_loss: 0.07052773075225936\n",
      "val_loss: 0.21112715989911118\n",
      "Progress: 99.5% ... Training loss: 0.061 ... Validation loss: 0.145iteration: 9948\n",
      "train_loss: 0.061070509638799154\n",
      "val_loss: 0.1456345767274108\n",
      "Progress: 99.5% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9949\n",
      "train_loss: 0.0560221883225212\n",
      "val_loss: 0.1599732001150029\n",
      "Progress: 99.5% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 9950\n",
      "train_loss: 0.06015365301940367\n",
      "val_loss: 0.15140221602850137\n",
      "Progress: 99.5% ... Training loss: 0.058 ... Validation loss: 0.180iteration: 9951\n",
      "train_loss: 0.05809240783514801\n",
      "val_loss: 0.18005608350297464\n",
      "Progress: 99.5% ... Training loss: 0.064 ... Validation loss: 0.151iteration: 9952\n",
      "train_loss: 0.06468814890082078\n",
      "val_loss: 0.151775759332904\n",
      "Progress: 99.5% ... Training loss: 0.067 ... Validation loss: 0.198iteration: 9953\n",
      "train_loss: 0.06700966267894497\n",
      "val_loss: 0.19842807595570425\n",
      "Progress: 99.5% ... Training loss: 0.062 ... Validation loss: 0.153iteration: 9954\n",
      "train_loss: 0.062371099692538595\n",
      "val_loss: 0.153256592991221\n",
      "Progress: 99.5% ... Training loss: 0.056 ... Validation loss: 0.165iteration: 9955\n",
      "train_loss: 0.056704245515055623\n",
      "val_loss: 0.1653304851753841\n",
      "Progress: 99.6% ... Training loss: 0.056 ... Validation loss: 0.149iteration: 9956\n",
      "train_loss: 0.056857675423486125\n",
      "val_loss: 0.1491105399267726\n",
      "Progress: 99.6% ... Training loss: 0.056 ... Validation loss: 0.160iteration: 9957\n",
      "train_loss: 0.05612401442474814\n",
      "val_loss: 0.16068077160561878\n",
      "Progress: 99.6% ... Training loss: 0.056 ... Validation loss: 0.157iteration: 9958\n",
      "train_loss: 0.05632953219028764\n",
      "val_loss: 0.15760019694636804\n",
      "Progress: 99.6% ... Training loss: 0.057 ... Validation loss: 0.155iteration: 9959\n",
      "train_loss: 0.05777521608917338\n",
      "val_loss: 0.15563289733757696\n",
      "Progress: 99.6% ... Training loss: 0.068 ... Validation loss: 0.170iteration: 9960\n",
      "train_loss: 0.06889686683541926\n",
      "val_loss: 0.17040121378937326\n",
      "Progress: 99.6% ... Training loss: 0.057 ... Validation loss: 0.151iteration: 9961\n",
      "train_loss: 0.057376114887417236\n",
      "val_loss: 0.15199762464478228\n",
      "Progress: 99.6% ... Training loss: 0.055 ... Validation loss: 0.158iteration: 9962\n",
      "train_loss: 0.05574908667420145\n",
      "val_loss: 0.15848117283274177\n",
      "Progress: 99.6% ... Training loss: 0.055 ... Validation loss: 0.152iteration: 9963\n",
      "train_loss: 0.055682424939787534\n",
      "val_loss: 0.1525214809955218\n",
      "Progress: 99.6% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9964\n",
      "train_loss: 0.05703017296199899\n",
      "val_loss: 0.16694387070929442\n",
      "Progress: 99.7% ... Training loss: 0.056 ... Validation loss: 0.152iteration: 9965\n",
      "train_loss: 0.05600240763441857\n",
      "val_loss: 0.1524803211512687\n",
      "Progress: 99.7% ... Training loss: 0.056 ... Validation loss: 0.155iteration: 9966\n",
      "train_loss: 0.056531351562129815\n",
      "val_loss: 0.15564705215396693\n",
      "Progress: 99.7% ... Training loss: 0.058 ... Validation loss: 0.147iteration: 9967\n",
      "train_loss: 0.058077752796150545\n",
      "val_loss: 0.1470741564178106\n",
      "Progress: 99.7% ... Training loss: 0.056 ... Validation loss: 0.154iteration: 9968\n",
      "train_loss: 0.056464626144575786\n",
      "val_loss: 0.15487638546630644\n",
      "Progress: 99.7% ... Training loss: 0.056 ... Validation loss: 0.145iteration: 9969\n",
      "train_loss: 0.05652007899100743\n",
      "val_loss: 0.14560611476249707\n",
      "Progress: 99.7% ... Training loss: 0.055 ... Validation loss: 0.149iteration: 9970\n",
      "train_loss: 0.05579610383512595\n",
      "val_loss: 0.1491237428840958\n",
      "Progress: 99.7% ... Training loss: 0.056 ... Validation loss: 0.151iteration: 9971\n",
      "train_loss: 0.05647302191709829\n",
      "val_loss: 0.15118488415649878\n",
      "Progress: 99.7% ... Training loss: 0.055 ... Validation loss: 0.160iteration: 9972\n",
      "train_loss: 0.055997803424800784\n",
      "val_loss: 0.1601448868906838\n",
      "Progress: 99.7% ... Training loss: 0.056 ... Validation loss: 0.151iteration: 9973\n",
      "train_loss: 0.05663972160763347\n",
      "val_loss: 0.15199976273757257\n",
      "Progress: 99.7% ... Training loss: 0.055 ... Validation loss: 0.157iteration: 9974\n",
      "train_loss: 0.0556314515763359\n",
      "val_loss: 0.15727175114825928\n",
      "Progress: 99.8% ... Training loss: 0.057 ... Validation loss: 0.152iteration: 9975\n",
      "train_loss: 0.057867546960456435\n",
      "val_loss: 0.15219244538499785\n",
      "Progress: 99.8% ... Training loss: 0.055 ... Validation loss: 0.156iteration: 9976\n",
      "train_loss: 0.05560712894459792\n",
      "val_loss: 0.15620318225916102\n",
      "Progress: 99.8% ... Training loss: 0.059 ... Validation loss: 0.156iteration: 9977\n",
      "train_loss: 0.05993130427253309\n",
      "val_loss: 0.15619559470932703\n",
      "Progress: 99.8% ... Training loss: 0.056 ... Validation loss: 0.166iteration: 9978\n",
      "train_loss: 0.056045058869327265\n",
      "val_loss: 0.16684749887290387\n",
      "Progress: 99.8% ... Training loss: 0.056 ... Validation loss: 0.149iteration: 9979\n",
      "train_loss: 0.05674852077938524\n",
      "val_loss: 0.14996406736090095\n",
      "Progress: 99.8% ... Training loss: 0.056 ... Validation loss: 0.149iteration: 9980\n",
      "train_loss: 0.05638729991890566\n",
      "val_loss: 0.1491768877379608\n",
      "Progress: 99.8% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9981\n",
      "train_loss: 0.057397008210431234\n",
      "val_loss: 0.1668618091699474\n",
      "Progress: 99.8% ... Training loss: 0.062 ... Validation loss: 0.146iteration: 9982\n",
      "train_loss: 0.06249240507174535\n",
      "val_loss: 0.1464593896889708\n",
      "Progress: 99.8% ... Training loss: 0.062 ... Validation loss: 0.172iteration: 9983\n",
      "train_loss: 0.06262615259509127\n",
      "val_loss: 0.1728219098500993\n",
      "Progress: 99.8% ... Training loss: 0.065 ... Validation loss: 0.157iteration: 9984\n",
      "train_loss: 0.0651000969605756\n",
      "val_loss: 0.15703422544468773\n",
      "Progress: 99.8% ... Training loss: 0.061 ... Validation loss: 0.179iteration: 9985\n",
      "train_loss: 0.061819795765826704\n",
      "val_loss: 0.17985641532937421\n",
      "Progress: 99.9% ... Training loss: 0.056 ... Validation loss: 0.151iteration: 9986\n",
      "train_loss: 0.056886546716639196\n",
      "val_loss: 0.1511023091268431\n",
      "Progress: 99.9% ... Training loss: 0.057 ... Validation loss: 0.146iteration: 9987\n",
      "train_loss: 0.057842166524231894\n",
      "val_loss: 0.14617211617345224\n",
      "Progress: 99.9% ... Training loss: 0.056 ... Validation loss: 0.151iteration: 9988\n",
      "train_loss: 0.056881055663682664\n",
      "val_loss: 0.1512209794541013\n",
      "Progress: 99.9% ... Training loss: 0.056 ... Validation loss: 0.159iteration: 9989\n",
      "train_loss: 0.05679559331685509\n",
      "val_loss: 0.1594358727618123\n",
      "Progress: 99.9% ... Training loss: 0.056 ... Validation loss: 0.165iteration: 9990\n",
      "train_loss: 0.05660429502699978\n",
      "val_loss: 0.16594778065985324\n",
      "Progress: 99.9% ... Training loss: 0.057 ... Validation loss: 0.166iteration: 9991\n",
      "train_loss: 0.0570314773591328\n",
      "val_loss: 0.16657057315625107\n",
      "Progress: 99.9% ... Training loss: 0.060 ... Validation loss: 0.151iteration: 9992\n",
      "train_loss: 0.060011705879119324\n",
      "val_loss: 0.15127367309046597\n",
      "Progress: 99.9% ... Training loss: 0.059 ... Validation loss: 0.161iteration: 9993\n",
      "train_loss: 0.05927351549662482\n",
      "val_loss: 0.1613474231333756\n",
      "Progress: 99.9% ... Training loss: 0.064 ... Validation loss: 0.148iteration: 9994\n",
      "train_loss: 0.06417526003007949\n",
      "val_loss: 0.1488781386304434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Training loss: 0.058 ... Validation loss: 0.178iteration: 9995\n",
      "train_loss: 0.058853202091933475\n",
      "val_loss: 0.17803823315874293\n",
      "Progress: 100.0% ... Training loss: 0.055 ... Validation loss: 0.161iteration: 9996\n",
      "train_loss: 0.055792284438796615\n",
      "val_loss: 0.16115797873892468\n",
      "Progress: 100.0% ... Training loss: 0.058 ... Validation loss: 0.165iteration: 9997\n",
      "train_loss: 0.058591128635900436\n",
      "val_loss: 0.16528570364578032\n",
      "Progress: 100.0% ... Training loss: 0.064 ... Validation loss: 0.150iteration: 9998\n",
      "train_loss: 0.0648073317204927\n",
      "val_loss: 0.15078543265248281\n",
      "Progress: 100.0% ... Training loss: 0.062 ... Validation loss: 0.177iteration: 9999\n",
      "train_loss: 0.06250731839161848\n",
      "val_loss: 0.17743465450723261\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "### Set the hyperparameters here ###\n",
    "iterations = 10000\n",
    "learning_rate = 0.5\n",
    "hidden_nodes = 10\n",
    "output_nodes = 1\n",
    "\n",
    "N_i = train_features.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for ii in range(iterations):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(train_features.index, size=128)\n",
    "    X, y = train_features.ix[batch].values, train_targets.ix[batch]['cnt']\n",
    "                             \n",
    "    network.train(X, y)\n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)\n",
    "    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)\n",
    "    #增加\n",
    "    print(\"iteration:\",ii),print(\"train_loss:\",train_loss),print(\"val_loss:\",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2019bb0ad68>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 373
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查预测结果\n",
    "\n",
    "使用测试数据看看网络对数据建模的效果如何。如果完全错了，请确保网络中的每步都正确实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2019bf31e10>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 272,
       "width": 502
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions = network.run(test_features).T*std + mean\n",
    "ax.plot(predictions[0], label='Prediction')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可选：思考下你的结果（我们不会评估这道题的答案）\n",
    "\n",
    " \n",
    "请针对你的结果回答以下问题。模型对数据的预测效果如何？哪里出现问题了？为何出现问题呢？\n",
    "\n",
    "> **注意**：你可以通过双击该单元编辑文本。如果想要预览文本，请按 Control + Enter\n",
    "\n",
    "#### 请将你的答案填写在下方\n",
    "#### 设置学习率0.5，隐藏层节点10\n",
    "- 第一次设置迭代次数为5000，随着迭代次数增加，训练误差与验证误差在不断变小\n",
    "- 第二次迭代次数为10000次，发现大概在6000次时，训练误差在0.06上下，验证误差在0.16上下，之后误差波动很大，预测结果不是很理想，希望把训练误差控制在0.01以内，验证误差控制在0.1以内\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
